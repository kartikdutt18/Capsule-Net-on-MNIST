{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CapsuleNet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kartikdutt18/Capsule-Net-on-MNIST/blob/master/CapsuleNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "sDPNb7AbTEF4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Importing Libraries**"
      ]
    },
    {
      "metadata": {
        "id": "yHBcBIBVS7Dw",
        "colab_type": "code",
        "outputId": "d21a9bcd-4a30-4438-9360-b36eadeb118d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install torchvision\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from numpy import prod\n",
        "from time import time\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/60/66415660aa46b23b5e1b72bc762e816736ce8d7260213e22365af51e8f9c/torch-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (591.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 591.8MB 25kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x62386000 @  0x7f007d0d42a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "Successfully installed torch-1.0.0\n",
            "Collecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 4.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Collecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 13.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Installing collected packages: pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.3.0 torchvision-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rTs_DZ2dTgfp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Defining The Layers**"
      ]
    },
    {
      "metadata": {
        "id": "3PA9Y4iPeWfC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### **Defining the Non-Linear function (Squash)**"
      ]
    },
    {
      "metadata": {
        "id": "OTJU0FSITWrp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def squash(vec,dim=-1):\n",
        "  squared_normal=torch.sum(vec**2,dim=dim,keepdim=True)\n",
        "  fn=squared_normal / (1 + squared_normal) * vec / (torch.sqrt(squared_normal) + 1e-8)\n",
        "  return fn  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HkkizorcfUe_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### **Defining Primary Capsules**"
      ]
    },
    {
      "metadata": {
        "id": "Z6I3DTMmTyfU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class PrimaryCapsules(nn.Module):\n",
        "  def __init__(self,in_channels, out_channels, dim_caps,kernel_size=9, stride=2, padding=0):\n",
        "    super(PrimaryCapsules,self).__init__()\n",
        "    self.dim_caps=dim_caps\n",
        "    self._caps_channel = int(out_channels / dim_caps)\n",
        "    self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "    \n",
        "  def forward(self,x):\n",
        "    out=self.conv(x)\n",
        "    out = out.view(out.size(0), self._caps_channel, out.size(2), out.size(3), self.dim_caps)\n",
        "    out = out.view(out.size(0), -1, self.dim_caps)\n",
        "    out=squash(out)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lLK5Hn5XoRAN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### **Defining routing between Capsules**"
      ]
    },
    {
      "metadata": {
        "id": "_1-iCxcbVe3H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Router(nn.Module):\n",
        "  def __init__(self,in_dim, in_caps, num_caps, dim_caps, num_routing):\n",
        "      super(Router,self).__init__()\n",
        "      self.in_dim = in_dim\n",
        "      self.in_caps = in_caps\n",
        "      self.num_caps = num_caps\n",
        "      self.dim_caps = dim_caps\n",
        "      self.num_routing = num_routing\n",
        "      self.W = nn.Parameter( 0.01 * torch.randn(1, num_caps, in_caps, dim_caps, in_dim ))\n",
        "    \n",
        "  def __repr__(self):\n",
        "      \n",
        "      tab= '  '\n",
        "      line = '\\n'\n",
        "      next = ' -> '\n",
        "      res = self.__class__.__name__ + '('\n",
        "      res = res + line + tab + '(' + str(0) + '): ' + 'CapsuleLinear('\n",
        "      res = res + str(self.in_dim) + ', ' + str(self.dim_caps) + ')'\n",
        "      res = res + line + tab + '(' + str(1) + '): ' + 'Routing('\n",
        "      res = res + 'Routing No =' + str(self.num_routing) + ')'\n",
        "      res = res + line + ')'\n",
        "      return res\n",
        "  \n",
        "  def forward(self,x):\n",
        "    batch_size = x.size(0)\n",
        "    x = x.unsqueeze(1).unsqueeze(4)\n",
        "    # W @ x =(1, num_caps, in_caps, dim_caps, in_dim) @ (batch_size, 1, in_caps, in_dim, 1) =(batch_size, num_caps, in_caps, dim_caps, 1)\n",
        "    u_hat = torch.matmul(self.W, x)\n",
        "    u_hat = u_hat.squeeze(-1)\n",
        "    #Prevent flow of Gradients\n",
        "    temp_u_hat = u_hat.detach()\n",
        "    b = torch.zeros(batch_size, self.num_caps, self.in_caps, 1)\n",
        "    \n",
        "    for route_iter in range(self.num_routing-1):\n",
        "      sc = F.softmax(b, dim=1)\n",
        "      # (batch_size, num_caps, in_caps, 1) * (batch_size, in_caps, num_caps, dim_caps) =(batch_size, num_caps, in_caps, dim_caps) sum across in_caps ->(batch_size, num_caps, dim_caps)\n",
        "      vec = (sc * temp_u_hat).sum(dim=2)\n",
        "      v = squash(vec)\n",
        "      uv = torch.matmul(temp_u_hat, v.unsqueeze(-1))\n",
        "      b += uv\n",
        "      \n",
        "    sc = F.softmax(b, dim=1)\n",
        "    vec = (sc * u_hat).sum(dim=2)\n",
        "    v = squash(vec)\n",
        "    return v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4SJWC9N8z1Gg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#**Defining the Network**"
      ]
    },
    {
      "metadata": {
        "id": "9-8EM0Z2xew5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CapsuleNet(nn.Module):\n",
        "\tdef __init__(self, img_shape, channels, primary_dim, num_classes, out_dim, num_routing, kernel_size=9):\n",
        "\t\tsuper(CapsuleNet,self).__init__()\n",
        "\t\tself.img_shape = img_shape\n",
        "\t\tself.num_classes = num_classes\n",
        "\n",
        "\t\tself.conv1 = nn.Conv2d(img_shape[0], channels, kernel_size, stride=1, bias=True)\n",
        "\t\tself.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "\t\tself.primary = PrimaryCapsules(channels, channels, primary_dim, kernel_size)\n",
        "\t\t\n",
        "\t\tprimary_caps = int(channels / primary_dim * ( img_shape[1] - 2*(kernel_size-1) ) * ( img_shape[2] - 2*(kernel_size-1) ) / 4)\n",
        "\t\tself.digits = Router(primary_dim, primary_caps, num_classes, out_dim, num_routing)\n",
        "\n",
        "\t\tself.decoder = nn.Sequential(\n",
        "\t\t\tnn.Linear(out_dim * num_classes, 512),\n",
        "\t\t\tnn.ReLU(inplace=True),\n",
        "\t\t\tnn.Linear(512, 1024),\n",
        "\t\t\tnn.ReLU(inplace=True),\n",
        "\t\t\tnn.Linear(1024, int(prod(img_shape)) ),\n",
        "\t\t\tnn.Sigmoid()\n",
        "\t\t)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tout = self.conv1(x)\n",
        "\t\tout = self.relu(out)\n",
        "\t\tout = self.primary(out)\n",
        "\t\tout = self.digits(out)\n",
        "\t\tpreds = torch.norm(out, dim=-1)\n",
        "\n",
        "\t\t# Reconstruct the *predicted* image\n",
        "\t\t_, max_length_idx = preds.max(dim=1)\t\n",
        "\t\ty = torch.eye(self.num_classes)\n",
        "\t\ty = y.index_select(dim=0, index=max_length_idx).unsqueeze(2)\n",
        "\n",
        "\t\treconstructions = self.decoder( (out*y).view(out.size(0), -1) )\n",
        "\t\treconstructions = reconstructions.view(-1, *self.img_shape)\n",
        "\n",
        "\t\treturn preds, reconstructions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8i0_9nGK6Eko",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Defining the Losses**"
      ]
    },
    {
      "metadata": {
        "id": "hx0dm3kM5oRS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MarginLoss(nn.Module):\n",
        "    def __init__(self, size_average=False, loss_lambda=0.5):\n",
        "            super(MarginLoss,self).__init__()\n",
        "            self.size_average = size_average\n",
        "            self.m_plus = 0.9\n",
        "            self.m_minus = 0.1\n",
        "            self.loss_lambda = loss_lambda\n",
        "    \n",
        "    def forward(self,inputs,labels):\n",
        "          L_k = labels * F.relu(self.m_plus - inputs)**2 + self.loss_lambda * (1 - labels) * F.relu(inputs - self.m_minus)**2\n",
        "          L_k.sum(dim=1)\n",
        "\n",
        "          if self.size_average:return L_k.mean()\n",
        "          else:return L_k.sum()\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S8yScD41_15O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CapsuleLoss(nn.Module):\n",
        "\tdef __init__(self,loss_lambda=0.5, recon_loss_scale=5e-4, size_average=False):\n",
        "\n",
        "\t\tsuper(CapsuleLoss,self).__init__()\n",
        "\t\tself.size_average = size_average\n",
        "\t\tself.margin_loss = MarginLoss(size_average=size_average, loss_lambda=loss_lambda)\n",
        "\t\tself.reconstruction_loss = nn.MSELoss(size_average=size_average)\n",
        "\t\tself.recon_loss_scale = recon_loss_scale\n",
        "\n",
        "\tdef forward(self,inputs, labels, images, reconstructions):\n",
        "\t\tmargin_loss = self.margin_loss(inputs, labels)\n",
        "\t\treconstruction_loss = self.reconstruction_loss(reconstructions, images)\n",
        "\t\tcaps_loss = (margin_loss + self.recon_loss_scale * reconstruction_loss)\n",
        "\n",
        "\t\treturn caps_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3lXo3lowAEE1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Making the Trainer**"
      ]
    },
    {
      "metadata": {
        "id": "Iyouc__XAiIq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "if not os.path.exists('Checkpoints/'):os.mkdir('Checkpoints/')\n",
        "SAVE_MODEL_PATH='Checkpoints/'\n",
        "class Trainer:\n",
        "  def __init__(self,loaders,net,epochs,batch_size,learning_rate,num_routing=3,lr_decay=0.8):\n",
        "    self.loaders=loaders\n",
        "    self.img_shape=self.loaders['train'].dataset[0][0].numpy().shape\n",
        "    self.epochs=epochs\n",
        "    self.net=net\n",
        "    \n",
        "    self.criterion=CapsuleLoss()\n",
        "    self.optimizer = optim.Adam(self.net.parameters(), lr=learning_rate)\n",
        "    self.scheduler = optim.lr_scheduler.ExponentialLR(self.optimizer, gamma=lr_decay)\n",
        "    print('Num params:', sum([prod(p.size()) for p in self.net.parameters()]))\n",
        "    \n",
        "  def __repr__(self):\n",
        "    return repr(self.net)\n",
        "  \n",
        "  def run(self):\n",
        "    classes=list(range(10))\n",
        "    print(8*'#','RUN STARTED',8*'#')\n",
        "    eye = torch.eye(len(classes))\n",
        "    \n",
        "    for epoch in range(1,self.epochs):\n",
        "      for phase in ['train', 'test']:\n",
        "        print(f'{phase}ing...'.upper())\n",
        "        if phase=='train':\n",
        "          self.net.train()\n",
        "        else: self.net.eval()\n",
        "        \n",
        "        t0=time()\n",
        "        running_loss=0.0\n",
        "        correct=0\n",
        "        total=0\n",
        "        \n",
        "        for i, (images, labels) in enumerate(self.loaders[phase]):\n",
        "          t1=time()\n",
        "          labels = eye[labels]\n",
        "          self.optimizer.zero_grad()\n",
        "          outputs, reconstructions = self.net(images)\n",
        "          loss = self.criterion(outputs, labels, images, reconstructions)\n",
        "          if phase == 'train':\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            \n",
        "          running_loss+=loss.item()\n",
        "          _, predicted = torch.max(outputs, 1)\n",
        "          \n",
        "          _,labels=torch.max(outputs,1)\n",
        "          total += labels.size(0)\n",
        "          \n",
        "          correct += (predicted == labels).sum()\n",
        "          print(' correct: ',correct,'Total: ',total)\n",
        "          accuracy=float(correct)/float(total)\n",
        "          if phase == 'train':\n",
        "            print(f'Epoch {epoch}, Batch {i+1}, Loss {running_loss/(i+1)}',f'Accuracy {accuracy} Time {round(time()-t1, 3)}s')\n",
        "        print(f'{phase.upper()} Epoch {epoch}, Loss {running_loss/(i+1)}',f'Accuracy {accuracy} Time {round(time()-t0, 3)}s')\n",
        "      self.scheduler.step()\n",
        "    error_rate = round((1-accuracy)*100, 2)\n",
        "    t2=str(time()).replace(\" \", \"-\")\n",
        "    torch.save(self.net.state_dict(), os.path.join(SAVE_MODEL_PATH, f'{error_rate}_{t2}.pth.tar'))\n",
        "    class_correct = list(0. for _ in classes)\n",
        "    class_total = list(0. for _ in classes)\n",
        "    for images, labels in self.loaders['test']:\n",
        "      outputs, reconstructions = self.net(images)\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      c = (predicted == labels).squeeze()\n",
        "      for i in range(labels.size(0)):\n",
        "        label = labels[i]\n",
        "        class_correct[label] += c[i].item()\n",
        "        class_total[label] += 1\n",
        "    \n",
        "    for i in range(len(classes)):\n",
        "        print('Accuracy of %5s : %2d %%' % (classes[i], 100 * class_correct[i] / class_total[i]))\n",
        "        \n",
        "\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yKizcm9RrKRg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **MNIST Datasets**"
      ]
    },
    {
      "metadata": {
        "id": "F9Ijl1_W7wLs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "BatchSize=batch_size=64\n",
        "size = 28\n",
        "classes = list(range(10))\n",
        "mean, std = ( ( 0.1307,), ( 0.3081,) )\n",
        "loader={}\n",
        "trainset = torchvision.datasets.MNIST(root='./MNIST', train=True,download=True, transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean,std)]))\n",
        "loader['train'] = torch.utils.data.DataLoader(trainset, batch_size=BatchSize,shuffle=True) \n",
        "testset = torchvision.datasets.MNIST(root='./MNIST', train=False,download=True, transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean,std)]))\n",
        "loader['test']=testloader = torch.utils.data.DataLoader(testset, batch_size=BatchSize,shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i1e-UzlQ-iib",
        "colab_type": "code",
        "outputId": "b3a24df1-a19f-412e-e3ba-0740e2dc963a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155115
        }
      },
      "cell_type": "code",
      "source": [
        "epochs=5\n",
        "net=CapsuleNet(img_shape=(1,28,28),channels=256,primary_dim=8,num_classes=10,out_dim=16,num_routing=3)\n",
        "classe = list(range(10))\n",
        "learning_rate=1e-3\n",
        "lr_decay=0.95\n",
        "caps_net=Trainer(loader,net=net,batch_size=128,learning_rate=learning_rate,num_routing=3, lr_decay=lr_decay,epochs=5)\n",
        "caps_net.run()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Num params: 8215568\n",
            "######## RUN STARTED ########\n",
            "TRAINING...\n",
            " correct:  tensor(64) Total:  64\n",
            "Epoch 1, Batch 1, Loss 82.69861602783203 Accuracy 1.0 Time 3.792s\n",
            " correct:  tensor(128) Total:  128\n",
            "Epoch 1, Batch 2, Loss 72.00513458251953 Accuracy 1.0 Time 3.257s\n",
            " correct:  tensor(192) Total:  192\n",
            "Epoch 1, Batch 3, Loss 73.6968510945638 Accuracy 1.0 Time 3.125s\n",
            " correct:  tensor(256) Total:  256\n",
            "Epoch 1, Batch 4, Loss 71.00421905517578 Accuracy 1.0 Time 3.107s\n",
            " correct:  tensor(320) Total:  320\n",
            "Epoch 1, Batch 5, Loss 69.04557495117187 Accuracy 1.0 Time 3.085s\n",
            " correct:  tensor(384) Total:  384\n",
            "Epoch 1, Batch 6, Loss 67.22044118245442 Accuracy 1.0 Time 3.103s\n",
            " correct:  tensor(448) Total:  448\n",
            "Epoch 1, Batch 7, Loss 65.4186156136649 Accuracy 1.0 Time 3.057s\n",
            " correct:  tensor(512) Total:  512\n",
            "Epoch 1, Batch 8, Loss 63.480666160583496 Accuracy 1.0 Time 3.163s\n",
            " correct:  tensor(576) Total:  576\n",
            "Epoch 1, Batch 9, Loss 61.81080288357205 Accuracy 1.0 Time 3.118s\n",
            " correct:  tensor(640) Total:  640\n",
            "Epoch 1, Batch 10, Loss 60.28843803405762 Accuracy 1.0 Time 3.107s\n",
            " correct:  tensor(704) Total:  704\n",
            "Epoch 1, Batch 11, Loss 58.482559550892226 Accuracy 1.0 Time 3.085s\n",
            " correct:  tensor(768) Total:  768\n",
            "Epoch 1, Batch 12, Loss 56.939606030782066 Accuracy 1.0 Time 3.12s\n",
            " correct:  tensor(832) Total:  832\n",
            "Epoch 1, Batch 13, Loss 55.55150838998648 Accuracy 1.0 Time 3.125s\n",
            " correct:  tensor(896) Total:  896\n",
            "Epoch 1, Batch 14, Loss 54.334141867501394 Accuracy 1.0 Time 3.113s\n",
            " correct:  tensor(960) Total:  960\n",
            "Epoch 1, Batch 15, Loss 52.9933364868164 Accuracy 1.0 Time 3.207s\n",
            " correct:  tensor(1024) Total:  1024\n",
            "Epoch 1, Batch 16, Loss 51.833982944488525 Accuracy 1.0 Time 3.12s\n",
            " correct:  tensor(1088) Total:  1088\n",
            "Epoch 1, Batch 17, Loss 50.75099653356216 Accuracy 1.0 Time 3.134s\n",
            " correct:  tensor(1152) Total:  1152\n",
            "Epoch 1, Batch 18, Loss 49.79166454739041 Accuracy 1.0 Time 3.089s\n",
            " correct:  tensor(1216) Total:  1216\n",
            "Epoch 1, Batch 19, Loss 48.95557463796515 Accuracy 1.0 Time 3.011s\n",
            " correct:  tensor(1280) Total:  1280\n",
            "Epoch 1, Batch 20, Loss 48.1814043045044 Accuracy 1.0 Time 3.018s\n",
            " correct:  tensor(1344) Total:  1344\n",
            "Epoch 1, Batch 21, Loss 47.35137685139974 Accuracy 1.0 Time 3.01s\n",
            " correct:  tensor(1408) Total:  1408\n",
            "Epoch 1, Batch 22, Loss 46.61496370488947 Accuracy 1.0 Time 3.065s\n",
            " correct:  tensor(1472) Total:  1472\n",
            "Epoch 1, Batch 23, Loss 45.921329000721805 Accuracy 1.0 Time 3.145s\n",
            " correct:  tensor(1536) Total:  1536\n",
            "Epoch 1, Batch 24, Loss 45.310121615727745 Accuracy 1.0 Time 3.102s\n",
            " correct:  tensor(1600) Total:  1600\n",
            "Epoch 1, Batch 25, Loss 44.8007950592041 Accuracy 1.0 Time 3.15s\n",
            " correct:  tensor(1664) Total:  1664\n",
            "Epoch 1, Batch 26, Loss 44.374541135934685 Accuracy 1.0 Time 3.096s\n",
            " correct:  tensor(1728) Total:  1728\n",
            "Epoch 1, Batch 27, Loss 43.99031554328071 Accuracy 1.0 Time 3.077s\n",
            " correct:  tensor(1792) Total:  1792\n",
            "Epoch 1, Batch 28, Loss 43.55461433955601 Accuracy 1.0 Time 3.054s\n",
            " correct:  tensor(1856) Total:  1856\n",
            "Epoch 1, Batch 29, Loss 43.071283406224744 Accuracy 1.0 Time 3.058s\n",
            " correct:  tensor(1920) Total:  1920\n",
            "Epoch 1, Batch 30, Loss 42.74618644714356 Accuracy 1.0 Time 3.076s\n",
            " correct:  tensor(1984) Total:  1984\n",
            "Epoch 1, Batch 31, Loss 42.381920106949345 Accuracy 1.0 Time 3.117s\n",
            " correct:  tensor(2048) Total:  2048\n",
            "Epoch 1, Batch 32, Loss 41.97995173931122 Accuracy 1.0 Time 3.073s\n",
            " correct:  tensor(2112) Total:  2112\n",
            "Epoch 1, Batch 33, Loss 41.642355312000625 Accuracy 1.0 Time 3.101s\n",
            " correct:  tensor(2176) Total:  2176\n",
            "Epoch 1, Batch 34, Loss 41.316233691047216 Accuracy 1.0 Time 3.13s\n",
            " correct:  tensor(2240) Total:  2240\n",
            "Epoch 1, Batch 35, Loss 40.924314389910016 Accuracy 1.0 Time 3.123s\n",
            " correct:  tensor(2304) Total:  2304\n",
            "Epoch 1, Batch 36, Loss 40.653235912323 Accuracy 1.0 Time 3.142s\n",
            " correct:  tensor(2368) Total:  2368\n",
            "Epoch 1, Batch 37, Loss 40.380783545004356 Accuracy 1.0 Time 3.348s\n",
            " correct:  tensor(2432) Total:  2432\n",
            "Epoch 1, Batch 38, Loss 40.134456835295026 Accuracy 1.0 Time 3.314s\n",
            " correct:  tensor(2496) Total:  2496\n",
            "Epoch 1, Batch 39, Loss 39.88146737905649 Accuracy 1.0 Time 3.331s\n",
            " correct:  tensor(2560) Total:  2560\n",
            "Epoch 1, Batch 40, Loss 39.587592935562135 Accuracy 1.0 Time 3.093s\n",
            " correct:  tensor(2624) Total:  2624\n",
            "Epoch 1, Batch 41, Loss 39.33225301416909 Accuracy 1.0 Time 3.089s\n",
            " correct:  tensor(2688) Total:  2688\n",
            "Epoch 1, Batch 42, Loss 39.08602964310419 Accuracy 1.0 Time 3.041s\n",
            " correct:  tensor(2752) Total:  2752\n",
            "Epoch 1, Batch 43, Loss 38.86000934866972 Accuracy 1.0 Time 3.085s\n",
            " correct:  tensor(2816) Total:  2816\n",
            "Epoch 1, Batch 44, Loss 38.645955389196224 Accuracy 1.0 Time 3.108s\n",
            " correct:  tensor(2880) Total:  2880\n",
            "Epoch 1, Batch 45, Loss 38.41889084710015 Accuracy 1.0 Time 3.087s\n",
            " correct:  tensor(2944) Total:  2944\n",
            "Epoch 1, Batch 46, Loss 38.2003114534461 Accuracy 1.0 Time 3.113s\n",
            " correct:  tensor(3008) Total:  3008\n",
            "Epoch 1, Batch 47, Loss 37.99227503512768 Accuracy 1.0 Time 3.083s\n",
            " correct:  tensor(3072) Total:  3072\n",
            "Epoch 1, Batch 48, Loss 37.82669881979624 Accuracy 1.0 Time 3.061s\n",
            " correct:  tensor(3136) Total:  3136\n",
            "Epoch 1, Batch 49, Loss 37.62343710296008 Accuracy 1.0 Time 3.068s\n",
            " correct:  tensor(3200) Total:  3200\n",
            "Epoch 1, Batch 50, Loss 37.43290542602539 Accuracy 1.0 Time 3.108s\n",
            " correct:  tensor(3264) Total:  3264\n",
            "Epoch 1, Batch 51, Loss 37.289205999935376 Accuracy 1.0 Time 3.04s\n",
            " correct:  tensor(3328) Total:  3328\n",
            "Epoch 1, Batch 52, Loss 37.130371864025406 Accuracy 1.0 Time 3.093s\n",
            " correct:  tensor(3392) Total:  3392\n",
            "Epoch 1, Batch 53, Loss 36.94006408835357 Accuracy 1.0 Time 3.093s\n",
            " correct:  tensor(3456) Total:  3456\n",
            "Epoch 1, Batch 54, Loss 36.79094854990641 Accuracy 1.0 Time 3.051s\n",
            " correct:  tensor(3520) Total:  3520\n",
            "Epoch 1, Batch 55, Loss 36.632001252607864 Accuracy 1.0 Time 3.02s\n",
            " correct:  tensor(3584) Total:  3584\n",
            "Epoch 1, Batch 56, Loss 36.44354820251465 Accuracy 1.0 Time 3.064s\n",
            " correct:  tensor(3648) Total:  3648\n",
            "Epoch 1, Batch 57, Loss 36.274598707232556 Accuracy 1.0 Time 3.024s\n",
            " correct:  tensor(3712) Total:  3712\n",
            "Epoch 1, Batch 58, Loss 36.08514858114308 Accuracy 1.0 Time 3.055s\n",
            " correct:  tensor(3776) Total:  3776\n",
            "Epoch 1, Batch 59, Loss 35.91779922226728 Accuracy 1.0 Time 3.104s\n",
            " correct:  tensor(3840) Total:  3840\n",
            "Epoch 1, Batch 60, Loss 35.75296592712402 Accuracy 1.0 Time 3.099s\n",
            " correct:  tensor(3904) Total:  3904\n",
            "Epoch 1, Batch 61, Loss 35.59657003058762 Accuracy 1.0 Time 3.084s\n",
            " correct:  tensor(3968) Total:  3968\n",
            "Epoch 1, Batch 62, Loss 35.442753084244266 Accuracy 1.0 Time 3.202s\n",
            " correct:  tensor(4032) Total:  4032\n",
            "Epoch 1, Batch 63, Loss 35.26704542977469 Accuracy 1.0 Time 3.177s\n",
            " correct:  tensor(4096) Total:  4096\n",
            "Epoch 1, Batch 64, Loss 35.11593872308731 Accuracy 1.0 Time 3.128s\n",
            " correct:  tensor(4160) Total:  4160\n",
            "Epoch 1, Batch 65, Loss 34.98050528306227 Accuracy 1.0 Time 3.057s\n",
            " correct:  tensor(4224) Total:  4224\n",
            "Epoch 1, Batch 66, Loss 34.85611039941961 Accuracy 1.0 Time 3.171s\n",
            " correct:  tensor(4288) Total:  4288\n",
            "Epoch 1, Batch 67, Loss 34.731599750803476 Accuracy 1.0 Time 3.085s\n",
            " correct:  tensor(4352) Total:  4352\n",
            "Epoch 1, Batch 68, Loss 34.579080637763525 Accuracy 1.0 Time 3.068s\n",
            " correct:  tensor(4416) Total:  4416\n",
            "Epoch 1, Batch 69, Loss 34.43782773225204 Accuracy 1.0 Time 3.058s\n",
            " correct:  tensor(4480) Total:  4480\n",
            "Epoch 1, Batch 70, Loss 34.313273675101144 Accuracy 1.0 Time 3.099s\n",
            " correct:  tensor(4544) Total:  4544\n",
            "Epoch 1, Batch 71, Loss 34.17538401106714 Accuracy 1.0 Time 3.116s\n",
            " correct:  tensor(4608) Total:  4608\n",
            "Epoch 1, Batch 72, Loss 34.03527029355367 Accuracy 1.0 Time 3.105s\n",
            " correct:  tensor(4672) Total:  4672\n",
            "Epoch 1, Batch 73, Loss 33.93006805524434 Accuracy 1.0 Time 3.15s\n",
            " correct:  tensor(4736) Total:  4736\n",
            "Epoch 1, Batch 74, Loss 33.815696097709036 Accuracy 1.0 Time 3.108s\n",
            " correct:  tensor(4800) Total:  4800\n",
            "Epoch 1, Batch 75, Loss 33.70751276652018 Accuracy 1.0 Time 3.055s\n",
            " correct:  tensor(4864) Total:  4864\n",
            "Epoch 1, Batch 76, Loss 33.59542259417082 Accuracy 1.0 Time 3.109s\n",
            " correct:  tensor(4928) Total:  4928\n",
            "Epoch 1, Batch 77, Loss 33.49837825824688 Accuracy 1.0 Time 3.063s\n",
            " correct:  tensor(4992) Total:  4992\n",
            "Epoch 1, Batch 78, Loss 33.40413367442596 Accuracy 1.0 Time 3.112s\n",
            " correct:  tensor(5056) Total:  5056\n",
            "Epoch 1, Batch 79, Loss 33.30453655387782 Accuracy 1.0 Time 3.06s\n",
            " correct:  tensor(5120) Total:  5120\n",
            "Epoch 1, Batch 80, Loss 33.19161207675934 Accuracy 1.0 Time 3.002s\n",
            " correct:  tensor(5184) Total:  5184\n",
            "Epoch 1, Batch 81, Loss 33.09773854856138 Accuracy 1.0 Time 3.112s\n",
            " correct:  tensor(5248) Total:  5248\n",
            "Epoch 1, Batch 82, Loss 33.007305354606814 Accuracy 1.0 Time 3.097s\n",
            " correct:  tensor(5312) Total:  5312\n",
            "Epoch 1, Batch 83, Loss 32.912324721554675 Accuracy 1.0 Time 3.068s\n",
            " correct:  tensor(5376) Total:  5376\n",
            "Epoch 1, Batch 84, Loss 32.791759581792924 Accuracy 1.0 Time 3.072s\n",
            " correct:  tensor(5440) Total:  5440\n",
            "Epoch 1, Batch 85, Loss 32.70269297431497 Accuracy 1.0 Time 3.105s\n",
            " correct:  tensor(5504) Total:  5504\n",
            "Epoch 1, Batch 86, Loss 32.598911174508025 Accuracy 1.0 Time 3.21s\n",
            " correct:  tensor(5568) Total:  5568\n",
            "Epoch 1, Batch 87, Loss 32.48850958374725 Accuracy 1.0 Time 3.321s\n",
            " correct:  tensor(5632) Total:  5632\n",
            "Epoch 1, Batch 88, Loss 32.393254713578656 Accuracy 1.0 Time 3.159s\n",
            " correct:  tensor(5696) Total:  5696\n",
            "Epoch 1, Batch 89, Loss 32.28542604339257 Accuracy 1.0 Time 3.064s\n",
            " correct:  tensor(5760) Total:  5760\n",
            "Epoch 1, Batch 90, Loss 32.196762021382646 Accuracy 1.0 Time 3.134s\n",
            " correct:  tensor(5824) Total:  5824\n",
            "Epoch 1, Batch 91, Loss 32.09414853106488 Accuracy 1.0 Time 3.03s\n",
            " correct:  tensor(5888) Total:  5888\n",
            "Epoch 1, Batch 92, Loss 31.994127439415973 Accuracy 1.0 Time 3.084s\n",
            " correct:  tensor(5952) Total:  5952\n",
            "Epoch 1, Batch 93, Loss 31.89922390189222 Accuracy 1.0 Time 3.168s\n",
            " correct:  tensor(6016) Total:  6016\n",
            "Epoch 1, Batch 94, Loss 31.81854696476713 Accuracy 1.0 Time 3.104s\n",
            " correct:  tensor(6080) Total:  6080\n",
            "Epoch 1, Batch 95, Loss 31.746203954596268 Accuracy 1.0 Time 3.051s\n",
            " correct:  tensor(6144) Total:  6144\n",
            "Epoch 1, Batch 96, Loss 31.65699241558711 Accuracy 1.0 Time 3.092s\n",
            " correct:  tensor(6208) Total:  6208\n",
            "Epoch 1, Batch 97, Loss 31.57264003557028 Accuracy 1.0 Time 3.124s\n",
            " correct:  tensor(6272) Total:  6272\n",
            "Epoch 1, Batch 98, Loss 31.477749590971033 Accuracy 1.0 Time 3.075s\n",
            " correct:  tensor(6336) Total:  6336\n",
            "Epoch 1, Batch 99, Loss 31.40519159490412 Accuracy 1.0 Time 3.082s\n",
            " correct:  tensor(6400) Total:  6400\n",
            "Epoch 1, Batch 100, Loss 31.321497268676758 Accuracy 1.0 Time 3.07s\n",
            " correct:  tensor(6464) Total:  6464\n",
            "Epoch 1, Batch 101, Loss 31.23144516142288 Accuracy 1.0 Time 3.035s\n",
            " correct:  tensor(6528) Total:  6528\n",
            "Epoch 1, Batch 102, Loss 31.15339843899596 Accuracy 1.0 Time 3.06s\n",
            " correct:  tensor(6592) Total:  6592\n",
            "Epoch 1, Batch 103, Loss 31.069716870206072 Accuracy 1.0 Time 3.117s\n",
            " correct:  tensor(6656) Total:  6656\n",
            "Epoch 1, Batch 104, Loss 30.97835328028752 Accuracy 1.0 Time 3.051s\n",
            " correct:  tensor(6720) Total:  6720\n",
            "Epoch 1, Batch 105, Loss 30.90691637311663 Accuracy 1.0 Time 3.1s\n",
            " correct:  tensor(6784) Total:  6784\n",
            "Epoch 1, Batch 106, Loss 30.81254138586656 Accuracy 1.0 Time 3.094s\n",
            " correct:  tensor(6848) Total:  6848\n",
            "Epoch 1, Batch 107, Loss 30.730277195155065 Accuracy 1.0 Time 3.072s\n",
            " correct:  tensor(6912) Total:  6912\n",
            "Epoch 1, Batch 108, Loss 30.646511042559588 Accuracy 1.0 Time 3.052s\n",
            " correct:  tensor(6976) Total:  6976\n",
            "Epoch 1, Batch 109, Loss 30.574132446849017 Accuracy 1.0 Time 3.094s\n",
            " correct:  tensor(7040) Total:  7040\n",
            "Epoch 1, Batch 110, Loss 30.508750273964623 Accuracy 1.0 Time 3.082s\n",
            " correct:  tensor(7104) Total:  7104\n",
            "Epoch 1, Batch 111, Loss 30.429508535711616 Accuracy 1.0 Time 3.058s\n",
            " correct:  tensor(7168) Total:  7168\n",
            "Epoch 1, Batch 112, Loss 30.36310182298933 Accuracy 1.0 Time 3.089s\n",
            " correct:  tensor(7232) Total:  7232\n",
            "Epoch 1, Batch 113, Loss 30.296643620043728 Accuracy 1.0 Time 3.03s\n",
            " correct:  tensor(7296) Total:  7296\n",
            "Epoch 1, Batch 114, Loss 30.23011137309827 Accuracy 1.0 Time 3.073s\n",
            " correct:  tensor(7360) Total:  7360\n",
            "Epoch 1, Batch 115, Loss 30.163564516150434 Accuracy 1.0 Time 3.033s\n",
            " correct:  tensor(7424) Total:  7424\n",
            "Epoch 1, Batch 116, Loss 30.08837129329813 Accuracy 1.0 Time 3.063s\n",
            " correct:  tensor(7488) Total:  7488\n",
            "Epoch 1, Batch 117, Loss 30.03062841627333 Accuracy 1.0 Time 3.077s\n",
            " correct:  tensor(7552) Total:  7552\n",
            "Epoch 1, Batch 118, Loss 29.978446814973477 Accuracy 1.0 Time 3.022s\n",
            " correct:  tensor(7616) Total:  7616\n",
            "Epoch 1, Batch 119, Loss 29.89251011760295 Accuracy 1.0 Time 3.08s\n",
            " correct:  tensor(7680) Total:  7680\n",
            "Epoch 1, Batch 120, Loss 29.82123328844706 Accuracy 1.0 Time 3.053s\n",
            " correct:  tensor(7744) Total:  7744\n",
            "Epoch 1, Batch 121, Loss 29.75599564796637 Accuracy 1.0 Time 3.1s\n",
            " correct:  tensor(7808) Total:  7808\n",
            "Epoch 1, Batch 122, Loss 29.67897440175541 Accuracy 1.0 Time 3.102s\n",
            " correct:  tensor(7872) Total:  7872\n",
            "Epoch 1, Batch 123, Loss 29.612035022518498 Accuracy 1.0 Time 3.12s\n",
            " correct:  tensor(7936) Total:  7936\n",
            "Epoch 1, Batch 124, Loss 29.54106947683519 Accuracy 1.0 Time 3.08s\n",
            " correct:  tensor(8000) Total:  8000\n",
            "Epoch 1, Batch 125, Loss 29.47654655456543 Accuracy 1.0 Time 3.123s\n",
            " correct:  tensor(8064) Total:  8064\n",
            "Epoch 1, Batch 126, Loss 29.421502052791535 Accuracy 1.0 Time 3.083s\n",
            " correct:  tensor(8128) Total:  8128\n",
            "Epoch 1, Batch 127, Loss 29.357545627383736 Accuracy 1.0 Time 3.06s\n",
            " correct:  tensor(8192) Total:  8192\n",
            "Epoch 1, Batch 128, Loss 29.302189275622368 Accuracy 1.0 Time 3.057s\n",
            " correct:  tensor(8256) Total:  8256\n",
            "Epoch 1, Batch 129, Loss 29.254837021347164 Accuracy 1.0 Time 3.108s\n",
            " correct:  tensor(8320) Total:  8320\n",
            "Epoch 1, Batch 130, Loss 29.210648624713603 Accuracy 1.0 Time 3.06s\n",
            " correct:  tensor(8384) Total:  8384\n",
            "Epoch 1, Batch 131, Loss 29.15511604483801 Accuracy 1.0 Time 3.225s\n",
            " correct:  tensor(8448) Total:  8448\n",
            "Epoch 1, Batch 132, Loss 29.10250132011645 Accuracy 1.0 Time 3.134s\n",
            " correct:  tensor(8512) Total:  8512\n",
            "Epoch 1, Batch 133, Loss 29.063281740461075 Accuracy 1.0 Time 3.068s\n",
            " correct:  tensor(8576) Total:  8576\n",
            "Epoch 1, Batch 134, Loss 29.00473539153142 Accuracy 1.0 Time 3.172s\n",
            " correct:  tensor(8640) Total:  8640\n",
            "Epoch 1, Batch 135, Loss 28.942289168746385 Accuracy 1.0 Time 3.069s\n",
            " correct:  tensor(8704) Total:  8704\n",
            "Epoch 1, Batch 136, Loss 28.890081756255206 Accuracy 1.0 Time 3.051s\n",
            " correct:  tensor(8768) Total:  8768\n",
            "Epoch 1, Batch 137, Loss 28.830485754639565 Accuracy 1.0 Time 3.312s\n",
            " correct:  tensor(8832) Total:  8832\n",
            "Epoch 1, Batch 138, Loss 28.770556850709777 Accuracy 1.0 Time 3.347s\n",
            " correct:  tensor(8896) Total:  8896\n",
            "Epoch 1, Batch 139, Loss 28.712795092905168 Accuracy 1.0 Time 3.323s\n",
            " correct:  tensor(8960) Total:  8960\n",
            "Epoch 1, Batch 140, Loss 28.65974501201085 Accuracy 1.0 Time 3.106s\n",
            " correct:  tensor(9024) Total:  9024\n",
            "Epoch 1, Batch 141, Loss 28.614052075866265 Accuracy 1.0 Time 3.085s\n",
            " correct:  tensor(9088) Total:  9088\n",
            "Epoch 1, Batch 142, Loss 28.564090836216028 Accuracy 1.0 Time 3.062s\n",
            " correct:  tensor(9152) Total:  9152\n",
            "Epoch 1, Batch 143, Loss 28.514058453219754 Accuracy 1.0 Time 3.072s\n",
            " correct:  tensor(9216) Total:  9216\n",
            "Epoch 1, Batch 144, Loss 28.46719053056505 Accuracy 1.0 Time 3.108s\n",
            " correct:  tensor(9280) Total:  9280\n",
            "Epoch 1, Batch 145, Loss 28.419323585773338 Accuracy 1.0 Time 3.064s\n",
            " correct:  tensor(9344) Total:  9344\n",
            "Epoch 1, Batch 146, Loss 28.362304360899206 Accuracy 1.0 Time 3.057s\n",
            " correct:  tensor(9408) Total:  9408\n",
            "Epoch 1, Batch 147, Loss 28.31970927180076 Accuracy 1.0 Time 3.102s\n",
            " correct:  tensor(9472) Total:  9472\n",
            "Epoch 1, Batch 148, Loss 28.26343127843496 Accuracy 1.0 Time 3.04s\n",
            " correct:  tensor(9536) Total:  9536\n",
            "Epoch 1, Batch 149, Loss 28.22643774147802 Accuracy 1.0 Time 3.053s\n",
            " correct:  tensor(9600) Total:  9600\n",
            "Epoch 1, Batch 150, Loss 28.18441411336263 Accuracy 1.0 Time 3.068s\n",
            " correct:  tensor(9664) Total:  9664\n",
            "Epoch 1, Batch 151, Loss 28.141072279570118 Accuracy 1.0 Time 3.147s\n",
            " correct:  tensor(9728) Total:  9728\n",
            "Epoch 1, Batch 152, Loss 28.0999054783269 Accuracy 1.0 Time 3.059s\n",
            " correct:  tensor(9792) Total:  9792\n",
            "Epoch 1, Batch 153, Loss 28.04140749164656 Accuracy 1.0 Time 3.152s\n",
            " correct:  tensor(9856) Total:  9856\n",
            "Epoch 1, Batch 154, Loss 28.0088167685967 Accuracy 1.0 Time 3.134s\n",
            " correct:  tensor(9920) Total:  9920\n",
            "Epoch 1, Batch 155, Loss 27.960084435247605 Accuracy 1.0 Time 3.043s\n",
            " correct:  tensor(9984) Total:  9984\n",
            "Epoch 1, Batch 156, Loss 27.915952413510052 Accuracy 1.0 Time 3.041s\n",
            " correct:  tensor(10048) Total:  10048\n",
            "Epoch 1, Batch 157, Loss 27.87651889339374 Accuracy 1.0 Time 3.072s\n",
            " correct:  tensor(10112) Total:  10112\n",
            "Epoch 1, Batch 158, Loss 27.835585075088694 Accuracy 1.0 Time 3.03s\n",
            " correct:  tensor(10176) Total:  10176\n",
            "Epoch 1, Batch 159, Loss 27.781730076052106 Accuracy 1.0 Time 3.035s\n",
            " correct:  tensor(10240) Total:  10240\n",
            "Epoch 1, Batch 160, Loss 27.733001613616942 Accuracy 1.0 Time 3.118s\n",
            " correct:  tensor(10304) Total:  10304\n",
            "Epoch 1, Batch 161, Loss 27.68822859533085 Accuracy 1.0 Time 3.145s\n",
            " correct:  tensor(10368) Total:  10368\n",
            "Epoch 1, Batch 162, Loss 27.649565590752495 Accuracy 1.0 Time 3.054s\n",
            " correct:  tensor(10432) Total:  10432\n",
            "Epoch 1, Batch 163, Loss 27.603617205941603 Accuracy 1.0 Time 3.114s\n",
            " correct:  tensor(10496) Total:  10496\n",
            "Epoch 1, Batch 164, Loss 27.56071908299516 Accuracy 1.0 Time 3.105s\n",
            " correct:  tensor(10560) Total:  10560\n",
            "Epoch 1, Batch 165, Loss 27.52081105781324 Accuracy 1.0 Time 3.104s\n",
            " correct:  tensor(10624) Total:  10624\n",
            "Epoch 1, Batch 166, Loss 27.4705171585083 Accuracy 1.0 Time 3.082s\n",
            " correct:  tensor(10688) Total:  10688\n",
            "Epoch 1, Batch 167, Loss 27.429464785638682 Accuracy 1.0 Time 3.056s\n",
            " correct:  tensor(10752) Total:  10752\n",
            "Epoch 1, Batch 168, Loss 27.396850495111373 Accuracy 1.0 Time 3.026s\n",
            " correct:  tensor(10816) Total:  10816\n",
            "Epoch 1, Batch 169, Loss 27.364593697722846 Accuracy 1.0 Time 3.079s\n",
            " correct:  tensor(10880) Total:  10880\n",
            "Epoch 1, Batch 170, Loss 27.32063632291906 Accuracy 1.0 Time 3.124s\n",
            " correct:  tensor(10944) Total:  10944\n",
            "Epoch 1, Batch 171, Loss 27.2775196276213 Accuracy 1.0 Time 3.026s\n",
            " correct:  tensor(11008) Total:  11008\n",
            "Epoch 1, Batch 172, Loss 27.240421417147616 Accuracy 1.0 Time 3.128s\n",
            " correct:  tensor(11072) Total:  11072\n",
            "Epoch 1, Batch 173, Loss 27.20961421900402 Accuracy 1.0 Time 3.047s\n",
            " correct:  tensor(11136) Total:  11136\n",
            "Epoch 1, Batch 174, Loss 27.171142008112763 Accuracy 1.0 Time 3.17s\n",
            " correct:  tensor(11200) Total:  11200\n",
            "Epoch 1, Batch 175, Loss 27.13549668448312 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(11264) Total:  11264\n",
            "Epoch 1, Batch 176, Loss 27.09159481525421 Accuracy 1.0 Time 3.051s\n",
            " correct:  tensor(11328) Total:  11328\n",
            "Epoch 1, Batch 177, Loss 27.055452077402233 Accuracy 1.0 Time 3.087s\n",
            " correct:  tensor(11392) Total:  11392\n",
            "Epoch 1, Batch 178, Loss 27.022593541091748 Accuracy 1.0 Time 3.084s\n",
            " correct:  tensor(11456) Total:  11456\n",
            "Epoch 1, Batch 179, Loss 26.978461942193228 Accuracy 1.0 Time 3.11s\n",
            " correct:  tensor(11520) Total:  11520\n",
            "Epoch 1, Batch 180, Loss 26.939418623182508 Accuracy 1.0 Time 3.062s\n",
            " correct:  tensor(11584) Total:  11584\n",
            "Epoch 1, Batch 181, Loss 26.907746499414603 Accuracy 1.0 Time 3.021s\n",
            " correct:  tensor(11648) Total:  11648\n",
            "Epoch 1, Batch 182, Loss 26.876038425571316 Accuracy 1.0 Time 3.12s\n",
            " correct:  tensor(11712) Total:  11712\n",
            "Epoch 1, Batch 183, Loss 26.83388011703074 Accuracy 1.0 Time 3.223s\n",
            " correct:  tensor(11776) Total:  11776\n",
            "Epoch 1, Batch 184, Loss 26.806925483371902 Accuracy 1.0 Time 3.084s\n",
            " correct:  tensor(11840) Total:  11840\n",
            "Epoch 1, Batch 185, Loss 26.77530521702122 Accuracy 1.0 Time 3.307s\n",
            " correct:  tensor(11904) Total:  11904\n",
            "Epoch 1, Batch 186, Loss 26.741623765678817 Accuracy 1.0 Time 3.322s\n",
            " correct:  tensor(11968) Total:  11968\n",
            "Epoch 1, Batch 187, Loss 26.700695864019547 Accuracy 1.0 Time 3.057s\n",
            " correct:  tensor(12032) Total:  12032\n",
            "Epoch 1, Batch 188, Loss 26.66187110860297 Accuracy 1.0 Time 3.134s\n",
            " correct:  tensor(12096) Total:  12096\n",
            "Epoch 1, Batch 189, Loss 26.631064409932133 Accuracy 1.0 Time 3.221s\n",
            " correct:  tensor(12160) Total:  12160\n",
            "Epoch 1, Batch 190, Loss 26.598882042734246 Accuracy 1.0 Time 3.148s\n",
            " correct:  tensor(12224) Total:  12224\n",
            "Epoch 1, Batch 191, Loss 26.561851112006224 Accuracy 1.0 Time 3.114s\n",
            " correct:  tensor(12288) Total:  12288\n",
            "Epoch 1, Batch 192, Loss 26.528127719958622 Accuracy 1.0 Time 3.089s\n",
            " correct:  tensor(12352) Total:  12352\n",
            "Epoch 1, Batch 193, Loss 26.491864792423545 Accuracy 1.0 Time 3.065s\n",
            " correct:  tensor(12416) Total:  12416\n",
            "Epoch 1, Batch 194, Loss 26.465581520316526 Accuracy 1.0 Time 3.109s\n",
            " correct:  tensor(12480) Total:  12480\n",
            "Epoch 1, Batch 195, Loss 26.4258667970315 Accuracy 1.0 Time 3.082s\n",
            " correct:  tensor(12544) Total:  12544\n",
            "Epoch 1, Batch 196, Loss 26.391626630510604 Accuracy 1.0 Time 3.063s\n",
            " correct:  tensor(12608) Total:  12608\n",
            "Epoch 1, Batch 197, Loss 26.36489001627501 Accuracy 1.0 Time 3.053s\n",
            " correct:  tensor(12672) Total:  12672\n",
            "Epoch 1, Batch 198, Loss 26.332441599682124 Accuracy 1.0 Time 3.12s\n",
            " correct:  tensor(12736) Total:  12736\n",
            "Epoch 1, Batch 199, Loss 26.3085034336876 Accuracy 1.0 Time 3.112s\n",
            " correct:  tensor(12800) Total:  12800\n",
            "Epoch 1, Batch 200, Loss 26.278164043426514 Accuracy 1.0 Time 3.078s\n",
            " correct:  tensor(12864) Total:  12864\n",
            "Epoch 1, Batch 201, Loss 26.241836680701716 Accuracy 1.0 Time 3.103s\n",
            " correct:  tensor(12928) Total:  12928\n",
            "Epoch 1, Batch 202, Loss 26.218101454253244 Accuracy 1.0 Time 3.09s\n",
            " correct:  tensor(12992) Total:  12992\n",
            "Epoch 1, Batch 203, Loss 26.1890669573704 Accuracy 1.0 Time 3.039s\n",
            " correct:  tensor(13056) Total:  13056\n",
            "Epoch 1, Batch 204, Loss 26.153666187735165 Accuracy 1.0 Time 3.043s\n",
            " correct:  tensor(13120) Total:  13120\n",
            "Epoch 1, Batch 205, Loss 26.12967530227289 Accuracy 1.0 Time 3.084s\n",
            " correct:  tensor(13184) Total:  13184\n",
            "Epoch 1, Batch 206, Loss 26.09858410103807 Accuracy 1.0 Time 3.124s\n",
            " correct:  tensor(13248) Total:  13248\n",
            "Epoch 1, Batch 207, Loss 26.079419702723406 Accuracy 1.0 Time 3.052s\n",
            " correct:  tensor(13312) Total:  13312\n",
            "Epoch 1, Batch 208, Loss 26.051767450112564 Accuracy 1.0 Time 3.066s\n",
            " correct:  tensor(13376) Total:  13376\n",
            "Epoch 1, Batch 209, Loss 26.03030604494816 Accuracy 1.0 Time 3.061s\n",
            " correct:  tensor(13440) Total:  13440\n",
            "Epoch 1, Batch 210, Loss 26.002154541015624 Accuracy 1.0 Time 3.1s\n",
            " correct:  tensor(13504) Total:  13504\n",
            "Epoch 1, Batch 211, Loss 25.978541351607625 Accuracy 1.0 Time 3.091s\n",
            " correct:  tensor(13568) Total:  13568\n",
            "Epoch 1, Batch 212, Loss 25.955320142350107 Accuracy 1.0 Time 3.068s\n",
            " correct:  tensor(13632) Total:  13632\n",
            "Epoch 1, Batch 213, Loss 25.932412098271186 Accuracy 1.0 Time 3.112s\n",
            " correct:  tensor(13696) Total:  13696\n",
            "Epoch 1, Batch 214, Loss 25.899231429411987 Accuracy 1.0 Time 3.069s\n",
            " correct:  tensor(13760) Total:  13760\n",
            "Epoch 1, Batch 215, Loss 25.875287273318268 Accuracy 1.0 Time 3.077s\n",
            " correct:  tensor(13824) Total:  13824\n",
            "Epoch 1, Batch 216, Loss 25.849197661435163 Accuracy 1.0 Time 3.062s\n",
            " correct:  tensor(13888) Total:  13888\n",
            "Epoch 1, Batch 217, Loss 25.818680767639442 Accuracy 1.0 Time 3.084s\n",
            " correct:  tensor(13952) Total:  13952\n",
            "Epoch 1, Batch 218, Loss 25.793812034326955 Accuracy 1.0 Time 3.0s\n",
            " correct:  tensor(14016) Total:  14016\n",
            "Epoch 1, Batch 219, Loss 25.769224933293312 Accuracy 1.0 Time 3.073s\n",
            " correct:  tensor(14080) Total:  14080\n",
            "Epoch 1, Batch 220, Loss 25.74457359313965 Accuracy 1.0 Time 3.077s\n",
            " correct:  tensor(14144) Total:  14144\n",
            "Epoch 1, Batch 221, Loss 25.7129562688629 Accuracy 1.0 Time 3.134s\n",
            " correct:  tensor(14208) Total:  14208\n",
            "Epoch 1, Batch 222, Loss 25.684409794506728 Accuracy 1.0 Time 3.041s\n",
            " correct:  tensor(14272) Total:  14272\n",
            "Epoch 1, Batch 223, Loss 25.656269424164776 Accuracy 1.0 Time 3.098s\n",
            " correct:  tensor(14336) Total:  14336\n",
            "Epoch 1, Batch 224, Loss 25.62900467429842 Accuracy 1.0 Time 3.083s\n",
            " correct:  tensor(14400) Total:  14400\n",
            "Epoch 1, Batch 225, Loss 25.60388222588433 Accuracy 1.0 Time 3.104s\n",
            " correct:  tensor(14464) Total:  14464\n",
            "Epoch 1, Batch 226, Loss 25.57304133356145 Accuracy 1.0 Time 3.057s\n",
            " correct:  tensor(14528) Total:  14528\n",
            "Epoch 1, Batch 227, Loss 25.54480209014489 Accuracy 1.0 Time 3.049s\n",
            " correct:  tensor(14592) Total:  14592\n",
            "Epoch 1, Batch 228, Loss 25.525445620218914 Accuracy 1.0 Time 3.075s\n",
            " correct:  tensor(14656) Total:  14656\n",
            "Epoch 1, Batch 229, Loss 25.5081823590541 Accuracy 1.0 Time 3.131s\n",
            " correct:  tensor(14720) Total:  14720\n",
            "Epoch 1, Batch 230, Loss 25.485147799616275 Accuracy 1.0 Time 3.074s\n",
            " correct:  tensor(14784) Total:  14784\n",
            "Epoch 1, Batch 231, Loss 25.45933803541836 Accuracy 1.0 Time 3.053s\n",
            " correct:  tensor(14848) Total:  14848\n",
            "Epoch 1, Batch 232, Loss 25.43194704220213 Accuracy 1.0 Time 3.07s\n",
            " correct:  tensor(14912) Total:  14912\n",
            "Epoch 1, Batch 233, Loss 25.407269571983765 Accuracy 1.0 Time 3.063s\n",
            " correct:  tensor(14976) Total:  14976\n",
            "Epoch 1, Batch 234, Loss 25.38110734662439 Accuracy 1.0 Time 3.104s\n",
            " correct:  tensor(15040) Total:  15040\n",
            "Epoch 1, Batch 235, Loss 25.358234900616583 Accuracy 1.0 Time 3.085s\n",
            " correct:  tensor(15104) Total:  15104\n",
            "Epoch 1, Batch 236, Loss 25.337806434954626 Accuracy 1.0 Time 3.096s\n",
            " correct:  tensor(15168) Total:  15168\n",
            "Epoch 1, Batch 237, Loss 25.311881592505088 Accuracy 1.0 Time 3.264s\n",
            " correct:  tensor(15232) Total:  15232\n",
            "Epoch 1, Batch 238, Loss 25.290821668480625 Accuracy 1.0 Time 3.305s\n",
            " correct:  tensor(15296) Total:  15296\n",
            "Epoch 1, Batch 239, Loss 25.26467468549018 Accuracy 1.0 Time 3.342s\n",
            " correct:  tensor(15360) Total:  15360\n",
            "Epoch 1, Batch 240, Loss 25.24118717511495 Accuracy 1.0 Time 3.162s\n",
            " correct:  tensor(15424) Total:  15424\n",
            "Epoch 1, Batch 241, Loss 25.215168600755113 Accuracy 1.0 Time 3.107s\n",
            " correct:  tensor(15488) Total:  15488\n",
            "Epoch 1, Batch 242, Loss 25.197257175918452 Accuracy 1.0 Time 3.059s\n",
            " correct:  tensor(15552) Total:  15552\n",
            "Epoch 1, Batch 243, Loss 25.16888842170621 Accuracy 1.0 Time 3.091s\n",
            " correct:  tensor(15616) Total:  15616\n",
            "Epoch 1, Batch 244, Loss 25.1441767958344 Accuracy 1.0 Time 3.117s\n",
            " correct:  tensor(15680) Total:  15680\n",
            "Epoch 1, Batch 245, Loss 25.12274673617616 Accuracy 1.0 Time 3.04s\n",
            " correct:  tensor(15744) Total:  15744\n",
            "Epoch 1, Batch 246, Loss 25.110529496417783 Accuracy 1.0 Time 3.101s\n",
            " correct:  tensor(15808) Total:  15808\n",
            "Epoch 1, Batch 247, Loss 25.091199921210286 Accuracy 1.0 Time 3.142s\n",
            " correct:  tensor(15872) Total:  15872\n",
            "Epoch 1, Batch 248, Loss 25.067703931562363 Accuracy 1.0 Time 3.069s\n",
            " correct:  tensor(15936) Total:  15936\n",
            "Epoch 1, Batch 249, Loss 25.047954298884992 Accuracy 1.0 Time 3.126s\n",
            " correct:  tensor(16000) Total:  16000\n",
            "Epoch 1, Batch 250, Loss 25.03150510406494 Accuracy 1.0 Time 3.1s\n",
            " correct:  tensor(16064) Total:  16064\n",
            "Epoch 1, Batch 251, Loss 25.015222108696562 Accuracy 1.0 Time 3.07s\n",
            " correct:  tensor(16128) Total:  16128\n",
            "Epoch 1, Batch 252, Loss 24.999634258330815 Accuracy 1.0 Time 3.125s\n",
            " correct:  tensor(16192) Total:  16192\n",
            "Epoch 1, Batch 253, Loss 24.977788668847367 Accuracy 1.0 Time 3.124s\n",
            " correct:  tensor(16256) Total:  16256\n",
            "Epoch 1, Batch 254, Loss 24.959217259264367 Accuracy 1.0 Time 3.033s\n",
            " correct:  tensor(16320) Total:  16320\n",
            "Epoch 1, Batch 255, Loss 24.94083136764227 Accuracy 1.0 Time 3.067s\n",
            " correct:  tensor(16384) Total:  16384\n",
            "Epoch 1, Batch 256, Loss 24.915896400809288 Accuracy 1.0 Time 3.137s\n",
            " correct:  tensor(16448) Total:  16448\n",
            "Epoch 1, Batch 257, Loss 24.89241453089139 Accuracy 1.0 Time 3.14s\n",
            " correct:  tensor(16512) Total:  16512\n",
            "Epoch 1, Batch 258, Loss 24.871692080830417 Accuracy 1.0 Time 3.122s\n",
            " correct:  tensor(16576) Total:  16576\n",
            "Epoch 1, Batch 259, Loss 24.85501766573048 Accuracy 1.0 Time 3.18s\n",
            " correct:  tensor(16640) Total:  16640\n",
            "Epoch 1, Batch 260, Loss 24.836474264585053 Accuracy 1.0 Time 3.221s\n",
            " correct:  tensor(16704) Total:  16704\n",
            "Epoch 1, Batch 261, Loss 24.817413220460388 Accuracy 1.0 Time 3.148s\n",
            " correct:  tensor(16768) Total:  16768\n",
            "Epoch 1, Batch 262, Loss 24.798124779271717 Accuracy 1.0 Time 3.131s\n",
            " correct:  tensor(16832) Total:  16832\n",
            "Epoch 1, Batch 263, Loss 24.776392519700664 Accuracy 1.0 Time 3.115s\n",
            " correct:  tensor(16896) Total:  16896\n",
            "Epoch 1, Batch 264, Loss 24.75648140184807 Accuracy 1.0 Time 3.136s\n",
            " correct:  tensor(16960) Total:  16960\n",
            "Epoch 1, Batch 265, Loss 24.738637139662256 Accuracy 1.0 Time 3.116s\n",
            " correct:  tensor(17024) Total:  17024\n",
            "Epoch 1, Batch 266, Loss 24.719262617871276 Accuracy 1.0 Time 3.139s\n",
            " correct:  tensor(17088) Total:  17088\n",
            "Epoch 1, Batch 267, Loss 24.697337611337726 Accuracy 1.0 Time 3.068s\n",
            " correct:  tensor(17152) Total:  17152\n",
            "Epoch 1, Batch 268, Loss 24.682654402149257 Accuracy 1.0 Time 3.079s\n",
            " correct:  tensor(17216) Total:  17216\n",
            "Epoch 1, Batch 269, Loss 24.668227412000466 Accuracy 1.0 Time 3.198s\n",
            " correct:  tensor(17280) Total:  17280\n",
            "Epoch 1, Batch 270, Loss 24.655866877237955 Accuracy 1.0 Time 3.12s\n",
            " correct:  tensor(17344) Total:  17344\n",
            "Epoch 1, Batch 271, Loss 24.64301831431934 Accuracy 1.0 Time 3.106s\n",
            " correct:  tensor(17408) Total:  17408\n",
            "Epoch 1, Batch 272, Loss 24.622522718766156 Accuracy 1.0 Time 3.11s\n",
            " correct:  tensor(17472) Total:  17472\n",
            "Epoch 1, Batch 273, Loss 24.59876226068853 Accuracy 1.0 Time 3.126s\n",
            " correct:  tensor(17536) Total:  17536\n",
            "Epoch 1, Batch 274, Loss 24.581439818779046 Accuracy 1.0 Time 3.148s\n",
            " correct:  tensor(17600) Total:  17600\n",
            "Epoch 1, Batch 275, Loss 24.56083599437367 Accuracy 1.0 Time 3.021s\n",
            " correct:  tensor(17664) Total:  17664\n",
            "Epoch 1, Batch 276, Loss 24.539711931477424 Accuracy 1.0 Time 3.096s\n",
            " correct:  tensor(17728) Total:  17728\n",
            "Epoch 1, Batch 277, Loss 24.52300597786473 Accuracy 1.0 Time 3.106s\n",
            " correct:  tensor(17792) Total:  17792\n",
            "Epoch 1, Batch 278, Loss 24.50702960885686 Accuracy 1.0 Time 3.146s\n",
            " correct:  tensor(17856) Total:  17856\n",
            "Epoch 1, Batch 279, Loss 24.4855711468659 Accuracy 1.0 Time 3.214s\n",
            " correct:  tensor(17920) Total:  17920\n",
            "Epoch 1, Batch 280, Loss 24.4698974609375 Accuracy 1.0 Time 3.148s\n",
            " correct:  tensor(17984) Total:  17984\n",
            "Epoch 1, Batch 281, Loss 24.448769681394314 Accuracy 1.0 Time 3.129s\n",
            " correct:  tensor(18048) Total:  18048\n",
            "Epoch 1, Batch 282, Loss 24.42983098402091 Accuracy 1.0 Time 3.174s\n",
            " correct:  tensor(18112) Total:  18112\n",
            "Epoch 1, Batch 283, Loss 24.40267711140663 Accuracy 1.0 Time 3.343s\n",
            " correct:  tensor(18176) Total:  18176\n",
            "Epoch 1, Batch 284, Loss 24.38713719811238 Accuracy 1.0 Time 3.264s\n",
            " correct:  tensor(18240) Total:  18240\n",
            "Epoch 1, Batch 285, Loss 24.368600611101115 Accuracy 1.0 Time 3.154s\n",
            " correct:  tensor(18304) Total:  18304\n",
            "Epoch 1, Batch 286, Loss 24.352789472033095 Accuracy 1.0 Time 3.14s\n",
            " correct:  tensor(18368) Total:  18368\n",
            "Epoch 1, Batch 287, Loss 24.337723655567768 Accuracy 1.0 Time 3.091s\n",
            " correct:  tensor(18432) Total:  18432\n",
            "Epoch 1, Batch 288, Loss 24.320769594775307 Accuracy 1.0 Time 3.101s\n",
            " correct:  tensor(18496) Total:  18496\n",
            "Epoch 1, Batch 289, Loss 24.31164501282583 Accuracy 1.0 Time 3.13s\n",
            " correct:  tensor(18560) Total:  18560\n",
            "Epoch 1, Batch 290, Loss 24.287513048895473 Accuracy 1.0 Time 3.12s\n",
            " correct:  tensor(18624) Total:  18624\n",
            "Epoch 1, Batch 291, Loss 24.270821436983613 Accuracy 1.0 Time 3.162s\n",
            " correct:  tensor(18688) Total:  18688\n",
            "Epoch 1, Batch 292, Loss 24.251501821491818 Accuracy 1.0 Time 3.109s\n",
            " correct:  tensor(18752) Total:  18752\n",
            "Epoch 1, Batch 293, Loss 24.234737226987455 Accuracy 1.0 Time 3.104s\n",
            " correct:  tensor(18816) Total:  18816\n",
            "Epoch 1, Batch 294, Loss 24.2200870189537 Accuracy 1.0 Time 3.197s\n",
            " correct:  tensor(18880) Total:  18880\n",
            "Epoch 1, Batch 295, Loss 24.20428995682021 Accuracy 1.0 Time 3.107s\n",
            " correct:  tensor(18944) Total:  18944\n",
            "Epoch 1, Batch 296, Loss 24.186125871297474 Accuracy 1.0 Time 3.132s\n",
            " correct:  tensor(19008) Total:  19008\n",
            "Epoch 1, Batch 297, Loss 24.168244384354615 Accuracy 1.0 Time 3.115s\n",
            " correct:  tensor(19072) Total:  19072\n",
            "Epoch 1, Batch 298, Loss 24.15422134911454 Accuracy 1.0 Time 3.137s\n",
            " correct:  tensor(19136) Total:  19136\n",
            "Epoch 1, Batch 299, Loss 24.135881085858298 Accuracy 1.0 Time 3.114s\n",
            " correct:  tensor(19200) Total:  19200\n",
            "Epoch 1, Batch 300, Loss 24.119450143178305 Accuracy 1.0 Time 3.11s\n",
            " correct:  tensor(19264) Total:  19264\n",
            "Epoch 1, Batch 301, Loss 24.10586528841443 Accuracy 1.0 Time 3.126s\n",
            " correct:  tensor(19328) Total:  19328\n",
            "Epoch 1, Batch 302, Loss 24.086146714671557 Accuracy 1.0 Time 3.092s\n",
            " correct:  tensor(19392) Total:  19392\n",
            "Epoch 1, Batch 303, Loss 24.07108199242318 Accuracy 1.0 Time 3.128s\n",
            " correct:  tensor(19456) Total:  19456\n",
            "Epoch 1, Batch 304, Loss 24.053645667276886 Accuracy 1.0 Time 3.179s\n",
            " correct:  tensor(19520) Total:  19520\n",
            "Epoch 1, Batch 305, Loss 24.040346164390687 Accuracy 1.0 Time 3.12s\n",
            " correct:  tensor(19584) Total:  19584\n",
            "Epoch 1, Batch 306, Loss 24.027776624642165 Accuracy 1.0 Time 3.132s\n",
            " correct:  tensor(19648) Total:  19648\n",
            "Epoch 1, Batch 307, Loss 24.01434291379848 Accuracy 1.0 Time 3.135s\n",
            " correct:  tensor(19712) Total:  19712\n",
            "Epoch 1, Batch 308, Loss 23.998405115944998 Accuracy 1.0 Time 3.123s\n",
            " correct:  tensor(19776) Total:  19776\n",
            "Epoch 1, Batch 309, Loss 23.981578209639366 Accuracy 1.0 Time 3.074s\n",
            " correct:  tensor(19840) Total:  19840\n",
            "Epoch 1, Batch 310, Loss 23.96458479358304 Accuracy 1.0 Time 3.097s\n",
            " correct:  tensor(19904) Total:  19904\n",
            "Epoch 1, Batch 311, Loss 23.95100868323225 Accuracy 1.0 Time 3.097s\n",
            " correct:  tensor(19968) Total:  19968\n",
            "Epoch 1, Batch 312, Loss 23.942729075749714 Accuracy 1.0 Time 3.053s\n",
            " correct:  tensor(20032) Total:  20032\n",
            "Epoch 1, Batch 313, Loss 23.92719900722321 Accuracy 1.0 Time 3.083s\n",
            " correct:  tensor(20096) Total:  20096\n",
            "Epoch 1, Batch 314, Loss 23.911458258416243 Accuracy 1.0 Time 3.093s\n",
            " correct:  tensor(20160) Total:  20160\n",
            "Epoch 1, Batch 315, Loss 23.900867274450878 Accuracy 1.0 Time 3.112s\n",
            " correct:  tensor(20224) Total:  20224\n",
            "Epoch 1, Batch 316, Loss 23.884163108053087 Accuracy 1.0 Time 3.096s\n",
            " correct:  tensor(20288) Total:  20288\n",
            "Epoch 1, Batch 317, Loss 23.871283883176016 Accuracy 1.0 Time 3.093s\n",
            " correct:  tensor(20352) Total:  20352\n",
            "Epoch 1, Batch 318, Loss 23.85639584139458 Accuracy 1.0 Time 3.112s\n",
            " correct:  tensor(20416) Total:  20416\n",
            "Epoch 1, Batch 319, Loss 23.845573311689133 Accuracy 1.0 Time 3.092s\n",
            " correct:  tensor(20480) Total:  20480\n",
            "Epoch 1, Batch 320, Loss 23.83048861026764 Accuracy 1.0 Time 3.075s\n",
            " correct:  tensor(20544) Total:  20544\n",
            "Epoch 1, Batch 321, Loss 23.819358219610198 Accuracy 1.0 Time 3.079s\n",
            " correct:  tensor(20608) Total:  20608\n",
            "Epoch 1, Batch 322, Loss 23.805802973160834 Accuracy 1.0 Time 3.117s\n",
            " correct:  tensor(20672) Total:  20672\n",
            "Epoch 1, Batch 323, Loss 23.79450699419429 Accuracy 1.0 Time 3.085s\n",
            " correct:  tensor(20736) Total:  20736\n",
            "Epoch 1, Batch 324, Loss 23.78015496407026 Accuracy 1.0 Time 3.115s\n",
            " correct:  tensor(20800) Total:  20800\n",
            "Epoch 1, Batch 325, Loss 23.77150133573092 Accuracy 1.0 Time 3.082s\n",
            " correct:  tensor(20864) Total:  20864\n",
            "Epoch 1, Batch 326, Loss 23.763604532721583 Accuracy 1.0 Time 3.064s\n",
            " correct:  tensor(20928) Total:  20928\n",
            "Epoch 1, Batch 327, Loss 23.74985774795579 Accuracy 1.0 Time 3.084s\n",
            " correct:  tensor(20992) Total:  20992\n",
            "Epoch 1, Batch 328, Loss 23.736148345761183 Accuracy 1.0 Time 3.064s\n",
            " correct:  tensor(21056) Total:  21056\n",
            "Epoch 1, Batch 329, Loss 23.72579314860892 Accuracy 1.0 Time 3.114s\n",
            " correct:  tensor(21120) Total:  21120\n",
            "Epoch 1, Batch 330, Loss 23.712519772847493 Accuracy 1.0 Time 3.061s\n",
            " correct:  tensor(21184) Total:  21184\n",
            "Epoch 1, Batch 331, Loss 23.69674124242316 Accuracy 1.0 Time 3.1s\n",
            " correct:  tensor(21248) Total:  21248\n",
            "Epoch 1, Batch 332, Loss 23.685695740113776 Accuracy 1.0 Time 3.19s\n",
            " correct:  tensor(21312) Total:  21312\n",
            "Epoch 1, Batch 333, Loss 23.66902127423444 Accuracy 1.0 Time 3.136s\n",
            " correct:  tensor(21376) Total:  21376\n",
            "Epoch 1, Batch 334, Loss 23.655165723697866 Accuracy 1.0 Time 3.063s\n",
            " correct:  tensor(21440) Total:  21440\n",
            "Epoch 1, Batch 335, Loss 23.64393299159719 Accuracy 1.0 Time 3.066s\n",
            " correct:  tensor(21504) Total:  21504\n",
            "Epoch 1, Batch 336, Loss 23.627679734002974 Accuracy 1.0 Time 3.35s\n",
            " correct:  tensor(21568) Total:  21568\n",
            "Epoch 1, Batch 337, Loss 23.615800382121733 Accuracy 1.0 Time 3.353s\n",
            " correct:  tensor(21632) Total:  21632\n",
            "Epoch 1, Batch 338, Loss 23.602998857667462 Accuracy 1.0 Time 3.359s\n",
            " correct:  tensor(21696) Total:  21696\n",
            "Epoch 1, Batch 339, Loss 23.589226736783278 Accuracy 1.0 Time 3.213s\n",
            " correct:  tensor(21760) Total:  21760\n",
            "Epoch 1, Batch 340, Loss 23.57644946154426 Accuracy 1.0 Time 3.144s\n",
            " correct:  tensor(21824) Total:  21824\n",
            "Epoch 1, Batch 341, Loss 23.55973260493572 Accuracy 1.0 Time 3.103s\n",
            " correct:  tensor(21888) Total:  21888\n",
            "Epoch 1, Batch 342, Loss 23.542783597756546 Accuracy 1.0 Time 3.095s\n",
            " correct:  tensor(21952) Total:  21952\n",
            "Epoch 1, Batch 343, Loss 23.53107520234133 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(22016) Total:  22016\n",
            "Epoch 1, Batch 344, Loss 23.522380712420443 Accuracy 1.0 Time 3.107s\n",
            " correct:  tensor(22080) Total:  22080\n",
            "Epoch 1, Batch 345, Loss 23.50926148234934 Accuracy 1.0 Time 3.178s\n",
            " correct:  tensor(22144) Total:  22144\n",
            "Epoch 1, Batch 346, Loss 23.494133469686343 Accuracy 1.0 Time 3.091s\n",
            " correct:  tensor(22208) Total:  22208\n",
            "Epoch 1, Batch 347, Loss 23.483297721796834 Accuracy 1.0 Time 3.084s\n",
            " correct:  tensor(22272) Total:  22272\n",
            "Epoch 1, Batch 348, Loss 23.471875042750916 Accuracy 1.0 Time 3.033s\n",
            " correct:  tensor(22336) Total:  22336\n",
            "Epoch 1, Batch 349, Loss 23.45632527414229 Accuracy 1.0 Time 3.076s\n",
            " correct:  tensor(22400) Total:  22400\n",
            "Epoch 1, Batch 350, Loss 23.44385009765625 Accuracy 1.0 Time 3.074s\n",
            " correct:  tensor(22464) Total:  22464\n",
            "Epoch 1, Batch 351, Loss 23.431179546562696 Accuracy 1.0 Time 3.113s\n",
            " correct:  tensor(22528) Total:  22528\n",
            "Epoch 1, Batch 352, Loss 23.419550245458428 Accuracy 1.0 Time 3.091s\n",
            " correct:  tensor(22592) Total:  22592\n",
            "Epoch 1, Batch 353, Loss 23.40630845780413 Accuracy 1.0 Time 3.124s\n",
            " correct:  tensor(22656) Total:  22656\n",
            "Epoch 1, Batch 354, Loss 23.394825111001225 Accuracy 1.0 Time 3.09s\n",
            " correct:  tensor(22720) Total:  22720\n",
            "Epoch 1, Batch 355, Loss 23.378775223208144 Accuracy 1.0 Time 3.18s\n",
            " correct:  tensor(22784) Total:  22784\n",
            "Epoch 1, Batch 356, Loss 23.364795749107103 Accuracy 1.0 Time 3.156s\n",
            " correct:  tensor(22848) Total:  22848\n",
            "Epoch 1, Batch 357, Loss 23.352557393349187 Accuracy 1.0 Time 3.096s\n",
            " correct:  tensor(22912) Total:  22912\n",
            "Epoch 1, Batch 358, Loss 23.338193760237882 Accuracy 1.0 Time 3.103s\n",
            " correct:  tensor(22976) Total:  22976\n",
            "Epoch 1, Batch 359, Loss 23.325054806255032 Accuracy 1.0 Time 3.108s\n",
            " correct:  tensor(23040) Total:  23040\n",
            "Epoch 1, Batch 360, Loss 23.315396886401707 Accuracy 1.0 Time 3.098s\n",
            " correct:  tensor(23104) Total:  23104\n",
            "Epoch 1, Batch 361, Loss 23.301121223005893 Accuracy 1.0 Time 3.1s\n",
            " correct:  tensor(23168) Total:  23168\n",
            "Epoch 1, Batch 362, Loss 23.286400958319394 Accuracy 1.0 Time 3.149s\n",
            " correct:  tensor(23232) Total:  23232\n",
            "Epoch 1, Batch 363, Loss 23.278259103948418 Accuracy 1.0 Time 3.117s\n",
            " correct:  tensor(23296) Total:  23296\n",
            "Epoch 1, Batch 364, Loss 23.264122009277344 Accuracy 1.0 Time 3.08s\n",
            " correct:  tensor(23360) Total:  23360\n",
            "Epoch 1, Batch 365, Loss 23.252746315525002 Accuracy 1.0 Time 3.133s\n",
            " correct:  tensor(23424) Total:  23424\n",
            "Epoch 1, Batch 366, Loss 23.240596604477513 Accuracy 1.0 Time 3.105s\n",
            " correct:  tensor(23488) Total:  23488\n",
            "Epoch 1, Batch 367, Loss 23.22623712815121 Accuracy 1.0 Time 3.073s\n",
            " correct:  tensor(23552) Total:  23552\n",
            "Epoch 1, Batch 368, Loss 23.210473495980967 Accuracy 1.0 Time 3.148s\n",
            " correct:  tensor(23616) Total:  23616\n",
            "Epoch 1, Batch 369, Loss 23.198269619205135 Accuracy 1.0 Time 3.194s\n",
            " correct:  tensor(23680) Total:  23680\n",
            "Epoch 1, Batch 370, Loss 23.185163884549528 Accuracy 1.0 Time 3.202s\n",
            " correct:  tensor(23744) Total:  23744\n",
            "Epoch 1, Batch 371, Loss 23.171707693135964 Accuracy 1.0 Time 3.147s\n",
            " correct:  tensor(23808) Total:  23808\n",
            "Epoch 1, Batch 372, Loss 23.159694738285516 Accuracy 1.0 Time 3.118s\n",
            " correct:  tensor(23872) Total:  23872\n",
            "Epoch 1, Batch 373, Loss 23.145888264633054 Accuracy 1.0 Time 3.096s\n",
            " correct:  tensor(23936) Total:  23936\n",
            "Epoch 1, Batch 374, Loss 23.130849914754776 Accuracy 1.0 Time 3.119s\n",
            " correct:  tensor(24000) Total:  24000\n",
            "Epoch 1, Batch 375, Loss 23.121189651489257 Accuracy 1.0 Time 3.193s\n",
            " correct:  tensor(24064) Total:  24064\n",
            "Epoch 1, Batch 376, Loss 23.108726765247102 Accuracy 1.0 Time 3.128s\n",
            " correct:  tensor(24128) Total:  24128\n",
            "Epoch 1, Batch 377, Loss 23.096065607247997 Accuracy 1.0 Time 3.079s\n",
            " correct:  tensor(24192) Total:  24192\n",
            "Epoch 1, Batch 378, Loss 23.083143274620095 Accuracy 1.0 Time 3.176s\n",
            " correct:  tensor(24256) Total:  24256\n",
            "Epoch 1, Batch 379, Loss 23.072461694400353 Accuracy 1.0 Time 3.213s\n",
            " correct:  tensor(24320) Total:  24320\n",
            "Epoch 1, Batch 380, Loss 23.057296115473697 Accuracy 1.0 Time 3.256s\n",
            " correct:  tensor(24384) Total:  24384\n",
            "Epoch 1, Batch 381, Loss 23.04616109592708 Accuracy 1.0 Time 3.286s\n",
            " correct:  tensor(24448) Total:  24448\n",
            "Epoch 1, Batch 382, Loss 23.035597861125208 Accuracy 1.0 Time 3.234s\n",
            " correct:  tensor(24512) Total:  24512\n",
            "Epoch 1, Batch 383, Loss 23.023950218220605 Accuracy 1.0 Time 3.146s\n",
            " correct:  tensor(24576) Total:  24576\n",
            "Epoch 1, Batch 384, Loss 23.013261392712593 Accuracy 1.0 Time 3.165s\n",
            " correct:  tensor(24640) Total:  24640\n",
            "Epoch 1, Batch 385, Loss 23.003265806916474 Accuracy 1.0 Time 3.122s\n",
            " correct:  tensor(24704) Total:  24704\n",
            "Epoch 1, Batch 386, Loss 22.990615029409142 Accuracy 1.0 Time 3.125s\n",
            " correct:  tensor(24768) Total:  24768\n",
            "Epoch 1, Batch 387, Loss 22.97915839656071 Accuracy 1.0 Time 3.116s\n",
            " correct:  tensor(24832) Total:  24832\n",
            "Epoch 1, Batch 388, Loss 22.973978961865928 Accuracy 1.0 Time 3.126s\n",
            " correct:  tensor(24896) Total:  24896\n",
            "Epoch 1, Batch 389, Loss 22.966021687328663 Accuracy 1.0 Time 3.128s\n",
            " correct:  tensor(24960) Total:  24960\n",
            "Epoch 1, Batch 390, Loss 22.95605319096492 Accuracy 1.0 Time 3.149s\n",
            " correct:  tensor(25024) Total:  25024\n",
            "Epoch 1, Batch 391, Loss 22.942871420584677 Accuracy 1.0 Time 3.174s\n",
            " correct:  tensor(25088) Total:  25088\n",
            "Epoch 1, Batch 392, Loss 22.936491791082886 Accuracy 1.0 Time 3.134s\n",
            " correct:  tensor(25152) Total:  25152\n",
            "Epoch 1, Batch 393, Loss 22.926590616163104 Accuracy 1.0 Time 3.183s\n",
            " correct:  tensor(25216) Total:  25216\n",
            "Epoch 1, Batch 394, Loss 22.915976485625137 Accuracy 1.0 Time 3.146s\n",
            " correct:  tensor(25280) Total:  25280\n",
            "Epoch 1, Batch 395, Loss 22.908167025409167 Accuracy 1.0 Time 3.271s\n",
            " correct:  tensor(25344) Total:  25344\n",
            "Epoch 1, Batch 396, Loss 22.895342436703768 Accuracy 1.0 Time 3.173s\n",
            " correct:  tensor(25408) Total:  25408\n",
            "Epoch 1, Batch 397, Loss 22.88438717724394 Accuracy 1.0 Time 3.127s\n",
            " correct:  tensor(25472) Total:  25472\n",
            "Epoch 1, Batch 398, Loss 22.875429910631038 Accuracy 1.0 Time 3.179s\n",
            " correct:  tensor(25536) Total:  25536\n",
            "Epoch 1, Batch 399, Loss 22.863260101853754 Accuracy 1.0 Time 3.136s\n",
            " correct:  tensor(25600) Total:  25600\n",
            "Epoch 1, Batch 400, Loss 22.8515242433548 Accuracy 1.0 Time 3.076s\n",
            " correct:  tensor(25664) Total:  25664\n",
            "Epoch 1, Batch 401, Loss 22.843033231701934 Accuracy 1.0 Time 3.115s\n",
            " correct:  tensor(25728) Total:  25728\n",
            "Epoch 1, Batch 402, Loss 22.83032989976418 Accuracy 1.0 Time 3.123s\n",
            " correct:  tensor(25792) Total:  25792\n",
            "Epoch 1, Batch 403, Loss 22.819240981828486 Accuracy 1.0 Time 3.125s\n",
            " correct:  tensor(25856) Total:  25856\n",
            "Epoch 1, Batch 404, Loss 22.8087797400975 Accuracy 1.0 Time 3.133s\n",
            " correct:  tensor(25920) Total:  25920\n",
            "Epoch 1, Batch 405, Loss 22.79576793246799 Accuracy 1.0 Time 3.168s\n",
            " correct:  tensor(25984) Total:  25984\n",
            "Epoch 1, Batch 406, Loss 22.786817080868875 Accuracy 1.0 Time 3.166s\n",
            " correct:  tensor(26048) Total:  26048\n",
            "Epoch 1, Batch 407, Loss 22.776363836752402 Accuracy 1.0 Time 3.125s\n",
            " correct:  tensor(26112) Total:  26112\n",
            "Epoch 1, Batch 408, Loss 22.766407312131395 Accuracy 1.0 Time 3.08s\n",
            " correct:  tensor(26176) Total:  26176\n",
            "Epoch 1, Batch 409, Loss 22.755884762789627 Accuracy 1.0 Time 3.13s\n",
            " correct:  tensor(26240) Total:  26240\n",
            "Epoch 1, Batch 410, Loss 22.744801377087104 Accuracy 1.0 Time 3.16s\n",
            " correct:  tensor(26304) Total:  26304\n",
            "Epoch 1, Batch 411, Loss 22.734125619967198 Accuracy 1.0 Time 3.123s\n",
            " correct:  tensor(26368) Total:  26368\n",
            "Epoch 1, Batch 412, Loss 22.72230968660521 Accuracy 1.0 Time 3.144s\n",
            " correct:  tensor(26432) Total:  26432\n",
            "Epoch 1, Batch 413, Loss 22.713259634613703 Accuracy 1.0 Time 3.112s\n",
            " correct:  tensor(26496) Total:  26496\n",
            "Epoch 1, Batch 414, Loss 22.70155019806203 Accuracy 1.0 Time 3.09s\n",
            " correct:  tensor(26560) Total:  26560\n",
            "Epoch 1, Batch 415, Loss 22.687906467483703 Accuracy 1.0 Time 3.109s\n",
            " correct:  tensor(26624) Total:  26624\n",
            "Epoch 1, Batch 416, Loss 22.674490864460285 Accuracy 1.0 Time 3.177s\n",
            " correct:  tensor(26688) Total:  26688\n",
            "Epoch 1, Batch 417, Loss 22.664530500233603 Accuracy 1.0 Time 3.164s\n",
            " correct:  tensor(26752) Total:  26752\n",
            "Epoch 1, Batch 418, Loss 22.655242472744444 Accuracy 1.0 Time 3.136s\n",
            " correct:  tensor(26816) Total:  26816\n",
            "Epoch 1, Batch 419, Loss 22.641757145701842 Accuracy 1.0 Time 3.131s\n",
            " correct:  tensor(26880) Total:  26880\n",
            "Epoch 1, Batch 420, Loss 22.63007320676531 Accuracy 1.0 Time 3.142s\n",
            " correct:  tensor(26944) Total:  26944\n",
            "Epoch 1, Batch 421, Loss 22.617042065799378 Accuracy 1.0 Time 3.143s\n",
            " correct:  tensor(27008) Total:  27008\n",
            "Epoch 1, Batch 422, Loss 22.608974953963294 Accuracy 1.0 Time 3.102s\n",
            " correct:  tensor(27072) Total:  27072\n",
            "Epoch 1, Batch 423, Loss 22.602841203658013 Accuracy 1.0 Time 3.073s\n",
            " correct:  tensor(27136) Total:  27136\n",
            "Epoch 1, Batch 424, Loss 22.593709140453697 Accuracy 1.0 Time 3.139s\n",
            " correct:  tensor(27200) Total:  27200\n",
            "Epoch 1, Batch 425, Loss 22.585409240722655 Accuracy 1.0 Time 3.099s\n",
            " correct:  tensor(27264) Total:  27264\n",
            "Epoch 1, Batch 426, Loss 22.573009482012107 Accuracy 1.0 Time 3.086s\n",
            " correct:  tensor(27328) Total:  27328\n",
            "Epoch 1, Batch 427, Loss 22.566250088622475 Accuracy 1.0 Time 3.042s\n",
            " correct:  tensor(27392) Total:  27392\n",
            "Epoch 1, Batch 428, Loss 22.555524545295217 Accuracy 1.0 Time 3.038s\n",
            " correct:  tensor(27456) Total:  27456\n",
            "Epoch 1, Batch 429, Loss 22.546102461559233 Accuracy 1.0 Time 3.106s\n",
            " correct:  tensor(27520) Total:  27520\n",
            "Epoch 1, Batch 430, Loss 22.53550266443297 Accuracy 1.0 Time 3.057s\n",
            " correct:  tensor(27584) Total:  27584\n",
            "Epoch 1, Batch 431, Loss 22.528052172915288 Accuracy 1.0 Time 3.137s\n",
            " correct:  tensor(27648) Total:  27648\n",
            "Epoch 1, Batch 432, Loss 22.517288605372112 Accuracy 1.0 Time 3.09s\n",
            " correct:  tensor(27712) Total:  27712\n",
            "Epoch 1, Batch 433, Loss 22.508014630500494 Accuracy 1.0 Time 3.096s\n",
            " correct:  tensor(27776) Total:  27776\n",
            "Epoch 1, Batch 434, Loss 22.50106051220872 Accuracy 1.0 Time 3.079s\n",
            " correct:  tensor(27840) Total:  27840\n",
            "Epoch 1, Batch 435, Loss 22.493382837580537 Accuracy 1.0 Time 3.313s\n",
            " correct:  tensor(27904) Total:  27904\n",
            "Epoch 1, Batch 436, Loss 22.48268263493109 Accuracy 1.0 Time 3.342s\n",
            " correct:  tensor(27968) Total:  27968\n",
            "Epoch 1, Batch 437, Loss 22.47472216171858 Accuracy 1.0 Time 3.349s\n",
            " correct:  tensor(28032) Total:  28032\n",
            "Epoch 1, Batch 438, Loss 22.463823954264324 Accuracy 1.0 Time 3.147s\n",
            " correct:  tensor(28096) Total:  28096\n",
            "Epoch 1, Batch 439, Loss 22.455552381372126 Accuracy 1.0 Time 3.086s\n",
            " correct:  tensor(28160) Total:  28160\n",
            "Epoch 1, Batch 440, Loss 22.442595195770263 Accuracy 1.0 Time 3.084s\n",
            " correct:  tensor(28224) Total:  28224\n",
            "Epoch 1, Batch 441, Loss 22.43383565024724 Accuracy 1.0 Time 3.126s\n",
            " correct:  tensor(28288) Total:  28288\n",
            "Epoch 1, Batch 442, Loss 22.42538752491118 Accuracy 1.0 Time 3.1s\n",
            " correct:  tensor(28352) Total:  28352\n",
            "Epoch 1, Batch 443, Loss 22.418277197982064 Accuracy 1.0 Time 3.133s\n",
            " correct:  tensor(28416) Total:  28416\n",
            "Epoch 1, Batch 444, Loss 22.407627561070896 Accuracy 1.0 Time 3.034s\n",
            " correct:  tensor(28480) Total:  28480\n",
            "Epoch 1, Batch 445, Loss 22.398702904347623 Accuracy 1.0 Time 3.085s\n",
            " correct:  tensor(28544) Total:  28544\n",
            "Epoch 1, Batch 446, Loss 22.38977395472505 Accuracy 1.0 Time 3.061s\n",
            " correct:  tensor(28608) Total:  28608\n",
            "Epoch 1, Batch 447, Loss 22.3802485113976 Accuracy 1.0 Time 3.101s\n",
            " correct:  tensor(28672) Total:  28672\n",
            "Epoch 1, Batch 448, Loss 22.36906409263611 Accuracy 1.0 Time 3.108s\n",
            " correct:  tensor(28736) Total:  28736\n",
            "Epoch 1, Batch 449, Loss 22.36275240042162 Accuracy 1.0 Time 3.056s\n",
            " correct:  tensor(28800) Total:  28800\n",
            "Epoch 1, Batch 450, Loss 22.353089849683972 Accuracy 1.0 Time 3.111s\n",
            " correct:  tensor(28864) Total:  28864\n",
            "Epoch 1, Batch 451, Loss 22.343748130713756 Accuracy 1.0 Time 3.151s\n",
            " correct:  tensor(28928) Total:  28928\n",
            "Epoch 1, Batch 452, Loss 22.334044321448403 Accuracy 1.0 Time 3.112s\n",
            " correct:  tensor(28992) Total:  28992\n",
            "Epoch 1, Batch 453, Loss 22.324639415109395 Accuracy 1.0 Time 3.096s\n",
            " correct:  tensor(29056) Total:  29056\n",
            "Epoch 1, Batch 454, Loss 22.31872167461244 Accuracy 1.0 Time 3.066s\n",
            " correct:  tensor(29120) Total:  29120\n",
            "Epoch 1, Batch 455, Loss 22.312328657213147 Accuracy 1.0 Time 3.111s\n",
            " correct:  tensor(29184) Total:  29184\n",
            "Epoch 1, Batch 456, Loss 22.301549468124122 Accuracy 1.0 Time 3.06s\n",
            " correct:  tensor(29248) Total:  29248\n",
            "Epoch 1, Batch 457, Loss 22.293994077185832 Accuracy 1.0 Time 3.002s\n",
            " correct:  tensor(29312) Total:  29312\n",
            "Epoch 1, Batch 458, Loss 22.286540185520224 Accuracy 1.0 Time 3.078s\n",
            " correct:  tensor(29376) Total:  29376\n",
            "Epoch 1, Batch 459, Loss 22.279800498147935 Accuracy 1.0 Time 3.016s\n",
            " correct:  tensor(29440) Total:  29440\n",
            "Epoch 1, Batch 460, Loss 22.2693284864011 Accuracy 1.0 Time 3.12s\n",
            " correct:  tensor(29504) Total:  29504\n",
            "Epoch 1, Batch 461, Loss 22.259268084213687 Accuracy 1.0 Time 3.057s\n",
            " correct:  tensor(29568) Total:  29568\n",
            "Epoch 1, Batch 462, Loss 22.254283062823408 Accuracy 1.0 Time 3.071s\n",
            " correct:  tensor(29632) Total:  29632\n",
            "Epoch 1, Batch 463, Loss 22.24559714315262 Accuracy 1.0 Time 3.061s\n",
            " correct:  tensor(29696) Total:  29696\n",
            "Epoch 1, Batch 464, Loss 22.236302318244146 Accuracy 1.0 Time 3.132s\n",
            " correct:  tensor(29760) Total:  29760\n",
            "Epoch 1, Batch 465, Loss 22.228729305472424 Accuracy 1.0 Time 3.087s\n",
            " correct:  tensor(29824) Total:  29824\n",
            "Epoch 1, Batch 466, Loss 22.2212157310846 Accuracy 1.0 Time 3.096s\n",
            " correct:  tensor(29888) Total:  29888\n",
            "Epoch 1, Batch 467, Loss 22.214446429266918 Accuracy 1.0 Time 3.067s\n",
            " correct:  tensor(29952) Total:  29952\n",
            "Epoch 1, Batch 468, Loss 22.20774179849869 Accuracy 1.0 Time 3.067s\n",
            " correct:  tensor(30016) Total:  30016\n",
            "Epoch 1, Batch 469, Loss 22.19822549311591 Accuracy 1.0 Time 3.049s\n",
            " correct:  tensor(30080) Total:  30080\n",
            "Epoch 1, Batch 470, Loss 22.18831183656733 Accuracy 1.0 Time 3.138s\n",
            " correct:  tensor(30144) Total:  30144\n",
            "Epoch 1, Batch 471, Loss 22.178057093529183 Accuracy 1.0 Time 3.16s\n",
            " correct:  tensor(30208) Total:  30208\n",
            "Epoch 1, Batch 472, Loss 22.172194953692163 Accuracy 1.0 Time 3.079s\n",
            " correct:  tensor(30272) Total:  30272\n",
            "Epoch 1, Batch 473, Loss 22.161817498237337 Accuracy 1.0 Time 3.054s\n",
            " correct:  tensor(30336) Total:  30336\n",
            "Epoch 1, Batch 474, Loss 22.15177140859612 Accuracy 1.0 Time 3.058s\n",
            " correct:  tensor(30400) Total:  30400\n",
            "Epoch 1, Batch 475, Loss 22.143916429218493 Accuracy 1.0 Time 3.114s\n",
            " correct:  tensor(30464) Total:  30464\n",
            "Epoch 1, Batch 476, Loss 22.13584238741578 Accuracy 1.0 Time 3.077s\n",
            " correct:  tensor(30528) Total:  30528\n",
            "Epoch 1, Batch 477, Loss 22.12798687596991 Accuracy 1.0 Time 3.094s\n",
            " correct:  tensor(30592) Total:  30592\n",
            "Epoch 1, Batch 478, Loss 22.119651072194884 Accuracy 1.0 Time 3.318s\n",
            " correct:  tensor(30656) Total:  30656\n",
            "Epoch 1, Batch 479, Loss 22.114494240109757 Accuracy 1.0 Time 3.223s\n",
            " correct:  tensor(30720) Total:  30720\n",
            "Epoch 1, Batch 480, Loss 22.10766600370407 Accuracy 1.0 Time 3.053s\n",
            " correct:  tensor(30784) Total:  30784\n",
            "Epoch 1, Batch 481, Loss 22.0984563966303 Accuracy 1.0 Time 3.047s\n",
            " correct:  tensor(30848) Total:  30848\n",
            "Epoch 1, Batch 482, Loss 22.088227216633523 Accuracy 1.0 Time 3.061s\n",
            " correct:  tensor(30912) Total:  30912\n",
            "Epoch 1, Batch 483, Loss 22.082053182534796 Accuracy 1.0 Time 3.104s\n",
            " correct:  tensor(30976) Total:  30976\n",
            "Epoch 1, Batch 484, Loss 22.07236256087122 Accuracy 1.0 Time 3.087s\n",
            " correct:  tensor(31040) Total:  31040\n",
            "Epoch 1, Batch 485, Loss 22.06354683158324 Accuracy 1.0 Time 3.186s\n",
            " correct:  tensor(31104) Total:  31104\n",
            "Epoch 1, Batch 486, Loss 22.056553436404883 Accuracy 1.0 Time 3.19s\n",
            " correct:  tensor(31168) Total:  31168\n",
            "Epoch 1, Batch 487, Loss 22.048167058329806 Accuracy 1.0 Time 3.105s\n",
            " correct:  tensor(31232) Total:  31232\n",
            "Epoch 1, Batch 488, Loss 22.037836821352847 Accuracy 1.0 Time 3.062s\n",
            " correct:  tensor(31296) Total:  31296\n",
            "Epoch 1, Batch 489, Loss 22.03139522373067 Accuracy 1.0 Time 3.102s\n",
            " correct:  tensor(31360) Total:  31360\n",
            "Epoch 1, Batch 490, Loss 22.021898417570153 Accuracy 1.0 Time 3.05s\n",
            " correct:  tensor(31424) Total:  31424\n",
            "Epoch 1, Batch 491, Loss 22.01349905212154 Accuracy 1.0 Time 3.037s\n",
            " correct:  tensor(31488) Total:  31488\n",
            "Epoch 1, Batch 492, Loss 22.006022495952077 Accuracy 1.0 Time 3.063s\n",
            " correct:  tensor(31552) Total:  31552\n",
            "Epoch 1, Batch 493, Loss 21.995424247416715 Accuracy 1.0 Time 3.08s\n",
            " correct:  tensor(31616) Total:  31616\n",
            "Epoch 1, Batch 494, Loss 21.985844195130383 Accuracy 1.0 Time 3.093s\n",
            " correct:  tensor(31680) Total:  31680\n",
            "Epoch 1, Batch 495, Loss 21.977173410280788 Accuracy 1.0 Time 3.049s\n",
            " correct:  tensor(31744) Total:  31744\n",
            "Epoch 1, Batch 496, Loss 21.970412062060447 Accuracy 1.0 Time 3.105s\n",
            " correct:  tensor(31808) Total:  31808\n",
            "Epoch 1, Batch 497, Loss 21.961062955184723 Accuracy 1.0 Time 3.086s\n",
            " correct:  tensor(31872) Total:  31872\n",
            "Epoch 1, Batch 498, Loss 21.958848095323187 Accuracy 1.0 Time 3.113s\n",
            " correct:  tensor(31936) Total:  31936\n",
            "Epoch 1, Batch 499, Loss 21.948580480051902 Accuracy 1.0 Time 3.137s\n",
            " correct:  tensor(32000) Total:  32000\n",
            "Epoch 1, Batch 500, Loss 21.942123989105223 Accuracy 1.0 Time 3.075s\n",
            " correct:  tensor(32064) Total:  32064\n",
            "Epoch 1, Batch 501, Loss 21.932769059659 Accuracy 1.0 Time 3.1s\n",
            " correct:  tensor(32128) Total:  32128\n",
            "Epoch 1, Batch 502, Loss 21.925762009335703 Accuracy 1.0 Time 3.116s\n",
            " correct:  tensor(32192) Total:  32192\n",
            "Epoch 1, Batch 503, Loss 21.916233904556062 Accuracy 1.0 Time 3.049s\n",
            " correct:  tensor(32256) Total:  32256\n",
            "Epoch 1, Batch 504, Loss 21.910035636689926 Accuracy 1.0 Time 3.019s\n",
            " correct:  tensor(32320) Total:  32320\n",
            "Epoch 1, Batch 505, Loss 21.900700295325553 Accuracy 1.0 Time 3.09s\n",
            " correct:  tensor(32384) Total:  32384\n",
            "Epoch 1, Batch 506, Loss 21.893696977215793 Accuracy 1.0 Time 3.057s\n",
            " correct:  tensor(32448) Total:  32448\n",
            "Epoch 1, Batch 507, Loss 21.8851016343698 Accuracy 1.0 Time 3.149s\n",
            " correct:  tensor(32512) Total:  32512\n",
            "Epoch 1, Batch 508, Loss 21.876920624980777 Accuracy 1.0 Time 3.015s\n",
            " correct:  tensor(32576) Total:  32576\n",
            "Epoch 1, Batch 509, Loss 21.870390586628194 Accuracy 1.0 Time 3.1s\n",
            " correct:  tensor(32640) Total:  32640\n",
            "Epoch 1, Batch 510, Loss 21.860180222754384 Accuracy 1.0 Time 3.048s\n",
            " correct:  tensor(32704) Total:  32704\n",
            "Epoch 1, Batch 511, Loss 21.851808749532978 Accuracy 1.0 Time 3.025s\n",
            " correct:  tensor(32768) Total:  32768\n",
            "Epoch 1, Batch 512, Loss 21.84541718289256 Accuracy 1.0 Time 3.076s\n",
            " correct:  tensor(32832) Total:  32832\n",
            "Epoch 1, Batch 513, Loss 21.837778816446225 Accuracy 1.0 Time 3.096s\n",
            " correct:  tensor(32896) Total:  32896\n",
            "Epoch 1, Batch 514, Loss 21.82783691242975 Accuracy 1.0 Time 3.065s\n",
            " correct:  tensor(32960) Total:  32960\n",
            "Epoch 1, Batch 515, Loss 21.81963197652576 Accuracy 1.0 Time 2.998s\n",
            " correct:  tensor(33024) Total:  33024\n",
            "Epoch 1, Batch 516, Loss 21.81054213989613 Accuracy 1.0 Time 3.073s\n",
            " correct:  tensor(33088) Total:  33088\n",
            "Epoch 1, Batch 517, Loss 21.800593499749958 Accuracy 1.0 Time 3.039s\n",
            " correct:  tensor(33152) Total:  33152\n",
            "Epoch 1, Batch 518, Loss 21.792449487222207 Accuracy 1.0 Time 3.071s\n",
            " correct:  tensor(33216) Total:  33216\n",
            "Epoch 1, Batch 519, Loss 21.785058769417176 Accuracy 1.0 Time 3.108s\n",
            " correct:  tensor(33280) Total:  33280\n",
            "Epoch 1, Batch 520, Loss 21.77734495309683 Accuracy 1.0 Time 3.043s\n",
            " correct:  tensor(33344) Total:  33344\n",
            "Epoch 1, Batch 521, Loss 21.772244367398137 Accuracy 1.0 Time 3.04s\n",
            " correct:  tensor(33408) Total:  33408\n",
            "Epoch 1, Batch 522, Loss 21.767022300954068 Accuracy 1.0 Time 3.065s\n",
            " correct:  tensor(33472) Total:  33472\n",
            "Epoch 1, Batch 523, Loss 21.757483932757697 Accuracy 1.0 Time 3.084s\n",
            " correct:  tensor(33536) Total:  33536\n",
            "Epoch 1, Batch 524, Loss 21.751001489071445 Accuracy 1.0 Time 3.11s\n",
            " correct:  tensor(33600) Total:  33600\n",
            "Epoch 1, Batch 525, Loss 21.742918882824124 Accuracy 1.0 Time 3.004s\n",
            " correct:  tensor(33664) Total:  33664\n",
            "Epoch 1, Batch 526, Loss 21.73552488102206 Accuracy 1.0 Time 3.018s\n",
            " correct:  tensor(33728) Total:  33728\n",
            "Epoch 1, Batch 527, Loss 21.730556770339184 Accuracy 1.0 Time 3.02s\n",
            " correct:  tensor(33792) Total:  33792\n",
            "Epoch 1, Batch 528, Loss 21.722147569511876 Accuracy 1.0 Time 3.055s\n",
            " correct:  tensor(33856) Total:  33856\n",
            "Epoch 1, Batch 529, Loss 21.714412433213234 Accuracy 1.0 Time 3.072s\n",
            " correct:  tensor(33920) Total:  33920\n",
            "Epoch 1, Batch 530, Loss 21.70865642259706 Accuracy 1.0 Time 3.031s\n",
            " correct:  tensor(33984) Total:  33984\n",
            "Epoch 1, Batch 531, Loss 21.701723120306845 Accuracy 1.0 Time 3.045s\n",
            " correct:  tensor(34048) Total:  34048\n",
            "Epoch 1, Batch 532, Loss 21.697462576672546 Accuracy 1.0 Time 2.992s\n",
            " correct:  tensor(34112) Total:  34112\n",
            "Epoch 1, Batch 533, Loss 21.690735847372945 Accuracy 1.0 Time 3.058s\n",
            " correct:  tensor(34176) Total:  34176\n",
            "Epoch 1, Batch 534, Loss 21.685289661536057 Accuracy 1.0 Time 3.046s\n",
            " correct:  tensor(34240) Total:  34240\n",
            "Epoch 1, Batch 535, Loss 21.68073959350586 Accuracy 1.0 Time 3.275s\n",
            " correct:  tensor(34304) Total:  34304\n",
            "Epoch 1, Batch 536, Loss 21.671504195056745 Accuracy 1.0 Time 3.352s\n",
            " correct:  tensor(34368) Total:  34368\n",
            "Epoch 1, Batch 537, Loss 21.663139893799965 Accuracy 1.0 Time 3.329s\n",
            " correct:  tensor(34432) Total:  34432\n",
            "Epoch 1, Batch 538, Loss 21.653571377013254 Accuracy 1.0 Time 3.177s\n",
            " correct:  tensor(34496) Total:  34496\n",
            "Epoch 1, Batch 539, Loss 21.646436829292707 Accuracy 1.0 Time 3.045s\n",
            " correct:  tensor(34560) Total:  34560\n",
            "Epoch 1, Batch 540, Loss 21.637907755816425 Accuracy 1.0 Time 3.084s\n",
            " correct:  tensor(34624) Total:  34624\n",
            "Epoch 1, Batch 541, Loss 21.630308232333878 Accuracy 1.0 Time 3.053s\n",
            " correct:  tensor(34688) Total:  34688\n",
            "Epoch 1, Batch 542, Loss 21.621844893451986 Accuracy 1.0 Time 3.045s\n",
            " correct:  tensor(34752) Total:  34752\n",
            "Epoch 1, Batch 543, Loss 21.617552906549218 Accuracy 1.0 Time 3.162s\n",
            " correct:  tensor(34816) Total:  34816\n",
            "Epoch 1, Batch 544, Loss 21.61030732533511 Accuracy 1.0 Time 3.028s\n",
            " correct:  tensor(34880) Total:  34880\n",
            "Epoch 1, Batch 545, Loss 21.605981634297503 Accuracy 1.0 Time 3.119s\n",
            " correct:  tensor(34944) Total:  34944\n",
            "Epoch 1, Batch 546, Loss 21.600128718784877 Accuracy 1.0 Time 3.084s\n",
            " correct:  tensor(35008) Total:  35008\n",
            "Epoch 1, Batch 547, Loss 21.592770776975307 Accuracy 1.0 Time 3.088s\n",
            " correct:  tensor(35072) Total:  35072\n",
            "Epoch 1, Batch 548, Loss 21.5861506949376 Accuracy 1.0 Time 3.083s\n",
            " correct:  tensor(35136) Total:  35136\n",
            "Epoch 1, Batch 549, Loss 21.578161826767772 Accuracy 1.0 Time 3.063s\n",
            " correct:  tensor(35200) Total:  35200\n",
            "Epoch 1, Batch 550, Loss 21.5701833412864 Accuracy 1.0 Time 3.081s\n",
            " correct:  tensor(35264) Total:  35264\n",
            "Epoch 1, Batch 551, Loss 21.561786021597804 Accuracy 1.0 Time 3.076s\n",
            " correct:  tensor(35328) Total:  35328\n",
            "Epoch 1, Batch 552, Loss 21.5545531735904 Accuracy 1.0 Time 3.032s\n",
            " correct:  tensor(35392) Total:  35392\n",
            "Epoch 1, Batch 553, Loss 21.548162515512644 Accuracy 1.0 Time 3.143s\n",
            " correct:  tensor(35456) Total:  35456\n",
            "Epoch 1, Batch 554, Loss 21.539650882624546 Accuracy 1.0 Time 3.08s\n",
            " correct:  tensor(35520) Total:  35520\n",
            "Epoch 1, Batch 555, Loss 21.531323730193815 Accuracy 1.0 Time 3.075s\n",
            " correct:  tensor(35584) Total:  35584\n",
            "Epoch 1, Batch 556, Loss 21.52750358307104 Accuracy 1.0 Time 3.031s\n",
            " correct:  tensor(35648) Total:  35648\n",
            "Epoch 1, Batch 557, Loss 21.52150514010253 Accuracy 1.0 Time 3.076s\n",
            " correct:  tensor(35712) Total:  35712\n",
            "Epoch 1, Batch 558, Loss 21.514557363311877 Accuracy 1.0 Time 3.1s\n",
            " correct:  tensor(35776) Total:  35776\n",
            "Epoch 1, Batch 559, Loss 21.507431760456992 Accuracy 1.0 Time 3.093s\n",
            " correct:  tensor(35840) Total:  35840\n",
            "Epoch 1, Batch 560, Loss 21.501273768288748 Accuracy 1.0 Time 3.069s\n",
            " correct:  tensor(35904) Total:  35904\n",
            "Epoch 1, Batch 561, Loss 21.49334212590454 Accuracy 1.0 Time 3.028s\n",
            " correct:  tensor(35968) Total:  35968\n",
            "Epoch 1, Batch 562, Loss 21.48605281856985 Accuracy 1.0 Time 3.111s\n",
            " correct:  tensor(36032) Total:  36032\n",
            "Epoch 1, Batch 563, Loss 21.47768821513039 Accuracy 1.0 Time 3.041s\n",
            " correct:  tensor(36096) Total:  36096\n",
            "Epoch 1, Batch 564, Loss 21.470677987903567 Accuracy 1.0 Time 3.067s\n",
            " correct:  tensor(36160) Total:  36160\n",
            "Epoch 1, Batch 565, Loss 21.464916830147263 Accuracy 1.0 Time 3.073s\n",
            " correct:  tensor(36224) Total:  36224\n",
            "Epoch 1, Batch 566, Loss 21.45676774270964 Accuracy 1.0 Time 3.08s\n",
            " correct:  tensor(36288) Total:  36288\n",
            "Epoch 1, Batch 567, Loss 21.45234099897758 Accuracy 1.0 Time 3.125s\n",
            " correct:  tensor(36352) Total:  36352\n",
            "Epoch 1, Batch 568, Loss 21.44352293350327 Accuracy 1.0 Time 3.207s\n",
            " correct:  tensor(36416) Total:  36416\n",
            "Epoch 1, Batch 569, Loss 21.43614081464878 Accuracy 1.0 Time 3.061s\n",
            " correct:  tensor(36480) Total:  36480\n",
            "Epoch 1, Batch 570, Loss 21.429538044176603 Accuracy 1.0 Time 3.043s\n",
            " correct:  tensor(36544) Total:  36544\n",
            "Epoch 1, Batch 571, Loss 21.423365751623898 Accuracy 1.0 Time 3.036s\n",
            " correct:  tensor(36608) Total:  36608\n",
            "Epoch 1, Batch 572, Loss 21.41651270606301 Accuracy 1.0 Time 3.101s\n",
            " correct:  tensor(36672) Total:  36672\n",
            "Epoch 1, Batch 573, Loss 21.408057500762673 Accuracy 1.0 Time 3.175s\n",
            " correct:  tensor(36736) Total:  36736\n",
            "Epoch 1, Batch 574, Loss 21.402125099394794 Accuracy 1.0 Time 3.136s\n",
            " correct:  tensor(36800) Total:  36800\n",
            "Epoch 1, Batch 575, Loss 21.393569594673487 Accuracy 1.0 Time 3.091s\n",
            " correct:  tensor(36864) Total:  36864\n",
            "Epoch 1, Batch 576, Loss 21.386429948939217 Accuracy 1.0 Time 3.175s\n",
            " correct:  tensor(36928) Total:  36928\n",
            "Epoch 1, Batch 577, Loss 21.37900986828465 Accuracy 1.0 Time 3.333s\n",
            " correct:  tensor(36992) Total:  36992\n",
            "Epoch 1, Batch 578, Loss 21.37062136772182 Accuracy 1.0 Time 3.238s\n",
            " correct:  tensor(37056) Total:  37056\n",
            "Epoch 1, Batch 579, Loss 21.36492645184611 Accuracy 1.0 Time 3.11s\n",
            " correct:  tensor(37120) Total:  37120\n",
            "Epoch 1, Batch 580, Loss 21.359864096805968 Accuracy 1.0 Time 3.007s\n",
            " correct:  tensor(37184) Total:  37184\n",
            "Epoch 1, Batch 581, Loss 21.355402042040932 Accuracy 1.0 Time 3.135s\n",
            " correct:  tensor(37248) Total:  37248\n",
            "Epoch 1, Batch 582, Loss 21.34851950222684 Accuracy 1.0 Time 3.168s\n",
            " correct:  tensor(37312) Total:  37312\n",
            "Epoch 1, Batch 583, Loss 21.340843305375113 Accuracy 1.0 Time 3.083s\n",
            " correct:  tensor(37376) Total:  37376\n",
            "Epoch 1, Batch 584, Loss 21.334052337359076 Accuracy 1.0 Time 3.084s\n",
            " correct:  tensor(37440) Total:  37440\n",
            "Epoch 1, Batch 585, Loss 21.329993298522428 Accuracy 1.0 Time 3.114s\n",
            " correct:  tensor(37504) Total:  37504\n",
            "Epoch 1, Batch 586, Loss 21.32330892191812 Accuracy 1.0 Time 3.129s\n",
            " correct:  tensor(37568) Total:  37568\n",
            "Epoch 1, Batch 587, Loss 21.3170875308867 Accuracy 1.0 Time 3.129s\n",
            " correct:  tensor(37632) Total:  37632\n",
            "Epoch 1, Batch 588, Loss 21.311614811825915 Accuracy 1.0 Time 3.103s\n",
            " correct:  tensor(37696) Total:  37696\n",
            "Epoch 1, Batch 589, Loss 21.305278651783137 Accuracy 1.0 Time 3.099s\n",
            " correct:  tensor(37760) Total:  37760\n",
            "Epoch 1, Batch 590, Loss 21.29776373394465 Accuracy 1.0 Time 3.126s\n",
            " correct:  tensor(37824) Total:  37824\n",
            "Epoch 1, Batch 591, Loss 21.29146195426205 Accuracy 1.0 Time 3.116s\n",
            " correct:  tensor(37888) Total:  37888\n",
            "Epoch 1, Batch 592, Loss 21.28365258590595 Accuracy 1.0 Time 3.166s\n",
            " correct:  tensor(37952) Total:  37952\n",
            "Epoch 1, Batch 593, Loss 21.278390668897966 Accuracy 1.0 Time 3.2s\n",
            " correct:  tensor(38016) Total:  38016\n",
            "Epoch 1, Batch 594, Loss 21.27085194122109 Accuracy 1.0 Time 3.13s\n",
            " correct:  tensor(38080) Total:  38080\n",
            "Epoch 1, Batch 595, Loss 21.266632471164737 Accuracy 1.0 Time 3.134s\n",
            " correct:  tensor(38144) Total:  38144\n",
            "Epoch 1, Batch 596, Loss 21.26203581790796 Accuracy 1.0 Time 3.164s\n",
            " correct:  tensor(38208) Total:  38208\n",
            "Epoch 1, Batch 597, Loss 21.25566587975277 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(38272) Total:  38272\n",
            "Epoch 1, Batch 598, Loss 21.250210835383488 Accuracy 1.0 Time 3.104s\n",
            " correct:  tensor(38336) Total:  38336\n",
            "Epoch 1, Batch 599, Loss 21.246798653833455 Accuracy 1.0 Time 3.119s\n",
            " correct:  tensor(38400) Total:  38400\n",
            "Epoch 1, Batch 600, Loss 21.24241708755493 Accuracy 1.0 Time 3.178s\n",
            " correct:  tensor(38464) Total:  38464\n",
            "Epoch 1, Batch 601, Loss 21.236154632441416 Accuracy 1.0 Time 3.181s\n",
            " correct:  tensor(38528) Total:  38528\n",
            "Epoch 1, Batch 602, Loss 21.232339136624258 Accuracy 1.0 Time 3.17s\n",
            " correct:  tensor(38592) Total:  38592\n",
            "Epoch 1, Batch 603, Loss 21.227596567637885 Accuracy 1.0 Time 3.178s\n",
            " correct:  tensor(38656) Total:  38656\n",
            "Epoch 1, Batch 604, Loss 21.22125800240119 Accuracy 1.0 Time 3.131s\n",
            " correct:  tensor(38720) Total:  38720\n",
            "Epoch 1, Batch 605, Loss 21.215298143497183 Accuracy 1.0 Time 3.195s\n",
            " correct:  tensor(38784) Total:  38784\n",
            "Epoch 1, Batch 606, Loss 21.208522903643818 Accuracy 1.0 Time 3.16s\n",
            " correct:  tensor(38848) Total:  38848\n",
            "Epoch 1, Batch 607, Loss 21.201181524864335 Accuracy 1.0 Time 3.158s\n",
            " correct:  tensor(38912) Total:  38912\n",
            "Epoch 1, Batch 608, Loss 21.194352325640228 Accuracy 1.0 Time 3.145s\n",
            " correct:  tensor(38976) Total:  38976\n",
            "Epoch 1, Batch 609, Loss 21.19001416737223 Accuracy 1.0 Time 3.085s\n",
            " correct:  tensor(39040) Total:  39040\n",
            "Epoch 1, Batch 610, Loss 21.18505169602691 Accuracy 1.0 Time 3.159s\n",
            " correct:  tensor(39104) Total:  39104\n",
            "Epoch 1, Batch 611, Loss 21.178220068374358 Accuracy 1.0 Time 3.156s\n",
            " correct:  tensor(39168) Total:  39168\n",
            "Epoch 1, Batch 612, Loss 21.174544960844752 Accuracy 1.0 Time 3.164s\n",
            " correct:  tensor(39232) Total:  39232\n",
            "Epoch 1, Batch 613, Loss 21.17200899046358 Accuracy 1.0 Time 3.158s\n",
            " correct:  tensor(39296) Total:  39296\n",
            "Epoch 1, Batch 614, Loss 21.166193757072723 Accuracy 1.0 Time 3.17s\n",
            " correct:  tensor(39360) Total:  39360\n",
            "Epoch 1, Batch 615, Loss 21.158765439289372 Accuracy 1.0 Time 3.158s\n",
            " correct:  tensor(39424) Total:  39424\n",
            "Epoch 1, Batch 616, Loss 21.153621781956065 Accuracy 1.0 Time 3.092s\n",
            " correct:  tensor(39488) Total:  39488\n",
            "Epoch 1, Batch 617, Loss 21.14878949083424 Accuracy 1.0 Time 3.083s\n",
            " correct:  tensor(39552) Total:  39552\n",
            "Epoch 1, Batch 618, Loss 21.142164816748362 Accuracy 1.0 Time 3.128s\n",
            " correct:  tensor(39616) Total:  39616\n",
            "Epoch 1, Batch 619, Loss 21.13675043232414 Accuracy 1.0 Time 3.114s\n",
            " correct:  tensor(39680) Total:  39680\n",
            "Epoch 1, Batch 620, Loss 21.13190179024973 Accuracy 1.0 Time 3.105s\n",
            " correct:  tensor(39744) Total:  39744\n",
            "Epoch 1, Batch 621, Loss 21.126791316721942 Accuracy 1.0 Time 3.237s\n",
            " correct:  tensor(39808) Total:  39808\n",
            "Epoch 1, Batch 622, Loss 21.121058375122463 Accuracy 1.0 Time 3.101s\n",
            " correct:  tensor(39872) Total:  39872\n",
            "Epoch 1, Batch 623, Loss 21.115001001863188 Accuracy 1.0 Time 3.057s\n",
            " correct:  tensor(39936) Total:  39936\n",
            "Epoch 1, Batch 624, Loss 21.10932325705504 Accuracy 1.0 Time 3.127s\n",
            " correct:  tensor(40000) Total:  40000\n",
            "Epoch 1, Batch 625, Loss 21.10476719970703 Accuracy 1.0 Time 3.085s\n",
            " correct:  tensor(40064) Total:  40064\n",
            "Epoch 1, Batch 626, Loss 21.098170972098938 Accuracy 1.0 Time 3.12s\n",
            " correct:  tensor(40128) Total:  40128\n",
            "Epoch 1, Batch 627, Loss 21.092972151590498 Accuracy 1.0 Time 3.13s\n",
            " correct:  tensor(40192) Total:  40192\n",
            "Epoch 1, Batch 628, Loss 21.085959805045157 Accuracy 1.0 Time 3.14s\n",
            " correct:  tensor(40256) Total:  40256\n",
            "Epoch 1, Batch 629, Loss 21.082451118384334 Accuracy 1.0 Time 3.114s\n",
            " correct:  tensor(40320) Total:  40320\n",
            "Epoch 1, Batch 630, Loss 21.0774013912867 Accuracy 1.0 Time 3.119s\n",
            " correct:  tensor(40384) Total:  40384\n",
            "Epoch 1, Batch 631, Loss 21.073833628804106 Accuracy 1.0 Time 3.152s\n",
            " correct:  tensor(40448) Total:  40448\n",
            "Epoch 1, Batch 632, Loss 21.068325890770442 Accuracy 1.0 Time 3.093s\n",
            " correct:  tensor(40512) Total:  40512\n",
            "Epoch 1, Batch 633, Loss 21.062206036294995 Accuracy 1.0 Time 3.127s\n",
            " correct:  tensor(40576) Total:  40576\n",
            "Epoch 1, Batch 634, Loss 21.058050658048515 Accuracy 1.0 Time 3.234s\n",
            " correct:  tensor(40640) Total:  40640\n",
            "Epoch 1, Batch 635, Loss 21.05292801744356 Accuracy 1.0 Time 3.415s\n",
            " correct:  tensor(40704) Total:  40704\n",
            "Epoch 1, Batch 636, Loss 21.04703047290538 Accuracy 1.0 Time 3.39s\n",
            " correct:  tensor(40768) Total:  40768\n",
            "Epoch 1, Batch 637, Loss 21.042284451998196 Accuracy 1.0 Time 3.29s\n",
            " correct:  tensor(40832) Total:  40832\n",
            "Epoch 1, Batch 638, Loss 21.03642790145635 Accuracy 1.0 Time 3.062s\n",
            " correct:  tensor(40896) Total:  40896\n",
            "Epoch 1, Batch 639, Loss 21.02991811247872 Accuracy 1.0 Time 3.089s\n",
            " correct:  tensor(40960) Total:  40960\n",
            "Epoch 1, Batch 640, Loss 21.024341574311258 Accuracy 1.0 Time 3.106s\n",
            " correct:  tensor(41024) Total:  41024\n",
            "Epoch 1, Batch 641, Loss 21.019595451176446 Accuracy 1.0 Time 3.154s\n",
            " correct:  tensor(41088) Total:  41088\n",
            "Epoch 1, Batch 642, Loss 21.01374537328322 Accuracy 1.0 Time 3.122s\n",
            " correct:  tensor(41152) Total:  41152\n",
            "Epoch 1, Batch 643, Loss 21.009193138609035 Accuracy 1.0 Time 3.184s\n",
            " correct:  tensor(41216) Total:  41216\n",
            "Epoch 1, Batch 644, Loss 21.003864839210273 Accuracy 1.0 Time 3.153s\n",
            " correct:  tensor(41280) Total:  41280\n",
            "Epoch 1, Batch 645, Loss 21.000859844592192 Accuracy 1.0 Time 3.072s\n",
            " correct:  tensor(41344) Total:  41344\n",
            "Epoch 1, Batch 646, Loss 20.993778872416115 Accuracy 1.0 Time 3.076s\n",
            " correct:  tensor(41408) Total:  41408\n",
            "Epoch 1, Batch 647, Loss 20.98860600042564 Accuracy 1.0 Time 3.064s\n",
            " correct:  tensor(41472) Total:  41472\n",
            "Epoch 1, Batch 648, Loss 20.982456395655504 Accuracy 1.0 Time 3.13s\n",
            " correct:  tensor(41536) Total:  41536\n",
            "Epoch 1, Batch 649, Loss 20.97543199771358 Accuracy 1.0 Time 3.087s\n",
            " correct:  tensor(41600) Total:  41600\n",
            "Epoch 1, Batch 650, Loss 20.970541745699368 Accuracy 1.0 Time 3.221s\n",
            " correct:  tensor(41664) Total:  41664\n",
            "Epoch 1, Batch 651, Loss 20.966119745726225 Accuracy 1.0 Time 3.14s\n",
            " correct:  tensor(41728) Total:  41728\n",
            "Epoch 1, Batch 652, Loss 20.960225702063436 Accuracy 1.0 Time 3.098s\n",
            " correct:  tensor(41792) Total:  41792\n",
            "Epoch 1, Batch 653, Loss 20.955661756156232 Accuracy 1.0 Time 3.109s\n",
            " correct:  tensor(41856) Total:  41856\n",
            "Epoch 1, Batch 654, Loss 20.949867508097892 Accuracy 1.0 Time 3.09s\n",
            " correct:  tensor(41920) Total:  41920\n",
            "Epoch 1, Batch 655, Loss 20.943081592239498 Accuracy 1.0 Time 3.098s\n",
            " correct:  tensor(41984) Total:  41984\n",
            "Epoch 1, Batch 656, Loss 20.93727212708171 Accuracy 1.0 Time 3.072s\n",
            " correct:  tensor(42048) Total:  42048\n",
            "Epoch 1, Batch 657, Loss 20.932417511032778 Accuracy 1.0 Time 3.091s\n",
            " correct:  tensor(42112) Total:  42112\n",
            "Epoch 1, Batch 658, Loss 20.92579123894132 Accuracy 1.0 Time 3.11s\n",
            " correct:  tensor(42176) Total:  42176\n",
            "Epoch 1, Batch 659, Loss 20.919332431914775 Accuracy 1.0 Time 3.062s\n",
            " correct:  tensor(42240) Total:  42240\n",
            "Epoch 1, Batch 660, Loss 20.915098051591354 Accuracy 1.0 Time 3.109s\n",
            " correct:  tensor(42304) Total:  42304\n",
            "Epoch 1, Batch 661, Loss 20.911100927171116 Accuracy 1.0 Time 3.104s\n",
            " correct:  tensor(42368) Total:  42368\n",
            "Epoch 1, Batch 662, Loss 20.90575247876954 Accuracy 1.0 Time 3.099s\n",
            " correct:  tensor(42432) Total:  42432\n",
            "Epoch 1, Batch 663, Loss 20.90047539988614 Accuracy 1.0 Time 3.179s\n",
            " correct:  tensor(42496) Total:  42496\n",
            "Epoch 1, Batch 664, Loss 20.896345388458435 Accuracy 1.0 Time 3.131s\n",
            " correct:  tensor(42560) Total:  42560\n",
            "Epoch 1, Batch 665, Loss 20.891812437817567 Accuracy 1.0 Time 3.098s\n",
            " correct:  tensor(42624) Total:  42624\n",
            "Epoch 1, Batch 666, Loss 20.88518566555447 Accuracy 1.0 Time 3.117s\n",
            " correct:  tensor(42688) Total:  42688\n",
            "Epoch 1, Batch 667, Loss 20.87997061630775 Accuracy 1.0 Time 3.052s\n",
            " correct:  tensor(42752) Total:  42752\n",
            "Epoch 1, Batch 668, Loss 20.874137847009532 Accuracy 1.0 Time 3.102s\n",
            " correct:  tensor(42816) Total:  42816\n",
            "Epoch 1, Batch 669, Loss 20.86781612390538 Accuracy 1.0 Time 3.166s\n",
            " correct:  tensor(42880) Total:  42880\n",
            "Epoch 1, Batch 670, Loss 20.86141422898022 Accuracy 1.0 Time 3.091s\n",
            " correct:  tensor(42944) Total:  42944\n",
            "Epoch 1, Batch 671, Loss 20.85465701823796 Accuracy 1.0 Time 3.057s\n",
            " correct:  tensor(43008) Total:  43008\n",
            "Epoch 1, Batch 672, Loss 20.849727275825682 Accuracy 1.0 Time 3.076s\n",
            " correct:  tensor(43072) Total:  43072\n",
            "Epoch 1, Batch 673, Loss 20.843821952105632 Accuracy 1.0 Time 3.064s\n",
            " correct:  tensor(43136) Total:  43136\n",
            "Epoch 1, Batch 674, Loss 20.837815032868427 Accuracy 1.0 Time 3.35s\n",
            " correct:  tensor(43200) Total:  43200\n",
            "Epoch 1, Batch 675, Loss 20.83170555114746 Accuracy 1.0 Time 3.19s\n",
            " correct:  tensor(43264) Total:  43264\n",
            "Epoch 1, Batch 676, Loss 20.82790928056254 Accuracy 1.0 Time 3.11s\n",
            " correct:  tensor(43328) Total:  43328\n",
            "Epoch 1, Batch 677, Loss 20.823617428056952 Accuracy 1.0 Time 3.104s\n",
            " correct:  tensor(43392) Total:  43392\n",
            "Epoch 1, Batch 678, Loss 20.81962932597923 Accuracy 1.0 Time 3.143s\n",
            " correct:  tensor(43456) Total:  43456\n",
            "Epoch 1, Batch 679, Loss 20.813121059681777 Accuracy 1.0 Time 3.133s\n",
            " correct:  tensor(43520) Total:  43520\n",
            "Epoch 1, Batch 680, Loss 20.808185493244846 Accuracy 1.0 Time 3.114s\n",
            " correct:  tensor(43584) Total:  43584\n",
            "Epoch 1, Batch 681, Loss 20.80344280517399 Accuracy 1.0 Time 3.108s\n",
            " correct:  tensor(43648) Total:  43648\n",
            "Epoch 1, Batch 682, Loss 20.798507201007386 Accuracy 1.0 Time 3.19s\n",
            " correct:  tensor(43712) Total:  43712\n",
            "Epoch 1, Batch 683, Loss 20.791869217968966 Accuracy 1.0 Time 3.127s\n",
            " correct:  tensor(43776) Total:  43776\n",
            "Epoch 1, Batch 684, Loss 20.788733610632825 Accuracy 1.0 Time 3.205s\n",
            " correct:  tensor(43840) Total:  43840\n",
            "Epoch 1, Batch 685, Loss 20.782785677387768 Accuracy 1.0 Time 3.09s\n",
            " correct:  tensor(43904) Total:  43904\n",
            "Epoch 1, Batch 686, Loss 20.77786019661684 Accuracy 1.0 Time 3.161s\n",
            " correct:  tensor(43968) Total:  43968\n",
            "Epoch 1, Batch 687, Loss 20.773195394435476 Accuracy 1.0 Time 3.131s\n",
            " correct:  tensor(44032) Total:  44032\n",
            "Epoch 1, Batch 688, Loss 20.76908768055051 Accuracy 1.0 Time 3.177s\n",
            " correct:  tensor(44096) Total:  44096\n",
            "Epoch 1, Batch 689, Loss 20.763900153349724 Accuracy 1.0 Time 3.182s\n",
            " correct:  tensor(44160) Total:  44160\n",
            "Epoch 1, Batch 690, Loss 20.759856652522434 Accuracy 1.0 Time 3.158s\n",
            " correct:  tensor(44224) Total:  44224\n",
            "Epoch 1, Batch 691, Loss 20.753967097457686 Accuracy 1.0 Time 3.126s\n",
            " correct:  tensor(44288) Total:  44288\n",
            "Epoch 1, Batch 692, Loss 20.748436619091585 Accuracy 1.0 Time 3.09s\n",
            " correct:  tensor(44352) Total:  44352\n",
            "Epoch 1, Batch 693, Loss 20.742682615227857 Accuracy 1.0 Time 3.118s\n",
            " correct:  tensor(44416) Total:  44416\n",
            "Epoch 1, Batch 694, Loss 20.73842282254002 Accuracy 1.0 Time 3.093s\n",
            " correct:  tensor(44480) Total:  44480\n",
            "Epoch 1, Batch 695, Loss 20.733244139856573 Accuracy 1.0 Time 3.111s\n",
            " correct:  tensor(44544) Total:  44544\n",
            "Epoch 1, Batch 696, Loss 20.728011915053443 Accuracy 1.0 Time 3.127s\n",
            " correct:  tensor(44608) Total:  44608\n",
            "Epoch 1, Batch 697, Loss 20.722918276465265 Accuracy 1.0 Time 3.105s\n",
            " correct:  tensor(44672) Total:  44672\n",
            "Epoch 1, Batch 698, Loss 20.717719154576518 Accuracy 1.0 Time 3.113s\n",
            " correct:  tensor(44736) Total:  44736\n",
            "Epoch 1, Batch 699, Loss 20.710860098209846 Accuracy 1.0 Time 3.21s\n",
            " correct:  tensor(44800) Total:  44800\n",
            "Epoch 1, Batch 700, Loss 20.704716946738106 Accuracy 1.0 Time 3.12s\n",
            " correct:  tensor(44864) Total:  44864\n",
            "Epoch 1, Batch 701, Loss 20.700546950314422 Accuracy 1.0 Time 3.092s\n",
            " correct:  tensor(44928) Total:  44928\n",
            "Epoch 1, Batch 702, Loss 20.695623547263295 Accuracy 1.0 Time 3.186s\n",
            " correct:  tensor(44992) Total:  44992\n",
            "Epoch 1, Batch 703, Loss 20.690889621696634 Accuracy 1.0 Time 3.152s\n",
            " correct:  tensor(45056) Total:  45056\n",
            "Epoch 1, Batch 704, Loss 20.68593703345819 Accuracy 1.0 Time 3.109s\n",
            " correct:  tensor(45120) Total:  45120\n",
            "Epoch 1, Batch 705, Loss 20.68247150664634 Accuracy 1.0 Time 3.211s\n",
            " correct:  tensor(45184) Total:  45184\n",
            "Epoch 1, Batch 706, Loss 20.676888417252062 Accuracy 1.0 Time 3.139s\n",
            " correct:  tensor(45248) Total:  45248\n",
            "Epoch 1, Batch 707, Loss 20.671655026318497 Accuracy 1.0 Time 3.213s\n",
            " correct:  tensor(45312) Total:  45312\n",
            "Epoch 1, Batch 708, Loss 20.668026059360827 Accuracy 1.0 Time 3.167s\n",
            " correct:  tensor(45376) Total:  45376\n",
            "Epoch 1, Batch 709, Loss 20.6633488664506 Accuracy 1.0 Time 3.184s\n",
            " correct:  tensor(45440) Total:  45440\n",
            "Epoch 1, Batch 710, Loss 20.658732011284627 Accuracy 1.0 Time 3.231s\n",
            " correct:  tensor(45504) Total:  45504\n",
            "Epoch 1, Batch 711, Loss 20.65455291576359 Accuracy 1.0 Time 3.133s\n",
            " correct:  tensor(45568) Total:  45568\n",
            "Epoch 1, Batch 712, Loss 20.649478483735844 Accuracy 1.0 Time 3.137s\n",
            " correct:  tensor(45632) Total:  45632\n",
            "Epoch 1, Batch 713, Loss 20.644373055092917 Accuracy 1.0 Time 3.151s\n",
            " correct:  tensor(45696) Total:  45696\n",
            "Epoch 1, Batch 714, Loss 20.638208685802812 Accuracy 1.0 Time 3.19s\n",
            " correct:  tensor(45760) Total:  45760\n",
            "Epoch 1, Batch 715, Loss 20.633346933751675 Accuracy 1.0 Time 3.077s\n",
            " correct:  tensor(45824) Total:  45824\n",
            "Epoch 1, Batch 716, Loss 20.629024590859867 Accuracy 1.0 Time 3.134s\n",
            " correct:  tensor(45888) Total:  45888\n",
            "Epoch 1, Batch 717, Loss 20.62331936515691 Accuracy 1.0 Time 3.168s\n",
            " correct:  tensor(45952) Total:  45952\n",
            "Epoch 1, Batch 718, Loss 20.619220893031045 Accuracy 1.0 Time 3.136s\n",
            " correct:  tensor(46016) Total:  46016\n",
            "Epoch 1, Batch 719, Loss 20.61300959739632 Accuracy 1.0 Time 3.101s\n",
            " correct:  tensor(46080) Total:  46080\n",
            "Epoch 1, Batch 720, Loss 20.60714803536733 Accuracy 1.0 Time 3.18s\n",
            " correct:  tensor(46144) Total:  46144\n",
            "Epoch 1, Batch 721, Loss 20.605761889115122 Accuracy 1.0 Time 3.144s\n",
            " correct:  tensor(46208) Total:  46208\n",
            "Epoch 1, Batch 722, Loss 20.601420458003755 Accuracy 1.0 Time 3.097s\n",
            " correct:  tensor(46272) Total:  46272\n",
            "Epoch 1, Batch 723, Loss 20.595413690780703 Accuracy 1.0 Time 3.11s\n",
            " correct:  tensor(46336) Total:  46336\n",
            "Epoch 1, Batch 724, Loss 20.591320277577605 Accuracy 1.0 Time 3.096s\n",
            " correct:  tensor(46400) Total:  46400\n",
            "Epoch 1, Batch 725, Loss 20.586502869704674 Accuracy 1.0 Time 3.113s\n",
            " correct:  tensor(46464) Total:  46464\n",
            "Epoch 1, Batch 726, Loss 20.58254324174781 Accuracy 1.0 Time 3.126s\n",
            " correct:  tensor(46528) Total:  46528\n",
            "Epoch 1, Batch 727, Loss 20.576596258759007 Accuracy 1.0 Time 3.199s\n",
            " correct:  tensor(46592) Total:  46592\n",
            "Epoch 1, Batch 728, Loss 20.5713807121738 Accuracy 1.0 Time 3.178s\n",
            " correct:  tensor(46656) Total:  46656\n",
            "Epoch 1, Batch 729, Loss 20.566056438597794 Accuracy 1.0 Time 3.128s\n",
            " correct:  tensor(46720) Total:  46720\n",
            "Epoch 1, Batch 730, Loss 20.561887615674163 Accuracy 1.0 Time 3.173s\n",
            " correct:  tensor(46784) Total:  46784\n",
            "Epoch 1, Batch 731, Loss 20.557052205264487 Accuracy 1.0 Time 3.125s\n",
            " correct:  tensor(46848) Total:  46848\n",
            "Epoch 1, Batch 732, Loss 20.5524159858787 Accuracy 1.0 Time 3.165s\n",
            " correct:  tensor(46912) Total:  46912\n",
            "Epoch 1, Batch 733, Loss 20.547474430495363 Accuracy 1.0 Time 3.33s\n",
            " correct:  tensor(46976) Total:  46976\n",
            "Epoch 1, Batch 734, Loss 20.54386934337564 Accuracy 1.0 Time 3.404s\n",
            " correct:  tensor(47040) Total:  47040\n",
            "Epoch 1, Batch 735, Loss 20.539154693707317 Accuracy 1.0 Time 3.367s\n",
            " correct:  tensor(47104) Total:  47104\n",
            "Epoch 1, Batch 736, Loss 20.533930537493333 Accuracy 1.0 Time 3.207s\n",
            " correct:  tensor(47168) Total:  47168\n",
            "Epoch 1, Batch 737, Loss 20.529988906115012 Accuracy 1.0 Time 3.18s\n",
            " correct:  tensor(47232) Total:  47232\n",
            "Epoch 1, Batch 738, Loss 20.526743687265288 Accuracy 1.0 Time 3.138s\n",
            " correct:  tensor(47296) Total:  47296\n",
            "Epoch 1, Batch 739, Loss 20.523255930217903 Accuracy 1.0 Time 3.186s\n",
            " correct:  tensor(47360) Total:  47360\n",
            "Epoch 1, Batch 740, Loss 20.519030906058646 Accuracy 1.0 Time 3.109s\n",
            " correct:  tensor(47424) Total:  47424\n",
            "Epoch 1, Batch 741, Loss 20.514611577537217 Accuracy 1.0 Time 3.178s\n",
            " correct:  tensor(47488) Total:  47488\n",
            "Epoch 1, Batch 742, Loss 20.51161518251157 Accuracy 1.0 Time 3.098s\n",
            " correct:  tensor(47552) Total:  47552\n",
            "Epoch 1, Batch 743, Loss 20.506661046722535 Accuracy 1.0 Time 3.107s\n",
            " correct:  tensor(47616) Total:  47616\n",
            "Epoch 1, Batch 744, Loss 20.50038657906235 Accuracy 1.0 Time 3.121s\n",
            " correct:  tensor(47680) Total:  47680\n",
            "Epoch 1, Batch 745, Loss 20.49656914832608 Accuracy 1.0 Time 3.13s\n",
            " correct:  tensor(47744) Total:  47744\n",
            "Epoch 1, Batch 746, Loss 20.49424222583106 Accuracy 1.0 Time 3.161s\n",
            " correct:  tensor(47808) Total:  47808\n",
            "Epoch 1, Batch 747, Loss 20.489986904813264 Accuracy 1.0 Time 3.104s\n",
            " correct:  tensor(47872) Total:  47872\n",
            "Epoch 1, Batch 748, Loss 20.48520880204471 Accuracy 1.0 Time 3.124s\n",
            " correct:  tensor(47936) Total:  47936\n",
            "Epoch 1, Batch 749, Loss 20.480443223614557 Accuracy 1.0 Time 3.115s\n",
            " correct:  tensor(48000) Total:  48000\n",
            "Epoch 1, Batch 750, Loss 20.477665028889973 Accuracy 1.0 Time 3.1s\n",
            " correct:  tensor(48064) Total:  48064\n",
            "Epoch 1, Batch 751, Loss 20.47209803178688 Accuracy 1.0 Time 3.146s\n",
            " correct:  tensor(48128) Total:  48128\n",
            "Epoch 1, Batch 752, Loss 20.46850426907235 Accuracy 1.0 Time 3.107s\n",
            " correct:  tensor(48192) Total:  48192\n",
            "Epoch 1, Batch 753, Loss 20.46473549213384 Accuracy 1.0 Time 3.103s\n",
            " correct:  tensor(48256) Total:  48256\n",
            "Epoch 1, Batch 754, Loss 20.459765123119404 Accuracy 1.0 Time 3.082s\n",
            " correct:  tensor(48320) Total:  48320\n",
            "Epoch 1, Batch 755, Loss 20.456037293996243 Accuracy 1.0 Time 3.121s\n",
            " correct:  tensor(48384) Total:  48384\n",
            "Epoch 1, Batch 756, Loss 20.451385059053937 Accuracy 1.0 Time 3.198s\n",
            " correct:  tensor(48448) Total:  48448\n",
            "Epoch 1, Batch 757, Loss 20.446144418955793 Accuracy 1.0 Time 3.137s\n",
            " correct:  tensor(48512) Total:  48512\n",
            "Epoch 1, Batch 758, Loss 20.441384599831615 Accuracy 1.0 Time 3.201s\n",
            " correct:  tensor(48576) Total:  48576\n",
            "Epoch 1, Batch 759, Loss 20.437283768484242 Accuracy 1.0 Time 3.223s\n",
            " correct:  tensor(48640) Total:  48640\n",
            "Epoch 1, Batch 760, Loss 20.433297107094212 Accuracy 1.0 Time 3.117s\n",
            " correct:  tensor(48704) Total:  48704\n",
            "Epoch 1, Batch 761, Loss 20.42915539233976 Accuracy 1.0 Time 3.221s\n",
            " correct:  tensor(48768) Total:  48768\n",
            "Epoch 1, Batch 762, Loss 20.424149736018943 Accuracy 1.0 Time 3.194s\n",
            " correct:  tensor(48832) Total:  48832\n",
            "Epoch 1, Batch 763, Loss 20.4196427646502 Accuracy 1.0 Time 3.155s\n",
            " correct:  tensor(48896) Total:  48896\n",
            "Epoch 1, Batch 764, Loss 20.415039152375066 Accuracy 1.0 Time 3.272s\n",
            " correct:  tensor(48960) Total:  48960\n",
            "Epoch 1, Batch 765, Loss 20.411203264722637 Accuracy 1.0 Time 3.202s\n",
            " correct:  tensor(49024) Total:  49024\n",
            "Epoch 1, Batch 766, Loss 20.406598932749297 Accuracy 1.0 Time 3.199s\n",
            " correct:  tensor(49088) Total:  49088\n",
            "Epoch 1, Batch 767, Loss 20.40209431753942 Accuracy 1.0 Time 3.188s\n",
            " correct:  tensor(49152) Total:  49152\n",
            "Epoch 1, Batch 768, Loss 20.397842975954216 Accuracy 1.0 Time 3.12s\n",
            " correct:  tensor(49216) Total:  49216\n",
            "Epoch 1, Batch 769, Loss 20.394052488131084 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(49280) Total:  49280\n",
            "Epoch 1, Batch 770, Loss 20.38958429113611 Accuracy 1.0 Time 3.224s\n",
            " correct:  tensor(49344) Total:  49344\n",
            "Epoch 1, Batch 771, Loss 20.385734570474785 Accuracy 1.0 Time 3.425s\n",
            " correct:  tensor(49408) Total:  49408\n",
            "Epoch 1, Batch 772, Loss 20.38230048560108 Accuracy 1.0 Time 3.166s\n",
            " correct:  tensor(49472) Total:  49472\n",
            "Epoch 1, Batch 773, Loss 20.377335937026245 Accuracy 1.0 Time 3.112s\n",
            " correct:  tensor(49536) Total:  49536\n",
            "Epoch 1, Batch 774, Loss 20.373099447836864 Accuracy 1.0 Time 3.103s\n",
            " correct:  tensor(49600) Total:  49600\n",
            "Epoch 1, Batch 775, Loss 20.370514575589088 Accuracy 1.0 Time 3.107s\n",
            " correct:  tensor(49664) Total:  49664\n",
            "Epoch 1, Batch 776, Loss 20.365875986433522 Accuracy 1.0 Time 3.078s\n",
            " correct:  tensor(49728) Total:  49728\n",
            "Epoch 1, Batch 777, Loss 20.36205095858187 Accuracy 1.0 Time 3.064s\n",
            " correct:  tensor(49792) Total:  49792\n",
            "Epoch 1, Batch 778, Loss 20.356341661095314 Accuracy 1.0 Time 3.134s\n",
            " correct:  tensor(49856) Total:  49856\n",
            "Epoch 1, Batch 779, Loss 20.35199258385635 Accuracy 1.0 Time 3.079s\n",
            " correct:  tensor(49920) Total:  49920\n",
            "Epoch 1, Batch 780, Loss 20.346012630218116 Accuracy 1.0 Time 3.124s\n",
            " correct:  tensor(49984) Total:  49984\n",
            "Epoch 1, Batch 781, Loss 20.340163587455407 Accuracy 1.0 Time 3.086s\n",
            " correct:  tensor(50048) Total:  50048\n",
            "Epoch 1, Batch 782, Loss 20.335911198345293 Accuracy 1.0 Time 3.131s\n",
            " correct:  tensor(50112) Total:  50112\n",
            "Epoch 1, Batch 783, Loss 20.332626923747448 Accuracy 1.0 Time 3.167s\n",
            " correct:  tensor(50176) Total:  50176\n",
            "Epoch 1, Batch 784, Loss 20.331213217608784 Accuracy 1.0 Time 3.144s\n",
            " correct:  tensor(50240) Total:  50240\n",
            "Epoch 1, Batch 785, Loss 20.328311233763483 Accuracy 1.0 Time 3.148s\n",
            " correct:  tensor(50304) Total:  50304\n",
            "Epoch 1, Batch 786, Loss 20.3237166259125 Accuracy 1.0 Time 3.124s\n",
            " correct:  tensor(50368) Total:  50368\n",
            "Epoch 1, Batch 787, Loss 20.31914436771727 Accuracy 1.0 Time 3.16s\n",
            " correct:  tensor(50432) Total:  50432\n",
            "Epoch 1, Batch 788, Loss 20.314967940906584 Accuracy 1.0 Time 3.073s\n",
            " correct:  tensor(50496) Total:  50496\n",
            "Epoch 1, Batch 789, Loss 20.310147075145416 Accuracy 1.0 Time 3.105s\n",
            " correct:  tensor(50560) Total:  50560\n",
            "Epoch 1, Batch 790, Loss 20.307159118410908 Accuracy 1.0 Time 3.151s\n",
            " correct:  tensor(50624) Total:  50624\n",
            "Epoch 1, Batch 791, Loss 20.303028906039735 Accuracy 1.0 Time 3.186s\n",
            " correct:  tensor(50688) Total:  50688\n",
            "Epoch 1, Batch 792, Loss 20.298993097411262 Accuracy 1.0 Time 3.123s\n",
            " correct:  tensor(50752) Total:  50752\n",
            "Epoch 1, Batch 793, Loss 20.296519050982983 Accuracy 1.0 Time 3.221s\n",
            " correct:  tensor(50816) Total:  50816\n",
            "Epoch 1, Batch 794, Loss 20.292796376370063 Accuracy 1.0 Time 3.186s\n",
            " correct:  tensor(50880) Total:  50880\n",
            "Epoch 1, Batch 795, Loss 20.28934823641987 Accuracy 1.0 Time 3.069s\n",
            " correct:  tensor(50944) Total:  50944\n",
            "Epoch 1, Batch 796, Loss 20.28482088371737 Accuracy 1.0 Time 3.11s\n",
            " correct:  tensor(51008) Total:  51008\n",
            "Epoch 1, Batch 797, Loss 20.28135701374547 Accuracy 1.0 Time 3.13s\n",
            " correct:  tensor(51072) Total:  51072\n",
            "Epoch 1, Batch 798, Loss 20.27794008685234 Accuracy 1.0 Time 3.103s\n",
            " correct:  tensor(51136) Total:  51136\n",
            "Epoch 1, Batch 799, Loss 20.271853054271023 Accuracy 1.0 Time 3.137s\n",
            " correct:  tensor(51200) Total:  51200\n",
            "Epoch 1, Batch 800, Loss 20.268055284023283 Accuracy 1.0 Time 3.14s\n",
            " correct:  tensor(51264) Total:  51264\n",
            "Epoch 1, Batch 801, Loss 20.266503397147456 Accuracy 1.0 Time 3.115s\n",
            " correct:  tensor(51328) Total:  51328\n",
            "Epoch 1, Batch 802, Loss 20.262976258770188 Accuracy 1.0 Time 3.107s\n",
            " correct:  tensor(51392) Total:  51392\n",
            "Epoch 1, Batch 803, Loss 20.259082725306378 Accuracy 1.0 Time 3.129s\n",
            " correct:  tensor(51456) Total:  51456\n",
            "Epoch 1, Batch 804, Loss 20.25430802682146 Accuracy 1.0 Time 3.108s\n",
            " correct:  tensor(51520) Total:  51520\n",
            "Epoch 1, Batch 805, Loss 20.250064700582755 Accuracy 1.0 Time 3.15s\n",
            " correct:  tensor(51584) Total:  51584\n",
            "Epoch 1, Batch 806, Loss 20.247217734457543 Accuracy 1.0 Time 3.094s\n",
            " correct:  tensor(51648) Total:  51648\n",
            "Epoch 1, Batch 807, Loss 20.24367632387296 Accuracy 1.0 Time 3.024s\n",
            " correct:  tensor(51712) Total:  51712\n",
            "Epoch 1, Batch 808, Loss 20.239444600473536 Accuracy 1.0 Time 3.117s\n",
            " correct:  tensor(51776) Total:  51776\n",
            "Epoch 1, Batch 809, Loss 20.235985311501107 Accuracy 1.0 Time 3.104s\n",
            " correct:  tensor(51840) Total:  51840\n",
            "Epoch 1, Batch 810, Loss 20.23298213158125 Accuracy 1.0 Time 3.137s\n",
            " correct:  tensor(51904) Total:  51904\n",
            "Epoch 1, Batch 811, Loss 20.228983195877547 Accuracy 1.0 Time 3.144s\n",
            " correct:  tensor(51968) Total:  51968\n",
            "Epoch 1, Batch 812, Loss 20.224752266418758 Accuracy 1.0 Time 3.197s\n",
            " correct:  tensor(52032) Total:  52032\n",
            "Epoch 1, Batch 813, Loss 20.221823700532994 Accuracy 1.0 Time 3.172s\n",
            " correct:  tensor(52096) Total:  52096\n",
            "Epoch 1, Batch 814, Loss 20.219014997271415 Accuracy 1.0 Time 3.184s\n",
            " correct:  tensor(52160) Total:  52160\n",
            "Epoch 1, Batch 815, Loss 20.21335378190491 Accuracy 1.0 Time 3.137s\n",
            " correct:  tensor(52224) Total:  52224\n",
            "Epoch 1, Batch 816, Loss 20.209320590776557 Accuracy 1.0 Time 3.123s\n",
            " correct:  tensor(52288) Total:  52288\n",
            "Epoch 1, Batch 817, Loss 20.203850160176195 Accuracy 1.0 Time 3.12s\n",
            " correct:  tensor(52352) Total:  52352\n",
            "Epoch 1, Batch 818, Loss 20.201021114013596 Accuracy 1.0 Time 3.099s\n",
            " correct:  tensor(52416) Total:  52416\n",
            "Epoch 1, Batch 819, Loss 20.19611287728334 Accuracy 1.0 Time 3.13s\n",
            " correct:  tensor(52480) Total:  52480\n",
            "Epoch 1, Batch 820, Loss 20.192645409049057 Accuracy 1.0 Time 3.112s\n",
            " correct:  tensor(52544) Total:  52544\n",
            "Epoch 1, Batch 821, Loss 20.187587237387135 Accuracy 1.0 Time 3.124s\n",
            " correct:  tensor(52608) Total:  52608\n",
            "Epoch 1, Batch 822, Loss 20.185273649628726 Accuracy 1.0 Time 3.182s\n",
            " correct:  tensor(52672) Total:  52672\n",
            "Epoch 1, Batch 823, Loss 20.183412286519715 Accuracy 1.0 Time 3.192s\n",
            " correct:  tensor(52736) Total:  52736\n",
            "Epoch 1, Batch 824, Loss 20.17987200704593 Accuracy 1.0 Time 3.12s\n",
            " correct:  tensor(52800) Total:  52800\n",
            "Epoch 1, Batch 825, Loss 20.17576270710338 Accuracy 1.0 Time 3.143s\n",
            " correct:  tensor(52864) Total:  52864\n",
            "Epoch 1, Batch 826, Loss 20.172972636419114 Accuracy 1.0 Time 3.137s\n",
            " correct:  tensor(52928) Total:  52928\n",
            "Epoch 1, Batch 827, Loss 20.167949264830806 Accuracy 1.0 Time 3.1s\n",
            " correct:  tensor(52992) Total:  52992\n",
            "Epoch 1, Batch 828, Loss 20.16363779127886 Accuracy 1.0 Time 3.181s\n",
            " correct:  tensor(53056) Total:  53056\n",
            "Epoch 1, Batch 829, Loss 20.159827287579617 Accuracy 1.0 Time 3.126s\n",
            " correct:  tensor(53120) Total:  53120\n",
            "Epoch 1, Batch 830, Loss 20.15556437825582 Accuracy 1.0 Time 3.153s\n",
            " correct:  tensor(53184) Total:  53184\n",
            "Epoch 1, Batch 831, Loss 20.151323714411216 Accuracy 1.0 Time 3.281s\n",
            " correct:  tensor(53248) Total:  53248\n",
            "Epoch 1, Batch 832, Loss 20.14821271139842 Accuracy 1.0 Time 3.417s\n",
            " correct:  tensor(53312) Total:  53312\n",
            "Epoch 1, Batch 833, Loss 20.143748358947462 Accuracy 1.0 Time 3.383s\n",
            " correct:  tensor(53376) Total:  53376\n",
            "Epoch 1, Batch 834, Loss 20.14070140029029 Accuracy 1.0 Time 3.3s\n",
            " correct:  tensor(53440) Total:  53440\n",
            "Epoch 1, Batch 835, Loss 20.13723657231131 Accuracy 1.0 Time 3.151s\n",
            " correct:  tensor(53504) Total:  53504\n",
            "Epoch 1, Batch 836, Loss 20.13342694574566 Accuracy 1.0 Time 3.087s\n",
            " correct:  tensor(53568) Total:  53568\n",
            "Epoch 1, Batch 837, Loss 20.12997870063554 Accuracy 1.0 Time 3.13s\n",
            " correct:  tensor(53632) Total:  53632\n",
            "Epoch 1, Batch 838, Loss 20.126231943508323 Accuracy 1.0 Time 3.099s\n",
            " correct:  tensor(53696) Total:  53696\n",
            "Epoch 1, Batch 839, Loss 20.123354609447382 Accuracy 1.0 Time 3.084s\n",
            " correct:  tensor(53760) Total:  53760\n",
            "Epoch 1, Batch 840, Loss 20.120401992116655 Accuracy 1.0 Time 3.148s\n",
            " correct:  tensor(53824) Total:  53824\n",
            "Epoch 1, Batch 841, Loss 20.117152134671933 Accuracy 1.0 Time 3.155s\n",
            " correct:  tensor(53888) Total:  53888\n",
            "Epoch 1, Batch 842, Loss 20.112256966407394 Accuracy 1.0 Time 3.162s\n",
            " correct:  tensor(53952) Total:  53952\n",
            "Epoch 1, Batch 843, Loss 20.10829068361511 Accuracy 1.0 Time 3.111s\n",
            " correct:  tensor(54016) Total:  54016\n",
            "Epoch 1, Batch 844, Loss 20.106014713856847 Accuracy 1.0 Time 3.138s\n",
            " correct:  tensor(54080) Total:  54080\n",
            "Epoch 1, Batch 845, Loss 20.102289496653178 Accuracy 1.0 Time 3.131s\n",
            " correct:  tensor(54144) Total:  54144\n",
            "Epoch 1, Batch 846, Loss 20.098307763994725 Accuracy 1.0 Time 3.142s\n",
            " correct:  tensor(54208) Total:  54208\n",
            "Epoch 1, Batch 847, Loss 20.095291972019595 Accuracy 1.0 Time 3.14s\n",
            " correct:  tensor(54272) Total:  54272\n",
            "Epoch 1, Batch 848, Loss 20.09032052755356 Accuracy 1.0 Time 3.125s\n",
            " correct:  tensor(54336) Total:  54336\n",
            "Epoch 1, Batch 849, Loss 20.08554594873399 Accuracy 1.0 Time 3.162s\n",
            " correct:  tensor(54400) Total:  54400\n",
            "Epoch 1, Batch 850, Loss 20.080950675291174 Accuracy 1.0 Time 3.135s\n",
            " correct:  tensor(54464) Total:  54464\n",
            "Epoch 1, Batch 851, Loss 20.07728203595315 Accuracy 1.0 Time 3.146s\n",
            " correct:  tensor(54528) Total:  54528\n",
            "Epoch 1, Batch 852, Loss 20.073417452019704 Accuracy 1.0 Time 3.139s\n",
            " correct:  tensor(54592) Total:  54592\n",
            "Epoch 1, Batch 853, Loss 20.071125163562414 Accuracy 1.0 Time 3.165s\n",
            " correct:  tensor(54656) Total:  54656\n",
            "Epoch 1, Batch 854, Loss 20.066437724602586 Accuracy 1.0 Time 3.209s\n",
            " correct:  tensor(54720) Total:  54720\n",
            "Epoch 1, Batch 855, Loss 20.06214521642317 Accuracy 1.0 Time 3.135s\n",
            " correct:  tensor(54784) Total:  54784\n",
            "Epoch 1, Batch 856, Loss 20.057212346068052 Accuracy 1.0 Time 3.136s\n",
            " correct:  tensor(54848) Total:  54848\n",
            "Epoch 1, Batch 857, Loss 20.0530849354409 Accuracy 1.0 Time 3.12s\n",
            " correct:  tensor(54912) Total:  54912\n",
            "Epoch 1, Batch 858, Loss 20.048713848585294 Accuracy 1.0 Time 3.102s\n",
            " correct:  tensor(54976) Total:  54976\n",
            "Epoch 1, Batch 859, Loss 20.04670324980588 Accuracy 1.0 Time 3.074s\n",
            " correct:  tensor(55040) Total:  55040\n",
            "Epoch 1, Batch 860, Loss 20.042601512199223 Accuracy 1.0 Time 3.14s\n",
            " correct:  tensor(55104) Total:  55104\n",
            "Epoch 1, Batch 861, Loss 20.038391035746752 Accuracy 1.0 Time 3.077s\n",
            " correct:  tensor(55168) Total:  55168\n",
            "Epoch 1, Batch 862, Loss 20.03500594340455 Accuracy 1.0 Time 3.124s\n",
            " correct:  tensor(55232) Total:  55232\n",
            "Epoch 1, Batch 863, Loss 20.03085439202407 Accuracy 1.0 Time 3.113s\n",
            " correct:  tensor(55296) Total:  55296\n",
            "Epoch 1, Batch 864, Loss 20.02644697383598 Accuracy 1.0 Time 3.133s\n",
            " correct:  tensor(55360) Total:  55360\n",
            "Epoch 1, Batch 865, Loss 20.021527557152545 Accuracy 1.0 Time 3.084s\n",
            " correct:  tensor(55424) Total:  55424\n",
            "Epoch 1, Batch 866, Loss 20.018169937177838 Accuracy 1.0 Time 3.097s\n",
            " correct:  tensor(55488) Total:  55488\n",
            "Epoch 1, Batch 867, Loss 20.016065479837348 Accuracy 1.0 Time 3.26s\n",
            " correct:  tensor(55552) Total:  55552\n",
            "Epoch 1, Batch 868, Loss 20.01361397224637 Accuracy 1.0 Time 3.364s\n",
            " correct:  tensor(55616) Total:  55616\n",
            "Epoch 1, Batch 869, Loss 20.010499415381183 Accuracy 1.0 Time 3.089s\n",
            " correct:  tensor(55680) Total:  55680\n",
            "Epoch 1, Batch 870, Loss 20.005961734947117 Accuracy 1.0 Time 3.089s\n",
            " correct:  tensor(55744) Total:  55744\n",
            "Epoch 1, Batch 871, Loss 20.00164691452044 Accuracy 1.0 Time 3.22s\n",
            " correct:  tensor(55808) Total:  55808\n",
            "Epoch 1, Batch 872, Loss 19.99850723612199 Accuracy 1.0 Time 3.121s\n",
            " correct:  tensor(55872) Total:  55872\n",
            "Epoch 1, Batch 873, Loss 19.99612724876622 Accuracy 1.0 Time 3.064s\n",
            " correct:  tensor(55936) Total:  55936\n",
            "Epoch 1, Batch 874, Loss 19.993280830863412 Accuracy 1.0 Time 3.105s\n",
            " correct:  tensor(56000) Total:  56000\n",
            "Epoch 1, Batch 875, Loss 19.990899794442313 Accuracy 1.0 Time 3.088s\n",
            " correct:  tensor(56064) Total:  56064\n",
            "Epoch 1, Batch 876, Loss 19.985695182460628 Accuracy 1.0 Time 3.124s\n",
            " correct:  tensor(56128) Total:  56128\n",
            "Epoch 1, Batch 877, Loss 19.98164347123393 Accuracy 1.0 Time 3.093s\n",
            " correct:  tensor(56192) Total:  56192\n",
            "Epoch 1, Batch 878, Loss 19.978325371318636 Accuracy 1.0 Time 3.084s\n",
            " correct:  tensor(56256) Total:  56256\n",
            "Epoch 1, Batch 879, Loss 19.975141257284985 Accuracy 1.0 Time 3.112s\n",
            " correct:  tensor(56320) Total:  56320\n",
            "Epoch 1, Batch 880, Loss 19.971245153383776 Accuracy 1.0 Time 3.104s\n",
            " correct:  tensor(56384) Total:  56384\n",
            "Epoch 1, Batch 881, Loss 19.966874668194947 Accuracy 1.0 Time 3.114s\n",
            " correct:  tensor(56448) Total:  56448\n",
            "Epoch 1, Batch 882, Loss 19.96301489135846 Accuracy 1.0 Time 3.079s\n",
            " correct:  tensor(56512) Total:  56512\n",
            "Epoch 1, Batch 883, Loss 19.95904966658941 Accuracy 1.0 Time 3.063s\n",
            " correct:  tensor(56576) Total:  56576\n",
            "Epoch 1, Batch 884, Loss 19.9547942698811 Accuracy 1.0 Time 3.068s\n",
            " correct:  tensor(56640) Total:  56640\n",
            "Epoch 1, Batch 885, Loss 19.95147801954194 Accuracy 1.0 Time 3.071s\n",
            " correct:  tensor(56704) Total:  56704\n",
            "Epoch 1, Batch 886, Loss 19.94829666210889 Accuracy 1.0 Time 3.122s\n",
            " correct:  tensor(56768) Total:  56768\n",
            "Epoch 1, Batch 887, Loss 19.94514402649717 Accuracy 1.0 Time 3.112s\n",
            " correct:  tensor(56832) Total:  56832\n",
            "Epoch 1, Batch 888, Loss 19.941187997122068 Accuracy 1.0 Time 3.103s\n",
            " correct:  tensor(56896) Total:  56896\n",
            "Epoch 1, Batch 889, Loss 19.937685852780252 Accuracy 1.0 Time 3.086s\n",
            " correct:  tensor(56960) Total:  56960\n",
            "Epoch 1, Batch 890, Loss 19.934104206856716 Accuracy 1.0 Time 3.063s\n",
            " correct:  tensor(57024) Total:  57024\n",
            "Epoch 1, Batch 891, Loss 19.932234862570542 Accuracy 1.0 Time 3.165s\n",
            " correct:  tensor(57088) Total:  57088\n",
            "Epoch 1, Batch 892, Loss 19.92970858026513 Accuracy 1.0 Time 3.132s\n",
            " correct:  tensor(57152) Total:  57152\n",
            "Epoch 1, Batch 893, Loss 19.926202948122782 Accuracy 1.0 Time 3.091s\n",
            " correct:  tensor(57216) Total:  57216\n",
            "Epoch 1, Batch 894, Loss 19.92270923354215 Accuracy 1.0 Time 3.106s\n",
            " correct:  tensor(57280) Total:  57280\n",
            "Epoch 1, Batch 895, Loss 19.91899724459515 Accuracy 1.0 Time 3.062s\n",
            " correct:  tensor(57344) Total:  57344\n",
            "Epoch 1, Batch 896, Loss 19.91554476001433 Accuracy 1.0 Time 3.104s\n",
            " correct:  tensor(57408) Total:  57408\n",
            "Epoch 1, Batch 897, Loss 19.913131593727613 Accuracy 1.0 Time 3.095s\n",
            " correct:  tensor(57472) Total:  57472\n",
            "Epoch 1, Batch 898, Loss 19.911456045435372 Accuracy 1.0 Time 3.124s\n",
            " correct:  tensor(57536) Total:  57536\n",
            "Epoch 1, Batch 899, Loss 19.908807524319354 Accuracy 1.0 Time 3.122s\n",
            " correct:  tensor(57600) Total:  57600\n",
            "Epoch 1, Batch 900, Loss 19.90565931002299 Accuracy 1.0 Time 3.15s\n",
            " correct:  tensor(57664) Total:  57664\n",
            "Epoch 1, Batch 901, Loss 19.901347592191875 Accuracy 1.0 Time 3.135s\n",
            " correct:  tensor(57728) Total:  57728\n",
            "Epoch 1, Batch 902, Loss 19.897916835057497 Accuracy 1.0 Time 3.111s\n",
            " correct:  tensor(57792) Total:  57792\n",
            "Epoch 1, Batch 903, Loss 19.895228016284086 Accuracy 1.0 Time 3.101s\n",
            " correct:  tensor(57856) Total:  57856\n",
            "Epoch 1, Batch 904, Loss 19.8922755159108 Accuracy 1.0 Time 3.142s\n",
            " correct:  tensor(57920) Total:  57920\n",
            "Epoch 1, Batch 905, Loss 19.888610261316458 Accuracy 1.0 Time 3.127s\n",
            " correct:  tensor(57984) Total:  57984\n",
            "Epoch 1, Batch 906, Loss 19.88561890351588 Accuracy 1.0 Time 3.151s\n",
            " correct:  tensor(58048) Total:  58048\n",
            "Epoch 1, Batch 907, Loss 19.88245637992514 Accuracy 1.0 Time 3.044s\n",
            " correct:  tensor(58112) Total:  58112\n",
            "Epoch 1, Batch 908, Loss 19.879324893069164 Accuracy 1.0 Time 3.175s\n",
            " correct:  tensor(58176) Total:  58176\n",
            "Epoch 1, Batch 909, Loss 19.87554626024202 Accuracy 1.0 Time 3.118s\n",
            " correct:  tensor(58240) Total:  58240\n",
            "Epoch 1, Batch 910, Loss 19.87181408536303 Accuracy 1.0 Time 3.12s\n",
            " correct:  tensor(58304) Total:  58304\n",
            "Epoch 1, Batch 911, Loss 19.86795115444715 Accuracy 1.0 Time 3.115s\n",
            " correct:  tensor(58368) Total:  58368\n",
            "Epoch 1, Batch 912, Loss 19.865084090776612 Accuracy 1.0 Time 3.148s\n",
            " correct:  tensor(58432) Total:  58432\n",
            "Epoch 1, Batch 913, Loss 19.86084174274223 Accuracy 1.0 Time 3.105s\n",
            " correct:  tensor(58496) Total:  58496\n",
            "Epoch 1, Batch 914, Loss 19.85767994093947 Accuracy 1.0 Time 3.089s\n",
            " correct:  tensor(58560) Total:  58560\n",
            "Epoch 1, Batch 915, Loss 19.85476049829702 Accuracy 1.0 Time 3.116s\n",
            " correct:  tensor(58624) Total:  58624\n",
            "Epoch 1, Batch 916, Loss 19.851732680891278 Accuracy 1.0 Time 3.036s\n",
            " correct:  tensor(58688) Total:  58688\n",
            "Epoch 1, Batch 917, Loss 19.84897866659882 Accuracy 1.0 Time 3.099s\n",
            " correct:  tensor(58752) Total:  58752\n",
            "Epoch 1, Batch 918, Loss 19.8459459899038 Accuracy 1.0 Time 3.052s\n",
            " correct:  tensor(58816) Total:  58816\n",
            "Epoch 1, Batch 919, Loss 19.841502420012397 Accuracy 1.0 Time 3.085s\n",
            " correct:  tensor(58880) Total:  58880\n",
            "Epoch 1, Batch 920, Loss 19.83814534208049 Accuracy 1.0 Time 3.075s\n",
            " correct:  tensor(58944) Total:  58944\n",
            "Epoch 1, Batch 921, Loss 19.83614258346806 Accuracy 1.0 Time 3.126s\n",
            " correct:  tensor(59008) Total:  59008\n",
            "Epoch 1, Batch 922, Loss 19.83412408208123 Accuracy 1.0 Time 3.084s\n",
            " correct:  tensor(59072) Total:  59072\n",
            "Epoch 1, Batch 923, Loss 19.83092743965554 Accuracy 1.0 Time 3.064s\n",
            " correct:  tensor(59136) Total:  59136\n",
            "Epoch 1, Batch 924, Loss 19.828512168033814 Accuracy 1.0 Time 3.083s\n",
            " correct:  tensor(59200) Total:  59200\n",
            "Epoch 1, Batch 925, Loss 19.824180539105388 Accuracy 1.0 Time 3.044s\n",
            " correct:  tensor(59264) Total:  59264\n",
            "Epoch 1, Batch 926, Loss 19.820378023398874 Accuracy 1.0 Time 3.096s\n",
            " correct:  tensor(59328) Total:  59328\n",
            "Epoch 1, Batch 927, Loss 19.81814483017083 Accuracy 1.0 Time 3.161s\n",
            " correct:  tensor(59392) Total:  59392\n",
            "Epoch 1, Batch 928, Loss 19.814516314144793 Accuracy 1.0 Time 3.107s\n",
            " correct:  tensor(59456) Total:  59456\n",
            "Epoch 1, Batch 929, Loss 19.811073637881243 Accuracy 1.0 Time 3.111s\n",
            " correct:  tensor(59520) Total:  59520\n",
            "Epoch 1, Batch 930, Loss 19.8074248098558 Accuracy 1.0 Time 3.174s\n",
            " correct:  tensor(59584) Total:  59584\n",
            "Epoch 1, Batch 931, Loss 19.804534469588102 Accuracy 1.0 Time 3.377s\n",
            " correct:  tensor(59648) Total:  59648\n",
            "Epoch 1, Batch 932, Loss 19.801303263897548 Accuracy 1.0 Time 3.343s\n",
            " correct:  tensor(59712) Total:  59712\n",
            "Epoch 1, Batch 933, Loss 19.798712719163966 Accuracy 1.0 Time 3.31s\n",
            " correct:  tensor(59776) Total:  59776\n",
            "Epoch 1, Batch 934, Loss 19.79622869266943 Accuracy 1.0 Time 3.127s\n",
            " correct:  tensor(59840) Total:  59840\n",
            "Epoch 1, Batch 935, Loss 19.79296099208893 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(59904) Total:  59904\n",
            "Epoch 1, Batch 936, Loss 19.789233719181812 Accuracy 1.0 Time 3.188s\n",
            " correct:  tensor(59968) Total:  59968\n",
            "Epoch 1, Batch 937, Loss 19.785609879386843 Accuracy 1.0 Time 3.113s\n",
            " correct:  tensor(60000) Total:  60000\n",
            "Epoch 1, Batch 938, Loss 19.773728062603265 Accuracy 1.0 Time 1.655s\n",
            "TRAIN Epoch 1, Loss 19.773728062603265 Accuracy 1.0 Time 2932.556s\n",
            "TESTING...\n",
            " correct:  tensor(64) Total:  64\n",
            " correct:  tensor(128) Total:  128\n",
            " correct:  tensor(192) Total:  192\n",
            " correct:  tensor(256) Total:  256\n",
            " correct:  tensor(320) Total:  320\n",
            " correct:  tensor(384) Total:  384\n",
            " correct:  tensor(448) Total:  448\n",
            " correct:  tensor(512) Total:  512\n",
            " correct:  tensor(576) Total:  576\n",
            " correct:  tensor(640) Total:  640\n",
            " correct:  tensor(704) Total:  704\n",
            " correct:  tensor(768) Total:  768\n",
            " correct:  tensor(832) Total:  832\n",
            " correct:  tensor(896) Total:  896\n",
            " correct:  tensor(960) Total:  960\n",
            " correct:  tensor(1024) Total:  1024\n",
            " correct:  tensor(1088) Total:  1088\n",
            " correct:  tensor(1152) Total:  1152\n",
            " correct:  tensor(1216) Total:  1216\n",
            " correct:  tensor(1280) Total:  1280\n",
            " correct:  tensor(1344) Total:  1344\n",
            " correct:  tensor(1408) Total:  1408\n",
            " correct:  tensor(1472) Total:  1472\n",
            " correct:  tensor(1536) Total:  1536\n",
            " correct:  tensor(1600) Total:  1600\n",
            " correct:  tensor(1664) Total:  1664\n",
            " correct:  tensor(1728) Total:  1728\n",
            " correct:  tensor(1792) Total:  1792\n",
            " correct:  tensor(1856) Total:  1856\n",
            " correct:  tensor(1920) Total:  1920\n",
            " correct:  tensor(1984) Total:  1984\n",
            " correct:  tensor(2048) Total:  2048\n",
            " correct:  tensor(2112) Total:  2112\n",
            " correct:  tensor(2176) Total:  2176\n",
            " correct:  tensor(2240) Total:  2240\n",
            " correct:  tensor(2304) Total:  2304\n",
            " correct:  tensor(2368) Total:  2368\n",
            " correct:  tensor(2432) Total:  2432\n",
            " correct:  tensor(2496) Total:  2496\n",
            " correct:  tensor(2560) Total:  2560\n",
            " correct:  tensor(2624) Total:  2624\n",
            " correct:  tensor(2688) Total:  2688\n",
            " correct:  tensor(2752) Total:  2752\n",
            " correct:  tensor(2816) Total:  2816\n",
            " correct:  tensor(2880) Total:  2880\n",
            " correct:  tensor(2944) Total:  2944\n",
            " correct:  tensor(3008) Total:  3008\n",
            " correct:  tensor(3072) Total:  3072\n",
            " correct:  tensor(3136) Total:  3136\n",
            " correct:  tensor(3200) Total:  3200\n",
            " correct:  tensor(3264) Total:  3264\n",
            " correct:  tensor(3328) Total:  3328\n",
            " correct:  tensor(3392) Total:  3392\n",
            " correct:  tensor(3456) Total:  3456\n",
            " correct:  tensor(3520) Total:  3520\n",
            " correct:  tensor(3584) Total:  3584\n",
            " correct:  tensor(3648) Total:  3648\n",
            " correct:  tensor(3712) Total:  3712\n",
            " correct:  tensor(3776) Total:  3776\n",
            " correct:  tensor(3840) Total:  3840\n",
            " correct:  tensor(3904) Total:  3904\n",
            " correct:  tensor(3968) Total:  3968\n",
            " correct:  tensor(4032) Total:  4032\n",
            " correct:  tensor(4096) Total:  4096\n",
            " correct:  tensor(4160) Total:  4160\n",
            " correct:  tensor(4224) Total:  4224\n",
            " correct:  tensor(4288) Total:  4288\n",
            " correct:  tensor(4352) Total:  4352\n",
            " correct:  tensor(4416) Total:  4416\n",
            " correct:  tensor(4480) Total:  4480\n",
            " correct:  tensor(4544) Total:  4544\n",
            " correct:  tensor(4608) Total:  4608\n",
            " correct:  tensor(4672) Total:  4672\n",
            " correct:  tensor(4736) Total:  4736\n",
            " correct:  tensor(4800) Total:  4800\n",
            " correct:  tensor(4864) Total:  4864\n",
            " correct:  tensor(4928) Total:  4928\n",
            " correct:  tensor(4992) Total:  4992\n",
            " correct:  tensor(5056) Total:  5056\n",
            " correct:  tensor(5120) Total:  5120\n",
            " correct:  tensor(5184) Total:  5184\n",
            " correct:  tensor(5248) Total:  5248\n",
            " correct:  tensor(5312) Total:  5312\n",
            " correct:  tensor(5376) Total:  5376\n",
            " correct:  tensor(5440) Total:  5440\n",
            " correct:  tensor(5504) Total:  5504\n",
            " correct:  tensor(5568) Total:  5568\n",
            " correct:  tensor(5632) Total:  5632\n",
            " correct:  tensor(5696) Total:  5696\n",
            " correct:  tensor(5760) Total:  5760\n",
            " correct:  tensor(5824) Total:  5824\n",
            " correct:  tensor(5888) Total:  5888\n",
            " correct:  tensor(5952) Total:  5952\n",
            " correct:  tensor(6016) Total:  6016\n",
            " correct:  tensor(6080) Total:  6080\n",
            " correct:  tensor(6144) Total:  6144\n",
            " correct:  tensor(6208) Total:  6208\n",
            " correct:  tensor(6272) Total:  6272\n",
            " correct:  tensor(6336) Total:  6336\n",
            " correct:  tensor(6400) Total:  6400\n",
            " correct:  tensor(6464) Total:  6464\n",
            " correct:  tensor(6528) Total:  6528\n",
            " correct:  tensor(6592) Total:  6592\n",
            " correct:  tensor(6656) Total:  6656\n",
            " correct:  tensor(6720) Total:  6720\n",
            " correct:  tensor(6784) Total:  6784\n",
            " correct:  tensor(6848) Total:  6848\n",
            " correct:  tensor(6912) Total:  6912\n",
            " correct:  tensor(6976) Total:  6976\n",
            " correct:  tensor(7040) Total:  7040\n",
            " correct:  tensor(7104) Total:  7104\n",
            " correct:  tensor(7168) Total:  7168\n",
            " correct:  tensor(7232) Total:  7232\n",
            " correct:  tensor(7296) Total:  7296\n",
            " correct:  tensor(7360) Total:  7360\n",
            " correct:  tensor(7424) Total:  7424\n",
            " correct:  tensor(7488) Total:  7488\n",
            " correct:  tensor(7552) Total:  7552\n",
            " correct:  tensor(7616) Total:  7616\n",
            " correct:  tensor(7680) Total:  7680\n",
            " correct:  tensor(7744) Total:  7744\n",
            " correct:  tensor(7808) Total:  7808\n",
            " correct:  tensor(7872) Total:  7872\n",
            " correct:  tensor(7936) Total:  7936\n",
            " correct:  tensor(8000) Total:  8000\n",
            " correct:  tensor(8064) Total:  8064\n",
            " correct:  tensor(8128) Total:  8128\n",
            " correct:  tensor(8192) Total:  8192\n",
            " correct:  tensor(8256) Total:  8256\n",
            " correct:  tensor(8320) Total:  8320\n",
            " correct:  tensor(8384) Total:  8384\n",
            " correct:  tensor(8448) Total:  8448\n",
            " correct:  tensor(8512) Total:  8512\n",
            " correct:  tensor(8576) Total:  8576\n",
            " correct:  tensor(8640) Total:  8640\n",
            " correct:  tensor(8704) Total:  8704\n",
            " correct:  tensor(8768) Total:  8768\n",
            " correct:  tensor(8832) Total:  8832\n",
            " correct:  tensor(8896) Total:  8896\n",
            " correct:  tensor(8960) Total:  8960\n",
            " correct:  tensor(9024) Total:  9024\n",
            " correct:  tensor(9088) Total:  9088\n",
            " correct:  tensor(9152) Total:  9152\n",
            " correct:  tensor(9216) Total:  9216\n",
            " correct:  tensor(9280) Total:  9280\n",
            " correct:  tensor(9344) Total:  9344\n",
            " correct:  tensor(9408) Total:  9408\n",
            " correct:  tensor(9472) Total:  9472\n",
            " correct:  tensor(9536) Total:  9536\n",
            " correct:  tensor(9600) Total:  9600\n",
            " correct:  tensor(9664) Total:  9664\n",
            " correct:  tensor(9728) Total:  9728\n",
            " correct:  tensor(9792) Total:  9792\n",
            " correct:  tensor(9856) Total:  9856\n",
            " correct:  tensor(9920) Total:  9920\n",
            " correct:  tensor(9984) Total:  9984\n",
            " correct:  tensor(10000) Total:  10000\n",
            "TEST Epoch 1, Loss 16.803988183379932 Accuracy 1.0 Time 166.17s\n",
            "TRAINING...\n",
            " correct:  tensor(64) Total:  64\n",
            "Epoch 2, Batch 1, Loss 16.57403564453125 Accuracy 1.0 Time 3.11s\n",
            " correct:  tensor(128) Total:  128\n",
            "Epoch 2, Batch 2, Loss 16.568881034851074 Accuracy 1.0 Time 3.162s\n",
            " correct:  tensor(192) Total:  192\n",
            "Epoch 2, Batch 3, Loss 17.037105560302734 Accuracy 1.0 Time 3.163s\n",
            " correct:  tensor(256) Total:  256\n",
            "Epoch 2, Batch 4, Loss 16.799665927886963 Accuracy 1.0 Time 3.13s\n",
            " correct:  tensor(320) Total:  320\n",
            "Epoch 2, Batch 5, Loss 16.701852798461914 Accuracy 1.0 Time 3.078s\n",
            " correct:  tensor(384) Total:  384\n",
            "Epoch 2, Batch 6, Loss 16.94915994008382 Accuracy 1.0 Time 3.05s\n",
            " correct:  tensor(448) Total:  448\n",
            "Epoch 2, Batch 7, Loss 16.929568426949636 Accuracy 1.0 Time 3.113s\n",
            " correct:  tensor(512) Total:  512\n",
            "Epoch 2, Batch 8, Loss 16.95337748527527 Accuracy 1.0 Time 3.053s\n",
            " correct:  tensor(576) Total:  576\n",
            "Epoch 2, Batch 9, Loss 16.805463155110676 Accuracy 1.0 Time 3.094s\n",
            " correct:  tensor(640) Total:  640\n",
            "Epoch 2, Batch 10, Loss 16.853971672058105 Accuracy 1.0 Time 3.098s\n",
            " correct:  tensor(704) Total:  704\n",
            "Epoch 2, Batch 11, Loss 16.82252242348411 Accuracy 1.0 Time 3.116s\n",
            " correct:  tensor(768) Total:  768\n",
            "Epoch 2, Batch 12, Loss 16.788994789123535 Accuracy 1.0 Time 3.12s\n",
            " correct:  tensor(832) Total:  832\n",
            "Epoch 2, Batch 13, Loss 16.75297296964205 Accuracy 1.0 Time 3.064s\n",
            " correct:  tensor(896) Total:  896\n",
            "Epoch 2, Batch 14, Loss 16.7282623563494 Accuracy 1.0 Time 3.077s\n",
            " correct:  tensor(960) Total:  960\n",
            "Epoch 2, Batch 15, Loss 16.697610346476235 Accuracy 1.0 Time 3.088s\n",
            " correct:  tensor(1024) Total:  1024\n",
            "Epoch 2, Batch 16, Loss 16.704354405403137 Accuracy 1.0 Time 3.05s\n",
            " correct:  tensor(1088) Total:  1088\n",
            "Epoch 2, Batch 17, Loss 16.683244144215305 Accuracy 1.0 Time 3.133s\n",
            " correct:  tensor(1152) Total:  1152\n",
            "Epoch 2, Batch 18, Loss 16.69098080529107 Accuracy 1.0 Time 3.075s\n",
            " correct:  tensor(1216) Total:  1216\n",
            "Epoch 2, Batch 19, Loss 16.704090319181745 Accuracy 1.0 Time 3.08s\n",
            " correct:  tensor(1280) Total:  1280\n",
            "Epoch 2, Batch 20, Loss 16.69064054489136 Accuracy 1.0 Time 3.056s\n",
            " correct:  tensor(1344) Total:  1344\n",
            "Epoch 2, Batch 21, Loss 16.68906048366002 Accuracy 1.0 Time 3.134s\n",
            " correct:  tensor(1408) Total:  1408\n",
            "Epoch 2, Batch 22, Loss 16.70646840875799 Accuracy 1.0 Time 3.095s\n",
            " correct:  tensor(1472) Total:  1472\n",
            "Epoch 2, Batch 23, Loss 16.83756977578868 Accuracy 1.0 Time 3.041s\n",
            " correct:  tensor(1536) Total:  1536\n",
            "Epoch 2, Batch 24, Loss 16.885686953862507 Accuracy 1.0 Time 3.093s\n",
            " correct:  tensor(1600) Total:  1600\n",
            "Epoch 2, Batch 25, Loss 16.90672966003418 Accuracy 1.0 Time 3.118s\n",
            " correct:  tensor(1664) Total:  1664\n",
            "Epoch 2, Batch 26, Loss 16.93528468792255 Accuracy 1.0 Time 3.113s\n",
            " correct:  tensor(1728) Total:  1728\n",
            "Epoch 2, Batch 27, Loss 16.91318596733941 Accuracy 1.0 Time 3.114s\n",
            " correct:  tensor(1792) Total:  1792\n",
            "Epoch 2, Batch 28, Loss 16.919286932264054 Accuracy 1.0 Time 3.101s\n",
            " correct:  tensor(1856) Total:  1856\n",
            "Epoch 2, Batch 29, Loss 16.92479863660089 Accuracy 1.0 Time 3.07s\n",
            " correct:  tensor(1920) Total:  1920\n",
            "Epoch 2, Batch 30, Loss 16.895205116271974 Accuracy 1.0 Time 3.139s\n",
            " correct:  tensor(1984) Total:  1984\n",
            "Epoch 2, Batch 31, Loss 16.926800389443674 Accuracy 1.0 Time 3.061s\n",
            " correct:  tensor(2048) Total:  2048\n",
            "Epoch 2, Batch 32, Loss 16.94808602333069 Accuracy 1.0 Time 3.172s\n",
            " correct:  tensor(2112) Total:  2112\n",
            "Epoch 2, Batch 33, Loss 16.955746563998137 Accuracy 1.0 Time 3.077s\n",
            " correct:  tensor(2176) Total:  2176\n",
            "Epoch 2, Batch 34, Loss 16.933807316948386 Accuracy 1.0 Time 3.088s\n",
            " correct:  tensor(2240) Total:  2240\n",
            "Epoch 2, Batch 35, Loss 16.90061209542411 Accuracy 1.0 Time 3.047s\n",
            " correct:  tensor(2304) Total:  2304\n",
            "Epoch 2, Batch 36, Loss 16.891271750132244 Accuracy 1.0 Time 3.161s\n",
            " correct:  tensor(2368) Total:  2368\n",
            "Epoch 2, Batch 37, Loss 16.90001920751623 Accuracy 1.0 Time 3.109s\n",
            " correct:  tensor(2432) Total:  2432\n",
            "Epoch 2, Batch 38, Loss 16.869804934451455 Accuracy 1.0 Time 3.064s\n",
            " correct:  tensor(2496) Total:  2496\n",
            "Epoch 2, Batch 39, Loss 16.868626814622147 Accuracy 1.0 Time 3.245s\n",
            " correct:  tensor(2560) Total:  2560\n",
            "Epoch 2, Batch 40, Loss 16.879810667037965 Accuracy 1.0 Time 3.342s\n",
            " correct:  tensor(2624) Total:  2624\n",
            "Epoch 2, Batch 41, Loss 16.882902657113423 Accuracy 1.0 Time 3.364s\n",
            " correct:  tensor(2688) Total:  2688\n",
            "Epoch 2, Batch 42, Loss 16.92414242880685 Accuracy 1.0 Time 3.21s\n",
            " correct:  tensor(2752) Total:  2752\n",
            "Epoch 2, Batch 43, Loss 16.911154369975247 Accuracy 1.0 Time 3.1s\n",
            " correct:  tensor(2816) Total:  2816\n",
            "Epoch 2, Batch 44, Loss 16.912748510187324 Accuracy 1.0 Time 3.127s\n",
            " correct:  tensor(2880) Total:  2880\n",
            "Epoch 2, Batch 45, Loss 16.902772521972658 Accuracy 1.0 Time 3.064s\n",
            " correct:  tensor(2944) Total:  2944\n",
            "Epoch 2, Batch 46, Loss 16.892931855243184 Accuracy 1.0 Time 3.068s\n",
            " correct:  tensor(3008) Total:  3008\n",
            "Epoch 2, Batch 47, Loss 16.888575168366128 Accuracy 1.0 Time 3.078s\n",
            " correct:  tensor(3072) Total:  3072\n",
            "Epoch 2, Batch 48, Loss 16.90066397190094 Accuracy 1.0 Time 3.091s\n",
            " correct:  tensor(3136) Total:  3136\n",
            "Epoch 2, Batch 49, Loss 16.90968303291165 Accuracy 1.0 Time 3.124s\n",
            " correct:  tensor(3200) Total:  3200\n",
            "Epoch 2, Batch 50, Loss 16.91202617645264 Accuracy 1.0 Time 3.11s\n",
            " correct:  tensor(3264) Total:  3264\n",
            "Epoch 2, Batch 51, Loss 16.947857987646962 Accuracy 1.0 Time 3.039s\n",
            " correct:  tensor(3328) Total:  3328\n",
            "Epoch 2, Batch 52, Loss 16.94194606634287 Accuracy 1.0 Time 3.073s\n",
            " correct:  tensor(3392) Total:  3392\n",
            "Epoch 2, Batch 53, Loss 16.947722560954542 Accuracy 1.0 Time 3.201s\n",
            " correct:  tensor(3456) Total:  3456\n",
            "Epoch 2, Batch 54, Loss 16.916052588710077 Accuracy 1.0 Time 3.125s\n",
            " correct:  tensor(3520) Total:  3520\n",
            "Epoch 2, Batch 55, Loss 16.90628518191251 Accuracy 1.0 Time 3.143s\n",
            " correct:  tensor(3584) Total:  3584\n",
            "Epoch 2, Batch 56, Loss 16.926772952079773 Accuracy 1.0 Time 3.111s\n",
            " correct:  tensor(3648) Total:  3648\n",
            "Epoch 2, Batch 57, Loss 16.936938218903123 Accuracy 1.0 Time 3.144s\n",
            " correct:  tensor(3712) Total:  3712\n",
            "Epoch 2, Batch 58, Loss 16.961555530285015 Accuracy 1.0 Time 3.113s\n",
            " correct:  tensor(3776) Total:  3776\n",
            "Epoch 2, Batch 59, Loss 16.945203651816158 Accuracy 1.0 Time 3.086s\n",
            " correct:  tensor(3840) Total:  3840\n",
            "Epoch 2, Batch 60, Loss 16.941611576080323 Accuracy 1.0 Time 3.106s\n",
            " correct:  tensor(3904) Total:  3904\n",
            "Epoch 2, Batch 61, Loss 16.949137797121143 Accuracy 1.0 Time 3.175s\n",
            " correct:  tensor(3968) Total:  3968\n",
            "Epoch 2, Batch 62, Loss 16.93758823025611 Accuracy 1.0 Time 3.114s\n",
            " correct:  tensor(4032) Total:  4032\n",
            "Epoch 2, Batch 63, Loss 16.945364270891464 Accuracy 1.0 Time 3.102s\n",
            " correct:  tensor(4096) Total:  4096\n",
            "Epoch 2, Batch 64, Loss 16.944779068231583 Accuracy 1.0 Time 3.046s\n",
            " correct:  tensor(4160) Total:  4160\n",
            "Epoch 2, Batch 65, Loss 16.953779337956355 Accuracy 1.0 Time 3.119s\n",
            " correct:  tensor(4224) Total:  4224\n",
            "Epoch 2, Batch 66, Loss 16.94948265769265 Accuracy 1.0 Time 3.08s\n",
            " correct:  tensor(4288) Total:  4288\n",
            "Epoch 2, Batch 67, Loss 16.940039079580735 Accuracy 1.0 Time 3.083s\n",
            " correct:  tensor(4352) Total:  4352\n",
            "Epoch 2, Batch 68, Loss 16.936605032752542 Accuracy 1.0 Time 3.095s\n",
            " correct:  tensor(4416) Total:  4416\n",
            "Epoch 2, Batch 69, Loss 16.92631879060165 Accuracy 1.0 Time 3.064s\n",
            " correct:  tensor(4480) Total:  4480\n",
            "Epoch 2, Batch 70, Loss 16.933294677734374 Accuracy 1.0 Time 3.055s\n",
            " correct:  tensor(4544) Total:  4544\n",
            "Epoch 2, Batch 71, Loss 16.92576964472381 Accuracy 1.0 Time 3.031s\n",
            " correct:  tensor(4608) Total:  4608\n",
            "Epoch 2, Batch 72, Loss 16.91253776020474 Accuracy 1.0 Time 3.288s\n",
            " correct:  tensor(4672) Total:  4672\n",
            "Epoch 2, Batch 73, Loss 16.899443443507366 Accuracy 1.0 Time 3.288s\n",
            " correct:  tensor(4736) Total:  4736\n",
            "Epoch 2, Batch 74, Loss 16.875498887654896 Accuracy 1.0 Time 3.056s\n",
            " correct:  tensor(4800) Total:  4800\n",
            "Epoch 2, Batch 75, Loss 16.882080370585125 Accuracy 1.0 Time 3.047s\n",
            " correct:  tensor(4864) Total:  4864\n",
            "Epoch 2, Batch 76, Loss 16.88343875031722 Accuracy 1.0 Time 3.087s\n",
            " correct:  tensor(4928) Total:  4928\n",
            "Epoch 2, Batch 77, Loss 16.869466348127887 Accuracy 1.0 Time 3.011s\n",
            " correct:  tensor(4992) Total:  4992\n",
            "Epoch 2, Batch 78, Loss 16.865418593088787 Accuracy 1.0 Time 3.097s\n",
            " correct:  tensor(5056) Total:  5056\n",
            "Epoch 2, Batch 79, Loss 16.85830634153342 Accuracy 1.0 Time 3.08s\n",
            " correct:  tensor(5120) Total:  5120\n",
            "Epoch 2, Batch 80, Loss 16.84804791212082 Accuracy 1.0 Time 3.083s\n",
            " correct:  tensor(5184) Total:  5184\n",
            "Epoch 2, Batch 81, Loss 16.850906007083847 Accuracy 1.0 Time 3.046s\n",
            " correct:  tensor(5248) Total:  5248\n",
            "Epoch 2, Batch 82, Loss 16.857717037200928 Accuracy 1.0 Time 3.061s\n",
            " correct:  tensor(5312) Total:  5312\n",
            "Epoch 2, Batch 83, Loss 16.849176188549364 Accuracy 1.0 Time 3.058s\n",
            " correct:  tensor(5376) Total:  5376\n",
            "Epoch 2, Batch 84, Loss 16.848119860603695 Accuracy 1.0 Time 3.029s\n",
            " correct:  tensor(5440) Total:  5440\n",
            "Epoch 2, Batch 85, Loss 16.83860919054817 Accuracy 1.0 Time 3.08s\n",
            " correct:  tensor(5504) Total:  5504\n",
            "Epoch 2, Batch 86, Loss 16.83490190949551 Accuracy 1.0 Time 3.07s\n",
            " correct:  tensor(5568) Total:  5568\n",
            "Epoch 2, Batch 87, Loss 16.827308534205645 Accuracy 1.0 Time 3.036s\n",
            " correct:  tensor(5632) Total:  5632\n",
            "Epoch 2, Batch 88, Loss 16.82509603283622 Accuracy 1.0 Time 3.058s\n",
            " correct:  tensor(5696) Total:  5696\n",
            "Epoch 2, Batch 89, Loss 16.818965193930637 Accuracy 1.0 Time 3.111s\n",
            " correct:  tensor(5760) Total:  5760\n",
            "Epoch 2, Batch 90, Loss 16.8205949889289 Accuracy 1.0 Time 3.047s\n",
            " correct:  tensor(5824) Total:  5824\n",
            "Epoch 2, Batch 91, Loss 16.827096928606977 Accuracy 1.0 Time 3.072s\n",
            " correct:  tensor(5888) Total:  5888\n",
            "Epoch 2, Batch 92, Loss 16.832299761150193 Accuracy 1.0 Time 3.031s\n",
            " correct:  tensor(5952) Total:  5952\n",
            "Epoch 2, Batch 93, Loss 16.847399793645387 Accuracy 1.0 Time 3.098s\n",
            " correct:  tensor(6016) Total:  6016\n",
            "Epoch 2, Batch 94, Loss 16.84387248627683 Accuracy 1.0 Time 3.047s\n",
            " correct:  tensor(6080) Total:  6080\n",
            "Epoch 2, Batch 95, Loss 16.84394524222926 Accuracy 1.0 Time 3.113s\n",
            " correct:  tensor(6144) Total:  6144\n",
            "Epoch 2, Batch 96, Loss 16.836858699719112 Accuracy 1.0 Time 3.131s\n",
            " correct:  tensor(6208) Total:  6208\n",
            "Epoch 2, Batch 97, Loss 16.83105442204426 Accuracy 1.0 Time 3.113s\n",
            " correct:  tensor(6272) Total:  6272\n",
            "Epoch 2, Batch 98, Loss 16.836666759179565 Accuracy 1.0 Time 3.054s\n",
            " correct:  tensor(6336) Total:  6336\n",
            "Epoch 2, Batch 99, Loss 16.825430725560043 Accuracy 1.0 Time 3.038s\n",
            " correct:  tensor(6400) Total:  6400\n",
            "Epoch 2, Batch 100, Loss 16.825489025115967 Accuracy 1.0 Time 3.12s\n",
            " correct:  tensor(6464) Total:  6464\n",
            "Epoch 2, Batch 101, Loss 16.823344636671614 Accuracy 1.0 Time 3.038s\n",
            " correct:  tensor(6528) Total:  6528\n",
            "Epoch 2, Batch 102, Loss 16.81329423306035 Accuracy 1.0 Time 3.07s\n",
            " correct:  tensor(6592) Total:  6592\n",
            "Epoch 2, Batch 103, Loss 16.805683534122206 Accuracy 1.0 Time 3.098s\n",
            " correct:  tensor(6656) Total:  6656\n",
            "Epoch 2, Batch 104, Loss 16.804415877048786 Accuracy 1.0 Time 3.089s\n",
            " correct:  tensor(6720) Total:  6720\n",
            "Epoch 2, Batch 105, Loss 16.80951150258382 Accuracy 1.0 Time 3.034s\n",
            " correct:  tensor(6784) Total:  6784\n",
            "Epoch 2, Batch 106, Loss 16.817872920126284 Accuracy 1.0 Time 3.079s\n",
            " correct:  tensor(6848) Total:  6848\n",
            "Epoch 2, Batch 107, Loss 16.81290040952023 Accuracy 1.0 Time 3.103s\n",
            " correct:  tensor(6912) Total:  6912\n",
            "Epoch 2, Batch 108, Loss 16.79963163976316 Accuracy 1.0 Time 3.143s\n",
            " correct:  tensor(6976) Total:  6976\n",
            "Epoch 2, Batch 109, Loss 16.801536218835672 Accuracy 1.0 Time 3.159s\n",
            " correct:  tensor(7040) Total:  7040\n",
            "Epoch 2, Batch 110, Loss 16.80203723040494 Accuracy 1.0 Time 3.121s\n",
            " correct:  tensor(7104) Total:  7104\n",
            "Epoch 2, Batch 111, Loss 16.795169289047653 Accuracy 1.0 Time 3.082s\n",
            " correct:  tensor(7168) Total:  7168\n",
            "Epoch 2, Batch 112, Loss 16.792492840971267 Accuracy 1.0 Time 3.047s\n",
            " correct:  tensor(7232) Total:  7232\n",
            "Epoch 2, Batch 113, Loss 16.781837176432653 Accuracy 1.0 Time 3.112s\n",
            " correct:  tensor(7296) Total:  7296\n",
            "Epoch 2, Batch 114, Loss 16.77744207884136 Accuracy 1.0 Time 3.118s\n",
            " correct:  tensor(7360) Total:  7360\n",
            "Epoch 2, Batch 115, Loss 16.774713582577913 Accuracy 1.0 Time 3.066s\n",
            " correct:  tensor(7424) Total:  7424\n",
            "Epoch 2, Batch 116, Loss 16.769886526568182 Accuracy 1.0 Time 3.076s\n",
            " correct:  tensor(7488) Total:  7488\n",
            "Epoch 2, Batch 117, Loss 16.76773333753276 Accuracy 1.0 Time 3.116s\n",
            " correct:  tensor(7552) Total:  7552\n",
            "Epoch 2, Batch 118, Loss 16.758891291537527 Accuracy 1.0 Time 3.064s\n",
            " correct:  tensor(7616) Total:  7616\n",
            "Epoch 2, Batch 119, Loss 16.74769754970775 Accuracy 1.0 Time 3.205s\n",
            " correct:  tensor(7680) Total:  7680\n",
            "Epoch 2, Batch 120, Loss 16.75194815794627 Accuracy 1.0 Time 3.173s\n",
            " correct:  tensor(7744) Total:  7744\n",
            "Epoch 2, Batch 121, Loss 16.74083543414912 Accuracy 1.0 Time 3.137s\n",
            " correct:  tensor(7808) Total:  7808\n",
            "Epoch 2, Batch 122, Loss 16.73069181598601 Accuracy 1.0 Time 3.112s\n",
            " correct:  tensor(7872) Total:  7872\n",
            "Epoch 2, Batch 123, Loss 16.733870064339985 Accuracy 1.0 Time 3.176s\n",
            " correct:  tensor(7936) Total:  7936\n",
            "Epoch 2, Batch 124, Loss 16.727537793497884 Accuracy 1.0 Time 3.215s\n",
            " correct:  tensor(8000) Total:  8000\n",
            "Epoch 2, Batch 125, Loss 16.729941917419435 Accuracy 1.0 Time 3.132s\n",
            " correct:  tensor(8064) Total:  8064\n",
            "Epoch 2, Batch 126, Loss 16.726373498401944 Accuracy 1.0 Time 3.115s\n",
            " correct:  tensor(8128) Total:  8128\n",
            "Epoch 2, Batch 127, Loss 16.726445273151548 Accuracy 1.0 Time 3.136s\n",
            " correct:  tensor(8192) Total:  8192\n",
            "Epoch 2, Batch 128, Loss 16.720622524619102 Accuracy 1.0 Time 3.147s\n",
            " correct:  tensor(8256) Total:  8256\n",
            "Epoch 2, Batch 129, Loss 16.714934859164927 Accuracy 1.0 Time 3.146s\n",
            " correct:  tensor(8320) Total:  8320\n",
            "Epoch 2, Batch 130, Loss 16.710795358511117 Accuracy 1.0 Time 3.22s\n",
            " correct:  tensor(8384) Total:  8384\n",
            "Epoch 2, Batch 131, Loss 16.711884534995974 Accuracy 1.0 Time 3.114s\n",
            " correct:  tensor(8448) Total:  8448\n",
            "Epoch 2, Batch 132, Loss 16.700926968545623 Accuracy 1.0 Time 3.101s\n",
            " correct:  tensor(8512) Total:  8512\n",
            "Epoch 2, Batch 133, Loss 16.701050148870713 Accuracy 1.0 Time 3.196s\n",
            " correct:  tensor(8576) Total:  8576\n",
            "Epoch 2, Batch 134, Loss 16.69030846410723 Accuracy 1.0 Time 3.111s\n",
            " correct:  tensor(8640) Total:  8640\n",
            "Epoch 2, Batch 135, Loss 16.68288984651919 Accuracy 1.0 Time 3.091s\n",
            " correct:  tensor(8704) Total:  8704\n",
            "Epoch 2, Batch 136, Loss 16.685744615162122 Accuracy 1.0 Time 3.103s\n",
            " correct:  tensor(8768) Total:  8768\n",
            "Epoch 2, Batch 137, Loss 16.679351583884582 Accuracy 1.0 Time 3.151s\n",
            " correct:  tensor(8832) Total:  8832\n",
            "Epoch 2, Batch 138, Loss 16.680876358695652 Accuracy 1.0 Time 3.182s\n",
            " correct:  tensor(8896) Total:  8896\n",
            "Epoch 2, Batch 139, Loss 16.680635397382776 Accuracy 1.0 Time 3.376s\n",
            " correct:  tensor(8960) Total:  8960\n",
            "Epoch 2, Batch 140, Loss 16.678306647709437 Accuracy 1.0 Time 3.361s\n",
            " correct:  tensor(9024) Total:  9024\n",
            "Epoch 2, Batch 141, Loss 16.670204730744057 Accuracy 1.0 Time 3.355s\n",
            " correct:  tensor(9088) Total:  9088\n",
            "Epoch 2, Batch 142, Loss 16.668286833964604 Accuracy 1.0 Time 3.137s\n",
            " correct:  tensor(9152) Total:  9152\n",
            "Epoch 2, Batch 143, Loss 16.670211165101378 Accuracy 1.0 Time 3.117s\n",
            " correct:  tensor(9216) Total:  9216\n",
            "Epoch 2, Batch 144, Loss 16.667003724310135 Accuracy 1.0 Time 3.093s\n",
            " correct:  tensor(9280) Total:  9280\n",
            "Epoch 2, Batch 145, Loss 16.67227249145508 Accuracy 1.0 Time 3.083s\n",
            " correct:  tensor(9344) Total:  9344\n",
            "Epoch 2, Batch 146, Loss 16.666038134326673 Accuracy 1.0 Time 3.149s\n",
            " correct:  tensor(9408) Total:  9408\n",
            "Epoch 2, Batch 147, Loss 16.664813255777165 Accuracy 1.0 Time 3.118s\n",
            " correct:  tensor(9472) Total:  9472\n",
            "Epoch 2, Batch 148, Loss 16.657875009485192 Accuracy 1.0 Time 3.176s\n",
            " correct:  tensor(9536) Total:  9536\n",
            "Epoch 2, Batch 149, Loss 16.65611887938224 Accuracy 1.0 Time 3.182s\n",
            " correct:  tensor(9600) Total:  9600\n",
            "Epoch 2, Batch 150, Loss 16.649081834157307 Accuracy 1.0 Time 3.147s\n",
            " correct:  tensor(9664) Total:  9664\n",
            "Epoch 2, Batch 151, Loss 16.64186398083011 Accuracy 1.0 Time 3.249s\n",
            " correct:  tensor(9728) Total:  9728\n",
            "Epoch 2, Batch 152, Loss 16.637463419060957 Accuracy 1.0 Time 3.157s\n",
            " correct:  tensor(9792) Total:  9792\n",
            "Epoch 2, Batch 153, Loss 16.63485053317999 Accuracy 1.0 Time 3.168s\n",
            " correct:  tensor(9856) Total:  9856\n",
            "Epoch 2, Batch 154, Loss 16.634671632345622 Accuracy 1.0 Time 3.15s\n",
            " correct:  tensor(9920) Total:  9920\n",
            "Epoch 2, Batch 155, Loss 16.63013592381631 Accuracy 1.0 Time 3.076s\n",
            " correct:  tensor(9984) Total:  9984\n",
            "Epoch 2, Batch 156, Loss 16.631926108629276 Accuracy 1.0 Time 3.156s\n",
            " correct:  tensor(10048) Total:  10048\n",
            "Epoch 2, Batch 157, Loss 16.636223543981078 Accuracy 1.0 Time 3.152s\n",
            " correct:  tensor(10112) Total:  10112\n",
            "Epoch 2, Batch 158, Loss 16.633811153943025 Accuracy 1.0 Time 3.149s\n",
            " correct:  tensor(10176) Total:  10176\n",
            "Epoch 2, Batch 159, Loss 16.631617060247457 Accuracy 1.0 Time 3.14s\n",
            " correct:  tensor(10240) Total:  10240\n",
            "Epoch 2, Batch 160, Loss 16.634753715991973 Accuracy 1.0 Time 3.126s\n",
            " correct:  tensor(10304) Total:  10304\n",
            "Epoch 2, Batch 161, Loss 16.627719209801338 Accuracy 1.0 Time 3.138s\n",
            " correct:  tensor(10368) Total:  10368\n",
            "Epoch 2, Batch 162, Loss 16.62584749268897 Accuracy 1.0 Time 3.123s\n",
            " correct:  tensor(10432) Total:  10432\n",
            "Epoch 2, Batch 163, Loss 16.624579710463074 Accuracy 1.0 Time 3.153s\n",
            " correct:  tensor(10496) Total:  10496\n",
            "Epoch 2, Batch 164, Loss 16.62161546218686 Accuracy 1.0 Time 3.178s\n",
            " correct:  tensor(10560) Total:  10560\n",
            "Epoch 2, Batch 165, Loss 16.61994801723596 Accuracy 1.0 Time 3.096s\n",
            " correct:  tensor(10624) Total:  10624\n",
            "Epoch 2, Batch 166, Loss 16.623095633035682 Accuracy 1.0 Time 3.116s\n",
            " correct:  tensor(10688) Total:  10688\n",
            "Epoch 2, Batch 167, Loss 16.62174457275939 Accuracy 1.0 Time 3.17s\n",
            " correct:  tensor(10752) Total:  10752\n",
            "Epoch 2, Batch 168, Loss 16.61918864363716 Accuracy 1.0 Time 3.119s\n",
            " correct:  tensor(10816) Total:  10816\n",
            "Epoch 2, Batch 169, Loss 16.6227240308502 Accuracy 1.0 Time 3.23s\n",
            " correct:  tensor(10880) Total:  10880\n",
            "Epoch 2, Batch 170, Loss 16.61860227023854 Accuracy 1.0 Time 3.361s\n",
            " correct:  tensor(10944) Total:  10944\n",
            "Epoch 2, Batch 171, Loss 16.616325088411745 Accuracy 1.0 Time 3.172s\n",
            " correct:  tensor(11008) Total:  11008\n",
            "Epoch 2, Batch 172, Loss 16.610394455665766 Accuracy 1.0 Time 3.134s\n",
            " correct:  tensor(11072) Total:  11072\n",
            "Epoch 2, Batch 173, Loss 16.609912155680572 Accuracy 1.0 Time 3.103s\n",
            " correct:  tensor(11136) Total:  11136\n",
            "Epoch 2, Batch 174, Loss 16.61277874036767 Accuracy 1.0 Time 3.08s\n",
            " correct:  tensor(11200) Total:  11200\n",
            "Epoch 2, Batch 175, Loss 16.613516605922154 Accuracy 1.0 Time 3.086s\n",
            " correct:  tensor(11264) Total:  11264\n",
            "Epoch 2, Batch 176, Loss 16.613714066418733 Accuracy 1.0 Time 3.187s\n",
            " correct:  tensor(11328) Total:  11328\n",
            "Epoch 2, Batch 177, Loss 16.609740833778165 Accuracy 1.0 Time 3.15s\n",
            " correct:  tensor(11392) Total:  11392\n",
            "Epoch 2, Batch 178, Loss 16.601773674568435 Accuracy 1.0 Time 3.102s\n",
            " correct:  tensor(11456) Total:  11456\n",
            "Epoch 2, Batch 179, Loss 16.599641048708442 Accuracy 1.0 Time 3.169s\n",
            " correct:  tensor(11520) Total:  11520\n",
            "Epoch 2, Batch 180, Loss 16.602740663952297 Accuracy 1.0 Time 3.15s\n",
            " correct:  tensor(11584) Total:  11584\n",
            "Epoch 2, Batch 181, Loss 16.608321548166856 Accuracy 1.0 Time 3.095s\n",
            " correct:  tensor(11648) Total:  11648\n",
            "Epoch 2, Batch 182, Loss 16.60720187491113 Accuracy 1.0 Time 3.059s\n",
            " correct:  tensor(11712) Total:  11712\n",
            "Epoch 2, Batch 183, Loss 16.604242976245985 Accuracy 1.0 Time 3.144s\n",
            " correct:  tensor(11776) Total:  11776\n",
            "Epoch 2, Batch 184, Loss 16.6030135932176 Accuracy 1.0 Time 3.119s\n",
            " correct:  tensor(11840) Total:  11840\n",
            "Epoch 2, Batch 185, Loss 16.604341759552828 Accuracy 1.0 Time 3.127s\n",
            " correct:  tensor(11904) Total:  11904\n",
            "Epoch 2, Batch 186, Loss 16.60557714585335 Accuracy 1.0 Time 3.119s\n",
            " correct:  tensor(11968) Total:  11968\n",
            "Epoch 2, Batch 187, Loss 16.605138375797374 Accuracy 1.0 Time 3.112s\n",
            " correct:  tensor(12032) Total:  12032\n",
            "Epoch 2, Batch 188, Loss 16.601965138252744 Accuracy 1.0 Time 3.158s\n",
            " correct:  tensor(12096) Total:  12096\n",
            "Epoch 2, Batch 189, Loss 16.600149876226194 Accuracy 1.0 Time 3.077s\n",
            " correct:  tensor(12160) Total:  12160\n",
            "Epoch 2, Batch 190, Loss 16.59066720259817 Accuracy 1.0 Time 3.126s\n",
            " correct:  tensor(12224) Total:  12224\n",
            "Epoch 2, Batch 191, Loss 16.580123596790575 Accuracy 1.0 Time 3.08s\n",
            " correct:  tensor(12288) Total:  12288\n",
            "Epoch 2, Batch 192, Loss 16.576120257377625 Accuracy 1.0 Time 3.125s\n",
            " correct:  tensor(12352) Total:  12352\n",
            "Epoch 2, Batch 193, Loss 16.578606561057924 Accuracy 1.0 Time 3.09s\n",
            " correct:  tensor(12416) Total:  12416\n",
            "Epoch 2, Batch 194, Loss 16.586005486163895 Accuracy 1.0 Time 3.065s\n",
            " correct:  tensor(12480) Total:  12480\n",
            "Epoch 2, Batch 195, Loss 16.584215643467047 Accuracy 1.0 Time 3.096s\n",
            " correct:  tensor(12544) Total:  12544\n",
            "Epoch 2, Batch 196, Loss 16.587745452413753 Accuracy 1.0 Time 3.093s\n",
            " correct:  tensor(12608) Total:  12608\n",
            "Epoch 2, Batch 197, Loss 16.585461398671725 Accuracy 1.0 Time 3.125s\n",
            " correct:  tensor(12672) Total:  12672\n",
            "Epoch 2, Batch 198, Loss 16.586479764996152 Accuracy 1.0 Time 3.063s\n",
            " correct:  tensor(12736) Total:  12736\n",
            "Epoch 2, Batch 199, Loss 16.58868657404454 Accuracy 1.0 Time 3.088s\n",
            " correct:  tensor(12800) Total:  12800\n",
            "Epoch 2, Batch 200, Loss 16.592420921325683 Accuracy 1.0 Time 3.058s\n",
            " correct:  tensor(12864) Total:  12864\n",
            "Epoch 2, Batch 201, Loss 16.58941581593224 Accuracy 1.0 Time 3.064s\n",
            " correct:  tensor(12928) Total:  12928\n",
            "Epoch 2, Batch 202, Loss 16.59063194293787 Accuracy 1.0 Time 3.063s\n",
            " correct:  tensor(12992) Total:  12992\n",
            "Epoch 2, Batch 203, Loss 16.586573018229068 Accuracy 1.0 Time 3.123s\n",
            " correct:  tensor(13056) Total:  13056\n",
            "Epoch 2, Batch 204, Loss 16.58662361724704 Accuracy 1.0 Time 3.093s\n",
            " correct:  tensor(13120) Total:  13120\n",
            "Epoch 2, Batch 205, Loss 16.584069931216355 Accuracy 1.0 Time 3.136s\n",
            " correct:  tensor(13184) Total:  13184\n",
            "Epoch 2, Batch 206, Loss 16.581388839240212 Accuracy 1.0 Time 3.151s\n",
            " correct:  tensor(13248) Total:  13248\n",
            "Epoch 2, Batch 207, Loss 16.58027944242321 Accuracy 1.0 Time 3.101s\n",
            " correct:  tensor(13312) Total:  13312\n",
            "Epoch 2, Batch 208, Loss 16.583020911766933 Accuracy 1.0 Time 3.124s\n",
            " correct:  tensor(13376) Total:  13376\n",
            "Epoch 2, Batch 209, Loss 16.58074157660087 Accuracy 1.0 Time 3.136s\n",
            " correct:  tensor(13440) Total:  13440\n",
            "Epoch 2, Batch 210, Loss 16.57852800005958 Accuracy 1.0 Time 3.1s\n",
            " correct:  tensor(13504) Total:  13504\n",
            "Epoch 2, Batch 211, Loss 16.57932554037085 Accuracy 1.0 Time 3.159s\n",
            " correct:  tensor(13568) Total:  13568\n",
            "Epoch 2, Batch 212, Loss 16.57481866062812 Accuracy 1.0 Time 3.127s\n",
            " correct:  tensor(13632) Total:  13632\n",
            "Epoch 2, Batch 213, Loss 16.572299737885526 Accuracy 1.0 Time 3.119s\n",
            " correct:  tensor(13696) Total:  13696\n",
            "Epoch 2, Batch 214, Loss 16.57260742811399 Accuracy 1.0 Time 3.199s\n",
            " correct:  tensor(13760) Total:  13760\n",
            "Epoch 2, Batch 215, Loss 16.567199298947358 Accuracy 1.0 Time 3.1s\n",
            " correct:  tensor(13824) Total:  13824\n",
            "Epoch 2, Batch 216, Loss 16.56239409799929 Accuracy 1.0 Time 3.136s\n",
            " correct:  tensor(13888) Total:  13888\n",
            "Epoch 2, Batch 217, Loss 16.560557387391544 Accuracy 1.0 Time 3.155s\n",
            " correct:  tensor(13952) Total:  13952\n",
            "Epoch 2, Batch 218, Loss 16.554617365565868 Accuracy 1.0 Time 3.124s\n",
            " correct:  tensor(14016) Total:  14016\n",
            "Epoch 2, Batch 219, Loss 16.55082954772531 Accuracy 1.0 Time 3.151s\n",
            " correct:  tensor(14080) Total:  14080\n",
            "Epoch 2, Batch 220, Loss 16.560787560723046 Accuracy 1.0 Time 3.136s\n",
            " correct:  tensor(14144) Total:  14144\n",
            "Epoch 2, Batch 221, Loss 16.56252707830921 Accuracy 1.0 Time 3.094s\n",
            " correct:  tensor(14208) Total:  14208\n",
            "Epoch 2, Batch 222, Loss 16.56106694968971 Accuracy 1.0 Time 3.111s\n",
            " correct:  tensor(14272) Total:  14272\n",
            "Epoch 2, Batch 223, Loss 16.558936846095886 Accuracy 1.0 Time 3.156s\n",
            " correct:  tensor(14336) Total:  14336\n",
            "Epoch 2, Batch 224, Loss 16.55749507887023 Accuracy 1.0 Time 3.12s\n",
            " correct:  tensor(14400) Total:  14400\n",
            "Epoch 2, Batch 225, Loss 16.561055988735625 Accuracy 1.0 Time 3.197s\n",
            " correct:  tensor(14464) Total:  14464\n",
            "Epoch 2, Batch 226, Loss 16.564124195976596 Accuracy 1.0 Time 3.123s\n",
            " correct:  tensor(14528) Total:  14528\n",
            "Epoch 2, Batch 227, Loss 16.562290943666703 Accuracy 1.0 Time 3.162s\n",
            " correct:  tensor(14592) Total:  14592\n",
            "Epoch 2, Batch 228, Loss 16.561718535005 Accuracy 1.0 Time 3.099s\n",
            " correct:  tensor(14656) Total:  14656\n",
            "Epoch 2, Batch 229, Loss 16.562414315069606 Accuracy 1.0 Time 3.147s\n",
            " correct:  tensor(14720) Total:  14720\n",
            "Epoch 2, Batch 230, Loss 16.561402797698975 Accuracy 1.0 Time 3.155s\n",
            " correct:  tensor(14784) Total:  14784\n",
            "Epoch 2, Batch 231, Loss 16.566836406658222 Accuracy 1.0 Time 3.124s\n",
            " correct:  tensor(14848) Total:  14848\n",
            "Epoch 2, Batch 232, Loss 16.565293242191448 Accuracy 1.0 Time 3.132s\n",
            " correct:  tensor(14912) Total:  14912\n",
            "Epoch 2, Batch 233, Loss 16.564093630712943 Accuracy 1.0 Time 3.252s\n",
            " correct:  tensor(14976) Total:  14976\n",
            "Epoch 2, Batch 234, Loss 16.563706393934723 Accuracy 1.0 Time 3.175s\n",
            " correct:  tensor(15040) Total:  15040\n",
            "Epoch 2, Batch 235, Loss 16.56417996426846 Accuracy 1.0 Time 3.237s\n",
            " correct:  tensor(15104) Total:  15104\n",
            "Epoch 2, Batch 236, Loss 16.55989193108122 Accuracy 1.0 Time 3.176s\n",
            " correct:  tensor(15168) Total:  15168\n",
            "Epoch 2, Batch 237, Loss 16.56246489915164 Accuracy 1.0 Time 3.254s\n",
            " correct:  tensor(15232) Total:  15232\n",
            "Epoch 2, Batch 238, Loss 16.566377395341377 Accuracy 1.0 Time 3.322s\n",
            " correct:  tensor(15296) Total:  15296\n",
            "Epoch 2, Batch 239, Loss 16.56853444805704 Accuracy 1.0 Time 3.346s\n",
            " correct:  tensor(15360) Total:  15360\n",
            "Epoch 2, Batch 240, Loss 16.566482150554656 Accuracy 1.0 Time 3.24s\n",
            " correct:  tensor(15424) Total:  15424\n",
            "Epoch 2, Batch 241, Loss 16.565921676604084 Accuracy 1.0 Time 3.135s\n",
            " correct:  tensor(15488) Total:  15488\n",
            "Epoch 2, Batch 242, Loss 16.569178411783266 Accuracy 1.0 Time 3.125s\n",
            " correct:  tensor(15552) Total:  15552\n",
            "Epoch 2, Batch 243, Loss 16.56677797105577 Accuracy 1.0 Time 3.08s\n",
            " correct:  tensor(15616) Total:  15616\n",
            "Epoch 2, Batch 244, Loss 16.568756197319654 Accuracy 1.0 Time 3.169s\n",
            " correct:  tensor(15680) Total:  15680\n",
            "Epoch 2, Batch 245, Loss 16.569708057325713 Accuracy 1.0 Time 3.164s\n",
            " correct:  tensor(15744) Total:  15744\n",
            "Epoch 2, Batch 246, Loss 16.57032471168332 Accuracy 1.0 Time 3.226s\n",
            " correct:  tensor(15808) Total:  15808\n",
            "Epoch 2, Batch 247, Loss 16.567346395268615 Accuracy 1.0 Time 3.083s\n",
            " correct:  tensor(15872) Total:  15872\n",
            "Epoch 2, Batch 248, Loss 16.565423442471413 Accuracy 1.0 Time 3.116s\n",
            " correct:  tensor(15936) Total:  15936\n",
            "Epoch 2, Batch 249, Loss 16.56243851673172 Accuracy 1.0 Time 3.147s\n",
            " correct:  tensor(16000) Total:  16000\n",
            "Epoch 2, Batch 250, Loss 16.560679027557374 Accuracy 1.0 Time 3.107s\n",
            " correct:  tensor(16064) Total:  16064\n",
            "Epoch 2, Batch 251, Loss 16.56022081337127 Accuracy 1.0 Time 3.129s\n",
            " correct:  tensor(16128) Total:  16128\n",
            "Epoch 2, Batch 252, Loss 16.560951743807113 Accuracy 1.0 Time 3.165s\n",
            " correct:  tensor(16192) Total:  16192\n",
            "Epoch 2, Batch 253, Loss 16.55743116823581 Accuracy 1.0 Time 3.106s\n",
            " correct:  tensor(16256) Total:  16256\n",
            "Epoch 2, Batch 254, Loss 16.554582317983073 Accuracy 1.0 Time 3.118s\n",
            " correct:  tensor(16320) Total:  16320\n",
            "Epoch 2, Batch 255, Loss 16.55117224898993 Accuracy 1.0 Time 3.038s\n",
            " correct:  tensor(16384) Total:  16384\n",
            "Epoch 2, Batch 256, Loss 16.549470022320747 Accuracy 1.0 Time 3.105s\n",
            " correct:  tensor(16448) Total:  16448\n",
            "Epoch 2, Batch 257, Loss 16.546613040137384 Accuracy 1.0 Time 3.125s\n",
            " correct:  tensor(16512) Total:  16512\n",
            "Epoch 2, Batch 258, Loss 16.543740483217462 Accuracy 1.0 Time 3.064s\n",
            " correct:  tensor(16576) Total:  16576\n",
            "Epoch 2, Batch 259, Loss 16.543387597131915 Accuracy 1.0 Time 3.137s\n",
            " correct:  tensor(16640) Total:  16640\n",
            "Epoch 2, Batch 260, Loss 16.54279688688425 Accuracy 1.0 Time 3.092s\n",
            " correct:  tensor(16704) Total:  16704\n",
            "Epoch 2, Batch 261, Loss 16.5422198452712 Accuracy 1.0 Time 3.083s\n",
            " correct:  tensor(16768) Total:  16768\n",
            "Epoch 2, Batch 262, Loss 16.53847430862543 Accuracy 1.0 Time 3.074s\n",
            " correct:  tensor(16832) Total:  16832\n",
            "Epoch 2, Batch 263, Loss 16.54423533827633 Accuracy 1.0 Time 3.08s\n",
            " correct:  tensor(16896) Total:  16896\n",
            "Epoch 2, Batch 264, Loss 16.54203422864278 Accuracy 1.0 Time 3.088s\n",
            " correct:  tensor(16960) Total:  16960\n",
            "Epoch 2, Batch 265, Loss 16.54088446419194 Accuracy 1.0 Time 3.166s\n",
            " correct:  tensor(17024) Total:  17024\n",
            "Epoch 2, Batch 266, Loss 16.539442894154025 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(17088) Total:  17088\n",
            "Epoch 2, Batch 267, Loss 16.535683196135675 Accuracy 1.0 Time 3.348s\n",
            " correct:  tensor(17152) Total:  17152\n",
            "Epoch 2, Batch 268, Loss 16.536424946429126 Accuracy 1.0 Time 3.218s\n",
            " correct:  tensor(17216) Total:  17216\n",
            "Epoch 2, Batch 269, Loss 16.536614354215146 Accuracy 1.0 Time 3.124s\n",
            " correct:  tensor(17280) Total:  17280\n",
            "Epoch 2, Batch 270, Loss 16.533657105763755 Accuracy 1.0 Time 3.104s\n",
            " correct:  tensor(17344) Total:  17344\n",
            "Epoch 2, Batch 271, Loss 16.53174616753835 Accuracy 1.0 Time 3.192s\n",
            " correct:  tensor(17408) Total:  17408\n",
            "Epoch 2, Batch 272, Loss 16.5313780132462 Accuracy 1.0 Time 3.127s\n",
            " correct:  tensor(17472) Total:  17472\n",
            "Epoch 2, Batch 273, Loss 16.53281183557196 Accuracy 1.0 Time 3.17s\n",
            " correct:  tensor(17536) Total:  17536\n",
            "Epoch 2, Batch 274, Loss 16.538247077134404 Accuracy 1.0 Time 3.147s\n",
            " correct:  tensor(17600) Total:  17600\n",
            "Epoch 2, Batch 275, Loss 16.533785868558017 Accuracy 1.0 Time 3.052s\n",
            " correct:  tensor(17664) Total:  17664\n",
            "Epoch 2, Batch 276, Loss 16.533616421879202 Accuracy 1.0 Time 3.181s\n",
            " correct:  tensor(17728) Total:  17728\n",
            "Epoch 2, Batch 277, Loss 16.528853533500367 Accuracy 1.0 Time 3.159s\n",
            " correct:  tensor(17792) Total:  17792\n",
            "Epoch 2, Batch 278, Loss 16.528376815988004 Accuracy 1.0 Time 3.186s\n",
            " correct:  tensor(17856) Total:  17856\n",
            "Epoch 2, Batch 279, Loss 16.52635539133489 Accuracy 1.0 Time 3.135s\n",
            " correct:  tensor(17920) Total:  17920\n",
            "Epoch 2, Batch 280, Loss 16.521100953647068 Accuracy 1.0 Time 3.11s\n",
            " correct:  tensor(17984) Total:  17984\n",
            "Epoch 2, Batch 281, Loss 16.520340403628094 Accuracy 1.0 Time 3.203s\n",
            " correct:  tensor(18048) Total:  18048\n",
            "Epoch 2, Batch 282, Loss 16.52377459005261 Accuracy 1.0 Time 3.129s\n",
            " correct:  tensor(18112) Total:  18112\n",
            "Epoch 2, Batch 283, Loss 16.525035201028885 Accuracy 1.0 Time 3.103s\n",
            " correct:  tensor(18176) Total:  18176\n",
            "Epoch 2, Batch 284, Loss 16.5254799245109 Accuracy 1.0 Time 3.124s\n",
            " correct:  tensor(18240) Total:  18240\n",
            "Epoch 2, Batch 285, Loss 16.524056079931427 Accuracy 1.0 Time 3.089s\n",
            " correct:  tensor(18304) Total:  18304\n",
            "Epoch 2, Batch 286, Loss 16.529901094369954 Accuracy 1.0 Time 3.089s\n",
            " correct:  tensor(18368) Total:  18368\n",
            "Epoch 2, Batch 287, Loss 16.528511708621778 Accuracy 1.0 Time 3.04s\n",
            " correct:  tensor(18432) Total:  18432\n",
            "Epoch 2, Batch 288, Loss 16.52642954720391 Accuracy 1.0 Time 3.077s\n",
            " correct:  tensor(18496) Total:  18496\n",
            "Epoch 2, Batch 289, Loss 16.526642584883216 Accuracy 1.0 Time 3.07s\n",
            " correct:  tensor(18560) Total:  18560\n",
            "Epoch 2, Batch 290, Loss 16.52590720078041 Accuracy 1.0 Time 3.166s\n",
            " correct:  tensor(18624) Total:  18624\n",
            "Epoch 2, Batch 291, Loss 16.52516141544093 Accuracy 1.0 Time 3.183s\n",
            " correct:  tensor(18688) Total:  18688\n",
            "Epoch 2, Batch 292, Loss 16.522666803778034 Accuracy 1.0 Time 3.098s\n",
            " correct:  tensor(18752) Total:  18752\n",
            "Epoch 2, Batch 293, Loss 16.521717224511676 Accuracy 1.0 Time 3.129s\n",
            " correct:  tensor(18816) Total:  18816\n",
            "Epoch 2, Batch 294, Loss 16.51997170156362 Accuracy 1.0 Time 3.076s\n",
            " correct:  tensor(18880) Total:  18880\n",
            "Epoch 2, Batch 295, Loss 16.517682492530952 Accuracy 1.0 Time 3.04s\n",
            " correct:  tensor(18944) Total:  18944\n",
            "Epoch 2, Batch 296, Loss 16.515355764208614 Accuracy 1.0 Time 3.03s\n",
            " correct:  tensor(19008) Total:  19008\n",
            "Epoch 2, Batch 297, Loss 16.520574746308505 Accuracy 1.0 Time 3.062s\n",
            " correct:  tensor(19072) Total:  19072\n",
            "Epoch 2, Batch 298, Loss 16.518170743980665 Accuracy 1.0 Time 3.117s\n",
            " correct:  tensor(19136) Total:  19136\n",
            "Epoch 2, Batch 299, Loss 16.516962488359432 Accuracy 1.0 Time 3.117s\n",
            " correct:  tensor(19200) Total:  19200\n",
            "Epoch 2, Batch 300, Loss 16.516228030522665 Accuracy 1.0 Time 3.048s\n",
            " correct:  tensor(19264) Total:  19264\n",
            "Epoch 2, Batch 301, Loss 16.516655484703293 Accuracy 1.0 Time 3.012s\n",
            " correct:  tensor(19328) Total:  19328\n",
            "Epoch 2, Batch 302, Loss 16.51808733971703 Accuracy 1.0 Time 3.135s\n",
            " correct:  tensor(19392) Total:  19392\n",
            "Epoch 2, Batch 303, Loss 16.51771336734885 Accuracy 1.0 Time 3.109s\n",
            " correct:  tensor(19456) Total:  19456\n",
            "Epoch 2, Batch 304, Loss 16.51778448569147 Accuracy 1.0 Time 3.065s\n",
            " correct:  tensor(19520) Total:  19520\n",
            "Epoch 2, Batch 305, Loss 16.51847505100438 Accuracy 1.0 Time 3.033s\n",
            " correct:  tensor(19584) Total:  19584\n",
            "Epoch 2, Batch 306, Loss 16.513976916768193 Accuracy 1.0 Time 3.077s\n",
            " correct:  tensor(19648) Total:  19648\n",
            "Epoch 2, Batch 307, Loss 16.514296488187213 Accuracy 1.0 Time 2.995s\n",
            " correct:  tensor(19712) Total:  19712\n",
            "Epoch 2, Batch 308, Loss 16.51195013677919 Accuracy 1.0 Time 3.061s\n",
            " correct:  tensor(19776) Total:  19776\n",
            "Epoch 2, Batch 309, Loss 16.510992497687973 Accuracy 1.0 Time 3.034s\n",
            " correct:  tensor(19840) Total:  19840\n",
            "Epoch 2, Batch 310, Loss 16.509605706122613 Accuracy 1.0 Time 3.104s\n",
            " correct:  tensor(19904) Total:  19904\n",
            "Epoch 2, Batch 311, Loss 16.51026673332288 Accuracy 1.0 Time 3.05s\n",
            " correct:  tensor(19968) Total:  19968\n",
            "Epoch 2, Batch 312, Loss 16.51230500600277 Accuracy 1.0 Time 3.05s\n",
            " correct:  tensor(20032) Total:  20032\n",
            "Epoch 2, Batch 313, Loss 16.51370903782951 Accuracy 1.0 Time 3.098s\n",
            " correct:  tensor(20096) Total:  20096\n",
            "Epoch 2, Batch 314, Loss 16.514605555564735 Accuracy 1.0 Time 3.12s\n",
            " correct:  tensor(20160) Total:  20160\n",
            "Epoch 2, Batch 315, Loss 16.51364926232232 Accuracy 1.0 Time 3.118s\n",
            " correct:  tensor(20224) Total:  20224\n",
            "Epoch 2, Batch 316, Loss 16.512966406496265 Accuracy 1.0 Time 3.103s\n",
            " correct:  tensor(20288) Total:  20288\n",
            "Epoch 2, Batch 317, Loss 16.510098664918534 Accuracy 1.0 Time 3.136s\n",
            " correct:  tensor(20352) Total:  20352\n",
            "Epoch 2, Batch 318, Loss 16.511403683596438 Accuracy 1.0 Time 3.109s\n",
            " correct:  tensor(20416) Total:  20416\n",
            "Epoch 2, Batch 319, Loss 16.5122496326889 Accuracy 1.0 Time 3.148s\n",
            " correct:  tensor(20480) Total:  20480\n",
            "Epoch 2, Batch 320, Loss 16.511919152736663 Accuracy 1.0 Time 3.08s\n",
            " correct:  tensor(20544) Total:  20544\n",
            "Epoch 2, Batch 321, Loss 16.51327116541402 Accuracy 1.0 Time 3.115s\n",
            " correct:  tensor(20608) Total:  20608\n",
            "Epoch 2, Batch 322, Loss 16.510472667883644 Accuracy 1.0 Time 3.145s\n",
            " correct:  tensor(20672) Total:  20672\n",
            "Epoch 2, Batch 323, Loss 16.51303921100156 Accuracy 1.0 Time 3.119s\n",
            " correct:  tensor(20736) Total:  20736\n",
            "Epoch 2, Batch 324, Loss 16.51166993306007 Accuracy 1.0 Time 3.15s\n",
            " correct:  tensor(20800) Total:  20800\n",
            "Epoch 2, Batch 325, Loss 16.512422323960525 Accuracy 1.0 Time 3.161s\n",
            " correct:  tensor(20864) Total:  20864\n",
            "Epoch 2, Batch 326, Loss 16.512542317981367 Accuracy 1.0 Time 3.118s\n",
            " correct:  tensor(20928) Total:  20928\n",
            "Epoch 2, Batch 327, Loss 16.51117295373106 Accuracy 1.0 Time 3.169s\n",
            " correct:  tensor(20992) Total:  20992\n",
            "Epoch 2, Batch 328, Loss 16.50750104973956 Accuracy 1.0 Time 3.195s\n",
            " correct:  tensor(21056) Total:  21056\n",
            "Epoch 2, Batch 329, Loss 16.506506853190597 Accuracy 1.0 Time 3.153s\n",
            " correct:  tensor(21120) Total:  21120\n",
            "Epoch 2, Batch 330, Loss 16.502425367181953 Accuracy 1.0 Time 3.075s\n",
            " correct:  tensor(21184) Total:  21184\n",
            "Epoch 2, Batch 331, Loss 16.500721288951862 Accuracy 1.0 Time 3.172s\n",
            " correct:  tensor(21248) Total:  21248\n",
            "Epoch 2, Batch 332, Loss 16.499724296202142 Accuracy 1.0 Time 3.15s\n",
            " correct:  tensor(21312) Total:  21312\n",
            "Epoch 2, Batch 333, Loss 16.499753384976774 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(21376) Total:  21376\n",
            "Epoch 2, Batch 334, Loss 16.498176860238264 Accuracy 1.0 Time 3.162s\n",
            " correct:  tensor(21440) Total:  21440\n",
            "Epoch 2, Batch 335, Loss 16.49993427333547 Accuracy 1.0 Time 3.113s\n",
            " correct:  tensor(21504) Total:  21504\n",
            "Epoch 2, Batch 336, Loss 16.499821214448836 Accuracy 1.0 Time 3.175s\n",
            " correct:  tensor(21568) Total:  21568\n",
            "Epoch 2, Batch 337, Loss 16.500349656056933 Accuracy 1.0 Time 3.378s\n",
            " correct:  tensor(21632) Total:  21632\n",
            "Epoch 2, Batch 338, Loss 16.49769173571344 Accuracy 1.0 Time 3.369s\n",
            " correct:  tensor(21696) Total:  21696\n",
            "Epoch 2, Batch 339, Loss 16.49663584773871 Accuracy 1.0 Time 3.31s\n",
            " correct:  tensor(21760) Total:  21760\n",
            "Epoch 2, Batch 340, Loss 16.499281361523796 Accuracy 1.0 Time 3.157s\n",
            " correct:  tensor(21824) Total:  21824\n",
            "Epoch 2, Batch 341, Loss 16.50099464036153 Accuracy 1.0 Time 3.155s\n",
            " correct:  tensor(21888) Total:  21888\n",
            "Epoch 2, Batch 342, Loss 16.502649608411286 Accuracy 1.0 Time 3.256s\n",
            " correct:  tensor(21952) Total:  21952\n",
            "Epoch 2, Batch 343, Loss 16.506839657663953 Accuracy 1.0 Time 3.121s\n",
            " correct:  tensor(22016) Total:  22016\n",
            "Epoch 2, Batch 344, Loss 16.505957642266917 Accuracy 1.0 Time 3.239s\n",
            " correct:  tensor(22080) Total:  22080\n",
            "Epoch 2, Batch 345, Loss 16.502478071572124 Accuracy 1.0 Time 3.171s\n",
            " correct:  tensor(22144) Total:  22144\n",
            "Epoch 2, Batch 346, Loss 16.50301879265405 Accuracy 1.0 Time 3.189s\n",
            " correct:  tensor(22208) Total:  22208\n",
            "Epoch 2, Batch 347, Loss 16.501406771305316 Accuracy 1.0 Time 3.15s\n",
            " correct:  tensor(22272) Total:  22272\n",
            "Epoch 2, Batch 348, Loss 16.49904684088696 Accuracy 1.0 Time 3.212s\n",
            " correct:  tensor(22336) Total:  22336\n",
            "Epoch 2, Batch 349, Loss 16.49942062509094 Accuracy 1.0 Time 3.172s\n",
            " correct:  tensor(22400) Total:  22400\n",
            "Epoch 2, Batch 350, Loss 16.498224051339285 Accuracy 1.0 Time 3.148s\n",
            " correct:  tensor(22464) Total:  22464\n",
            "Epoch 2, Batch 351, Loss 16.502206074206576 Accuracy 1.0 Time 3.15s\n",
            " correct:  tensor(22528) Total:  22528\n",
            "Epoch 2, Batch 352, Loss 16.500645504756406 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(22592) Total:  22592\n",
            "Epoch 2, Batch 353, Loss 16.49916670477762 Accuracy 1.0 Time 3.126s\n",
            " correct:  tensor(22656) Total:  22656\n",
            "Epoch 2, Batch 354, Loss 16.49736848658761 Accuracy 1.0 Time 3.19s\n",
            " correct:  tensor(22720) Total:  22720\n",
            "Epoch 2, Batch 355, Loss 16.497219120616645 Accuracy 1.0 Time 3.099s\n",
            " correct:  tensor(22784) Total:  22784\n",
            "Epoch 2, Batch 356, Loss 16.494802472296726 Accuracy 1.0 Time 3.143s\n",
            " correct:  tensor(22848) Total:  22848\n",
            "Epoch 2, Batch 357, Loss 16.49760660069997 Accuracy 1.0 Time 3.11s\n",
            " correct:  tensor(22912) Total:  22912\n",
            "Epoch 2, Batch 358, Loss 16.49642232809653 Accuracy 1.0 Time 3.125s\n",
            " correct:  tensor(22976) Total:  22976\n",
            "Epoch 2, Batch 359, Loss 16.49436243795751 Accuracy 1.0 Time 3.132s\n",
            " correct:  tensor(23040) Total:  23040\n",
            "Epoch 2, Batch 360, Loss 16.49404096338484 Accuracy 1.0 Time 3.162s\n",
            " correct:  tensor(23104) Total:  23104\n",
            "Epoch 2, Batch 361, Loss 16.493032204477412 Accuracy 1.0 Time 3.132s\n",
            " correct:  tensor(23168) Total:  23168\n",
            "Epoch 2, Batch 362, Loss 16.49238959201792 Accuracy 1.0 Time 3.119s\n",
            " correct:  tensor(23232) Total:  23232\n",
            "Epoch 2, Batch 363, Loss 16.496059956301014 Accuracy 1.0 Time 3.063s\n",
            " correct:  tensor(23296) Total:  23296\n",
            "Epoch 2, Batch 364, Loss 16.492459326000002 Accuracy 1.0 Time 3.307s\n",
            " correct:  tensor(23360) Total:  23360\n",
            "Epoch 2, Batch 365, Loss 16.490853116283677 Accuracy 1.0 Time 3.318s\n",
            " correct:  tensor(23424) Total:  23424\n",
            "Epoch 2, Batch 366, Loss 16.48921042583028 Accuracy 1.0 Time 3.157s\n",
            " correct:  tensor(23488) Total:  23488\n",
            "Epoch 2, Batch 367, Loss 16.489392059700364 Accuracy 1.0 Time 3.175s\n",
            " correct:  tensor(23552) Total:  23552\n",
            "Epoch 2, Batch 368, Loss 16.488517118536908 Accuracy 1.0 Time 3.098s\n",
            " correct:  tensor(23616) Total:  23616\n",
            "Epoch 2, Batch 369, Loss 16.488336185775797 Accuracy 1.0 Time 3.116s\n",
            " correct:  tensor(23680) Total:  23680\n",
            "Epoch 2, Batch 370, Loss 16.48825803705164 Accuracy 1.0 Time 3.102s\n",
            " correct:  tensor(23744) Total:  23744\n",
            "Epoch 2, Batch 371, Loss 16.484731918396655 Accuracy 1.0 Time 3.068s\n",
            " correct:  tensor(23808) Total:  23808\n",
            "Epoch 2, Batch 372, Loss 16.483636915042837 Accuracy 1.0 Time 3.104s\n",
            " correct:  tensor(23872) Total:  23872\n",
            "Epoch 2, Batch 373, Loss 16.48230232310359 Accuracy 1.0 Time 3.118s\n",
            " correct:  tensor(23936) Total:  23936\n",
            "Epoch 2, Batch 374, Loss 16.485388115765577 Accuracy 1.0 Time 3.076s\n",
            " correct:  tensor(24000) Total:  24000\n",
            "Epoch 2, Batch 375, Loss 16.48440045928955 Accuracy 1.0 Time 3.071s\n",
            " correct:  tensor(24064) Total:  24064\n",
            "Epoch 2, Batch 376, Loss 16.484136695557453 Accuracy 1.0 Time 3.061s\n",
            " correct:  tensor(24128) Total:  24128\n",
            "Epoch 2, Batch 377, Loss 16.484922801151832 Accuracy 1.0 Time 3.086s\n",
            " correct:  tensor(24192) Total:  24192\n",
            "Epoch 2, Batch 378, Loss 16.482936571514795 Accuracy 1.0 Time 3.069s\n",
            " correct:  tensor(24256) Total:  24256\n",
            "Epoch 2, Batch 379, Loss 16.483554100298317 Accuracy 1.0 Time 3.083s\n",
            " correct:  tensor(24320) Total:  24320\n",
            "Epoch 2, Batch 380, Loss 16.481620678148772 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(24384) Total:  24384\n",
            "Epoch 2, Batch 381, Loss 16.478873906173106 Accuracy 1.0 Time 3.092s\n",
            " correct:  tensor(24448) Total:  24448\n",
            "Epoch 2, Batch 382, Loss 16.476782232054866 Accuracy 1.0 Time 3.072s\n",
            " correct:  tensor(24512) Total:  24512\n",
            "Epoch 2, Batch 383, Loss 16.476563827176008 Accuracy 1.0 Time 3.078s\n",
            " correct:  tensor(24576) Total:  24576\n",
            "Epoch 2, Batch 384, Loss 16.47388830035925 Accuracy 1.0 Time 3.101s\n",
            " correct:  tensor(24640) Total:  24640\n",
            "Epoch 2, Batch 385, Loss 16.471409398859198 Accuracy 1.0 Time 3.107s\n",
            " correct:  tensor(24704) Total:  24704\n",
            "Epoch 2, Batch 386, Loss 16.47066713244186 Accuracy 1.0 Time 3.183s\n",
            " correct:  tensor(24768) Total:  24768\n",
            "Epoch 2, Batch 387, Loss 16.47085583117581 Accuracy 1.0 Time 3.041s\n",
            " correct:  tensor(24832) Total:  24832\n",
            "Epoch 2, Batch 388, Loss 16.46863463736072 Accuracy 1.0 Time 3.097s\n",
            " correct:  tensor(24896) Total:  24896\n",
            "Epoch 2, Batch 389, Loss 16.468238396632334 Accuracy 1.0 Time 3.119s\n",
            " correct:  tensor(24960) Total:  24960\n",
            "Epoch 2, Batch 390, Loss 16.467996472578783 Accuracy 1.0 Time 3.033s\n",
            " correct:  tensor(25024) Total:  25024\n",
            "Epoch 2, Batch 391, Loss 16.465613570054778 Accuracy 1.0 Time 3.104s\n",
            " correct:  tensor(25088) Total:  25088\n",
            "Epoch 2, Batch 392, Loss 16.467472453506627 Accuracy 1.0 Time 3.065s\n",
            " correct:  tensor(25152) Total:  25152\n",
            "Epoch 2, Batch 393, Loss 16.466429744361314 Accuracy 1.0 Time 3.11s\n",
            " correct:  tensor(25216) Total:  25216\n",
            "Epoch 2, Batch 394, Loss 16.465523393021016 Accuracy 1.0 Time 3.092s\n",
            " correct:  tensor(25280) Total:  25280\n",
            "Epoch 2, Batch 395, Loss 16.463402424582952 Accuracy 1.0 Time 3.063s\n",
            " correct:  tensor(25344) Total:  25344\n",
            "Epoch 2, Batch 396, Loss 16.462038418259283 Accuracy 1.0 Time 3.102s\n",
            " correct:  tensor(25408) Total:  25408\n",
            "Epoch 2, Batch 397, Loss 16.460845334704036 Accuracy 1.0 Time 3.088s\n",
            " correct:  tensor(25472) Total:  25472\n",
            "Epoch 2, Batch 398, Loss 16.459292797587025 Accuracy 1.0 Time 3.061s\n",
            " correct:  tensor(25536) Total:  25536\n",
            "Epoch 2, Batch 399, Loss 16.461361072415993 Accuracy 1.0 Time 3.09s\n",
            " correct:  tensor(25600) Total:  25600\n",
            "Epoch 2, Batch 400, Loss 16.46007381439209 Accuracy 1.0 Time 3.112s\n",
            " correct:  tensor(25664) Total:  25664\n",
            "Epoch 2, Batch 401, Loss 16.460100835100967 Accuracy 1.0 Time 3.114s\n",
            " correct:  tensor(25728) Total:  25728\n",
            "Epoch 2, Batch 402, Loss 16.460135018647605 Accuracy 1.0 Time 3.065s\n",
            " correct:  tensor(25792) Total:  25792\n",
            "Epoch 2, Batch 403, Loss 16.459050938154274 Accuracy 1.0 Time 3.063s\n",
            " correct:  tensor(25856) Total:  25856\n",
            "Epoch 2, Batch 404, Loss 16.457309446712532 Accuracy 1.0 Time 3.062s\n",
            " correct:  tensor(25920) Total:  25920\n",
            "Epoch 2, Batch 405, Loss 16.455991474198708 Accuracy 1.0 Time 3.086s\n",
            " correct:  tensor(25984) Total:  25984\n",
            "Epoch 2, Batch 406, Loss 16.456461072555317 Accuracy 1.0 Time 3.159s\n",
            " correct:  tensor(26048) Total:  26048\n",
            "Epoch 2, Batch 407, Loss 16.454765804862507 Accuracy 1.0 Time 3.126s\n",
            " correct:  tensor(26112) Total:  26112\n",
            "Epoch 2, Batch 408, Loss 16.455860888256748 Accuracy 1.0 Time 3.08s\n",
            " correct:  tensor(26176) Total:  26176\n",
            "Epoch 2, Batch 409, Loss 16.454428560867868 Accuracy 1.0 Time 3.066s\n",
            " correct:  tensor(26240) Total:  26240\n",
            "Epoch 2, Batch 410, Loss 16.454825478065306 Accuracy 1.0 Time 3.042s\n",
            " correct:  tensor(26304) Total:  26304\n",
            "Epoch 2, Batch 411, Loss 16.455599144427445 Accuracy 1.0 Time 3.057s\n",
            " correct:  tensor(26368) Total:  26368\n",
            "Epoch 2, Batch 412, Loss 16.45234226023109 Accuracy 1.0 Time 3.054s\n",
            " correct:  tensor(26432) Total:  26432\n",
            "Epoch 2, Batch 413, Loss 16.45041138785226 Accuracy 1.0 Time 3.072s\n",
            " correct:  tensor(26496) Total:  26496\n",
            "Epoch 2, Batch 414, Loss 16.452808942195873 Accuracy 1.0 Time 3.167s\n",
            " correct:  tensor(26560) Total:  26560\n",
            "Epoch 2, Batch 415, Loss 16.45109015085611 Accuracy 1.0 Time 3.091s\n",
            " correct:  tensor(26624) Total:  26624\n",
            "Epoch 2, Batch 416, Loss 16.4494359264007 Accuracy 1.0 Time 3.084s\n",
            " correct:  tensor(26688) Total:  26688\n",
            "Epoch 2, Batch 417, Loss 16.449400625068794 Accuracy 1.0 Time 3.093s\n",
            " correct:  tensor(26752) Total:  26752\n",
            "Epoch 2, Batch 418, Loss 16.446704367131137 Accuracy 1.0 Time 3.168s\n",
            " correct:  tensor(26816) Total:  26816\n",
            "Epoch 2, Batch 419, Loss 16.448345448350565 Accuracy 1.0 Time 3.054s\n",
            " correct:  tensor(26880) Total:  26880\n",
            "Epoch 2, Batch 420, Loss 16.44935093380156 Accuracy 1.0 Time 3.125s\n",
            " correct:  tensor(26944) Total:  26944\n",
            "Epoch 2, Batch 421, Loss 16.448556292934825 Accuracy 1.0 Time 3.103s\n",
            " correct:  tensor(27008) Total:  27008\n",
            "Epoch 2, Batch 422, Loss 16.448413487294275 Accuracy 1.0 Time 3.153s\n",
            " correct:  tensor(27072) Total:  27072\n",
            "Epoch 2, Batch 423, Loss 16.450053442736326 Accuracy 1.0 Time 3.095s\n",
            " correct:  tensor(27136) Total:  27136\n",
            "Epoch 2, Batch 424, Loss 16.448451824907988 Accuracy 1.0 Time 3.126s\n",
            " correct:  tensor(27200) Total:  27200\n",
            "Epoch 2, Batch 425, Loss 16.44611750434427 Accuracy 1.0 Time 3.149s\n",
            " correct:  tensor(27264) Total:  27264\n",
            "Epoch 2, Batch 426, Loss 16.446452127376073 Accuracy 1.0 Time 3.133s\n",
            " correct:  tensor(27328) Total:  27328\n",
            "Epoch 2, Batch 427, Loss 16.447219960304277 Accuracy 1.0 Time 3.233s\n",
            " correct:  tensor(27392) Total:  27392\n",
            "Epoch 2, Batch 428, Loss 16.447300242486403 Accuracy 1.0 Time 3.137s\n",
            " correct:  tensor(27456) Total:  27456\n",
            "Epoch 2, Batch 429, Loss 16.44563387268351 Accuracy 1.0 Time 3.073s\n",
            " correct:  tensor(27520) Total:  27520\n",
            "Epoch 2, Batch 430, Loss 16.445442133171614 Accuracy 1.0 Time 3.171s\n",
            " correct:  tensor(27584) Total:  27584\n",
            "Epoch 2, Batch 431, Loss 16.447393567822097 Accuracy 1.0 Time 3.119s\n",
            " correct:  tensor(27648) Total:  27648\n",
            "Epoch 2, Batch 432, Loss 16.445548371032434 Accuracy 1.0 Time 3.091s\n",
            " correct:  tensor(27712) Total:  27712\n",
            "Epoch 2, Batch 433, Loss 16.442788331117697 Accuracy 1.0 Time 3.094s\n",
            " correct:  tensor(27776) Total:  27776\n",
            "Epoch 2, Batch 434, Loss 16.444467801652195 Accuracy 1.0 Time 3.136s\n",
            " correct:  tensor(27840) Total:  27840\n",
            "Epoch 2, Batch 435, Loss 16.442276735415405 Accuracy 1.0 Time 3.115s\n",
            " correct:  tensor(27904) Total:  27904\n",
            "Epoch 2, Batch 436, Loss 16.440557665781142 Accuracy 1.0 Time 3.384s\n",
            " correct:  tensor(27968) Total:  27968\n",
            "Epoch 2, Batch 437, Loss 16.441499459116084 Accuracy 1.0 Time 3.375s\n",
            " correct:  tensor(28032) Total:  28032\n",
            "Epoch 2, Batch 438, Loss 16.4416198011947 Accuracy 1.0 Time 3.357s\n",
            " correct:  tensor(28096) Total:  28096\n",
            "Epoch 2, Batch 439, Loss 16.43871099041915 Accuracy 1.0 Time 3.134s\n",
            " correct:  tensor(28160) Total:  28160\n",
            "Epoch 2, Batch 440, Loss 16.435058509219775 Accuracy 1.0 Time 3.114s\n",
            " correct:  tensor(28224) Total:  28224\n",
            "Epoch 2, Batch 441, Loss 16.435958782291195 Accuracy 1.0 Time 3.14s\n",
            " correct:  tensor(28288) Total:  28288\n",
            "Epoch 2, Batch 442, Loss 16.436341710759503 Accuracy 1.0 Time 3.159s\n",
            " correct:  tensor(28352) Total:  28352\n",
            "Epoch 2, Batch 443, Loss 16.433524861561793 Accuracy 1.0 Time 3.178s\n",
            " correct:  tensor(28416) Total:  28416\n",
            "Epoch 2, Batch 444, Loss 16.43542704281506 Accuracy 1.0 Time 3.203s\n",
            " correct:  tensor(28480) Total:  28480\n",
            "Epoch 2, Batch 445, Loss 16.435614540871608 Accuracy 1.0 Time 3.139s\n",
            " correct:  tensor(28544) Total:  28544\n",
            "Epoch 2, Batch 446, Loss 16.43524439345561 Accuracy 1.0 Time 3.115s\n",
            " correct:  tensor(28608) Total:  28608\n",
            "Epoch 2, Batch 447, Loss 16.43579289310464 Accuracy 1.0 Time 3.14s\n",
            " correct:  tensor(28672) Total:  28672\n",
            "Epoch 2, Batch 448, Loss 16.436834284237452 Accuracy 1.0 Time 3.111s\n",
            " correct:  tensor(28736) Total:  28736\n",
            "Epoch 2, Batch 449, Loss 16.436111569139104 Accuracy 1.0 Time 3.12s\n",
            " correct:  tensor(28800) Total:  28800\n",
            "Epoch 2, Batch 450, Loss 16.43480906592475 Accuracy 1.0 Time 3.171s\n",
            " correct:  tensor(28864) Total:  28864\n",
            "Epoch 2, Batch 451, Loss 16.432883296467512 Accuracy 1.0 Time 3.133s\n",
            " correct:  tensor(28928) Total:  28928\n",
            "Epoch 2, Batch 452, Loss 16.431525335902663 Accuracy 1.0 Time 3.157s\n",
            " correct:  tensor(28992) Total:  28992\n",
            "Epoch 2, Batch 453, Loss 16.431897502334966 Accuracy 1.0 Time 3.172s\n",
            " correct:  tensor(29056) Total:  29056\n",
            "Epoch 2, Batch 454, Loss 16.4327383125406 Accuracy 1.0 Time 3.083s\n",
            " correct:  tensor(29120) Total:  29120\n",
            "Epoch 2, Batch 455, Loss 16.430060193826865 Accuracy 1.0 Time 3.172s\n",
            " correct:  tensor(29184) Total:  29184\n",
            "Epoch 2, Batch 456, Loss 16.430336075916625 Accuracy 1.0 Time 3.15s\n",
            " correct:  tensor(29248) Total:  29248\n",
            "Epoch 2, Batch 457, Loss 16.430440174411633 Accuracy 1.0 Time 3.134s\n",
            " correct:  tensor(29312) Total:  29312\n",
            "Epoch 2, Batch 458, Loss 16.4285027678877 Accuracy 1.0 Time 3.147s\n",
            " correct:  tensor(29376) Total:  29376\n",
            "Epoch 2, Batch 459, Loss 16.427722866498826 Accuracy 1.0 Time 3.107s\n",
            " correct:  tensor(29440) Total:  29440\n",
            "Epoch 2, Batch 460, Loss 16.4252709824106 Accuracy 1.0 Time 3.08s\n",
            " correct:  tensor(29504) Total:  29504\n",
            "Epoch 2, Batch 461, Loss 16.42403092870485 Accuracy 1.0 Time 3.258s\n",
            " correct:  tensor(29568) Total:  29568\n",
            "Epoch 2, Batch 462, Loss 16.42381510796485 Accuracy 1.0 Time 3.346s\n",
            " correct:  tensor(29632) Total:  29632\n",
            "Epoch 2, Batch 463, Loss 16.42223886073794 Accuracy 1.0 Time 3.185s\n",
            " correct:  tensor(29696) Total:  29696\n",
            "Epoch 2, Batch 464, Loss 16.42094818682506 Accuracy 1.0 Time 3.142s\n",
            " correct:  tensor(29760) Total:  29760\n",
            "Epoch 2, Batch 465, Loss 16.422060691669422 Accuracy 1.0 Time 3.111s\n",
            " correct:  tensor(29824) Total:  29824\n",
            "Epoch 2, Batch 466, Loss 16.421245224997723 Accuracy 1.0 Time 3.073s\n",
            " correct:  tensor(29888) Total:  29888\n",
            "Epoch 2, Batch 467, Loss 16.422251856556798 Accuracy 1.0 Time 3.153s\n",
            " correct:  tensor(29952) Total:  29952\n",
            "Epoch 2, Batch 468, Loss 16.418530074959126 Accuracy 1.0 Time 3.13s\n",
            " correct:  tensor(30016) Total:  30016\n",
            "Epoch 2, Batch 469, Loss 16.41697480785313 Accuracy 1.0 Time 3.135s\n",
            " correct:  tensor(30080) Total:  30080\n",
            "Epoch 2, Batch 470, Loss 16.416609169574492 Accuracy 1.0 Time 3.119s\n",
            " correct:  tensor(30144) Total:  30144\n",
            "Epoch 2, Batch 471, Loss 16.415633904199943 Accuracy 1.0 Time 3.131s\n",
            " correct:  tensor(30208) Total:  30208\n",
            "Epoch 2, Batch 472, Loss 16.414624113147543 Accuracy 1.0 Time 3.148s\n",
            " correct:  tensor(30272) Total:  30272\n",
            "Epoch 2, Batch 473, Loss 16.414638220885845 Accuracy 1.0 Time 3.121s\n",
            " correct:  tensor(30336) Total:  30336\n",
            "Epoch 2, Batch 474, Loss 16.413993944095658 Accuracy 1.0 Time 3.166s\n",
            " correct:  tensor(30400) Total:  30400\n",
            "Epoch 2, Batch 475, Loss 16.41354600605212 Accuracy 1.0 Time 3.193s\n",
            " correct:  tensor(30464) Total:  30464\n",
            "Epoch 2, Batch 476, Loss 16.413991158749877 Accuracy 1.0 Time 3.186s\n",
            " correct:  tensor(30528) Total:  30528\n",
            "Epoch 2, Batch 477, Loss 16.413822885829198 Accuracy 1.0 Time 3.103s\n",
            " correct:  tensor(30592) Total:  30592\n",
            "Epoch 2, Batch 478, Loss 16.41234834423624 Accuracy 1.0 Time 3.182s\n",
            " correct:  tensor(30656) Total:  30656\n",
            "Epoch 2, Batch 479, Loss 16.411259559599493 Accuracy 1.0 Time 3.111s\n",
            " correct:  tensor(30720) Total:  30720\n",
            "Epoch 2, Batch 480, Loss 16.41004208922386 Accuracy 1.0 Time 3.107s\n",
            " correct:  tensor(30784) Total:  30784\n",
            "Epoch 2, Batch 481, Loss 16.409607946748793 Accuracy 1.0 Time 3.163s\n",
            " correct:  tensor(30848) Total:  30848\n",
            "Epoch 2, Batch 482, Loss 16.40765548840598 Accuracy 1.0 Time 3.212s\n",
            " correct:  tensor(30912) Total:  30912\n",
            "Epoch 2, Batch 483, Loss 16.406126379719925 Accuracy 1.0 Time 3.213s\n",
            " correct:  tensor(30976) Total:  30976\n",
            "Epoch 2, Batch 484, Loss 16.405264143116217 Accuracy 1.0 Time 3.16s\n",
            " correct:  tensor(31040) Total:  31040\n",
            "Epoch 2, Batch 485, Loss 16.40598182088321 Accuracy 1.0 Time 3.196s\n",
            " correct:  tensor(31104) Total:  31104\n",
            "Epoch 2, Batch 486, Loss 16.405430862442458 Accuracy 1.0 Time 3.242s\n",
            " correct:  tensor(31168) Total:  31168\n",
            "Epoch 2, Batch 487, Loss 16.405313356701107 Accuracy 1.0 Time 3.182s\n",
            " correct:  tensor(31232) Total:  31232\n",
            "Epoch 2, Batch 488, Loss 16.404416850355805 Accuracy 1.0 Time 3.16s\n",
            " correct:  tensor(31296) Total:  31296\n",
            "Epoch 2, Batch 489, Loss 16.403910026472282 Accuracy 1.0 Time 3.144s\n",
            " correct:  tensor(31360) Total:  31360\n",
            "Epoch 2, Batch 490, Loss 16.402621847269486 Accuracy 1.0 Time 3.208s\n",
            " correct:  tensor(31424) Total:  31424\n",
            "Epoch 2, Batch 491, Loss 16.400872315991435 Accuracy 1.0 Time 3.18s\n",
            " correct:  tensor(31488) Total:  31488\n",
            "Epoch 2, Batch 492, Loss 16.401653667775594 Accuracy 1.0 Time 3.094s\n",
            " correct:  tensor(31552) Total:  31552\n",
            "Epoch 2, Batch 493, Loss 16.39971973542993 Accuracy 1.0 Time 3.159s\n",
            " correct:  tensor(31616) Total:  31616\n",
            "Epoch 2, Batch 494, Loss 16.39781307498453 Accuracy 1.0 Time 3.076s\n",
            " correct:  tensor(31680) Total:  31680\n",
            "Epoch 2, Batch 495, Loss 16.39615850159616 Accuracy 1.0 Time 3.137s\n",
            " correct:  tensor(31744) Total:  31744\n",
            "Epoch 2, Batch 496, Loss 16.396049255324947 Accuracy 1.0 Time 3.081s\n",
            " correct:  tensor(31808) Total:  31808\n",
            "Epoch 2, Batch 497, Loss 16.39486483069253 Accuracy 1.0 Time 3.147s\n",
            " correct:  tensor(31872) Total:  31872\n",
            "Epoch 2, Batch 498, Loss 16.39443279365938 Accuracy 1.0 Time 3.148s\n",
            " correct:  tensor(31936) Total:  31936\n",
            "Epoch 2, Batch 499, Loss 16.394704939129358 Accuracy 1.0 Time 3.196s\n",
            " correct:  tensor(32000) Total:  32000\n",
            "Epoch 2, Batch 500, Loss 16.39326028251648 Accuracy 1.0 Time 3.124s\n",
            " correct:  tensor(32064) Total:  32064\n",
            "Epoch 2, Batch 501, Loss 16.3923655546116 Accuracy 1.0 Time 3.162s\n",
            " correct:  tensor(32128) Total:  32128\n",
            "Epoch 2, Batch 502, Loss 16.392751638632845 Accuracy 1.0 Time 3.113s\n",
            " correct:  tensor(32192) Total:  32192\n",
            "Epoch 2, Batch 503, Loss 16.39304778590117 Accuracy 1.0 Time 3.126s\n",
            " correct:  tensor(32256) Total:  32256\n",
            "Epoch 2, Batch 504, Loss 16.393260393823898 Accuracy 1.0 Time 3.183s\n",
            " correct:  tensor(32320) Total:  32320\n",
            "Epoch 2, Batch 505, Loss 16.395022603780916 Accuracy 1.0 Time 3.072s\n",
            " correct:  tensor(32384) Total:  32384\n",
            "Epoch 2, Batch 506, Loss 16.393454983300373 Accuracy 1.0 Time 3.181s\n",
            " correct:  tensor(32448) Total:  32448\n",
            "Epoch 2, Batch 507, Loss 16.392296439326962 Accuracy 1.0 Time 3.158s\n",
            " correct:  tensor(32512) Total:  32512\n",
            "Epoch 2, Batch 508, Loss 16.394042758491096 Accuracy 1.0 Time 3.192s\n",
            " correct:  tensor(32576) Total:  32576\n",
            "Epoch 2, Batch 509, Loss 16.395019220228512 Accuracy 1.0 Time 3.144s\n",
            " correct:  tensor(32640) Total:  32640\n",
            "Epoch 2, Batch 510, Loss 16.39429417778464 Accuracy 1.0 Time 3.176s\n",
            " correct:  tensor(32704) Total:  32704\n",
            "Epoch 2, Batch 511, Loss 16.394207646468846 Accuracy 1.0 Time 3.135s\n",
            " correct:  tensor(32768) Total:  32768\n",
            "Epoch 2, Batch 512, Loss 16.393201772123575 Accuracy 1.0 Time 3.163s\n",
            " correct:  tensor(32832) Total:  32832\n",
            "Epoch 2, Batch 513, Loss 16.39272349247923 Accuracy 1.0 Time 3.143s\n",
            " correct:  tensor(32896) Total:  32896\n",
            "Epoch 2, Batch 514, Loss 16.393009022516036 Accuracy 1.0 Time 3.162s\n",
            " correct:  tensor(32960) Total:  32960\n",
            "Epoch 2, Batch 515, Loss 16.393826208762754 Accuracy 1.0 Time 3.158s\n",
            " correct:  tensor(33024) Total:  33024\n",
            "Epoch 2, Batch 516, Loss 16.393894879392874 Accuracy 1.0 Time 3.129s\n",
            " correct:  tensor(33088) Total:  33088\n",
            "Epoch 2, Batch 517, Loss 16.393679052533678 Accuracy 1.0 Time 3.118s\n",
            " correct:  tensor(33152) Total:  33152\n",
            "Epoch 2, Batch 518, Loss 16.3915510564237 Accuracy 1.0 Time 3.149s\n",
            " correct:  tensor(33216) Total:  33216\n",
            "Epoch 2, Batch 519, Loss 16.39129214387868 Accuracy 1.0 Time 3.172s\n",
            " correct:  tensor(33280) Total:  33280\n",
            "Epoch 2, Batch 520, Loss 16.390396569325375 Accuracy 1.0 Time 3.251s\n",
            " correct:  tensor(33344) Total:  33344\n",
            "Epoch 2, Batch 521, Loss 16.389277264161212 Accuracy 1.0 Time 3.154s\n",
            " correct:  tensor(33408) Total:  33408\n",
            "Epoch 2, Batch 522, Loss 16.38790735887842 Accuracy 1.0 Time 3.211s\n",
            " correct:  tensor(33472) Total:  33472\n",
            "Epoch 2, Batch 523, Loss 16.389145632318968 Accuracy 1.0 Time 3.167s\n",
            " correct:  tensor(33536) Total:  33536\n",
            "Epoch 2, Batch 524, Loss 16.389083350887734 Accuracy 1.0 Time 3.221s\n",
            " correct:  tensor(33600) Total:  33600\n",
            "Epoch 2, Batch 525, Loss 16.391566218421573 Accuracy 1.0 Time 3.23s\n",
            " correct:  tensor(33664) Total:  33664\n",
            "Epoch 2, Batch 526, Loss 16.390270898550636 Accuracy 1.0 Time 3.216s\n",
            " correct:  tensor(33728) Total:  33728\n",
            "Epoch 2, Batch 527, Loss 16.392853483529425 Accuracy 1.0 Time 3.167s\n",
            " correct:  tensor(33792) Total:  33792\n",
            "Epoch 2, Batch 528, Loss 16.395615474744275 Accuracy 1.0 Time 3.18s\n",
            " correct:  tensor(33856) Total:  33856\n",
            "Epoch 2, Batch 529, Loss 16.394522924729692 Accuracy 1.0 Time 3.218s\n",
            " correct:  tensor(33920) Total:  33920\n",
            "Epoch 2, Batch 530, Loss 16.392624387201273 Accuracy 1.0 Time 3.209s\n",
            " correct:  tensor(33984) Total:  33984\n",
            "Epoch 2, Batch 531, Loss 16.394013051034815 Accuracy 1.0 Time 3.191s\n",
            " correct:  tensor(34048) Total:  34048\n",
            "Epoch 2, Batch 532, Loss 16.394185392480146 Accuracy 1.0 Time 3.247s\n",
            " correct:  tensor(34112) Total:  34112\n",
            "Epoch 2, Batch 533, Loss 16.392826405967153 Accuracy 1.0 Time 3.308s\n",
            " correct:  tensor(34176) Total:  34176\n",
            "Epoch 2, Batch 534, Loss 16.39234546061312 Accuracy 1.0 Time 3.406s\n",
            " correct:  tensor(34240) Total:  34240\n",
            "Epoch 2, Batch 535, Loss 16.390309214369157 Accuracy 1.0 Time 3.397s\n",
            " correct:  tensor(34304) Total:  34304\n",
            "Epoch 2, Batch 536, Loss 16.39166231297735 Accuracy 1.0 Time 3.333s\n",
            " correct:  tensor(34368) Total:  34368\n",
            "Epoch 2, Batch 537, Loss 16.393476372546544 Accuracy 1.0 Time 3.095s\n",
            " correct:  tensor(34432) Total:  34432\n",
            "Epoch 2, Batch 538, Loss 16.39199303517111 Accuracy 1.0 Time 3.277s\n",
            " correct:  tensor(34496) Total:  34496\n",
            "Epoch 2, Batch 539, Loss 16.393592807932556 Accuracy 1.0 Time 3.178s\n",
            " correct:  tensor(34560) Total:  34560\n",
            "Epoch 2, Batch 540, Loss 16.393724439762256 Accuracy 1.0 Time 3.168s\n",
            " correct:  tensor(34624) Total:  34624\n",
            "Epoch 2, Batch 541, Loss 16.394028342805818 Accuracy 1.0 Time 3.165s\n",
            " correct:  tensor(34688) Total:  34688\n",
            "Epoch 2, Batch 542, Loss 16.393534816939013 Accuracy 1.0 Time 3.144s\n",
            " correct:  tensor(34752) Total:  34752\n",
            "Epoch 2, Batch 543, Loss 16.39399915417694 Accuracy 1.0 Time 3.187s\n",
            " correct:  tensor(34816) Total:  34816\n",
            "Epoch 2, Batch 544, Loss 16.392409918939364 Accuracy 1.0 Time 3.158s\n",
            " correct:  tensor(34880) Total:  34880\n",
            "Epoch 2, Batch 545, Loss 16.393109195604236 Accuracy 1.0 Time 3.183s\n",
            " correct:  tensor(34944) Total:  34944\n",
            "Epoch 2, Batch 546, Loss 16.393629337841773 Accuracy 1.0 Time 3.136s\n",
            " correct:  tensor(35008) Total:  35008\n",
            "Epoch 2, Batch 547, Loss 16.393113115369946 Accuracy 1.0 Time 3.172s\n",
            " correct:  tensor(35072) Total:  35072\n",
            "Epoch 2, Batch 548, Loss 16.391188990460694 Accuracy 1.0 Time 3.167s\n",
            " correct:  tensor(35136) Total:  35136\n",
            "Epoch 2, Batch 549, Loss 16.390618572686755 Accuracy 1.0 Time 3.143s\n",
            " correct:  tensor(35200) Total:  35200\n",
            "Epoch 2, Batch 550, Loss 16.389006202004172 Accuracy 1.0 Time 3.106s\n",
            " correct:  tensor(35264) Total:  35264\n",
            "Epoch 2, Batch 551, Loss 16.389357118554642 Accuracy 1.0 Time 3.219s\n",
            " correct:  tensor(35328) Total:  35328\n",
            "Epoch 2, Batch 552, Loss 16.387319635653842 Accuracy 1.0 Time 3.189s\n",
            " correct:  tensor(35392) Total:  35392\n",
            "Epoch 2, Batch 553, Loss 16.385998413748162 Accuracy 1.0 Time 3.19s\n",
            " correct:  tensor(35456) Total:  35456\n",
            "Epoch 2, Batch 554, Loss 16.385443869910947 Accuracy 1.0 Time 3.146s\n",
            " correct:  tensor(35520) Total:  35520\n",
            "Epoch 2, Batch 555, Loss 16.386105690346106 Accuracy 1.0 Time 3.136s\n",
            " correct:  tensor(35584) Total:  35584\n",
            "Epoch 2, Batch 556, Loss 16.385708294326452 Accuracy 1.0 Time 3.106s\n",
            " correct:  tensor(35648) Total:  35648\n",
            "Epoch 2, Batch 557, Loss 16.387937912179066 Accuracy 1.0 Time 3.274s\n",
            " correct:  tensor(35712) Total:  35712\n",
            "Epoch 2, Batch 558, Loss 16.386656060440995 Accuracy 1.0 Time 3.326s\n",
            " correct:  tensor(35776) Total:  35776\n",
            "Epoch 2, Batch 559, Loss 16.38428665870844 Accuracy 1.0 Time 3.178s\n",
            " correct:  tensor(35840) Total:  35840\n",
            "Epoch 2, Batch 560, Loss 16.383281091281347 Accuracy 1.0 Time 3.029s\n",
            " correct:  tensor(35904) Total:  35904\n",
            "Epoch 2, Batch 561, Loss 16.383732454024535 Accuracy 1.0 Time 3.123s\n",
            " correct:  tensor(35968) Total:  35968\n",
            "Epoch 2, Batch 562, Loss 16.383363683028577 Accuracy 1.0 Time 3.084s\n",
            " correct:  tensor(36032) Total:  36032\n",
            "Epoch 2, Batch 563, Loss 16.380677182551597 Accuracy 1.0 Time 3.155s\n",
            " correct:  tensor(36096) Total:  36096\n",
            "Epoch 2, Batch 564, Loss 16.379058415162646 Accuracy 1.0 Time 3.182s\n",
            " correct:  tensor(36160) Total:  36160\n",
            "Epoch 2, Batch 565, Loss 16.37783061171 Accuracy 1.0 Time 3.175s\n",
            " correct:  tensor(36224) Total:  36224\n",
            "Epoch 2, Batch 566, Loss 16.377053968476744 Accuracy 1.0 Time 3.041s\n",
            " correct:  tensor(36288) Total:  36288\n",
            "Epoch 2, Batch 567, Loss 16.376878879688405 Accuracy 1.0 Time 3.1s\n",
            " correct:  tensor(36352) Total:  36352\n",
            "Epoch 2, Batch 568, Loss 16.377346461927388 Accuracy 1.0 Time 3.079s\n",
            " correct:  tensor(36416) Total:  36416\n",
            "Epoch 2, Batch 569, Loss 16.37726078469221 Accuracy 1.0 Time 3.147s\n",
            " correct:  tensor(36480) Total:  36480\n",
            "Epoch 2, Batch 570, Loss 16.375922378740814 Accuracy 1.0 Time 3.064s\n",
            " correct:  tensor(36544) Total:  36544\n",
            "Epoch 2, Batch 571, Loss 16.375493199938024 Accuracy 1.0 Time 3.122s\n",
            " correct:  tensor(36608) Total:  36608\n",
            "Epoch 2, Batch 572, Loss 16.374985236387985 Accuracy 1.0 Time 3.119s\n",
            " correct:  tensor(36672) Total:  36672\n",
            "Epoch 2, Batch 573, Loss 16.37407574811739 Accuracy 1.0 Time 3.1s\n",
            " correct:  tensor(36736) Total:  36736\n",
            "Epoch 2, Batch 574, Loss 16.373227943526743 Accuracy 1.0 Time 3.052s\n",
            " correct:  tensor(36800) Total:  36800\n",
            "Epoch 2, Batch 575, Loss 16.372910507865573 Accuracy 1.0 Time 3.089s\n",
            " correct:  tensor(36864) Total:  36864\n",
            "Epoch 2, Batch 576, Loss 16.371819343831802 Accuracy 1.0 Time 3.22s\n",
            " correct:  tensor(36928) Total:  36928\n",
            "Epoch 2, Batch 577, Loss 16.371815405437395 Accuracy 1.0 Time 3.156s\n",
            " correct:  tensor(36992) Total:  36992\n",
            "Epoch 2, Batch 578, Loss 16.37068499660822 Accuracy 1.0 Time 3.123s\n",
            " correct:  tensor(37056) Total:  37056\n",
            "Epoch 2, Batch 579, Loss 16.37142600090804 Accuracy 1.0 Time 3.063s\n",
            " correct:  tensor(37120) Total:  37120\n",
            "Epoch 2, Batch 580, Loss 16.37039434498754 Accuracy 1.0 Time 3.066s\n",
            " correct:  tensor(37184) Total:  37184\n",
            "Epoch 2, Batch 581, Loss 16.370642532374895 Accuracy 1.0 Time 3.059s\n",
            " correct:  tensor(37248) Total:  37248\n",
            "Epoch 2, Batch 582, Loss 16.36979302507905 Accuracy 1.0 Time 3.099s\n",
            " correct:  tensor(37312) Total:  37312\n",
            "Epoch 2, Batch 583, Loss 16.36798015221519 Accuracy 1.0 Time 3.096s\n",
            " correct:  tensor(37376) Total:  37376\n",
            "Epoch 2, Batch 584, Loss 16.367225454278188 Accuracy 1.0 Time 3.104s\n",
            " correct:  tensor(37440) Total:  37440\n",
            "Epoch 2, Batch 585, Loss 16.36533896291358 Accuracy 1.0 Time 3.131s\n",
            " correct:  tensor(37504) Total:  37504\n",
            "Epoch 2, Batch 586, Loss 16.366818782819415 Accuracy 1.0 Time 3.073s\n",
            " correct:  tensor(37568) Total:  37568\n",
            "Epoch 2, Batch 587, Loss 16.364558067191805 Accuracy 1.0 Time 3.111s\n",
            " correct:  tensor(37632) Total:  37632\n",
            "Epoch 2, Batch 588, Loss 16.362555748751376 Accuracy 1.0 Time 3.081s\n",
            " correct:  tensor(37696) Total:  37696\n",
            "Epoch 2, Batch 589, Loss 16.362261610241617 Accuracy 1.0 Time 3.063s\n",
            " correct:  tensor(37760) Total:  37760\n",
            "Epoch 2, Batch 590, Loss 16.36181999303527 Accuracy 1.0 Time 3.091s\n",
            " correct:  tensor(37824) Total:  37824\n",
            "Epoch 2, Batch 591, Loss 16.362122171017926 Accuracy 1.0 Time 3.107s\n",
            " correct:  tensor(37888) Total:  37888\n",
            "Epoch 2, Batch 592, Loss 16.363150453245318 Accuracy 1.0 Time 3.062s\n",
            " correct:  tensor(37952) Total:  37952\n",
            "Epoch 2, Batch 593, Loss 16.363478274610753 Accuracy 1.0 Time 3.093s\n",
            " correct:  tensor(38016) Total:  38016\n",
            "Epoch 2, Batch 594, Loss 16.36237042038529 Accuracy 1.0 Time 3.116s\n",
            " correct:  tensor(38080) Total:  38080\n",
            "Epoch 2, Batch 595, Loss 16.362952560937707 Accuracy 1.0 Time 3.056s\n",
            " correct:  tensor(38144) Total:  38144\n",
            "Epoch 2, Batch 596, Loss 16.362756642719244 Accuracy 1.0 Time 3.105s\n",
            " correct:  tensor(38208) Total:  38208\n",
            "Epoch 2, Batch 597, Loss 16.36182364746554 Accuracy 1.0 Time 3.099s\n",
            " correct:  tensor(38272) Total:  38272\n",
            "Epoch 2, Batch 598, Loss 16.361035286383487 Accuracy 1.0 Time 3.069s\n",
            " correct:  tensor(38336) Total:  38336\n",
            "Epoch 2, Batch 599, Loss 16.362937844456336 Accuracy 1.0 Time 3.076s\n",
            " correct:  tensor(38400) Total:  38400\n",
            "Epoch 2, Batch 600, Loss 16.360816961924236 Accuracy 1.0 Time 3.08s\n",
            " correct:  tensor(38464) Total:  38464\n",
            "Epoch 2, Batch 601, Loss 16.36049554351959 Accuracy 1.0 Time 3.081s\n",
            " correct:  tensor(38528) Total:  38528\n",
            "Epoch 2, Batch 602, Loss 16.35916896199071 Accuracy 1.0 Time 3.09s\n",
            " correct:  tensor(38592) Total:  38592\n",
            "Epoch 2, Batch 603, Loss 16.357840558585046 Accuracy 1.0 Time 3.138s\n",
            " correct:  tensor(38656) Total:  38656\n",
            "Epoch 2, Batch 604, Loss 16.35689748833511 Accuracy 1.0 Time 3.125s\n",
            " correct:  tensor(38720) Total:  38720\n",
            "Epoch 2, Batch 605, Loss 16.357400485897852 Accuracy 1.0 Time 3.142s\n",
            " correct:  tensor(38784) Total:  38784\n",
            "Epoch 2, Batch 606, Loss 16.357399089108206 Accuracy 1.0 Time 3.083s\n",
            " correct:  tensor(38848) Total:  38848\n",
            "Epoch 2, Batch 607, Loss 16.357297295597082 Accuracy 1.0 Time 3.033s\n",
            " correct:  tensor(38912) Total:  38912\n",
            "Epoch 2, Batch 608, Loss 16.35535360166901 Accuracy 1.0 Time 3.098s\n",
            " correct:  tensor(38976) Total:  38976\n",
            "Epoch 2, Batch 609, Loss 16.35324499289978 Accuracy 1.0 Time 3.088s\n",
            " correct:  tensor(39040) Total:  39040\n",
            "Epoch 2, Batch 610, Loss 16.35411515626751 Accuracy 1.0 Time 3.045s\n",
            " correct:  tensor(39104) Total:  39104\n",
            "Epoch 2, Batch 611, Loss 16.35488423505118 Accuracy 1.0 Time 3.072s\n",
            " correct:  tensor(39168) Total:  39168\n",
            "Epoch 2, Batch 612, Loss 16.353390974156998 Accuracy 1.0 Time 3.077s\n",
            " correct:  tensor(39232) Total:  39232\n",
            "Epoch 2, Batch 613, Loss 16.351822468818305 Accuracy 1.0 Time 3.055s\n",
            " correct:  tensor(39296) Total:  39296\n",
            "Epoch 2, Batch 614, Loss 16.351270439570424 Accuracy 1.0 Time 3.064s\n",
            " correct:  tensor(39360) Total:  39360\n",
            "Epoch 2, Batch 615, Loss 16.352278937363042 Accuracy 1.0 Time 3.122s\n",
            " correct:  tensor(39424) Total:  39424\n",
            "Epoch 2, Batch 616, Loss 16.351461348595556 Accuracy 1.0 Time 3.107s\n",
            " correct:  tensor(39488) Total:  39488\n",
            "Epoch 2, Batch 617, Loss 16.35000729367644 Accuracy 1.0 Time 3.042s\n",
            " correct:  tensor(39552) Total:  39552\n",
            "Epoch 2, Batch 618, Loss 16.34923727535507 Accuracy 1.0 Time 3.098s\n",
            " correct:  tensor(39616) Total:  39616\n",
            "Epoch 2, Batch 619, Loss 16.349315714181337 Accuracy 1.0 Time 3.048s\n",
            " correct:  tensor(39680) Total:  39680\n",
            "Epoch 2, Batch 620, Loss 16.347839090901036 Accuracy 1.0 Time 3.041s\n",
            " correct:  tensor(39744) Total:  39744\n",
            "Epoch 2, Batch 621, Loss 16.346733429581647 Accuracy 1.0 Time 3.094s\n",
            " correct:  tensor(39808) Total:  39808\n",
            "Epoch 2, Batch 622, Loss 16.344910503583705 Accuracy 1.0 Time 3.066s\n",
            " correct:  tensor(39872) Total:  39872\n",
            "Epoch 2, Batch 623, Loss 16.34395272544261 Accuracy 1.0 Time 3.108s\n",
            " correct:  tensor(39936) Total:  39936\n",
            "Epoch 2, Batch 624, Loss 16.341722008509514 Accuracy 1.0 Time 3.067s\n",
            " correct:  tensor(40000) Total:  40000\n",
            "Epoch 2, Batch 625, Loss 16.337918753051756 Accuracy 1.0 Time 3.051s\n",
            " correct:  tensor(40064) Total:  40064\n",
            "Epoch 2, Batch 626, Loss 16.337854001468745 Accuracy 1.0 Time 3.085s\n",
            " correct:  tensor(40128) Total:  40128\n",
            "Epoch 2, Batch 627, Loss 16.33824721097566 Accuracy 1.0 Time 3.102s\n",
            " correct:  tensor(40192) Total:  40192\n",
            "Epoch 2, Batch 628, Loss 16.338431382634838 Accuracy 1.0 Time 3.195s\n",
            " correct:  tensor(40256) Total:  40256\n",
            "Epoch 2, Batch 629, Loss 16.337679385380827 Accuracy 1.0 Time 3.164s\n",
            " correct:  tensor(40320) Total:  40320\n",
            "Epoch 2, Batch 630, Loss 16.33636413907248 Accuracy 1.0 Time 3.09s\n",
            " correct:  tensor(40384) Total:  40384\n",
            "Epoch 2, Batch 631, Loss 16.33600969964464 Accuracy 1.0 Time 3.046s\n",
            " correct:  tensor(40448) Total:  40448\n",
            "Epoch 2, Batch 632, Loss 16.335207676585718 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(40512) Total:  40512\n",
            "Epoch 2, Batch 633, Loss 16.33560350306603 Accuracy 1.0 Time 3.392s\n",
            " correct:  tensor(40576) Total:  40576\n",
            "Epoch 2, Batch 634, Loss 16.33466566097849 Accuracy 1.0 Time 3.395s\n",
            " correct:  tensor(40640) Total:  40640\n",
            "Epoch 2, Batch 635, Loss 16.33389248284768 Accuracy 1.0 Time 3.378s\n",
            " correct:  tensor(40704) Total:  40704\n",
            "Epoch 2, Batch 636, Loss 16.33470330148373 Accuracy 1.0 Time 3.204s\n",
            " correct:  tensor(40768) Total:  40768\n",
            "Epoch 2, Batch 637, Loss 16.334810188089847 Accuracy 1.0 Time 3.181s\n",
            " correct:  tensor(40832) Total:  40832\n",
            "Epoch 2, Batch 638, Loss 16.334686913086703 Accuracy 1.0 Time 3.147s\n",
            " correct:  tensor(40896) Total:  40896\n",
            "Epoch 2, Batch 639, Loss 16.333042922340834 Accuracy 1.0 Time 3.245s\n",
            " correct:  tensor(40960) Total:  40960\n",
            "Epoch 2, Batch 640, Loss 16.332903845608236 Accuracy 1.0 Time 3.225s\n",
            " correct:  tensor(41024) Total:  41024\n",
            "Epoch 2, Batch 641, Loss 16.331929720136195 Accuracy 1.0 Time 3.118s\n",
            " correct:  tensor(41088) Total:  41088\n",
            "Epoch 2, Batch 642, Loss 16.33116338408996 Accuracy 1.0 Time 3.215s\n",
            " correct:  tensor(41152) Total:  41152\n",
            "Epoch 2, Batch 643, Loss 16.330994706873206 Accuracy 1.0 Time 3.159s\n",
            " correct:  tensor(41216) Total:  41216\n",
            "Epoch 2, Batch 644, Loss 16.330586994656866 Accuracy 1.0 Time 3.158s\n",
            " correct:  tensor(41280) Total:  41280\n",
            "Epoch 2, Batch 645, Loss 16.33084405558978 Accuracy 1.0 Time 3.154s\n",
            " correct:  tensor(41344) Total:  41344\n",
            "Epoch 2, Batch 646, Loss 16.330148770713215 Accuracy 1.0 Time 3.186s\n",
            " correct:  tensor(41408) Total:  41408\n",
            "Epoch 2, Batch 647, Loss 16.32917729190549 Accuracy 1.0 Time 3.131s\n",
            " correct:  tensor(41472) Total:  41472\n",
            "Epoch 2, Batch 648, Loss 16.330685128400354 Accuracy 1.0 Time 3.178s\n",
            " correct:  tensor(41536) Total:  41536\n",
            "Epoch 2, Batch 649, Loss 16.329308440762418 Accuracy 1.0 Time 3.21s\n",
            " correct:  tensor(41600) Total:  41600\n",
            "Epoch 2, Batch 650, Loss 16.329429696890024 Accuracy 1.0 Time 3.208s\n",
            " correct:  tensor(41664) Total:  41664\n",
            "Epoch 2, Batch 651, Loss 16.329380202403268 Accuracy 1.0 Time 3.193s\n",
            " correct:  tensor(41728) Total:  41728\n",
            "Epoch 2, Batch 652, Loss 16.328944759134867 Accuracy 1.0 Time 3.173s\n",
            " correct:  tensor(41792) Total:  41792\n",
            "Epoch 2, Batch 653, Loss 16.327419868107047 Accuracy 1.0 Time 3.194s\n",
            " correct:  tensor(41856) Total:  41856\n",
            "Epoch 2, Batch 654, Loss 16.32666563331534 Accuracy 1.0 Time 3.162s\n",
            " correct:  tensor(41920) Total:  41920\n",
            "Epoch 2, Batch 655, Loss 16.326748750410008 Accuracy 1.0 Time 3.377s\n",
            " correct:  tensor(41984) Total:  41984\n",
            "Epoch 2, Batch 656, Loss 16.327372661451015 Accuracy 1.0 Time 3.314s\n",
            " correct:  tensor(42048) Total:  42048\n",
            "Epoch 2, Batch 657, Loss 16.328028372615076 Accuracy 1.0 Time 3.192s\n",
            " correct:  tensor(42112) Total:  42112\n",
            "Epoch 2, Batch 658, Loss 16.32720619132091 Accuracy 1.0 Time 3.178s\n",
            " correct:  tensor(42176) Total:  42176\n",
            "Epoch 2, Batch 659, Loss 16.328076456674857 Accuracy 1.0 Time 3.181s\n",
            " correct:  tensor(42240) Total:  42240\n",
            "Epoch 2, Batch 660, Loss 16.328222355698095 Accuracy 1.0 Time 3.153s\n",
            " correct:  tensor(42304) Total:  42304\n",
            "Epoch 2, Batch 661, Loss 16.32878002853509 Accuracy 1.0 Time 3.208s\n",
            " correct:  tensor(42368) Total:  42368\n",
            "Epoch 2, Batch 662, Loss 16.329550290755996 Accuracy 1.0 Time 3.129s\n",
            " correct:  tensor(42432) Total:  42432\n",
            "Epoch 2, Batch 663, Loss 16.330013398850845 Accuracy 1.0 Time 3.151s\n",
            " correct:  tensor(42496) Total:  42496\n",
            "Epoch 2, Batch 664, Loss 16.32964186496045 Accuracy 1.0 Time 3.204s\n",
            " correct:  tensor(42560) Total:  42560\n",
            "Epoch 2, Batch 665, Loss 16.32807397053654 Accuracy 1.0 Time 3.16s\n",
            " correct:  tensor(42624) Total:  42624\n",
            "Epoch 2, Batch 666, Loss 16.328371756785625 Accuracy 1.0 Time 3.179s\n",
            " correct:  tensor(42688) Total:  42688\n",
            "Epoch 2, Batch 667, Loss 16.32878410047677 Accuracy 1.0 Time 3.125s\n",
            " correct:  tensor(42752) Total:  42752\n",
            "Epoch 2, Batch 668, Loss 16.329151394838345 Accuracy 1.0 Time 3.145s\n",
            " correct:  tensor(42816) Total:  42816\n",
            "Epoch 2, Batch 669, Loss 16.329704987450388 Accuracy 1.0 Time 3.113s\n",
            " correct:  tensor(42880) Total:  42880\n",
            "Epoch 2, Batch 670, Loss 16.330133775454847 Accuracy 1.0 Time 3.156s\n",
            " correct:  tensor(42944) Total:  42944\n",
            "Epoch 2, Batch 671, Loss 16.33004493969208 Accuracy 1.0 Time 3.139s\n",
            " correct:  tensor(43008) Total:  43008\n",
            "Epoch 2, Batch 672, Loss 16.330813974142075 Accuracy 1.0 Time 3.182s\n",
            " correct:  tensor(43072) Total:  43072\n",
            "Epoch 2, Batch 673, Loss 16.32995079318269 Accuracy 1.0 Time 3.132s\n",
            " correct:  tensor(43136) Total:  43136\n",
            "Epoch 2, Batch 674, Loss 16.329286170642526 Accuracy 1.0 Time 3.149s\n",
            " correct:  tensor(43200) Total:  43200\n",
            "Epoch 2, Batch 675, Loss 16.330062614723488 Accuracy 1.0 Time 3.113s\n",
            " correct:  tensor(43264) Total:  43264\n",
            "Epoch 2, Batch 676, Loss 16.328912122715153 Accuracy 1.0 Time 3.047s\n",
            " correct:  tensor(43328) Total:  43328\n",
            "Epoch 2, Batch 677, Loss 16.329721721271646 Accuracy 1.0 Time 3.122s\n",
            " correct:  tensor(43392) Total:  43392\n",
            "Epoch 2, Batch 678, Loss 16.329176939342577 Accuracy 1.0 Time 3.078s\n",
            " correct:  tensor(43456) Total:  43456\n",
            "Epoch 2, Batch 679, Loss 16.328750829457885 Accuracy 1.0 Time 3.122s\n",
            " correct:  tensor(43520) Total:  43520\n",
            "Epoch 2, Batch 680, Loss 16.327869094119354 Accuracy 1.0 Time 3.123s\n",
            " correct:  tensor(43584) Total:  43584\n",
            "Epoch 2, Batch 681, Loss 16.32893657964407 Accuracy 1.0 Time 3.099s\n",
            " correct:  tensor(43648) Total:  43648\n",
            "Epoch 2, Batch 682, Loss 16.329060897100007 Accuracy 1.0 Time 3.077s\n",
            " correct:  tensor(43712) Total:  43712\n",
            "Epoch 2, Batch 683, Loss 16.32812966504523 Accuracy 1.0 Time 3.123s\n",
            " correct:  tensor(43776) Total:  43776\n",
            "Epoch 2, Batch 684, Loss 16.328175412284004 Accuracy 1.0 Time 3.128s\n",
            " correct:  tensor(43840) Total:  43840\n",
            "Epoch 2, Batch 685, Loss 16.32777761055605 Accuracy 1.0 Time 3.085s\n",
            " correct:  tensor(43904) Total:  43904\n",
            "Epoch 2, Batch 686, Loss 16.328688039028957 Accuracy 1.0 Time 3.045s\n",
            " correct:  tensor(43968) Total:  43968\n",
            "Epoch 2, Batch 687, Loss 16.327590467627914 Accuracy 1.0 Time 3.077s\n",
            " correct:  tensor(44032) Total:  44032\n",
            "Epoch 2, Batch 688, Loss 16.3290686662807 Accuracy 1.0 Time 2.989s\n",
            " correct:  tensor(44096) Total:  44096\n",
            "Epoch 2, Batch 689, Loss 16.328415369606848 Accuracy 1.0 Time 3.049s\n",
            " correct:  tensor(44160) Total:  44160\n",
            "Epoch 2, Batch 690, Loss 16.328365023239797 Accuracy 1.0 Time 3.092s\n",
            " correct:  tensor(44224) Total:  44224\n",
            "Epoch 2, Batch 691, Loss 16.327723078030754 Accuracy 1.0 Time 3.144s\n",
            " correct:  tensor(44288) Total:  44288\n",
            "Epoch 2, Batch 692, Loss 16.32693748529247 Accuracy 1.0 Time 3.129s\n",
            " correct:  tensor(44352) Total:  44352\n",
            "Epoch 2, Batch 693, Loss 16.32577712264068 Accuracy 1.0 Time 3.086s\n",
            " correct:  tensor(44416) Total:  44416\n",
            "Epoch 2, Batch 694, Loss 16.32407121905676 Accuracy 1.0 Time 3.082s\n",
            " correct:  tensor(44480) Total:  44480\n",
            "Epoch 2, Batch 695, Loss 16.32365740220324 Accuracy 1.0 Time 3.109s\n",
            " correct:  tensor(44544) Total:  44544\n",
            "Epoch 2, Batch 696, Loss 16.322836014046064 Accuracy 1.0 Time 3.046s\n",
            " correct:  tensor(44608) Total:  44608\n",
            "Epoch 2, Batch 697, Loss 16.32176898781162 Accuracy 1.0 Time 3.063s\n",
            " correct:  tensor(44672) Total:  44672\n",
            "Epoch 2, Batch 698, Loss 16.321234761814676 Accuracy 1.0 Time 3.037s\n",
            " correct:  tensor(44736) Total:  44736\n",
            "Epoch 2, Batch 699, Loss 16.31959387706926 Accuracy 1.0 Time 3.065s\n",
            " correct:  tensor(44800) Total:  44800\n",
            "Epoch 2, Batch 700, Loss 16.320694994245258 Accuracy 1.0 Time 3.13s\n",
            " correct:  tensor(44864) Total:  44864\n",
            "Epoch 2, Batch 701, Loss 16.319696328439317 Accuracy 1.0 Time 3.123s\n",
            " correct:  tensor(44928) Total:  44928\n",
            "Epoch 2, Batch 702, Loss 16.32082571263327 Accuracy 1.0 Time 3.071s\n",
            " correct:  tensor(44992) Total:  44992\n",
            "Epoch 2, Batch 703, Loss 16.32027533247664 Accuracy 1.0 Time 3.104s\n",
            " correct:  tensor(45056) Total:  45056\n",
            "Epoch 2, Batch 704, Loss 16.320514023303986 Accuracy 1.0 Time 3.12s\n",
            " correct:  tensor(45120) Total:  45120\n",
            "Epoch 2, Batch 705, Loss 16.31956609631261 Accuracy 1.0 Time 3.076s\n",
            " correct:  tensor(45184) Total:  45184\n",
            "Epoch 2, Batch 706, Loss 16.319969844547952 Accuracy 1.0 Time 3.061s\n",
            " correct:  tensor(45248) Total:  45248\n",
            "Epoch 2, Batch 707, Loss 16.31964946568771 Accuracy 1.0 Time 3.102s\n",
            " correct:  tensor(45312) Total:  45312\n",
            "Epoch 2, Batch 708, Loss 16.319242811472403 Accuracy 1.0 Time 3.076s\n",
            " correct:  tensor(45376) Total:  45376\n",
            "Epoch 2, Batch 709, Loss 16.319185560950103 Accuracy 1.0 Time 3.058s\n",
            " correct:  tensor(45440) Total:  45440\n",
            "Epoch 2, Batch 710, Loss 16.319596881597814 Accuracy 1.0 Time 3.092s\n",
            " correct:  tensor(45504) Total:  45504\n",
            "Epoch 2, Batch 711, Loss 16.321007212170738 Accuracy 1.0 Time 3.057s\n",
            " correct:  tensor(45568) Total:  45568\n",
            "Epoch 2, Batch 712, Loss 16.321559584542605 Accuracy 1.0 Time 3.074s\n",
            " correct:  tensor(45632) Total:  45632\n",
            "Epoch 2, Batch 713, Loss 16.320351579246147 Accuracy 1.0 Time 3.088s\n",
            " correct:  tensor(45696) Total:  45696\n",
            "Epoch 2, Batch 714, Loss 16.321363640098678 Accuracy 1.0 Time 3.075s\n",
            " correct:  tensor(45760) Total:  45760\n",
            "Epoch 2, Batch 715, Loss 16.32108905098655 Accuracy 1.0 Time 3.107s\n",
            " correct:  tensor(45824) Total:  45824\n",
            "Epoch 2, Batch 716, Loss 16.318676861970783 Accuracy 1.0 Time 3.091s\n",
            " correct:  tensor(45888) Total:  45888\n",
            "Epoch 2, Batch 717, Loss 16.3180372206237 Accuracy 1.0 Time 3.089s\n",
            " correct:  tensor(45952) Total:  45952\n",
            "Epoch 2, Batch 718, Loss 16.31640997172066 Accuracy 1.0 Time 3.108s\n",
            " correct:  tensor(46016) Total:  46016\n",
            "Epoch 2, Batch 719, Loss 16.31649083122922 Accuracy 1.0 Time 3.152s\n",
            " correct:  tensor(46080) Total:  46080\n",
            "Epoch 2, Batch 720, Loss 16.315963125228883 Accuracy 1.0 Time 3.147s\n",
            " correct:  tensor(46144) Total:  46144\n",
            "Epoch 2, Batch 721, Loss 16.314424557097247 Accuracy 1.0 Time 3.098s\n",
            " correct:  tensor(46208) Total:  46208\n",
            "Epoch 2, Batch 722, Loss 16.314386518378008 Accuracy 1.0 Time 3.039s\n",
            " correct:  tensor(46272) Total:  46272\n",
            "Epoch 2, Batch 723, Loss 16.312689525292953 Accuracy 1.0 Time 3.086s\n",
            " correct:  tensor(46336) Total:  46336\n",
            "Epoch 2, Batch 724, Loss 16.3131220261695 Accuracy 1.0 Time 3.238s\n",
            " correct:  tensor(46400) Total:  46400\n",
            "Epoch 2, Batch 725, Loss 16.312333584489494 Accuracy 1.0 Time 3.092s\n",
            " correct:  tensor(46464) Total:  46464\n",
            "Epoch 2, Batch 726, Loss 16.313542221531723 Accuracy 1.0 Time 3.15s\n",
            " correct:  tensor(46528) Total:  46528\n",
            "Epoch 2, Batch 727, Loss 16.312337397873154 Accuracy 1.0 Time 3.078s\n",
            " correct:  tensor(46592) Total:  46592\n",
            "Epoch 2, Batch 728, Loss 16.3121117078341 Accuracy 1.0 Time 3.079s\n",
            " correct:  tensor(46656) Total:  46656\n",
            "Epoch 2, Batch 729, Loss 16.312378589017893 Accuracy 1.0 Time 3.09s\n",
            " correct:  tensor(46720) Total:  46720\n",
            "Epoch 2, Batch 730, Loss 16.312315159627836 Accuracy 1.0 Time 3.127s\n",
            " correct:  tensor(46784) Total:  46784\n",
            "Epoch 2, Batch 731, Loss 16.312075334436752 Accuracy 1.0 Time 3.196s\n",
            " correct:  tensor(46848) Total:  46848\n",
            "Epoch 2, Batch 732, Loss 16.313088161697806 Accuracy 1.0 Time 3.35s\n",
            " correct:  tensor(46912) Total:  46912\n",
            "Epoch 2, Batch 733, Loss 16.31255689519489 Accuracy 1.0 Time 3.342s\n",
            " correct:  tensor(46976) Total:  46976\n",
            "Epoch 2, Batch 734, Loss 16.31320011908092 Accuracy 1.0 Time 3.296s\n",
            " correct:  tensor(47040) Total:  47040\n",
            "Epoch 2, Batch 735, Loss 16.31212532536513 Accuracy 1.0 Time 3.151s\n",
            " correct:  tensor(47104) Total:  47104\n",
            "Epoch 2, Batch 736, Loss 16.311401448819947 Accuracy 1.0 Time 3.103s\n",
            " correct:  tensor(47168) Total:  47168\n",
            "Epoch 2, Batch 737, Loss 16.311801658071364 Accuracy 1.0 Time 3.142s\n",
            " correct:  tensor(47232) Total:  47232\n",
            "Epoch 2, Batch 738, Loss 16.311091509614858 Accuracy 1.0 Time 3.169s\n",
            " correct:  tensor(47296) Total:  47296\n",
            "Epoch 2, Batch 739, Loss 16.310995482624143 Accuracy 1.0 Time 3.086s\n",
            " correct:  tensor(47360) Total:  47360\n",
            "Epoch 2, Batch 740, Loss 16.310928518707687 Accuracy 1.0 Time 3.126s\n",
            " correct:  tensor(47424) Total:  47424\n",
            "Epoch 2, Batch 741, Loss 16.30997696000072 Accuracy 1.0 Time 3.154s\n",
            " correct:  tensor(47488) Total:  47488\n",
            "Epoch 2, Batch 742, Loss 16.308526000565273 Accuracy 1.0 Time 3.164s\n",
            " correct:  tensor(47552) Total:  47552\n",
            "Epoch 2, Batch 743, Loss 16.30881995949418 Accuracy 1.0 Time 3.156s\n",
            " correct:  tensor(47616) Total:  47616\n",
            "Epoch 2, Batch 744, Loss 16.309906821097098 Accuracy 1.0 Time 3.147s\n",
            " correct:  tensor(47680) Total:  47680\n",
            "Epoch 2, Batch 745, Loss 16.308828662385878 Accuracy 1.0 Time 3.225s\n",
            " correct:  tensor(47744) Total:  47744\n",
            "Epoch 2, Batch 746, Loss 16.308574294276916 Accuracy 1.0 Time 3.159s\n",
            " correct:  tensor(47808) Total:  47808\n",
            "Epoch 2, Batch 747, Loss 16.3081821801672 Accuracy 1.0 Time 3.178s\n",
            " correct:  tensor(47872) Total:  47872\n",
            "Epoch 2, Batch 748, Loss 16.307388885773438 Accuracy 1.0 Time 3.135s\n",
            " correct:  tensor(47936) Total:  47936\n",
            "Epoch 2, Batch 749, Loss 16.306853574490514 Accuracy 1.0 Time 3.209s\n",
            " correct:  tensor(48000) Total:  48000\n",
            "Epoch 2, Batch 750, Loss 16.307040314992268 Accuracy 1.0 Time 3.129s\n",
            " correct:  tensor(48064) Total:  48064\n",
            "Epoch 2, Batch 751, Loss 16.30536795487893 Accuracy 1.0 Time 3.15s\n",
            " correct:  tensor(48128) Total:  48128\n",
            "Epoch 2, Batch 752, Loss 16.304308362463686 Accuracy 1.0 Time 3.228s\n",
            " correct:  tensor(48192) Total:  48192\n",
            "Epoch 2, Batch 753, Loss 16.30389238892007 Accuracy 1.0 Time 3.326s\n",
            " correct:  tensor(48256) Total:  48256\n",
            "Epoch 2, Batch 754, Loss 16.30454892884515 Accuracy 1.0 Time 3.159s\n",
            " correct:  tensor(48320) Total:  48320\n",
            "Epoch 2, Batch 755, Loss 16.305936267675943 Accuracy 1.0 Time 3.171s\n",
            " correct:  tensor(48384) Total:  48384\n",
            "Epoch 2, Batch 756, Loss 16.305928790380083 Accuracy 1.0 Time 3.316s\n",
            " correct:  tensor(48448) Total:  48448\n",
            "Epoch 2, Batch 757, Loss 16.30542454788833 Accuracy 1.0 Time 3.214s\n",
            " correct:  tensor(48512) Total:  48512\n",
            "Epoch 2, Batch 758, Loss 16.306147678546353 Accuracy 1.0 Time 3.154s\n",
            " correct:  tensor(48576) Total:  48576\n",
            "Epoch 2, Batch 759, Loss 16.30523654751784 Accuracy 1.0 Time 3.179s\n",
            " correct:  tensor(48640) Total:  48640\n",
            "Epoch 2, Batch 760, Loss 16.304019230290464 Accuracy 1.0 Time 3.192s\n",
            " correct:  tensor(48704) Total:  48704\n",
            "Epoch 2, Batch 761, Loss 16.30254441460548 Accuracy 1.0 Time 3.183s\n",
            " correct:  tensor(48768) Total:  48768\n",
            "Epoch 2, Batch 762, Loss 16.301637560676713 Accuracy 1.0 Time 3.186s\n",
            " correct:  tensor(48832) Total:  48832\n",
            "Epoch 2, Batch 763, Loss 16.300451871139508 Accuracy 1.0 Time 3.12s\n",
            " correct:  tensor(48896) Total:  48896\n",
            "Epoch 2, Batch 764, Loss 16.297831209542238 Accuracy 1.0 Time 3.124s\n",
            " correct:  tensor(48960) Total:  48960\n",
            "Epoch 2, Batch 765, Loss 16.29764470742419 Accuracy 1.0 Time 3.158s\n",
            " correct:  tensor(49024) Total:  49024\n",
            "Epoch 2, Batch 766, Loss 16.297355443939527 Accuracy 1.0 Time 3.168s\n",
            " correct:  tensor(49088) Total:  49088\n",
            "Epoch 2, Batch 767, Loss 16.297556519042093 Accuracy 1.0 Time 3.16s\n",
            " correct:  tensor(49152) Total:  49152\n",
            "Epoch 2, Batch 768, Loss 16.29689420138796 Accuracy 1.0 Time 3.131s\n",
            " correct:  tensor(49216) Total:  49216\n",
            "Epoch 2, Batch 769, Loss 16.297295250415182 Accuracy 1.0 Time 3.152s\n",
            " correct:  tensor(49280) Total:  49280\n",
            "Epoch 2, Batch 770, Loss 16.29775553988172 Accuracy 1.0 Time 3.15s\n",
            " correct:  tensor(49344) Total:  49344\n",
            "Epoch 2, Batch 771, Loss 16.297043833751157 Accuracy 1.0 Time 3.188s\n",
            " correct:  tensor(49408) Total:  49408\n",
            "Epoch 2, Batch 772, Loss 16.29719987187361 Accuracy 1.0 Time 3.167s\n",
            " correct:  tensor(49472) Total:  49472\n",
            "Epoch 2, Batch 773, Loss 16.295790268872103 Accuracy 1.0 Time 3.157s\n",
            " correct:  tensor(49536) Total:  49536\n",
            "Epoch 2, Batch 774, Loss 16.297111190874755 Accuracy 1.0 Time 3.148s\n",
            " correct:  tensor(49600) Total:  49600\n",
            "Epoch 2, Batch 775, Loss 16.296963341005387 Accuracy 1.0 Time 3.183s\n",
            " correct:  tensor(49664) Total:  49664\n",
            "Epoch 2, Batch 776, Loss 16.296768729219732 Accuracy 1.0 Time 3.104s\n",
            " correct:  tensor(49728) Total:  49728\n",
            "Epoch 2, Batch 777, Loss 16.296817781995777 Accuracy 1.0 Time 3.175s\n",
            " correct:  tensor(49792) Total:  49792\n",
            "Epoch 2, Batch 778, Loss 16.296389068613323 Accuracy 1.0 Time 3.179s\n",
            " correct:  tensor(49856) Total:  49856\n",
            "Epoch 2, Batch 779, Loss 16.295476198502467 Accuracy 1.0 Time 3.226s\n",
            " correct:  tensor(49920) Total:  49920\n",
            "Epoch 2, Batch 780, Loss 16.29450423411834 Accuracy 1.0 Time 3.174s\n",
            " correct:  tensor(49984) Total:  49984\n",
            "Epoch 2, Batch 781, Loss 16.294432304274867 Accuracy 1.0 Time 3.175s\n",
            " correct:  tensor(50048) Total:  50048\n",
            "Epoch 2, Batch 782, Loss 16.295368989715186 Accuracy 1.0 Time 3.178s\n",
            " correct:  tensor(50112) Total:  50112\n",
            "Epoch 2, Batch 783, Loss 16.294884171370192 Accuracy 1.0 Time 3.166s\n",
            " correct:  tensor(50176) Total:  50176\n",
            "Epoch 2, Batch 784, Loss 16.29402220249176 Accuracy 1.0 Time 3.148s\n",
            " correct:  tensor(50240) Total:  50240\n",
            "Epoch 2, Batch 785, Loss 16.293082076880584 Accuracy 1.0 Time 3.125s\n",
            " correct:  tensor(50304) Total:  50304\n",
            "Epoch 2, Batch 786, Loss 16.292619804692936 Accuracy 1.0 Time 3.172s\n",
            " correct:  tensor(50368) Total:  50368\n",
            "Epoch 2, Batch 787, Loss 16.29278735799438 Accuracy 1.0 Time 3.14s\n",
            " correct:  tensor(50432) Total:  50432\n",
            "Epoch 2, Batch 788, Loss 16.292878165462902 Accuracy 1.0 Time 3.145s\n",
            " correct:  tensor(50496) Total:  50496\n",
            "Epoch 2, Batch 789, Loss 16.29190110012001 Accuracy 1.0 Time 3.172s\n",
            " correct:  tensor(50560) Total:  50560\n",
            "Epoch 2, Batch 790, Loss 16.291779304456107 Accuracy 1.0 Time 3.12s\n",
            " correct:  tensor(50624) Total:  50624\n",
            "Epoch 2, Batch 791, Loss 16.291909478256283 Accuracy 1.0 Time 3.111s\n",
            " correct:  tensor(50688) Total:  50688\n",
            "Epoch 2, Batch 792, Loss 16.290000215925353 Accuracy 1.0 Time 3.164s\n",
            " correct:  tensor(50752) Total:  50752\n",
            "Epoch 2, Batch 793, Loss 16.2901480808234 Accuracy 1.0 Time 3.108s\n",
            " correct:  tensor(50816) Total:  50816\n",
            "Epoch 2, Batch 794, Loss 16.290267354595272 Accuracy 1.0 Time 3.061s\n",
            " correct:  tensor(50880) Total:  50880\n",
            "Epoch 2, Batch 795, Loss 16.288635238311574 Accuracy 1.0 Time 3.125s\n",
            " correct:  tensor(50944) Total:  50944\n",
            "Epoch 2, Batch 796, Loss 16.288012844833297 Accuracy 1.0 Time 3.129s\n",
            " correct:  tensor(51008) Total:  51008\n",
            "Epoch 2, Batch 797, Loss 16.28687572718564 Accuracy 1.0 Time 3.099s\n",
            " correct:  tensor(51072) Total:  51072\n",
            "Epoch 2, Batch 798, Loss 16.28661212227996 Accuracy 1.0 Time 3.101s\n",
            " correct:  tensor(51136) Total:  51136\n",
            "Epoch 2, Batch 799, Loss 16.285845308936434 Accuracy 1.0 Time 3.166s\n",
            " correct:  tensor(51200) Total:  51200\n",
            "Epoch 2, Batch 800, Loss 16.284834877252578 Accuracy 1.0 Time 3.189s\n",
            " correct:  tensor(51264) Total:  51264\n",
            "Epoch 2, Batch 801, Loss 16.28457334544626 Accuracy 1.0 Time 3.161s\n",
            " correct:  tensor(51328) Total:  51328\n",
            "Epoch 2, Batch 802, Loss 16.28474834970108 Accuracy 1.0 Time 3.102s\n",
            " correct:  tensor(51392) Total:  51392\n",
            "Epoch 2, Batch 803, Loss 16.28410666075026 Accuracy 1.0 Time 3.144s\n",
            " correct:  tensor(51456) Total:  51456\n",
            "Epoch 2, Batch 804, Loss 16.28324061958351 Accuracy 1.0 Time 3.089s\n",
            " correct:  tensor(51520) Total:  51520\n",
            "Epoch 2, Batch 805, Loss 16.282591778299082 Accuracy 1.0 Time 3.164s\n",
            " correct:  tensor(51584) Total:  51584\n",
            "Epoch 2, Batch 806, Loss 16.28047112968956 Accuracy 1.0 Time 3.184s\n",
            " correct:  tensor(51648) Total:  51648\n",
            "Epoch 2, Batch 807, Loss 16.279782713567457 Accuracy 1.0 Time 3.167s\n",
            " correct:  tensor(51712) Total:  51712\n",
            "Epoch 2, Batch 808, Loss 16.279264074741025 Accuracy 1.0 Time 3.098s\n",
            " correct:  tensor(51776) Total:  51776\n",
            "Epoch 2, Batch 809, Loss 16.277011323181455 Accuracy 1.0 Time 3.152s\n",
            " correct:  tensor(51840) Total:  51840\n",
            "Epoch 2, Batch 810, Loss 16.27751319437851 Accuracy 1.0 Time 3.145s\n",
            " correct:  tensor(51904) Total:  51904\n",
            "Epoch 2, Batch 811, Loss 16.277300164379117 Accuracy 1.0 Time 3.136s\n",
            " correct:  tensor(51968) Total:  51968\n",
            "Epoch 2, Batch 812, Loss 16.276219163622176 Accuracy 1.0 Time 3.128s\n",
            " correct:  tensor(52032) Total:  52032\n",
            "Epoch 2, Batch 813, Loss 16.275217249824554 Accuracy 1.0 Time 3.091s\n",
            " correct:  tensor(52096) Total:  52096\n",
            "Epoch 2, Batch 814, Loss 16.27507443510051 Accuracy 1.0 Time 3.122s\n",
            " correct:  tensor(52160) Total:  52160\n",
            "Epoch 2, Batch 815, Loss 16.275161587533777 Accuracy 1.0 Time 3.161s\n",
            " correct:  tensor(52224) Total:  52224\n",
            "Epoch 2, Batch 816, Loss 16.276544815185023 Accuracy 1.0 Time 3.175s\n",
            " correct:  tensor(52288) Total:  52288\n",
            "Epoch 2, Batch 817, Loss 16.2773788515182 Accuracy 1.0 Time 3.169s\n",
            " correct:  tensor(52352) Total:  52352\n",
            "Epoch 2, Batch 818, Loss 16.276327468946686 Accuracy 1.0 Time 3.188s\n",
            " correct:  tensor(52416) Total:  52416\n",
            "Epoch 2, Batch 819, Loss 16.27613080202878 Accuracy 1.0 Time 3.197s\n",
            " correct:  tensor(52480) Total:  52480\n",
            "Epoch 2, Batch 820, Loss 16.276189422607423 Accuracy 1.0 Time 3.111s\n",
            " correct:  tensor(52544) Total:  52544\n",
            "Epoch 2, Batch 821, Loss 16.27565630518998 Accuracy 1.0 Time 3.217s\n",
            " correct:  tensor(52608) Total:  52608\n",
            "Epoch 2, Batch 822, Loss 16.273496029150746 Accuracy 1.0 Time 3.239s\n",
            " correct:  tensor(52672) Total:  52672\n",
            "Epoch 2, Batch 823, Loss 16.2725890357758 Accuracy 1.0 Time 3.199s\n",
            " correct:  tensor(52736) Total:  52736\n",
            "Epoch 2, Batch 824, Loss 16.271628751338106 Accuracy 1.0 Time 3.229s\n",
            " correct:  tensor(52800) Total:  52800\n",
            "Epoch 2, Batch 825, Loss 16.27066859852184 Accuracy 1.0 Time 3.211s\n",
            " correct:  tensor(52864) Total:  52864\n",
            "Epoch 2, Batch 826, Loss 16.2704568013152 Accuracy 1.0 Time 3.145s\n",
            " correct:  tensor(52928) Total:  52928\n",
            "Epoch 2, Batch 827, Loss 16.269816707671048 Accuracy 1.0 Time 3.265s\n",
            " correct:  tensor(52992) Total:  52992\n",
            "Epoch 2, Batch 828, Loss 16.269443740015443 Accuracy 1.0 Time 3.158s\n",
            " correct:  tensor(53056) Total:  53056\n",
            "Epoch 2, Batch 829, Loss 16.27236262506685 Accuracy 1.0 Time 3.287s\n",
            " correct:  tensor(53120) Total:  53120\n",
            "Epoch 2, Batch 830, Loss 16.271527482228105 Accuracy 1.0 Time 3.387s\n",
            " correct:  tensor(53184) Total:  53184\n",
            "Epoch 2, Batch 831, Loss 16.271007763923436 Accuracy 1.0 Time 3.369s\n",
            " correct:  tensor(53248) Total:  53248\n",
            "Epoch 2, Batch 832, Loss 16.271321242818466 Accuracy 1.0 Time 3.245s\n",
            " correct:  tensor(53312) Total:  53312\n",
            "Epoch 2, Batch 833, Loss 16.270434221585973 Accuracy 1.0 Time 3.162s\n",
            " correct:  tensor(53376) Total:  53376\n",
            "Epoch 2, Batch 834, Loss 16.27110820079593 Accuracy 1.0 Time 3.161s\n",
            " correct:  tensor(53440) Total:  53440\n",
            "Epoch 2, Batch 835, Loss 16.270022433389446 Accuracy 1.0 Time 3.132s\n",
            " correct:  tensor(53504) Total:  53504\n",
            "Epoch 2, Batch 836, Loss 16.26877560912137 Accuracy 1.0 Time 3.175s\n",
            " correct:  tensor(53568) Total:  53568\n",
            "Epoch 2, Batch 837, Loss 16.269578220454026 Accuracy 1.0 Time 3.173s\n",
            " correct:  tensor(53632) Total:  53632\n",
            "Epoch 2, Batch 838, Loss 16.269576721373493 Accuracy 1.0 Time 3.183s\n",
            " correct:  tensor(53696) Total:  53696\n",
            "Epoch 2, Batch 839, Loss 16.2691440605009 Accuracy 1.0 Time 3.103s\n",
            " correct:  tensor(53760) Total:  53760\n",
            "Epoch 2, Batch 840, Loss 16.268784101804098 Accuracy 1.0 Time 3.133s\n",
            " correct:  tensor(53824) Total:  53824\n",
            "Epoch 2, Batch 841, Loss 16.267599693234835 Accuracy 1.0 Time 3.163s\n",
            " correct:  tensor(53888) Total:  53888\n",
            "Epoch 2, Batch 842, Loss 16.2662531406466 Accuracy 1.0 Time 3.101s\n",
            " correct:  tensor(53952) Total:  53952\n",
            "Epoch 2, Batch 843, Loss 16.26470694423146 Accuracy 1.0 Time 3.158s\n",
            " correct:  tensor(54016) Total:  54016\n",
            "Epoch 2, Batch 844, Loss 16.26442298730968 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(54080) Total:  54080\n",
            "Epoch 2, Batch 845, Loss 16.26444383542213 Accuracy 1.0 Time 3.151s\n",
            " correct:  tensor(54144) Total:  54144\n",
            "Epoch 2, Batch 846, Loss 16.26323880653291 Accuracy 1.0 Time 3.146s\n",
            " correct:  tensor(54208) Total:  54208\n",
            "Epoch 2, Batch 847, Loss 16.26371245964077 Accuracy 1.0 Time 3.156s\n",
            " correct:  tensor(54272) Total:  54272\n",
            "Epoch 2, Batch 848, Loss 16.262433325344663 Accuracy 1.0 Time 3.234s\n",
            " correct:  tensor(54336) Total:  54336\n",
            "Epoch 2, Batch 849, Loss 16.26236490647279 Accuracy 1.0 Time 3.439s\n",
            " correct:  tensor(54400) Total:  54400\n",
            "Epoch 2, Batch 850, Loss 16.261988329045913 Accuracy 1.0 Time 3.244s\n",
            " correct:  tensor(54464) Total:  54464\n",
            "Epoch 2, Batch 851, Loss 16.262402772062956 Accuracy 1.0 Time 3.259s\n",
            " correct:  tensor(54528) Total:  54528\n",
            "Epoch 2, Batch 852, Loss 16.26225560036064 Accuracy 1.0 Time 3.191s\n",
            " correct:  tensor(54592) Total:  54592\n",
            "Epoch 2, Batch 853, Loss 16.26180658195111 Accuracy 1.0 Time 3.203s\n",
            " correct:  tensor(54656) Total:  54656\n",
            "Epoch 2, Batch 854, Loss 16.26161789503254 Accuracy 1.0 Time 3.157s\n",
            " correct:  tensor(54720) Total:  54720\n",
            "Epoch 2, Batch 855, Loss 16.26104071544625 Accuracy 1.0 Time 3.161s\n",
            " correct:  tensor(54784) Total:  54784\n",
            "Epoch 2, Batch 856, Loss 16.259636326370952 Accuracy 1.0 Time 3.172s\n",
            " correct:  tensor(54848) Total:  54848\n",
            "Epoch 2, Batch 857, Loss 16.25926585069476 Accuracy 1.0 Time 3.169s\n",
            " correct:  tensor(54912) Total:  54912\n",
            "Epoch 2, Batch 858, Loss 16.25967859610533 Accuracy 1.0 Time 3.158s\n",
            " correct:  tensor(54976) Total:  54976\n",
            "Epoch 2, Batch 859, Loss 16.259718609633353 Accuracy 1.0 Time 3.165s\n",
            " correct:  tensor(55040) Total:  55040\n",
            "Epoch 2, Batch 860, Loss 16.25834011366201 Accuracy 1.0 Time 3.182s\n",
            " correct:  tensor(55104) Total:  55104\n",
            "Epoch 2, Batch 861, Loss 16.25810641666461 Accuracy 1.0 Time 3.169s\n",
            " correct:  tensor(55168) Total:  55168\n",
            "Epoch 2, Batch 862, Loss 16.257014012392048 Accuracy 1.0 Time 3.287s\n",
            " correct:  tensor(55232) Total:  55232\n",
            "Epoch 2, Batch 863, Loss 16.255677759163902 Accuracy 1.0 Time 3.162s\n",
            " correct:  tensor(55296) Total:  55296\n",
            "Epoch 2, Batch 864, Loss 16.254382268146234 Accuracy 1.0 Time 3.113s\n",
            " correct:  tensor(55360) Total:  55360\n",
            "Epoch 2, Batch 865, Loss 16.25324534708365 Accuracy 1.0 Time 3.114s\n",
            " correct:  tensor(55424) Total:  55424\n",
            "Epoch 2, Batch 866, Loss 16.253013072333214 Accuracy 1.0 Time 3.148s\n",
            " correct:  tensor(55488) Total:  55488\n",
            "Epoch 2, Batch 867, Loss 16.254677895718654 Accuracy 1.0 Time 3.117s\n",
            " correct:  tensor(55552) Total:  55552\n",
            "Epoch 2, Batch 868, Loss 16.25400152294317 Accuracy 1.0 Time 3.138s\n",
            " correct:  tensor(55616) Total:  55616\n",
            "Epoch 2, Batch 869, Loss 16.25338219444003 Accuracy 1.0 Time 3.104s\n",
            " correct:  tensor(55680) Total:  55680\n",
            "Epoch 2, Batch 870, Loss 16.25388138869713 Accuracy 1.0 Time 3.04s\n",
            " correct:  tensor(55744) Total:  55744\n",
            "Epoch 2, Batch 871, Loss 16.254124892154874 Accuracy 1.0 Time 3.093s\n",
            " correct:  tensor(55808) Total:  55808\n",
            "Epoch 2, Batch 872, Loss 16.253972703163775 Accuracy 1.0 Time 3.093s\n",
            " correct:  tensor(55872) Total:  55872\n",
            "Epoch 2, Batch 873, Loss 16.254608945202037 Accuracy 1.0 Time 3.094s\n",
            " correct:  tensor(55936) Total:  55936\n",
            "Epoch 2, Batch 874, Loss 16.254344551732395 Accuracy 1.0 Time 3.068s\n",
            " correct:  tensor(56000) Total:  56000\n",
            "Epoch 2, Batch 875, Loss 16.2538475189209 Accuracy 1.0 Time 3.109s\n",
            " correct:  tensor(56064) Total:  56064\n",
            "Epoch 2, Batch 876, Loss 16.254393133398604 Accuracy 1.0 Time 3.113s\n",
            " correct:  tensor(56128) Total:  56128\n",
            "Epoch 2, Batch 877, Loss 16.254466962651218 Accuracy 1.0 Time 3.144s\n",
            " correct:  tensor(56192) Total:  56192\n",
            "Epoch 2, Batch 878, Loss 16.25475189626081 Accuracy 1.0 Time 3.145s\n",
            " correct:  tensor(56256) Total:  56256\n",
            "Epoch 2, Batch 879, Loss 16.255969042121617 Accuracy 1.0 Time 3.105s\n",
            " correct:  tensor(56320) Total:  56320\n",
            "Epoch 2, Batch 880, Loss 16.256223830309782 Accuracy 1.0 Time 3.1s\n",
            " correct:  tensor(56384) Total:  56384\n",
            "Epoch 2, Batch 881, Loss 16.254995862417406 Accuracy 1.0 Time 3.113s\n",
            " correct:  tensor(56448) Total:  56448\n",
            "Epoch 2, Batch 882, Loss 16.25506275743584 Accuracy 1.0 Time 3.103s\n",
            " correct:  tensor(56512) Total:  56512\n",
            "Epoch 2, Batch 883, Loss 16.25613362170832 Accuracy 1.0 Time 3.085s\n",
            " correct:  tensor(56576) Total:  56576\n",
            "Epoch 2, Batch 884, Loss 16.256270964221176 Accuracy 1.0 Time 3.054s\n",
            " correct:  tensor(56640) Total:  56640\n",
            "Epoch 2, Batch 885, Loss 16.254674667961854 Accuracy 1.0 Time 3.072s\n",
            " correct:  tensor(56704) Total:  56704\n",
            "Epoch 2, Batch 886, Loss 16.255530734094485 Accuracy 1.0 Time 3.191s\n",
            " correct:  tensor(56768) Total:  56768\n",
            "Epoch 2, Batch 887, Loss 16.25492941567164 Accuracy 1.0 Time 3.18s\n",
            " correct:  tensor(56832) Total:  56832\n",
            "Epoch 2, Batch 888, Loss 16.254603884241604 Accuracy 1.0 Time 3.162s\n",
            " correct:  tensor(56896) Total:  56896\n",
            "Epoch 2, Batch 889, Loss 16.254244846219557 Accuracy 1.0 Time 3.107s\n",
            " correct:  tensor(56960) Total:  56960\n",
            "Epoch 2, Batch 890, Loss 16.254658045393697 Accuracy 1.0 Time 3.128s\n",
            " correct:  tensor(57024) Total:  57024\n",
            "Epoch 2, Batch 891, Loss 16.255730482330492 Accuracy 1.0 Time 3.145s\n",
            " correct:  tensor(57088) Total:  57088\n",
            "Epoch 2, Batch 892, Loss 16.254389972430175 Accuracy 1.0 Time 3.081s\n",
            " correct:  tensor(57152) Total:  57152\n",
            "Epoch 2, Batch 893, Loss 16.254383422492733 Accuracy 1.0 Time 3.022s\n",
            " correct:  tensor(57216) Total:  57216\n",
            "Epoch 2, Batch 894, Loss 16.25440636790572 Accuracy 1.0 Time 3.107s\n",
            " correct:  tensor(57280) Total:  57280\n",
            "Epoch 2, Batch 895, Loss 16.254483530907656 Accuracy 1.0 Time 3.137s\n",
            " correct:  tensor(57344) Total:  57344\n",
            "Epoch 2, Batch 896, Loss 16.25543970508235 Accuracy 1.0 Time 3.087s\n",
            " correct:  tensor(57408) Total:  57408\n",
            "Epoch 2, Batch 897, Loss 16.25511858481893 Accuracy 1.0 Time 3.121s\n",
            " correct:  tensor(57472) Total:  57472\n",
            "Epoch 2, Batch 898, Loss 16.254785077874537 Accuracy 1.0 Time 3.11s\n",
            " correct:  tensor(57536) Total:  57536\n",
            "Epoch 2, Batch 899, Loss 16.25541844118689 Accuracy 1.0 Time 3.08s\n",
            " correct:  tensor(57600) Total:  57600\n",
            "Epoch 2, Batch 900, Loss 16.25621784740024 Accuracy 1.0 Time 3.084s\n",
            " correct:  tensor(57664) Total:  57664\n",
            "Epoch 2, Batch 901, Loss 16.25696223636843 Accuracy 1.0 Time 3.058s\n",
            " correct:  tensor(57728) Total:  57728\n",
            "Epoch 2, Batch 902, Loss 16.257138699491378 Accuracy 1.0 Time 3.072s\n",
            " correct:  tensor(57792) Total:  57792\n",
            "Epoch 2, Batch 903, Loss 16.256759347841722 Accuracy 1.0 Time 3.085s\n",
            " correct:  tensor(57856) Total:  57856\n",
            "Epoch 2, Batch 904, Loss 16.255212496867223 Accuracy 1.0 Time 3.077s\n",
            " correct:  tensor(57920) Total:  57920\n",
            "Epoch 2, Batch 905, Loss 16.25458655278327 Accuracy 1.0 Time 3.102s\n",
            " correct:  tensor(57984) Total:  57984\n",
            "Epoch 2, Batch 906, Loss 16.25385551157903 Accuracy 1.0 Time 3.088s\n",
            " correct:  tensor(58048) Total:  58048\n",
            "Epoch 2, Batch 907, Loss 16.25243973232551 Accuracy 1.0 Time 3.091s\n",
            " correct:  tensor(58112) Total:  58112\n",
            "Epoch 2, Batch 908, Loss 16.25230164464875 Accuracy 1.0 Time 3.095s\n",
            " correct:  tensor(58176) Total:  58176\n",
            "Epoch 2, Batch 909, Loss 16.250884030911788 Accuracy 1.0 Time 3.035s\n",
            " correct:  tensor(58240) Total:  58240\n",
            "Epoch 2, Batch 910, Loss 16.251270997393263 Accuracy 1.0 Time 3.116s\n",
            " correct:  tensor(58304) Total:  58304\n",
            "Epoch 2, Batch 911, Loss 16.250239793084454 Accuracy 1.0 Time 3.133s\n",
            " correct:  tensor(58368) Total:  58368\n",
            "Epoch 2, Batch 912, Loss 16.249361780651828 Accuracy 1.0 Time 3.11s\n",
            " correct:  tensor(58432) Total:  58432\n",
            "Epoch 2, Batch 913, Loss 16.248779784679936 Accuracy 1.0 Time 3.112s\n",
            " correct:  tensor(58496) Total:  58496\n",
            "Epoch 2, Batch 914, Loss 16.24789150225516 Accuracy 1.0 Time 3.193s\n",
            " correct:  tensor(58560) Total:  58560\n",
            "Epoch 2, Batch 915, Loss 16.247372137392805 Accuracy 1.0 Time 3.147s\n",
            " correct:  tensor(58624) Total:  58624\n",
            "Epoch 2, Batch 916, Loss 16.24754746303808 Accuracy 1.0 Time 3.096s\n",
            " correct:  tensor(58688) Total:  58688\n",
            "Epoch 2, Batch 917, Loss 16.24750580886458 Accuracy 1.0 Time 3.104s\n",
            " correct:  tensor(58752) Total:  58752\n",
            "Epoch 2, Batch 918, Loss 16.24646577440316 Accuracy 1.0 Time 3.102s\n",
            " correct:  tensor(58816) Total:  58816\n",
            "Epoch 2, Batch 919, Loss 16.246370873850758 Accuracy 1.0 Time 3.066s\n",
            " correct:  tensor(58880) Total:  58880\n",
            "Epoch 2, Batch 920, Loss 16.245568115814873 Accuracy 1.0 Time 3.191s\n",
            " correct:  tensor(58944) Total:  58944\n",
            "Epoch 2, Batch 921, Loss 16.2447651532781 Accuracy 1.0 Time 3.121s\n",
            " correct:  tensor(59008) Total:  59008\n",
            "Epoch 2, Batch 922, Loss 16.244926310930232 Accuracy 1.0 Time 3.096s\n",
            " correct:  tensor(59072) Total:  59072\n",
            "Epoch 2, Batch 923, Loss 16.24437827382031 Accuracy 1.0 Time 3.056s\n",
            " correct:  tensor(59136) Total:  59136\n",
            "Epoch 2, Batch 924, Loss 16.243942518770954 Accuracy 1.0 Time 3.061s\n",
            " correct:  tensor(59200) Total:  59200\n",
            "Epoch 2, Batch 925, Loss 16.244074362677498 Accuracy 1.0 Time 3.189s\n",
            " correct:  tensor(59264) Total:  59264\n",
            "Epoch 2, Batch 926, Loss 16.244409309865073 Accuracy 1.0 Time 3.101s\n",
            " correct:  tensor(59328) Total:  59328\n",
            "Epoch 2, Batch 927, Loss 16.24362850189209 Accuracy 1.0 Time 3.067s\n",
            " correct:  tensor(59392) Total:  59392\n",
            "Epoch 2, Batch 928, Loss 16.24274599963221 Accuracy 1.0 Time 3.289s\n",
            " correct:  tensor(59456) Total:  59456\n",
            "Epoch 2, Batch 929, Loss 16.243405722955842 Accuracy 1.0 Time 3.341s\n",
            " correct:  tensor(59520) Total:  59520\n",
            "Epoch 2, Batch 930, Loss 16.241989394157162 Accuracy 1.0 Time 3.369s\n",
            " correct:  tensor(59584) Total:  59584\n",
            "Epoch 2, Batch 931, Loss 16.24150096204944 Accuracy 1.0 Time 3.125s\n",
            " correct:  tensor(59648) Total:  59648\n",
            "Epoch 2, Batch 932, Loss 16.24137134204095 Accuracy 1.0 Time 3.06s\n",
            " correct:  tensor(59712) Total:  59712\n",
            "Epoch 2, Batch 933, Loss 16.240390606887342 Accuracy 1.0 Time 3.091s\n",
            " correct:  tensor(59776) Total:  59776\n",
            "Epoch 2, Batch 934, Loss 16.24041099977187 Accuracy 1.0 Time 3.142s\n",
            " correct:  tensor(59840) Total:  59840\n",
            "Epoch 2, Batch 935, Loss 16.239039676074675 Accuracy 1.0 Time 3.104s\n",
            " correct:  tensor(59904) Total:  59904\n",
            "Epoch 2, Batch 936, Loss 16.23905897853721 Accuracy 1.0 Time 3.149s\n",
            " correct:  tensor(59968) Total:  59968\n",
            "Epoch 2, Batch 937, Loss 16.23825750381486 Accuracy 1.0 Time 3.14s\n",
            " correct:  tensor(60000) Total:  60000\n",
            "Epoch 2, Batch 938, Loss 16.229183221422534 Accuracy 1.0 Time 1.626s\n",
            "TRAIN Epoch 2, Loss 16.229183221422534 Accuracy 1.0 Time 2946.147s\n",
            "TESTING...\n",
            " correct:  tensor(64) Total:  64\n",
            " correct:  tensor(128) Total:  128\n",
            " correct:  tensor(192) Total:  192\n",
            " correct:  tensor(256) Total:  256\n",
            " correct:  tensor(320) Total:  320\n",
            " correct:  tensor(384) Total:  384\n",
            " correct:  tensor(448) Total:  448\n",
            " correct:  tensor(512) Total:  512\n",
            " correct:  tensor(576) Total:  576\n",
            " correct:  tensor(640) Total:  640\n",
            " correct:  tensor(704) Total:  704\n",
            " correct:  tensor(768) Total:  768\n",
            " correct:  tensor(832) Total:  832\n",
            " correct:  tensor(896) Total:  896\n",
            " correct:  tensor(960) Total:  960\n",
            " correct:  tensor(1024) Total:  1024\n",
            " correct:  tensor(1088) Total:  1088\n",
            " correct:  tensor(1152) Total:  1152\n",
            " correct:  tensor(1216) Total:  1216\n",
            " correct:  tensor(1280) Total:  1280\n",
            " correct:  tensor(1344) Total:  1344\n",
            " correct:  tensor(1408) Total:  1408\n",
            " correct:  tensor(1472) Total:  1472\n",
            " correct:  tensor(1536) Total:  1536\n",
            " correct:  tensor(1600) Total:  1600\n",
            " correct:  tensor(1664) Total:  1664\n",
            " correct:  tensor(1728) Total:  1728\n",
            " correct:  tensor(1792) Total:  1792\n",
            " correct:  tensor(1856) Total:  1856\n",
            " correct:  tensor(1920) Total:  1920\n",
            " correct:  tensor(1984) Total:  1984\n",
            " correct:  tensor(2048) Total:  2048\n",
            " correct:  tensor(2112) Total:  2112\n",
            " correct:  tensor(2176) Total:  2176\n",
            " correct:  tensor(2240) Total:  2240\n",
            " correct:  tensor(2304) Total:  2304\n",
            " correct:  tensor(2368) Total:  2368\n",
            " correct:  tensor(2432) Total:  2432\n",
            " correct:  tensor(2496) Total:  2496\n",
            " correct:  tensor(2560) Total:  2560\n",
            " correct:  tensor(2624) Total:  2624\n",
            " correct:  tensor(2688) Total:  2688\n",
            " correct:  tensor(2752) Total:  2752\n",
            " correct:  tensor(2816) Total:  2816\n",
            " correct:  tensor(2880) Total:  2880\n",
            " correct:  tensor(2944) Total:  2944\n",
            " correct:  tensor(3008) Total:  3008\n",
            " correct:  tensor(3072) Total:  3072\n",
            " correct:  tensor(3136) Total:  3136\n",
            " correct:  tensor(3200) Total:  3200\n",
            " correct:  tensor(3264) Total:  3264\n",
            " correct:  tensor(3328) Total:  3328\n",
            " correct:  tensor(3392) Total:  3392\n",
            " correct:  tensor(3456) Total:  3456\n",
            " correct:  tensor(3520) Total:  3520\n",
            " correct:  tensor(3584) Total:  3584\n",
            " correct:  tensor(3648) Total:  3648\n",
            " correct:  tensor(3712) Total:  3712\n",
            " correct:  tensor(3776) Total:  3776\n",
            " correct:  tensor(3840) Total:  3840\n",
            " correct:  tensor(3904) Total:  3904\n",
            " correct:  tensor(3968) Total:  3968\n",
            " correct:  tensor(4032) Total:  4032\n",
            " correct:  tensor(4096) Total:  4096\n",
            " correct:  tensor(4160) Total:  4160\n",
            " correct:  tensor(4224) Total:  4224\n",
            " correct:  tensor(4288) Total:  4288\n",
            " correct:  tensor(4352) Total:  4352\n",
            " correct:  tensor(4416) Total:  4416\n",
            " correct:  tensor(4480) Total:  4480\n",
            " correct:  tensor(4544) Total:  4544\n",
            " correct:  tensor(4608) Total:  4608\n",
            " correct:  tensor(4672) Total:  4672\n",
            " correct:  tensor(4736) Total:  4736\n",
            " correct:  tensor(4800) Total:  4800\n",
            " correct:  tensor(4864) Total:  4864\n",
            " correct:  tensor(4928) Total:  4928\n",
            " correct:  tensor(4992) Total:  4992\n",
            " correct:  tensor(5056) Total:  5056\n",
            " correct:  tensor(5120) Total:  5120\n",
            " correct:  tensor(5184) Total:  5184\n",
            " correct:  tensor(5248) Total:  5248\n",
            " correct:  tensor(5312) Total:  5312\n",
            " correct:  tensor(5376) Total:  5376\n",
            " correct:  tensor(5440) Total:  5440\n",
            " correct:  tensor(5504) Total:  5504\n",
            " correct:  tensor(5568) Total:  5568\n",
            " correct:  tensor(5632) Total:  5632\n",
            " correct:  tensor(5696) Total:  5696\n",
            " correct:  tensor(5760) Total:  5760\n",
            " correct:  tensor(5824) Total:  5824\n",
            " correct:  tensor(5888) Total:  5888\n",
            " correct:  tensor(5952) Total:  5952\n",
            " correct:  tensor(6016) Total:  6016\n",
            " correct:  tensor(6080) Total:  6080\n",
            " correct:  tensor(6144) Total:  6144\n",
            " correct:  tensor(6208) Total:  6208\n",
            " correct:  tensor(6272) Total:  6272\n",
            " correct:  tensor(6336) Total:  6336\n",
            " correct:  tensor(6400) Total:  6400\n",
            " correct:  tensor(6464) Total:  6464\n",
            " correct:  tensor(6528) Total:  6528\n",
            " correct:  tensor(6592) Total:  6592\n",
            " correct:  tensor(6656) Total:  6656\n",
            " correct:  tensor(6720) Total:  6720\n",
            " correct:  tensor(6784) Total:  6784\n",
            " correct:  tensor(6848) Total:  6848\n",
            " correct:  tensor(6912) Total:  6912\n",
            " correct:  tensor(6976) Total:  6976\n",
            " correct:  tensor(7040) Total:  7040\n",
            " correct:  tensor(7104) Total:  7104\n",
            " correct:  tensor(7168) Total:  7168\n",
            " correct:  tensor(7232) Total:  7232\n",
            " correct:  tensor(7296) Total:  7296\n",
            " correct:  tensor(7360) Total:  7360\n",
            " correct:  tensor(7424) Total:  7424\n",
            " correct:  tensor(7488) Total:  7488\n",
            " correct:  tensor(7552) Total:  7552\n",
            " correct:  tensor(7616) Total:  7616\n",
            " correct:  tensor(7680) Total:  7680\n",
            " correct:  tensor(7744) Total:  7744\n",
            " correct:  tensor(7808) Total:  7808\n",
            " correct:  tensor(7872) Total:  7872\n",
            " correct:  tensor(7936) Total:  7936\n",
            " correct:  tensor(8000) Total:  8000\n",
            " correct:  tensor(8064) Total:  8064\n",
            " correct:  tensor(8128) Total:  8128\n",
            " correct:  tensor(8192) Total:  8192\n",
            " correct:  tensor(8256) Total:  8256\n",
            " correct:  tensor(8320) Total:  8320\n",
            " correct:  tensor(8384) Total:  8384\n",
            " correct:  tensor(8448) Total:  8448\n",
            " correct:  tensor(8512) Total:  8512\n",
            " correct:  tensor(8576) Total:  8576\n",
            " correct:  tensor(8640) Total:  8640\n",
            " correct:  tensor(8704) Total:  8704\n",
            " correct:  tensor(8768) Total:  8768\n",
            " correct:  tensor(8832) Total:  8832\n",
            " correct:  tensor(8896) Total:  8896\n",
            " correct:  tensor(8960) Total:  8960\n",
            " correct:  tensor(9024) Total:  9024\n",
            " correct:  tensor(9088) Total:  9088\n",
            " correct:  tensor(9152) Total:  9152\n",
            " correct:  tensor(9216) Total:  9216\n",
            " correct:  tensor(9280) Total:  9280\n",
            " correct:  tensor(9344) Total:  9344\n",
            " correct:  tensor(9408) Total:  9408\n",
            " correct:  tensor(9472) Total:  9472\n",
            " correct:  tensor(9536) Total:  9536\n",
            " correct:  tensor(9600) Total:  9600\n",
            " correct:  tensor(9664) Total:  9664\n",
            " correct:  tensor(9728) Total:  9728\n",
            " correct:  tensor(9792) Total:  9792\n",
            " correct:  tensor(9856) Total:  9856\n",
            " correct:  tensor(9920) Total:  9920\n",
            " correct:  tensor(9984) Total:  9984\n",
            " correct:  tensor(10000) Total:  10000\n",
            "TEST Epoch 2, Loss 15.915660044190231 Accuracy 1.0 Time 166.055s\n",
            "TRAINING...\n",
            " correct:  tensor(64) Total:  64\n",
            "Epoch 3, Batch 1, Loss 15.580666542053223 Accuracy 1.0 Time 3.151s\n",
            " correct:  tensor(128) Total:  128\n",
            "Epoch 3, Batch 2, Loss 15.431747436523438 Accuracy 1.0 Time 3.177s\n",
            " correct:  tensor(192) Total:  192\n",
            "Epoch 3, Batch 3, Loss 15.63633918762207 Accuracy 1.0 Time 3.161s\n",
            " correct:  tensor(256) Total:  256\n",
            "Epoch 3, Batch 4, Loss 15.60352087020874 Accuracy 1.0 Time 3.091s\n",
            " correct:  tensor(320) Total:  320\n",
            "Epoch 3, Batch 5, Loss 15.628534889221191 Accuracy 1.0 Time 3.13s\n",
            " correct:  tensor(384) Total:  384\n",
            "Epoch 3, Batch 6, Loss 15.626012007395426 Accuracy 1.0 Time 3.115s\n",
            " correct:  tensor(448) Total:  448\n",
            "Epoch 3, Batch 7, Loss 15.455515589032855 Accuracy 1.0 Time 3.078s\n",
            " correct:  tensor(512) Total:  512\n",
            "Epoch 3, Batch 8, Loss 15.424892783164978 Accuracy 1.0 Time 3.17s\n",
            " correct:  tensor(576) Total:  576\n",
            "Epoch 3, Batch 9, Loss 15.458578321668837 Accuracy 1.0 Time 3.208s\n",
            " correct:  tensor(640) Total:  640\n",
            "Epoch 3, Batch 10, Loss 15.496307754516602 Accuracy 1.0 Time 3.169s\n",
            " correct:  tensor(704) Total:  704\n",
            "Epoch 3, Batch 11, Loss 15.49787460673939 Accuracy 1.0 Time 3.164s\n",
            " correct:  tensor(768) Total:  768\n",
            "Epoch 3, Batch 12, Loss 15.530197938283285 Accuracy 1.0 Time 3.239s\n",
            " correct:  tensor(832) Total:  832\n",
            "Epoch 3, Batch 13, Loss 15.511751395005446 Accuracy 1.0 Time 3.158s\n",
            " correct:  tensor(896) Total:  896\n",
            "Epoch 3, Batch 14, Loss 15.454745837620326 Accuracy 1.0 Time 3.161s\n",
            " correct:  tensor(960) Total:  960\n",
            "Epoch 3, Batch 15, Loss 15.42857093811035 Accuracy 1.0 Time 3.167s\n",
            " correct:  tensor(1024) Total:  1024\n",
            "Epoch 3, Batch 16, Loss 15.53192925453186 Accuracy 1.0 Time 3.161s\n",
            " correct:  tensor(1088) Total:  1088\n",
            "Epoch 3, Batch 17, Loss 15.471908569335938 Accuracy 1.0 Time 3.119s\n",
            " correct:  tensor(1152) Total:  1152\n",
            "Epoch 3, Batch 18, Loss 15.492731730143229 Accuracy 1.0 Time 3.16s\n",
            " correct:  tensor(1216) Total:  1216\n",
            "Epoch 3, Batch 19, Loss 15.5159643072831 Accuracy 1.0 Time 3.188s\n",
            " correct:  tensor(1280) Total:  1280\n",
            "Epoch 3, Batch 20, Loss 15.585240936279297 Accuracy 1.0 Time 3.133s\n",
            " correct:  tensor(1344) Total:  1344\n",
            "Epoch 3, Batch 21, Loss 15.541732197716122 Accuracy 1.0 Time 3.155s\n",
            " correct:  tensor(1408) Total:  1408\n",
            "Epoch 3, Batch 22, Loss 15.553731528195469 Accuracy 1.0 Time 3.204s\n",
            " correct:  tensor(1472) Total:  1472\n",
            "Epoch 3, Batch 23, Loss 15.58239567798117 Accuracy 1.0 Time 3.169s\n",
            " correct:  tensor(1536) Total:  1536\n",
            "Epoch 3, Batch 24, Loss 15.577542980511984 Accuracy 1.0 Time 3.089s\n",
            " correct:  tensor(1600) Total:  1600\n",
            "Epoch 3, Batch 25, Loss 15.581300811767578 Accuracy 1.0 Time 3.174s\n",
            " correct:  tensor(1664) Total:  1664\n",
            "Epoch 3, Batch 26, Loss 15.589320659637451 Accuracy 1.0 Time 3.178s\n",
            " correct:  tensor(1728) Total:  1728\n",
            "Epoch 3, Batch 27, Loss 15.616869220027217 Accuracy 1.0 Time 3.159s\n",
            " correct:  tensor(1792) Total:  1792\n",
            "Epoch 3, Batch 28, Loss 15.606989588056292 Accuracy 1.0 Time 3.162s\n",
            " correct:  tensor(1856) Total:  1856\n",
            "Epoch 3, Batch 29, Loss 15.612440503876785 Accuracy 1.0 Time 3.133s\n",
            " correct:  tensor(1920) Total:  1920\n",
            "Epoch 3, Batch 30, Loss 15.585963980356853 Accuracy 1.0 Time 3.12s\n",
            " correct:  tensor(1984) Total:  1984\n",
            "Epoch 3, Batch 31, Loss 15.608338909764443 Accuracy 1.0 Time 3.169s\n",
            " correct:  tensor(2048) Total:  2048\n",
            "Epoch 3, Batch 32, Loss 15.602523118257523 Accuracy 1.0 Time 3.16s\n",
            " correct:  tensor(2112) Total:  2112\n",
            "Epoch 3, Batch 33, Loss 15.57545193758878 Accuracy 1.0 Time 3.152s\n",
            " correct:  tensor(2176) Total:  2176\n",
            "Epoch 3, Batch 34, Loss 15.567078450146843 Accuracy 1.0 Time 3.194s\n",
            " correct:  tensor(2240) Total:  2240\n",
            "Epoch 3, Batch 35, Loss 15.58222462790353 Accuracy 1.0 Time 3.131s\n",
            " correct:  tensor(2304) Total:  2304\n",
            "Epoch 3, Batch 36, Loss 15.575835784276327 Accuracy 1.0 Time 3.29s\n",
            " correct:  tensor(2368) Total:  2368\n",
            "Epoch 3, Batch 37, Loss 15.563468881555506 Accuracy 1.0 Time 3.344s\n",
            " correct:  tensor(2432) Total:  2432\n",
            "Epoch 3, Batch 38, Loss 15.53868768089696 Accuracy 1.0 Time 3.336s\n",
            " correct:  tensor(2496) Total:  2496\n",
            "Epoch 3, Batch 39, Loss 15.561193832984337 Accuracy 1.0 Time 3.244s\n",
            " correct:  tensor(2560) Total:  2560\n",
            "Epoch 3, Batch 40, Loss 15.542725586891175 Accuracy 1.0 Time 3.226s\n",
            " correct:  tensor(2624) Total:  2624\n",
            "Epoch 3, Batch 41, Loss 15.538975296950921 Accuracy 1.0 Time 3.18s\n",
            " correct:  tensor(2688) Total:  2688\n",
            "Epoch 3, Batch 42, Loss 15.525483562832786 Accuracy 1.0 Time 3.196s\n",
            " correct:  tensor(2752) Total:  2752\n",
            "Epoch 3, Batch 43, Loss 15.523646798244743 Accuracy 1.0 Time 3.24s\n",
            " correct:  tensor(2816) Total:  2816\n",
            "Epoch 3, Batch 44, Loss 15.51670148155906 Accuracy 1.0 Time 3.202s\n",
            " correct:  tensor(2880) Total:  2880\n",
            "Epoch 3, Batch 45, Loss 15.49148309495714 Accuracy 1.0 Time 3.163s\n",
            " correct:  tensor(2944) Total:  2944\n",
            "Epoch 3, Batch 46, Loss 15.490671945654828 Accuracy 1.0 Time 3.169s\n",
            " correct:  tensor(3008) Total:  3008\n",
            "Epoch 3, Batch 47, Loss 15.489292956413106 Accuracy 1.0 Time 3.164s\n",
            " correct:  tensor(3072) Total:  3072\n",
            "Epoch 3, Batch 48, Loss 15.507736563682556 Accuracy 1.0 Time 3.127s\n",
            " correct:  tensor(3136) Total:  3136\n",
            "Epoch 3, Batch 49, Loss 15.51218591417585 Accuracy 1.0 Time 3.155s\n",
            " correct:  tensor(3200) Total:  3200\n",
            "Epoch 3, Batch 50, Loss 15.517915668487548 Accuracy 1.0 Time 3.188s\n",
            " correct:  tensor(3264) Total:  3264\n",
            "Epoch 3, Batch 51, Loss 15.497942531810088 Accuracy 1.0 Time 3.114s\n",
            " correct:  tensor(3328) Total:  3328\n",
            "Epoch 3, Batch 52, Loss 15.497519603142372 Accuracy 1.0 Time 3.293s\n",
            " correct:  tensor(3392) Total:  3392\n",
            "Epoch 3, Batch 53, Loss 15.492950187539154 Accuracy 1.0 Time 3.294s\n",
            " correct:  tensor(3456) Total:  3456\n",
            "Epoch 3, Batch 54, Loss 15.495009969781947 Accuracy 1.0 Time 3.148s\n",
            " correct:  tensor(3520) Total:  3520\n",
            "Epoch 3, Batch 55, Loss 15.496205780722878 Accuracy 1.0 Time 3.232s\n",
            " correct:  tensor(3584) Total:  3584\n",
            "Epoch 3, Batch 56, Loss 15.50025018623897 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(3648) Total:  3648\n",
            "Epoch 3, Batch 57, Loss 15.50529792852569 Accuracy 1.0 Time 3.122s\n",
            " correct:  tensor(3712) Total:  3712\n",
            "Epoch 3, Batch 58, Loss 15.502601080927356 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(3776) Total:  3776\n",
            "Epoch 3, Batch 59, Loss 15.509956376027253 Accuracy 1.0 Time 3.236s\n",
            " correct:  tensor(3840) Total:  3840\n",
            "Epoch 3, Batch 60, Loss 15.507988675435383 Accuracy 1.0 Time 3.259s\n",
            " correct:  tensor(3904) Total:  3904\n",
            "Epoch 3, Batch 61, Loss 15.507065350892114 Accuracy 1.0 Time 3.213s\n",
            " correct:  tensor(3968) Total:  3968\n",
            "Epoch 3, Batch 62, Loss 15.502962189335976 Accuracy 1.0 Time 3.222s\n",
            " correct:  tensor(4032) Total:  4032\n",
            "Epoch 3, Batch 63, Loss 15.508218795534164 Accuracy 1.0 Time 3.145s\n",
            " correct:  tensor(4096) Total:  4096\n",
            "Epoch 3, Batch 64, Loss 15.496863409876823 Accuracy 1.0 Time 3.213s\n",
            " correct:  tensor(4160) Total:  4160\n",
            "Epoch 3, Batch 65, Loss 15.496166845468375 Accuracy 1.0 Time 3.215s\n",
            " correct:  tensor(4224) Total:  4224\n",
            "Epoch 3, Batch 66, Loss 15.493545402180064 Accuracy 1.0 Time 3.264s\n",
            " correct:  tensor(4288) Total:  4288\n",
            "Epoch 3, Batch 67, Loss 15.49159790152934 Accuracy 1.0 Time 3.213s\n",
            " correct:  tensor(4352) Total:  4352\n",
            "Epoch 3, Batch 68, Loss 15.495674511965584 Accuracy 1.0 Time 3.197s\n",
            " correct:  tensor(4416) Total:  4416\n",
            "Epoch 3, Batch 69, Loss 15.486045284547668 Accuracy 1.0 Time 3.183s\n",
            " correct:  tensor(4480) Total:  4480\n",
            "Epoch 3, Batch 70, Loss 15.492942946297783 Accuracy 1.0 Time 3.232s\n",
            " correct:  tensor(4544) Total:  4544\n",
            "Epoch 3, Batch 71, Loss 15.490676638106224 Accuracy 1.0 Time 3.228s\n",
            " correct:  tensor(4608) Total:  4608\n",
            "Epoch 3, Batch 72, Loss 15.490707649124992 Accuracy 1.0 Time 3.156s\n",
            " correct:  tensor(4672) Total:  4672\n",
            "Epoch 3, Batch 73, Loss 15.487589000022574 Accuracy 1.0 Time 3.179s\n",
            " correct:  tensor(4736) Total:  4736\n",
            "Epoch 3, Batch 74, Loss 15.476785981977308 Accuracy 1.0 Time 3.224s\n",
            " correct:  tensor(4800) Total:  4800\n",
            "Epoch 3, Batch 75, Loss 15.47767370859782 Accuracy 1.0 Time 3.188s\n",
            " correct:  tensor(4864) Total:  4864\n",
            "Epoch 3, Batch 76, Loss 15.475616517819857 Accuracy 1.0 Time 3.234s\n",
            " correct:  tensor(4928) Total:  4928\n",
            "Epoch 3, Batch 77, Loss 15.465062636833686 Accuracy 1.0 Time 3.158s\n",
            " correct:  tensor(4992) Total:  4992\n",
            "Epoch 3, Batch 78, Loss 15.462196411230625 Accuracy 1.0 Time 3.242s\n",
            " correct:  tensor(5056) Total:  5056\n",
            "Epoch 3, Batch 79, Loss 15.460866855669625 Accuracy 1.0 Time 3.27s\n",
            " correct:  tensor(5120) Total:  5120\n",
            "Epoch 3, Batch 80, Loss 15.457035720348358 Accuracy 1.0 Time 3.212s\n",
            " correct:  tensor(5184) Total:  5184\n",
            "Epoch 3, Batch 81, Loss 15.46527882564215 Accuracy 1.0 Time 3.194s\n",
            " correct:  tensor(5248) Total:  5248\n",
            "Epoch 3, Batch 82, Loss 15.461623971055193 Accuracy 1.0 Time 3.162s\n",
            " correct:  tensor(5312) Total:  5312\n",
            "Epoch 3, Batch 83, Loss 15.461830598762237 Accuracy 1.0 Time 3.205s\n",
            " correct:  tensor(5376) Total:  5376\n",
            "Epoch 3, Batch 84, Loss 15.46703967593965 Accuracy 1.0 Time 3.195s\n",
            " correct:  tensor(5440) Total:  5440\n",
            "Epoch 3, Batch 85, Loss 15.469570216010599 Accuracy 1.0 Time 3.193s\n",
            " correct:  tensor(5504) Total:  5504\n",
            "Epoch 3, Batch 86, Loss 15.466735950736112 Accuracy 1.0 Time 3.197s\n",
            " correct:  tensor(5568) Total:  5568\n",
            "Epoch 3, Batch 87, Loss 15.463034366739207 Accuracy 1.0 Time 3.272s\n",
            " correct:  tensor(5632) Total:  5632\n",
            "Epoch 3, Batch 88, Loss 15.462900335138494 Accuracy 1.0 Time 3.139s\n",
            " correct:  tensor(5696) Total:  5696\n",
            "Epoch 3, Batch 89, Loss 15.468092296900375 Accuracy 1.0 Time 3.229s\n",
            " correct:  tensor(5760) Total:  5760\n",
            "Epoch 3, Batch 90, Loss 15.469780084821913 Accuracy 1.0 Time 3.215s\n",
            " correct:  tensor(5824) Total:  5824\n",
            "Epoch 3, Batch 91, Loss 15.458562400314834 Accuracy 1.0 Time 3.178s\n",
            " correct:  tensor(5888) Total:  5888\n",
            "Epoch 3, Batch 92, Loss 15.456739964692488 Accuracy 1.0 Time 3.182s\n",
            " correct:  tensor(5952) Total:  5952\n",
            "Epoch 3, Batch 93, Loss 15.453747523728238 Accuracy 1.0 Time 3.217s\n",
            " correct:  tensor(6016) Total:  6016\n",
            "Epoch 3, Batch 94, Loss 15.448925698057135 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(6080) Total:  6080\n",
            "Epoch 3, Batch 95, Loss 15.45224330299779 Accuracy 1.0 Time 3.128s\n",
            " correct:  tensor(6144) Total:  6144\n",
            "Epoch 3, Batch 96, Loss 15.443677713473638 Accuracy 1.0 Time 3.1s\n",
            " correct:  tensor(6208) Total:  6208\n",
            "Epoch 3, Batch 97, Loss 15.440430356055192 Accuracy 1.0 Time 3.183s\n",
            " correct:  tensor(6272) Total:  6272\n",
            "Epoch 3, Batch 98, Loss 15.44508883417869 Accuracy 1.0 Time 3.13s\n",
            " correct:  tensor(6336) Total:  6336\n",
            "Epoch 3, Batch 99, Loss 15.454202112525401 Accuracy 1.0 Time 3.249s\n",
            " correct:  tensor(6400) Total:  6400\n",
            "Epoch 3, Batch 100, Loss 15.45900855064392 Accuracy 1.0 Time 3.268s\n",
            " correct:  tensor(6464) Total:  6464\n",
            "Epoch 3, Batch 101, Loss 15.449546049136927 Accuracy 1.0 Time 3.163s\n",
            " correct:  tensor(6528) Total:  6528\n",
            "Epoch 3, Batch 102, Loss 15.460783500297397 Accuracy 1.0 Time 3.144s\n",
            " correct:  tensor(6592) Total:  6592\n",
            "Epoch 3, Batch 103, Loss 15.460899815976042 Accuracy 1.0 Time 3.204s\n",
            " correct:  tensor(6656) Total:  6656\n",
            "Epoch 3, Batch 104, Loss 15.461824875611525 Accuracy 1.0 Time 3.246s\n",
            " correct:  tensor(6720) Total:  6720\n",
            "Epoch 3, Batch 105, Loss 15.46042452312651 Accuracy 1.0 Time 3.246s\n",
            " correct:  tensor(6784) Total:  6784\n",
            "Epoch 3, Batch 106, Loss 15.460709382902902 Accuracy 1.0 Time 3.128s\n",
            " correct:  tensor(6848) Total:  6848\n",
            "Epoch 3, Batch 107, Loss 15.457565619566731 Accuracy 1.0 Time 3.181s\n",
            " correct:  tensor(6912) Total:  6912\n",
            "Epoch 3, Batch 108, Loss 15.454372812200475 Accuracy 1.0 Time 3.144s\n",
            " correct:  tensor(6976) Total:  6976\n",
            "Epoch 3, Batch 109, Loss 15.464717235040228 Accuracy 1.0 Time 3.156s\n",
            " correct:  tensor(7040) Total:  7040\n",
            "Epoch 3, Batch 110, Loss 15.469261048056863 Accuracy 1.0 Time 3.126s\n",
            " correct:  tensor(7104) Total:  7104\n",
            "Epoch 3, Batch 111, Loss 15.465494276167036 Accuracy 1.0 Time 3.142s\n",
            " correct:  tensor(7168) Total:  7168\n",
            "Epoch 3, Batch 112, Loss 15.464240874562945 Accuracy 1.0 Time 3.178s\n",
            " correct:  tensor(7232) Total:  7232\n",
            "Epoch 3, Batch 113, Loss 15.461494876220163 Accuracy 1.0 Time 3.228s\n",
            " correct:  tensor(7296) Total:  7296\n",
            "Epoch 3, Batch 114, Loss 15.458353519439697 Accuracy 1.0 Time 3.252s\n",
            " correct:  tensor(7360) Total:  7360\n",
            "Epoch 3, Batch 115, Loss 15.461109103327212 Accuracy 1.0 Time 3.255s\n",
            " correct:  tensor(7424) Total:  7424\n",
            "Epoch 3, Batch 116, Loss 15.458341738273358 Accuracy 1.0 Time 3.206s\n",
            " correct:  tensor(7488) Total:  7488\n",
            "Epoch 3, Batch 117, Loss 15.459731126442934 Accuracy 1.0 Time 3.26s\n",
            " correct:  tensor(7552) Total:  7552\n",
            "Epoch 3, Batch 118, Loss 15.455298771292476 Accuracy 1.0 Time 3.331s\n",
            " correct:  tensor(7616) Total:  7616\n",
            "Epoch 3, Batch 119, Loss 15.455248632350889 Accuracy 1.0 Time 3.144s\n",
            " correct:  tensor(7680) Total:  7680\n",
            "Epoch 3, Batch 120, Loss 15.45851555665334 Accuracy 1.0 Time 3.262s\n",
            " correct:  tensor(7744) Total:  7744\n",
            "Epoch 3, Batch 121, Loss 15.461984500412113 Accuracy 1.0 Time 3.069s\n",
            " correct:  tensor(7808) Total:  7808\n",
            "Epoch 3, Batch 122, Loss 15.471992234714696 Accuracy 1.0 Time 3.143s\n",
            " correct:  tensor(7872) Total:  7872\n",
            "Epoch 3, Batch 123, Loss 15.477526323582099 Accuracy 1.0 Time 3.136s\n",
            " correct:  tensor(7936) Total:  7936\n",
            "Epoch 3, Batch 124, Loss 15.473882575188913 Accuracy 1.0 Time 3.087s\n",
            " correct:  tensor(8000) Total:  8000\n",
            "Epoch 3, Batch 125, Loss 15.468011993408203 Accuracy 1.0 Time 3.083s\n",
            " correct:  tensor(8064) Total:  8064\n",
            "Epoch 3, Batch 126, Loss 15.465318452744256 Accuracy 1.0 Time 3.144s\n",
            " correct:  tensor(8128) Total:  8128\n",
            "Epoch 3, Batch 127, Loss 15.469243064640075 Accuracy 1.0 Time 3.15s\n",
            " correct:  tensor(8192) Total:  8192\n",
            "Epoch 3, Batch 128, Loss 15.47198773175478 Accuracy 1.0 Time 3.166s\n",
            " correct:  tensor(8256) Total:  8256\n",
            "Epoch 3, Batch 129, Loss 15.467682845832766 Accuracy 1.0 Time 3.129s\n",
            " correct:  tensor(8320) Total:  8320\n",
            "Epoch 3, Batch 130, Loss 15.466671540186955 Accuracy 1.0 Time 3.091s\n",
            " correct:  tensor(8384) Total:  8384\n",
            "Epoch 3, Batch 131, Loss 15.46643014718558 Accuracy 1.0 Time 3.192s\n",
            " correct:  tensor(8448) Total:  8448\n",
            "Epoch 3, Batch 132, Loss 15.464228536143448 Accuracy 1.0 Time 3.197s\n",
            " correct:  tensor(8512) Total:  8512\n",
            "Epoch 3, Batch 133, Loss 15.46609898617393 Accuracy 1.0 Time 3.288s\n",
            " correct:  tensor(8576) Total:  8576\n",
            "Epoch 3, Batch 134, Loss 15.45672882137014 Accuracy 1.0 Time 3.406s\n",
            " correct:  tensor(8640) Total:  8640\n",
            "Epoch 3, Batch 135, Loss 15.45835391857006 Accuracy 1.0 Time 3.345s\n",
            " correct:  tensor(8704) Total:  8704\n",
            "Epoch 3, Batch 136, Loss 15.460427971447215 Accuracy 1.0 Time 3.172s\n",
            " correct:  tensor(8768) Total:  8768\n",
            "Epoch 3, Batch 137, Loss 15.456630379614168 Accuracy 1.0 Time 3.165s\n",
            " correct:  tensor(8832) Total:  8832\n",
            "Epoch 3, Batch 138, Loss 15.447503469992375 Accuracy 1.0 Time 3.271s\n",
            " correct:  tensor(8896) Total:  8896\n",
            "Epoch 3, Batch 139, Loss 15.441983319014954 Accuracy 1.0 Time 3.229s\n",
            " correct:  tensor(8960) Total:  8960\n",
            "Epoch 3, Batch 140, Loss 15.439364392416818 Accuracy 1.0 Time 3.177s\n",
            " correct:  tensor(9024) Total:  9024\n",
            "Epoch 3, Batch 141, Loss 15.44189858943858 Accuracy 1.0 Time 3.149s\n",
            " correct:  tensor(9088) Total:  9088\n",
            "Epoch 3, Batch 142, Loss 15.44442702682925 Accuracy 1.0 Time 3.257s\n",
            " correct:  tensor(9152) Total:  9152\n",
            "Epoch 3, Batch 143, Loss 15.450006198216151 Accuracy 1.0 Time 3.151s\n",
            " correct:  tensor(9216) Total:  9216\n",
            "Epoch 3, Batch 144, Loss 15.450034982628292 Accuracy 1.0 Time 3.174s\n",
            " correct:  tensor(9280) Total:  9280\n",
            "Epoch 3, Batch 145, Loss 15.44606946090172 Accuracy 1.0 Time 3.175s\n",
            " correct:  tensor(9344) Total:  9344\n",
            "Epoch 3, Batch 146, Loss 15.443926510745532 Accuracy 1.0 Time 3.251s\n",
            " correct:  tensor(9408) Total:  9408\n",
            "Epoch 3, Batch 147, Loss 15.441483037001422 Accuracy 1.0 Time 3.284s\n",
            " correct:  tensor(9472) Total:  9472\n",
            "Epoch 3, Batch 148, Loss 15.437659843547925 Accuracy 1.0 Time 3.33s\n",
            " correct:  tensor(9536) Total:  9536\n",
            "Epoch 3, Batch 149, Loss 15.440787513784114 Accuracy 1.0 Time 3.158s\n",
            " correct:  tensor(9600) Total:  9600\n",
            "Epoch 3, Batch 150, Loss 15.439638697306314 Accuracy 1.0 Time 3.174s\n",
            " correct:  tensor(9664) Total:  9664\n",
            "Epoch 3, Batch 151, Loss 15.440160839762909 Accuracy 1.0 Time 3.274s\n",
            " correct:  tensor(9728) Total:  9728\n",
            "Epoch 3, Batch 152, Loss 15.435280410866989 Accuracy 1.0 Time 3.097s\n",
            " correct:  tensor(9792) Total:  9792\n",
            "Epoch 3, Batch 153, Loss 15.433341687021692 Accuracy 1.0 Time 3.128s\n",
            " correct:  tensor(9856) Total:  9856\n",
            "Epoch 3, Batch 154, Loss 15.427793168402337 Accuracy 1.0 Time 3.13s\n",
            " correct:  tensor(9920) Total:  9920\n",
            "Epoch 3, Batch 155, Loss 15.429757604291362 Accuracy 1.0 Time 3.179s\n",
            " correct:  tensor(9984) Total:  9984\n",
            "Epoch 3, Batch 156, Loss 15.431688883365728 Accuracy 1.0 Time 3.219s\n",
            " correct:  tensor(10048) Total:  10048\n",
            "Epoch 3, Batch 157, Loss 15.428643378482503 Accuracy 1.0 Time 3.118s\n",
            " correct:  tensor(10112) Total:  10112\n",
            "Epoch 3, Batch 158, Loss 15.428730029094067 Accuracy 1.0 Time 3.171s\n",
            " correct:  tensor(10176) Total:  10176\n",
            "Epoch 3, Batch 159, Loss 15.42871770318949 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(10240) Total:  10240\n",
            "Epoch 3, Batch 160, Loss 15.435099005699158 Accuracy 1.0 Time 3.155s\n",
            " correct:  tensor(10304) Total:  10304\n",
            "Epoch 3, Batch 161, Loss 15.43808977352166 Accuracy 1.0 Time 3.176s\n",
            " correct:  tensor(10368) Total:  10368\n",
            "Epoch 3, Batch 162, Loss 15.43742785041715 Accuracy 1.0 Time 3.157s\n",
            " correct:  tensor(10432) Total:  10432\n",
            "Epoch 3, Batch 163, Loss 15.441241515926057 Accuracy 1.0 Time 3.142s\n",
            " correct:  tensor(10496) Total:  10496\n",
            "Epoch 3, Batch 164, Loss 15.436721476112924 Accuracy 1.0 Time 3.172s\n",
            " correct:  tensor(10560) Total:  10560\n",
            "Epoch 3, Batch 165, Loss 15.440164774114436 Accuracy 1.0 Time 3.259s\n",
            " correct:  tensor(10624) Total:  10624\n",
            "Epoch 3, Batch 166, Loss 15.438783553709467 Accuracy 1.0 Time 3.319s\n",
            " correct:  tensor(10688) Total:  10688\n",
            "Epoch 3, Batch 167, Loss 15.436347955715156 Accuracy 1.0 Time 3.214s\n",
            " correct:  tensor(10752) Total:  10752\n",
            "Epoch 3, Batch 168, Loss 15.435636719067892 Accuracy 1.0 Time 3.267s\n",
            " correct:  tensor(10816) Total:  10816\n",
            "Epoch 3, Batch 169, Loss 15.435313117574658 Accuracy 1.0 Time 3.189s\n",
            " correct:  tensor(10880) Total:  10880\n",
            "Epoch 3, Batch 170, Loss 15.436996482400334 Accuracy 1.0 Time 3.173s\n",
            " correct:  tensor(10944) Total:  10944\n",
            "Epoch 3, Batch 171, Loss 15.434584450303463 Accuracy 1.0 Time 3.169s\n",
            " correct:  tensor(11008) Total:  11008\n",
            "Epoch 3, Batch 172, Loss 15.438045085862626 Accuracy 1.0 Time 3.176s\n",
            " correct:  tensor(11072) Total:  11072\n",
            "Epoch 3, Batch 173, Loss 15.434280621523113 Accuracy 1.0 Time 3.131s\n",
            " correct:  tensor(11136) Total:  11136\n",
            "Epoch 3, Batch 174, Loss 15.436165535587003 Accuracy 1.0 Time 3.106s\n",
            " correct:  tensor(11200) Total:  11200\n",
            "Epoch 3, Batch 175, Loss 15.437509073529924 Accuracy 1.0 Time 3.128s\n",
            " correct:  tensor(11264) Total:  11264\n",
            "Epoch 3, Batch 176, Loss 15.4353469285098 Accuracy 1.0 Time 3.18s\n",
            " correct:  tensor(11328) Total:  11328\n",
            "Epoch 3, Batch 177, Loss 15.433375584877144 Accuracy 1.0 Time 3.146s\n",
            " correct:  tensor(11392) Total:  11392\n",
            "Epoch 3, Batch 178, Loss 15.434357010916377 Accuracy 1.0 Time 3.121s\n",
            " correct:  tensor(11456) Total:  11456\n",
            "Epoch 3, Batch 179, Loss 15.43425056521453 Accuracy 1.0 Time 3.155s\n",
            " correct:  tensor(11520) Total:  11520\n",
            "Epoch 3, Batch 180, Loss 15.4330313205719 Accuracy 1.0 Time 3.258s\n",
            " correct:  tensor(11584) Total:  11584\n",
            "Epoch 3, Batch 181, Loss 15.431553898595315 Accuracy 1.0 Time 3.186s\n",
            " correct:  tensor(11648) Total:  11648\n",
            "Epoch 3, Batch 182, Loss 15.434388637542725 Accuracy 1.0 Time 3.197s\n",
            " correct:  tensor(11712) Total:  11712\n",
            "Epoch 3, Batch 183, Loss 15.435197621746793 Accuracy 1.0 Time 3.254s\n",
            " correct:  tensor(11776) Total:  11776\n",
            "Epoch 3, Batch 184, Loss 15.431589551593946 Accuracy 1.0 Time 3.158s\n",
            " correct:  tensor(11840) Total:  11840\n",
            "Epoch 3, Batch 185, Loss 15.428741310738229 Accuracy 1.0 Time 3.139s\n",
            " correct:  tensor(11904) Total:  11904\n",
            "Epoch 3, Batch 186, Loss 15.423665908075147 Accuracy 1.0 Time 3.15s\n",
            " correct:  tensor(11968) Total:  11968\n",
            "Epoch 3, Batch 187, Loss 15.420332806633118 Accuracy 1.0 Time 3.181s\n",
            " correct:  tensor(12032) Total:  12032\n",
            "Epoch 3, Batch 188, Loss 15.42504277635128 Accuracy 1.0 Time 3.225s\n",
            " correct:  tensor(12096) Total:  12096\n",
            "Epoch 3, Batch 189, Loss 15.425990356970086 Accuracy 1.0 Time 3.198s\n",
            " correct:  tensor(12160) Total:  12160\n",
            "Epoch 3, Batch 190, Loss 15.425260558881257 Accuracy 1.0 Time 3.223s\n",
            " correct:  tensor(12224) Total:  12224\n",
            "Epoch 3, Batch 191, Loss 15.42328239860335 Accuracy 1.0 Time 3.167s\n",
            " correct:  tensor(12288) Total:  12288\n",
            "Epoch 3, Batch 192, Loss 15.425143892566362 Accuracy 1.0 Time 3.138s\n",
            " correct:  tensor(12352) Total:  12352\n",
            "Epoch 3, Batch 193, Loss 15.42675226834154 Accuracy 1.0 Time 3.164s\n",
            " correct:  tensor(12416) Total:  12416\n",
            "Epoch 3, Batch 194, Loss 15.427179479107414 Accuracy 1.0 Time 3.126s\n",
            " correct:  tensor(12480) Total:  12480\n",
            "Epoch 3, Batch 195, Loss 15.428899794358474 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(12544) Total:  12544\n",
            "Epoch 3, Batch 196, Loss 15.427975878423574 Accuracy 1.0 Time 3.162s\n",
            " correct:  tensor(12608) Total:  12608\n",
            "Epoch 3, Batch 197, Loss 15.428495474878302 Accuracy 1.0 Time 3.238s\n",
            " correct:  tensor(12672) Total:  12672\n",
            "Epoch 3, Batch 198, Loss 15.42797641561489 Accuracy 1.0 Time 3.201s\n",
            " correct:  tensor(12736) Total:  12736\n",
            "Epoch 3, Batch 199, Loss 15.42946149356401 Accuracy 1.0 Time 3.15s\n",
            " correct:  tensor(12800) Total:  12800\n",
            "Epoch 3, Batch 200, Loss 15.430539498329162 Accuracy 1.0 Time 3.246s\n",
            " correct:  tensor(12864) Total:  12864\n",
            "Epoch 3, Batch 201, Loss 15.430452147526527 Accuracy 1.0 Time 3.207s\n",
            " correct:  tensor(12928) Total:  12928\n",
            "Epoch 3, Batch 202, Loss 15.429948126915658 Accuracy 1.0 Time 3.194s\n",
            " correct:  tensor(12992) Total:  12992\n",
            "Epoch 3, Batch 203, Loss 15.428694438464536 Accuracy 1.0 Time 3.206s\n",
            " correct:  tensor(13056) Total:  13056\n",
            "Epoch 3, Batch 204, Loss 15.42878091101553 Accuracy 1.0 Time 3.176s\n",
            " correct:  tensor(13120) Total:  13120\n",
            "Epoch 3, Batch 205, Loss 15.426533312913849 Accuracy 1.0 Time 3.216s\n",
            " correct:  tensor(13184) Total:  13184\n",
            "Epoch 3, Batch 206, Loss 15.434067040971183 Accuracy 1.0 Time 3.217s\n",
            " correct:  tensor(13248) Total:  13248\n",
            "Epoch 3, Batch 207, Loss 15.431891667094208 Accuracy 1.0 Time 3.257s\n",
            " correct:  tensor(13312) Total:  13312\n",
            "Epoch 3, Batch 208, Loss 15.43692266024076 Accuracy 1.0 Time 3.19s\n",
            " correct:  tensor(13376) Total:  13376\n",
            "Epoch 3, Batch 209, Loss 15.436318306261272 Accuracy 1.0 Time 3.16s\n",
            " correct:  tensor(13440) Total:  13440\n",
            "Epoch 3, Batch 210, Loss 15.434779062725248 Accuracy 1.0 Time 3.21s\n",
            " correct:  tensor(13504) Total:  13504\n",
            "Epoch 3, Batch 211, Loss 15.436340029205756 Accuracy 1.0 Time 3.171s\n",
            " correct:  tensor(13568) Total:  13568\n",
            "Epoch 3, Batch 212, Loss 15.435484476809231 Accuracy 1.0 Time 3.312s\n",
            " correct:  tensor(13632) Total:  13632\n",
            "Epoch 3, Batch 213, Loss 15.43341689042642 Accuracy 1.0 Time 3.198s\n",
            " correct:  tensor(13696) Total:  13696\n",
            "Epoch 3, Batch 214, Loss 15.434299339757901 Accuracy 1.0 Time 3.199s\n",
            " correct:  tensor(13760) Total:  13760\n",
            "Epoch 3, Batch 215, Loss 15.436082343168037 Accuracy 1.0 Time 3.175s\n",
            " correct:  tensor(13824) Total:  13824\n",
            "Epoch 3, Batch 216, Loss 15.43167511622111 Accuracy 1.0 Time 3.204s\n",
            " correct:  tensor(13888) Total:  13888\n",
            "Epoch 3, Batch 217, Loss 15.433869287165628 Accuracy 1.0 Time 3.145s\n",
            " correct:  tensor(13952) Total:  13952\n",
            "Epoch 3, Batch 218, Loss 15.432367631054799 Accuracy 1.0 Time 3.216s\n",
            " correct:  tensor(14016) Total:  14016\n",
            "Epoch 3, Batch 219, Loss 15.432413353767569 Accuracy 1.0 Time 3.226s\n",
            " correct:  tensor(14080) Total:  14080\n",
            "Epoch 3, Batch 220, Loss 15.431806018135765 Accuracy 1.0 Time 3.211s\n",
            " correct:  tensor(14144) Total:  14144\n",
            "Epoch 3, Batch 221, Loss 15.435739266926348 Accuracy 1.0 Time 3.208s\n",
            " correct:  tensor(14208) Total:  14208\n",
            "Epoch 3, Batch 222, Loss 15.43521260785627 Accuracy 1.0 Time 3.187s\n",
            " correct:  tensor(14272) Total:  14272\n",
            "Epoch 3, Batch 223, Loss 15.433477765241546 Accuracy 1.0 Time 3.153s\n",
            " correct:  tensor(14336) Total:  14336\n",
            "Epoch 3, Batch 224, Loss 15.431465881211418 Accuracy 1.0 Time 3.152s\n",
            " correct:  tensor(14400) Total:  14400\n",
            "Epoch 3, Batch 225, Loss 15.436254052056206 Accuracy 1.0 Time 3.135s\n",
            " correct:  tensor(14464) Total:  14464\n",
            "Epoch 3, Batch 226, Loss 15.436463469952608 Accuracy 1.0 Time 3.161s\n",
            " correct:  tensor(14528) Total:  14528\n",
            "Epoch 3, Batch 227, Loss 15.432977243667132 Accuracy 1.0 Time 3.158s\n",
            " correct:  tensor(14592) Total:  14592\n",
            "Epoch 3, Batch 228, Loss 15.427759567896524 Accuracy 1.0 Time 3.216s\n",
            " correct:  tensor(14656) Total:  14656\n",
            "Epoch 3, Batch 229, Loss 15.428369347185027 Accuracy 1.0 Time 3.072s\n",
            " correct:  tensor(14720) Total:  14720\n",
            "Epoch 3, Batch 230, Loss 15.426306940161664 Accuracy 1.0 Time 3.321s\n",
            " correct:  tensor(14784) Total:  14784\n",
            "Epoch 3, Batch 231, Loss 15.4248209082203 Accuracy 1.0 Time 3.406s\n",
            " correct:  tensor(14848) Total:  14848\n",
            "Epoch 3, Batch 232, Loss 15.425570866157269 Accuracy 1.0 Time 3.359s\n",
            " correct:  tensor(14912) Total:  14912\n",
            "Epoch 3, Batch 233, Loss 15.425418186597046 Accuracy 1.0 Time 3.205s\n",
            " correct:  tensor(14976) Total:  14976\n",
            "Epoch 3, Batch 234, Loss 15.425115886916462 Accuracy 1.0 Time 3.21s\n",
            " correct:  tensor(15040) Total:  15040\n",
            "Epoch 3, Batch 235, Loss 15.427045814027178 Accuracy 1.0 Time 3.178s\n",
            " correct:  tensor(15104) Total:  15104\n",
            "Epoch 3, Batch 236, Loss 15.424882298808987 Accuracy 1.0 Time 3.227s\n",
            " correct:  tensor(15168) Total:  15168\n",
            "Epoch 3, Batch 237, Loss 15.423793559335959 Accuracy 1.0 Time 3.19s\n",
            " correct:  tensor(15232) Total:  15232\n",
            "Epoch 3, Batch 238, Loss 15.423857773051543 Accuracy 1.0 Time 3.155s\n",
            " correct:  tensor(15296) Total:  15296\n",
            "Epoch 3, Batch 239, Loss 15.424532726718791 Accuracy 1.0 Time 3.132s\n",
            " correct:  tensor(15360) Total:  15360\n",
            "Epoch 3, Batch 240, Loss 15.425672463576 Accuracy 1.0 Time 3.129s\n",
            " correct:  tensor(15424) Total:  15424\n",
            "Epoch 3, Batch 241, Loss 15.428783539419847 Accuracy 1.0 Time 3.115s\n",
            " correct:  tensor(15488) Total:  15488\n",
            "Epoch 3, Batch 242, Loss 15.425936564926273 Accuracy 1.0 Time 3.15s\n",
            " correct:  tensor(15552) Total:  15552\n",
            "Epoch 3, Batch 243, Loss 15.42934870621795 Accuracy 1.0 Time 3.371s\n",
            " correct:  tensor(15616) Total:  15616\n",
            "Epoch 3, Batch 244, Loss 15.426574007409517 Accuracy 1.0 Time 3.206s\n",
            " correct:  tensor(15680) Total:  15680\n",
            "Epoch 3, Batch 245, Loss 15.424990669561891 Accuracy 1.0 Time 3.115s\n",
            " correct:  tensor(15744) Total:  15744\n",
            "Epoch 3, Batch 246, Loss 15.42299948281389 Accuracy 1.0 Time 3.121s\n",
            " correct:  tensor(15808) Total:  15808\n",
            "Epoch 3, Batch 247, Loss 15.421523499585357 Accuracy 1.0 Time 3.181s\n",
            " correct:  tensor(15872) Total:  15872\n",
            "Epoch 3, Batch 248, Loss 15.422672067919086 Accuracy 1.0 Time 3.136s\n",
            " correct:  tensor(15936) Total:  15936\n",
            "Epoch 3, Batch 249, Loss 15.425161534045117 Accuracy 1.0 Time 3.103s\n",
            " correct:  tensor(16000) Total:  16000\n",
            "Epoch 3, Batch 250, Loss 15.426413204193116 Accuracy 1.0 Time 3.163s\n",
            " correct:  tensor(16064) Total:  16064\n",
            "Epoch 3, Batch 251, Loss 15.422530307237846 Accuracy 1.0 Time 3.148s\n",
            " correct:  tensor(16128) Total:  16128\n",
            "Epoch 3, Batch 252, Loss 15.424449352991013 Accuracy 1.0 Time 3.09s\n",
            " correct:  tensor(16192) Total:  16192\n",
            "Epoch 3, Batch 253, Loss 15.424752815910008 Accuracy 1.0 Time 3.083s\n",
            " correct:  tensor(16256) Total:  16256\n",
            "Epoch 3, Batch 254, Loss 15.423249623906894 Accuracy 1.0 Time 3.096s\n",
            " correct:  tensor(16320) Total:  16320\n",
            "Epoch 3, Batch 255, Loss 15.42382380915623 Accuracy 1.0 Time 3.124s\n",
            " correct:  tensor(16384) Total:  16384\n",
            "Epoch 3, Batch 256, Loss 15.424477074295282 Accuracy 1.0 Time 3.134s\n",
            " correct:  tensor(16448) Total:  16448\n",
            "Epoch 3, Batch 257, Loss 15.423063567640252 Accuracy 1.0 Time 3.114s\n",
            " correct:  tensor(16512) Total:  16512\n",
            "Epoch 3, Batch 258, Loss 15.420291619707447 Accuracy 1.0 Time 3.125s\n",
            " correct:  tensor(16576) Total:  16576\n",
            "Epoch 3, Batch 259, Loss 15.422792442056663 Accuracy 1.0 Time 3.092s\n",
            " correct:  tensor(16640) Total:  16640\n",
            "Epoch 3, Batch 260, Loss 15.420860448250403 Accuracy 1.0 Time 3.089s\n",
            " correct:  tensor(16704) Total:  16704\n",
            "Epoch 3, Batch 261, Loss 15.420952588662335 Accuracy 1.0 Time 3.147s\n",
            " correct:  tensor(16768) Total:  16768\n",
            "Epoch 3, Batch 262, Loss 15.421678885248781 Accuracy 1.0 Time 3.095s\n",
            " correct:  tensor(16832) Total:  16832\n",
            "Epoch 3, Batch 263, Loss 15.420124869836147 Accuracy 1.0 Time 3.197s\n",
            " correct:  tensor(16896) Total:  16896\n",
            "Epoch 3, Batch 264, Loss 15.415847156987045 Accuracy 1.0 Time 3.062s\n",
            " correct:  tensor(16960) Total:  16960\n",
            "Epoch 3, Batch 265, Loss 15.413380439326449 Accuracy 1.0 Time 3.107s\n",
            " correct:  tensor(17024) Total:  17024\n",
            "Epoch 3, Batch 266, Loss 15.415142460873252 Accuracy 1.0 Time 3.104s\n",
            " correct:  tensor(17088) Total:  17088\n",
            "Epoch 3, Batch 267, Loss 15.417663895682002 Accuracy 1.0 Time 3.103s\n",
            " correct:  tensor(17152) Total:  17152\n",
            "Epoch 3, Batch 268, Loss 15.416807149773213 Accuracy 1.0 Time 3.082s\n",
            " correct:  tensor(17216) Total:  17216\n",
            "Epoch 3, Batch 269, Loss 15.419484592281753 Accuracy 1.0 Time 3.195s\n",
            " correct:  tensor(17280) Total:  17280\n",
            "Epoch 3, Batch 270, Loss 15.41879822059914 Accuracy 1.0 Time 3.138s\n",
            " correct:  tensor(17344) Total:  17344\n",
            "Epoch 3, Batch 271, Loss 15.420870897074908 Accuracy 1.0 Time 3.121s\n",
            " correct:  tensor(17408) Total:  17408\n",
            "Epoch 3, Batch 272, Loss 15.419173766584958 Accuracy 1.0 Time 3.068s\n",
            " correct:  tensor(17472) Total:  17472\n",
            "Epoch 3, Batch 273, Loss 15.419572473882319 Accuracy 1.0 Time 3.095s\n",
            " correct:  tensor(17536) Total:  17536\n",
            "Epoch 3, Batch 274, Loss 15.417649564951876 Accuracy 1.0 Time 3.128s\n",
            " correct:  tensor(17600) Total:  17600\n",
            "Epoch 3, Batch 275, Loss 15.418239912553267 Accuracy 1.0 Time 3.148s\n",
            " correct:  tensor(17664) Total:  17664\n",
            "Epoch 3, Batch 276, Loss 15.41598974103513 Accuracy 1.0 Time 3.202s\n",
            " correct:  tensor(17728) Total:  17728\n",
            "Epoch 3, Batch 277, Loss 15.411885605822402 Accuracy 1.0 Time 3.18s\n",
            " correct:  tensor(17792) Total:  17792\n",
            "Epoch 3, Batch 278, Loss 15.412774075707086 Accuracy 1.0 Time 3.175s\n",
            " correct:  tensor(17856) Total:  17856\n",
            "Epoch 3, Batch 279, Loss 15.411306090679647 Accuracy 1.0 Time 3.096s\n",
            " correct:  tensor(17920) Total:  17920\n",
            "Epoch 3, Batch 280, Loss 15.412337333815438 Accuracy 1.0 Time 3.156s\n",
            " correct:  tensor(17984) Total:  17984\n",
            "Epoch 3, Batch 281, Loss 15.411683221728776 Accuracy 1.0 Time 3.12s\n",
            " correct:  tensor(18048) Total:  18048\n",
            "Epoch 3, Batch 282, Loss 15.410561172674734 Accuracy 1.0 Time 3.101s\n",
            " correct:  tensor(18112) Total:  18112\n",
            "Epoch 3, Batch 283, Loss 15.410169699166772 Accuracy 1.0 Time 3.192s\n",
            " correct:  tensor(18176) Total:  18176\n",
            "Epoch 3, Batch 284, Loss 15.412968880693677 Accuracy 1.0 Time 3.193s\n",
            " correct:  tensor(18240) Total:  18240\n",
            "Epoch 3, Batch 285, Loss 15.412162201864678 Accuracy 1.0 Time 3.094s\n",
            " correct:  tensor(18304) Total:  18304\n",
            "Epoch 3, Batch 286, Loss 15.41430684403106 Accuracy 1.0 Time 3.094s\n",
            " correct:  tensor(18368) Total:  18368\n",
            "Epoch 3, Batch 287, Loss 15.41349850299051 Accuracy 1.0 Time 3.108s\n",
            " correct:  tensor(18432) Total:  18432\n",
            "Epoch 3, Batch 288, Loss 15.415887117385864 Accuracy 1.0 Time 3.184s\n",
            " correct:  tensor(18496) Total:  18496\n",
            "Epoch 3, Batch 289, Loss 15.416364894193762 Accuracy 1.0 Time 3.12s\n",
            " correct:  tensor(18560) Total:  18560\n",
            "Epoch 3, Batch 290, Loss 15.412927394077697 Accuracy 1.0 Time 3.086s\n",
            " correct:  tensor(18624) Total:  18624\n",
            "Epoch 3, Batch 291, Loss 15.415661470996556 Accuracy 1.0 Time 3.124s\n",
            " correct:  tensor(18688) Total:  18688\n",
            "Epoch 3, Batch 292, Loss 15.41552615818912 Accuracy 1.0 Time 3.073s\n",
            " correct:  tensor(18752) Total:  18752\n",
            "Epoch 3, Batch 293, Loss 15.414478109965145 Accuracy 1.0 Time 3.148s\n",
            " correct:  tensor(18816) Total:  18816\n",
            "Epoch 3, Batch 294, Loss 15.41231835618311 Accuracy 1.0 Time 3.137s\n",
            " correct:  tensor(18880) Total:  18880\n",
            "Epoch 3, Batch 295, Loss 15.410513945757332 Accuracy 1.0 Time 3.186s\n",
            " correct:  tensor(18944) Total:  18944\n",
            "Epoch 3, Batch 296, Loss 15.41096905759863 Accuracy 1.0 Time 3.135s\n",
            " correct:  tensor(19008) Total:  19008\n",
            "Epoch 3, Batch 297, Loss 15.412713169650196 Accuracy 1.0 Time 3.158s\n",
            " correct:  tensor(19072) Total:  19072\n",
            "Epoch 3, Batch 298, Loss 15.410948945371896 Accuracy 1.0 Time 3.16s\n",
            " correct:  tensor(19136) Total:  19136\n",
            "Epoch 3, Batch 299, Loss 15.408640858321684 Accuracy 1.0 Time 3.14s\n",
            " correct:  tensor(19200) Total:  19200\n",
            "Epoch 3, Batch 300, Loss 15.409340022404988 Accuracy 1.0 Time 3.168s\n",
            " correct:  tensor(19264) Total:  19264\n",
            "Epoch 3, Batch 301, Loss 15.410811256332652 Accuracy 1.0 Time 3.253s\n",
            " correct:  tensor(19328) Total:  19328\n",
            "Epoch 3, Batch 302, Loss 15.412971130270043 Accuracy 1.0 Time 3.126s\n",
            " correct:  tensor(19392) Total:  19392\n",
            "Epoch 3, Batch 303, Loss 15.411062426299544 Accuracy 1.0 Time 3.11s\n",
            " correct:  tensor(19456) Total:  19456\n",
            "Epoch 3, Batch 304, Loss 15.408137271278783 Accuracy 1.0 Time 3.179s\n",
            " correct:  tensor(19520) Total:  19520\n",
            "Epoch 3, Batch 305, Loss 15.407547250341196 Accuracy 1.0 Time 3.133s\n",
            " correct:  tensor(19584) Total:  19584\n",
            "Epoch 3, Batch 306, Loss 15.404700338450912 Accuracy 1.0 Time 3.125s\n",
            " correct:  tensor(19648) Total:  19648\n",
            "Epoch 3, Batch 307, Loss 15.403245692920995 Accuracy 1.0 Time 3.277s\n",
            " correct:  tensor(19712) Total:  19712\n",
            "Epoch 3, Batch 308, Loss 15.404975321385768 Accuracy 1.0 Time 3.224s\n",
            " correct:  tensor(19776) Total:  19776\n",
            "Epoch 3, Batch 309, Loss 15.404339876761329 Accuracy 1.0 Time 3.153s\n",
            " correct:  tensor(19840) Total:  19840\n",
            "Epoch 3, Batch 310, Loss 15.401404048550514 Accuracy 1.0 Time 3.108s\n",
            " correct:  tensor(19904) Total:  19904\n",
            "Epoch 3, Batch 311, Loss 15.400040092958898 Accuracy 1.0 Time 3.135s\n",
            " correct:  tensor(19968) Total:  19968\n",
            "Epoch 3, Batch 312, Loss 15.398113956818214 Accuracy 1.0 Time 3.106s\n",
            " correct:  tensor(20032) Total:  20032\n",
            "Epoch 3, Batch 313, Loss 15.398225567973078 Accuracy 1.0 Time 3.119s\n",
            " correct:  tensor(20096) Total:  20096\n",
            "Epoch 3, Batch 314, Loss 15.397313215170696 Accuracy 1.0 Time 3.179s\n",
            " correct:  tensor(20160) Total:  20160\n",
            "Epoch 3, Batch 315, Loss 15.396517435709635 Accuracy 1.0 Time 3.113s\n",
            " correct:  tensor(20224) Total:  20224\n",
            "Epoch 3, Batch 316, Loss 15.397157895414136 Accuracy 1.0 Time 3.156s\n",
            " correct:  tensor(20288) Total:  20288\n",
            "Epoch 3, Batch 317, Loss 15.39975084190489 Accuracy 1.0 Time 3.146s\n",
            " correct:  tensor(20352) Total:  20352\n",
            "Epoch 3, Batch 318, Loss 15.399789192391642 Accuracy 1.0 Time 3.151s\n",
            " correct:  tensor(20416) Total:  20416\n",
            "Epoch 3, Batch 319, Loss 15.399100773013124 Accuracy 1.0 Time 3.071s\n",
            " correct:  tensor(20480) Total:  20480\n",
            "Epoch 3, Batch 320, Loss 15.398033821582795 Accuracy 1.0 Time 3.123s\n",
            " correct:  tensor(20544) Total:  20544\n",
            "Epoch 3, Batch 321, Loss 15.397445943125312 Accuracy 1.0 Time 3.166s\n",
            " correct:  tensor(20608) Total:  20608\n",
            "Epoch 3, Batch 322, Loss 15.401753244933134 Accuracy 1.0 Time 3.055s\n",
            " correct:  tensor(20672) Total:  20672\n",
            "Epoch 3, Batch 323, Loss 15.39869778222713 Accuracy 1.0 Time 3.076s\n",
            " correct:  tensor(20736) Total:  20736\n",
            "Epoch 3, Batch 324, Loss 15.400838890193421 Accuracy 1.0 Time 3.1s\n",
            " correct:  tensor(20800) Total:  20800\n",
            "Epoch 3, Batch 325, Loss 15.398962757404034 Accuracy 1.0 Time 3.075s\n",
            " correct:  tensor(20864) Total:  20864\n",
            "Epoch 3, Batch 326, Loss 15.396974970226639 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(20928) Total:  20928\n",
            "Epoch 3, Batch 327, Loss 15.394109536383858 Accuracy 1.0 Time 3.124s\n",
            " correct:  tensor(20992) Total:  20992\n",
            "Epoch 3, Batch 328, Loss 15.396019857104232 Accuracy 1.0 Time 3.132s\n",
            " correct:  tensor(21056) Total:  21056\n",
            "Epoch 3, Batch 329, Loss 15.397674583736524 Accuracy 1.0 Time 3.354s\n",
            " correct:  tensor(21120) Total:  21120\n",
            "Epoch 3, Batch 330, Loss 15.40017810012355 Accuracy 1.0 Time 3.378s\n",
            " correct:  tensor(21184) Total:  21184\n",
            "Epoch 3, Batch 331, Loss 15.399690241972126 Accuracy 1.0 Time 3.334s\n",
            " correct:  tensor(21248) Total:  21248\n",
            "Epoch 3, Batch 332, Loss 15.397296388465238 Accuracy 1.0 Time 3.12s\n",
            " correct:  tensor(21312) Total:  21312\n",
            "Epoch 3, Batch 333, Loss 15.398896847401295 Accuracy 1.0 Time 3.142s\n",
            " correct:  tensor(21376) Total:  21376\n",
            "Epoch 3, Batch 334, Loss 15.397035961379547 Accuracy 1.0 Time 3.115s\n",
            " correct:  tensor(21440) Total:  21440\n",
            "Epoch 3, Batch 335, Loss 15.398982617392468 Accuracy 1.0 Time 3.165s\n",
            " correct:  tensor(21504) Total:  21504\n",
            "Epoch 3, Batch 336, Loss 15.39902404092607 Accuracy 1.0 Time 3.125s\n",
            " correct:  tensor(21568) Total:  21568\n",
            "Epoch 3, Batch 337, Loss 15.396232466315658 Accuracy 1.0 Time 3.135s\n",
            " correct:  tensor(21632) Total:  21632\n",
            "Epoch 3, Batch 338, Loss 15.395014892668414 Accuracy 1.0 Time 3.1s\n",
            " correct:  tensor(21696) Total:  21696\n",
            "Epoch 3, Batch 339, Loss 15.397183167899252 Accuracy 1.0 Time 3.156s\n",
            " correct:  tensor(21760) Total:  21760\n",
            "Epoch 3, Batch 340, Loss 15.394686163173002 Accuracy 1.0 Time 3.378s\n",
            " correct:  tensor(21824) Total:  21824\n",
            "Epoch 3, Batch 341, Loss 15.393920753009159 Accuracy 1.0 Time 3.168s\n",
            " correct:  tensor(21888) Total:  21888\n",
            "Epoch 3, Batch 342, Loss 15.392221171953524 Accuracy 1.0 Time 3.188s\n",
            " correct:  tensor(21952) Total:  21952\n",
            "Epoch 3, Batch 343, Loss 15.390393866046873 Accuracy 1.0 Time 3.123s\n",
            " correct:  tensor(22016) Total:  22016\n",
            "Epoch 3, Batch 344, Loss 15.390332424363425 Accuracy 1.0 Time 3.124s\n",
            " correct:  tensor(22080) Total:  22080\n",
            "Epoch 3, Batch 345, Loss 15.39068798949753 Accuracy 1.0 Time 3.177s\n",
            " correct:  tensor(22144) Total:  22144\n",
            "Epoch 3, Batch 346, Loss 15.391867899481271 Accuracy 1.0 Time 3.055s\n",
            " correct:  tensor(22208) Total:  22208\n",
            "Epoch 3, Batch 347, Loss 15.389158202179571 Accuracy 1.0 Time 3.144s\n",
            " correct:  tensor(22272) Total:  22272\n",
            "Epoch 3, Batch 348, Loss 15.388685917032175 Accuracy 1.0 Time 3.082s\n",
            " correct:  tensor(22336) Total:  22336\n",
            "Epoch 3, Batch 349, Loss 15.387712284623722 Accuracy 1.0 Time 3.065s\n",
            " correct:  tensor(22400) Total:  22400\n",
            "Epoch 3, Batch 350, Loss 15.388003749847412 Accuracy 1.0 Time 3.135s\n",
            " correct:  tensor(22464) Total:  22464\n",
            "Epoch 3, Batch 351, Loss 15.388865310582 Accuracy 1.0 Time 3.117s\n",
            " correct:  tensor(22528) Total:  22528\n",
            "Epoch 3, Batch 352, Loss 15.387206600470977 Accuracy 1.0 Time 3.079s\n",
            " correct:  tensor(22592) Total:  22592\n",
            "Epoch 3, Batch 353, Loss 15.386726106530208 Accuracy 1.0 Time 3.067s\n",
            " correct:  tensor(22656) Total:  22656\n",
            "Epoch 3, Batch 354, Loss 15.385524628526074 Accuracy 1.0 Time 3.094s\n",
            " correct:  tensor(22720) Total:  22720\n",
            "Epoch 3, Batch 355, Loss 15.383355038602588 Accuracy 1.0 Time 3.112s\n",
            " correct:  tensor(22784) Total:  22784\n",
            "Epoch 3, Batch 356, Loss 15.386021788200635 Accuracy 1.0 Time 3.076s\n",
            " correct:  tensor(22848) Total:  22848\n",
            "Epoch 3, Batch 357, Loss 15.385478706253009 Accuracy 1.0 Time 3.097s\n",
            " correct:  tensor(22912) Total:  22912\n",
            "Epoch 3, Batch 358, Loss 15.385233868433776 Accuracy 1.0 Time 3.115s\n",
            " correct:  tensor(22976) Total:  22976\n",
            "Epoch 3, Batch 359, Loss 15.385023799779356 Accuracy 1.0 Time 3.075s\n",
            " correct:  tensor(23040) Total:  23040\n",
            "Epoch 3, Batch 360, Loss 15.38648326396942 Accuracy 1.0 Time 3.027s\n",
            " correct:  tensor(23104) Total:  23104\n",
            "Epoch 3, Batch 361, Loss 15.387643645014458 Accuracy 1.0 Time 3.11s\n",
            " correct:  tensor(23168) Total:  23168\n",
            "Epoch 3, Batch 362, Loss 15.386391803045958 Accuracy 1.0 Time 3.118s\n",
            " correct:  tensor(23232) Total:  23232\n",
            "Epoch 3, Batch 363, Loss 15.385116135778507 Accuracy 1.0 Time 3.151s\n",
            " correct:  tensor(23296) Total:  23296\n",
            "Epoch 3, Batch 364, Loss 15.383646055892273 Accuracy 1.0 Time 3.102s\n",
            " correct:  tensor(23360) Total:  23360\n",
            "Epoch 3, Batch 365, Loss 15.3846937414718 Accuracy 1.0 Time 3.123s\n",
            " correct:  tensor(23424) Total:  23424\n",
            "Epoch 3, Batch 366, Loss 15.386677867076436 Accuracy 1.0 Time 3.093s\n",
            " correct:  tensor(23488) Total:  23488\n",
            "Epoch 3, Batch 367, Loss 15.3861448485455 Accuracy 1.0 Time 3.183s\n",
            " correct:  tensor(23552) Total:  23552\n",
            "Epoch 3, Batch 368, Loss 15.385787601056306 Accuracy 1.0 Time 3.152s\n",
            " correct:  tensor(23616) Total:  23616\n",
            "Epoch 3, Batch 369, Loss 15.385607287812686 Accuracy 1.0 Time 3.074s\n",
            " correct:  tensor(23680) Total:  23680\n",
            "Epoch 3, Batch 370, Loss 15.386147004204828 Accuracy 1.0 Time 3.097s\n",
            " correct:  tensor(23744) Total:  23744\n",
            "Epoch 3, Batch 371, Loss 15.38659016334143 Accuracy 1.0 Time 3.107s\n",
            " correct:  tensor(23808) Total:  23808\n",
            "Epoch 3, Batch 372, Loss 15.386737420994749 Accuracy 1.0 Time 3.153s\n",
            " correct:  tensor(23872) Total:  23872\n",
            "Epoch 3, Batch 373, Loss 15.385979025996722 Accuracy 1.0 Time 3.101s\n",
            " correct:  tensor(23936) Total:  23936\n",
            "Epoch 3, Batch 374, Loss 15.385190785249923 Accuracy 1.0 Time 3.107s\n",
            " correct:  tensor(24000) Total:  24000\n",
            "Epoch 3, Batch 375, Loss 15.385099934895834 Accuracy 1.0 Time 3.177s\n",
            " correct:  tensor(24064) Total:  24064\n",
            "Epoch 3, Batch 376, Loss 15.384973820219649 Accuracy 1.0 Time 3.173s\n",
            " correct:  tensor(24128) Total:  24128\n",
            "Epoch 3, Batch 377, Loss 15.383509274186759 Accuracy 1.0 Time 3.161s\n",
            " correct:  tensor(24192) Total:  24192\n",
            "Epoch 3, Batch 378, Loss 15.382979259289131 Accuracy 1.0 Time 3.134s\n",
            " correct:  tensor(24256) Total:  24256\n",
            "Epoch 3, Batch 379, Loss 15.38527498018773 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(24320) Total:  24320\n",
            "Epoch 3, Batch 380, Loss 15.387346430828696 Accuracy 1.0 Time 3.18s\n",
            " correct:  tensor(24384) Total:  24384\n",
            "Epoch 3, Batch 381, Loss 15.384058829680516 Accuracy 1.0 Time 3.205s\n",
            " correct:  tensor(24448) Total:  24448\n",
            "Epoch 3, Batch 382, Loss 15.382255262105252 Accuracy 1.0 Time 3.152s\n",
            " correct:  tensor(24512) Total:  24512\n",
            "Epoch 3, Batch 383, Loss 15.38240460378383 Accuracy 1.0 Time 3.236s\n",
            " correct:  tensor(24576) Total:  24576\n",
            "Epoch 3, Batch 384, Loss 15.382131579021612 Accuracy 1.0 Time 3.154s\n",
            " correct:  tensor(24640) Total:  24640\n",
            "Epoch 3, Batch 385, Loss 15.3807394126793 Accuracy 1.0 Time 3.108s\n",
            " correct:  tensor(24704) Total:  24704\n",
            "Epoch 3, Batch 386, Loss 15.381195550137852 Accuracy 1.0 Time 3.105s\n",
            " correct:  tensor(24768) Total:  24768\n",
            "Epoch 3, Batch 387, Loss 15.379886782446572 Accuracy 1.0 Time 3.182s\n",
            " correct:  tensor(24832) Total:  24832\n",
            "Epoch 3, Batch 388, Loss 15.379728164869485 Accuracy 1.0 Time 3.107s\n",
            " correct:  tensor(24896) Total:  24896\n",
            "Epoch 3, Batch 389, Loss 15.379811416861331 Accuracy 1.0 Time 3.176s\n",
            " correct:  tensor(24960) Total:  24960\n",
            "Epoch 3, Batch 390, Loss 15.38208682231414 Accuracy 1.0 Time 3.13s\n",
            " correct:  tensor(25024) Total:  25024\n",
            "Epoch 3, Batch 391, Loss 15.384686194417421 Accuracy 1.0 Time 3.162s\n",
            " correct:  tensor(25088) Total:  25088\n",
            "Epoch 3, Batch 392, Loss 15.385030880266306 Accuracy 1.0 Time 3.108s\n",
            " correct:  tensor(25152) Total:  25152\n",
            "Epoch 3, Batch 393, Loss 15.384738412522177 Accuracy 1.0 Time 3.159s\n",
            " correct:  tensor(25216) Total:  25216\n",
            "Epoch 3, Batch 394, Loss 15.383896425895884 Accuracy 1.0 Time 3.222s\n",
            " correct:  tensor(25280) Total:  25280\n",
            "Epoch 3, Batch 395, Loss 15.385512851763375 Accuracy 1.0 Time 3.074s\n",
            " correct:  tensor(25344) Total:  25344\n",
            "Epoch 3, Batch 396, Loss 15.385886430740356 Accuracy 1.0 Time 3.227s\n",
            " correct:  tensor(25408) Total:  25408\n",
            "Epoch 3, Batch 397, Loss 15.385571061814161 Accuracy 1.0 Time 3.139s\n",
            " correct:  tensor(25472) Total:  25472\n",
            "Epoch 3, Batch 398, Loss 15.384105179178055 Accuracy 1.0 Time 3.136s\n",
            " correct:  tensor(25536) Total:  25536\n",
            "Epoch 3, Batch 399, Loss 15.384621964361434 Accuracy 1.0 Time 3.118s\n",
            " correct:  tensor(25600) Total:  25600\n",
            "Epoch 3, Batch 400, Loss 15.386993639469146 Accuracy 1.0 Time 3.187s\n",
            " correct:  tensor(25664) Total:  25664\n",
            "Epoch 3, Batch 401, Loss 15.386610801677751 Accuracy 1.0 Time 3.086s\n",
            " correct:  tensor(25728) Total:  25728\n",
            "Epoch 3, Batch 402, Loss 15.386022427781898 Accuracy 1.0 Time 3.202s\n",
            " correct:  tensor(25792) Total:  25792\n",
            "Epoch 3, Batch 403, Loss 15.386724095782334 Accuracy 1.0 Time 3.157s\n",
            " correct:  tensor(25856) Total:  25856\n",
            "Epoch 3, Batch 404, Loss 15.387042949695399 Accuracy 1.0 Time 3.092s\n",
            " correct:  tensor(25920) Total:  25920\n",
            "Epoch 3, Batch 405, Loss 15.385743346037689 Accuracy 1.0 Time 3.108s\n",
            " correct:  tensor(25984) Total:  25984\n",
            "Epoch 3, Batch 406, Loss 15.38581751837519 Accuracy 1.0 Time 3.172s\n",
            " correct:  tensor(26048) Total:  26048\n",
            "Epoch 3, Batch 407, Loss 15.385520813213226 Accuracy 1.0 Time 3.166s\n",
            " correct:  tensor(26112) Total:  26112\n",
            "Epoch 3, Batch 408, Loss 15.383109265682744 Accuracy 1.0 Time 3.155s\n",
            " correct:  tensor(26176) Total:  26176\n",
            "Epoch 3, Batch 409, Loss 15.383101843388564 Accuracy 1.0 Time 3.163s\n",
            " correct:  tensor(26240) Total:  26240\n",
            "Epoch 3, Batch 410, Loss 15.382838339921905 Accuracy 1.0 Time 3.157s\n",
            " correct:  tensor(26304) Total:  26304\n",
            "Epoch 3, Batch 411, Loss 15.383727990217743 Accuracy 1.0 Time 3.106s\n",
            " correct:  tensor(26368) Total:  26368\n",
            "Epoch 3, Batch 412, Loss 15.382850065972042 Accuracy 1.0 Time 3.137s\n",
            " correct:  tensor(26432) Total:  26432\n",
            "Epoch 3, Batch 413, Loss 15.38355391827969 Accuracy 1.0 Time 3.145s\n",
            " correct:  tensor(26496) Total:  26496\n",
            "Epoch 3, Batch 414, Loss 15.385045470822837 Accuracy 1.0 Time 3.079s\n",
            " correct:  tensor(26560) Total:  26560\n",
            "Epoch 3, Batch 415, Loss 15.384056658917164 Accuracy 1.0 Time 3.164s\n",
            " correct:  tensor(26624) Total:  26624\n",
            "Epoch 3, Batch 416, Loss 15.386999636888504 Accuracy 1.0 Time 3.102s\n",
            " correct:  tensor(26688) Total:  26688\n",
            "Epoch 3, Batch 417, Loss 15.385243555338834 Accuracy 1.0 Time 3.085s\n",
            " correct:  tensor(26752) Total:  26752\n",
            "Epoch 3, Batch 418, Loss 15.385720462890333 Accuracy 1.0 Time 3.111s\n",
            " correct:  tensor(26816) Total:  26816\n",
            "Epoch 3, Batch 419, Loss 15.387803612574757 Accuracy 1.0 Time 3.091s\n",
            " correct:  tensor(26880) Total:  26880\n",
            "Epoch 3, Batch 420, Loss 15.387712244760422 Accuracy 1.0 Time 3.084s\n",
            " correct:  tensor(26944) Total:  26944\n",
            "Epoch 3, Batch 421, Loss 15.386764283984403 Accuracy 1.0 Time 3.196s\n",
            " correct:  tensor(27008) Total:  27008\n",
            "Epoch 3, Batch 422, Loss 15.387480679281515 Accuracy 1.0 Time 3.153s\n",
            " correct:  tensor(27072) Total:  27072\n",
            "Epoch 3, Batch 423, Loss 15.38799422514354 Accuracy 1.0 Time 3.09s\n",
            " correct:  tensor(27136) Total:  27136\n",
            "Epoch 3, Batch 424, Loss 15.387445922167796 Accuracy 1.0 Time 3.11s\n",
            " correct:  tensor(27200) Total:  27200\n",
            "Epoch 3, Batch 425, Loss 15.388352562399472 Accuracy 1.0 Time 3.064s\n",
            " correct:  tensor(27264) Total:  27264\n",
            "Epoch 3, Batch 426, Loss 15.390202070066067 Accuracy 1.0 Time 3.126s\n",
            " correct:  tensor(27328) Total:  27328\n",
            "Epoch 3, Batch 427, Loss 15.390466145106725 Accuracy 1.0 Time 3.252s\n",
            " correct:  tensor(27392) Total:  27392\n",
            "Epoch 3, Batch 428, Loss 15.391719586381289 Accuracy 1.0 Time 3.337s\n",
            " correct:  tensor(27456) Total:  27456\n",
            "Epoch 3, Batch 429, Loss 15.390916799887632 Accuracy 1.0 Time 3.369s\n",
            " correct:  tensor(27520) Total:  27520\n",
            "Epoch 3, Batch 430, Loss 15.39143541025561 Accuracy 1.0 Time 3.23s\n",
            " correct:  tensor(27584) Total:  27584\n",
            "Epoch 3, Batch 431, Loss 15.389936402889527 Accuracy 1.0 Time 3.119s\n",
            " correct:  tensor(27648) Total:  27648\n",
            "Epoch 3, Batch 432, Loss 15.388725289592037 Accuracy 1.0 Time 3.241s\n",
            " correct:  tensor(27712) Total:  27712\n",
            "Epoch 3, Batch 433, Loss 15.390056887620041 Accuracy 1.0 Time 3.062s\n",
            " correct:  tensor(27776) Total:  27776\n",
            "Epoch 3, Batch 434, Loss 15.389354519031015 Accuracy 1.0 Time 3.148s\n",
            " correct:  tensor(27840) Total:  27840\n",
            "Epoch 3, Batch 435, Loss 15.390250727774083 Accuracy 1.0 Time 3.135s\n",
            " correct:  tensor(27904) Total:  27904\n",
            "Epoch 3, Batch 436, Loss 15.389209915738586 Accuracy 1.0 Time 3.093s\n",
            " correct:  tensor(27968) Total:  27968\n",
            "Epoch 3, Batch 437, Loss 15.389060210308712 Accuracy 1.0 Time 3.318s\n",
            " correct:  tensor(28032) Total:  28032\n",
            "Epoch 3, Batch 438, Loss 15.388831796167104 Accuracy 1.0 Time 3.223s\n",
            " correct:  tensor(28096) Total:  28096\n",
            "Epoch 3, Batch 439, Loss 15.390594677935972 Accuracy 1.0 Time 3.108s\n",
            " correct:  tensor(28160) Total:  28160\n",
            "Epoch 3, Batch 440, Loss 15.39123153036291 Accuracy 1.0 Time 3.151s\n",
            " correct:  tensor(28224) Total:  28224\n",
            "Epoch 3, Batch 441, Loss 15.390548777418072 Accuracy 1.0 Time 3.155s\n",
            " correct:  tensor(28288) Total:  28288\n",
            "Epoch 3, Batch 442, Loss 15.392187858598804 Accuracy 1.0 Time 3.133s\n",
            " correct:  tensor(28352) Total:  28352\n",
            "Epoch 3, Batch 443, Loss 15.391400685966957 Accuracy 1.0 Time 3.097s\n",
            " correct:  tensor(28416) Total:  28416\n",
            "Epoch 3, Batch 444, Loss 15.390998260394946 Accuracy 1.0 Time 3.132s\n",
            " correct:  tensor(28480) Total:  28480\n",
            "Epoch 3, Batch 445, Loss 15.392875255627578 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(28544) Total:  28544\n",
            "Epoch 3, Batch 446, Loss 15.394852514224203 Accuracy 1.0 Time 3.124s\n",
            " correct:  tensor(28608) Total:  28608\n",
            "Epoch 3, Batch 447, Loss 15.394326342298948 Accuracy 1.0 Time 3.119s\n",
            " correct:  tensor(28672) Total:  28672\n",
            "Epoch 3, Batch 448, Loss 15.398362906915802 Accuracy 1.0 Time 3.134s\n",
            " correct:  tensor(28736) Total:  28736\n",
            "Epoch 3, Batch 449, Loss 15.397633699107011 Accuracy 1.0 Time 3.117s\n",
            " correct:  tensor(28800) Total:  28800\n",
            "Epoch 3, Batch 450, Loss 15.399790895250108 Accuracy 1.0 Time 3.109s\n",
            " correct:  tensor(28864) Total:  28864\n",
            "Epoch 3, Batch 451, Loss 15.399636325709308 Accuracy 1.0 Time 3.099s\n",
            " correct:  tensor(28928) Total:  28928\n",
            "Epoch 3, Batch 452, Loss 15.399076963947937 Accuracy 1.0 Time 3.131s\n",
            " correct:  tensor(28992) Total:  28992\n",
            "Epoch 3, Batch 453, Loss 15.399290465887546 Accuracy 1.0 Time 3.092s\n",
            " correct:  tensor(29056) Total:  29056\n",
            "Epoch 3, Batch 454, Loss 15.400046766592018 Accuracy 1.0 Time 3.142s\n",
            " correct:  tensor(29120) Total:  29120\n",
            "Epoch 3, Batch 455, Loss 15.399310480893313 Accuracy 1.0 Time 3.098s\n",
            " correct:  tensor(29184) Total:  29184\n",
            "Epoch 3, Batch 456, Loss 15.397803373504104 Accuracy 1.0 Time 3.083s\n",
            " correct:  tensor(29248) Total:  29248\n",
            "Epoch 3, Batch 457, Loss 15.397341655171935 Accuracy 1.0 Time 3.125s\n",
            " correct:  tensor(29312) Total:  29312\n",
            "Epoch 3, Batch 458, Loss 15.395464045512103 Accuracy 1.0 Time 3.122s\n",
            " correct:  tensor(29376) Total:  29376\n",
            "Epoch 3, Batch 459, Loss 15.39662694619372 Accuracy 1.0 Time 3.182s\n",
            " correct:  tensor(29440) Total:  29440\n",
            "Epoch 3, Batch 460, Loss 15.398785041726153 Accuracy 1.0 Time 3.154s\n",
            " correct:  tensor(29504) Total:  29504\n",
            "Epoch 3, Batch 461, Loss 15.397937834650731 Accuracy 1.0 Time 3.138s\n",
            " correct:  tensor(29568) Total:  29568\n",
            "Epoch 3, Batch 462, Loss 15.398466756333521 Accuracy 1.0 Time 3.103s\n",
            " correct:  tensor(29632) Total:  29632\n",
            "Epoch 3, Batch 463, Loss 15.398098609874882 Accuracy 1.0 Time 3.11s\n",
            " correct:  tensor(29696) Total:  29696\n",
            "Epoch 3, Batch 464, Loss 15.397016993884382 Accuracy 1.0 Time 3.126s\n",
            " correct:  tensor(29760) Total:  29760\n",
            "Epoch 3, Batch 465, Loss 15.396003573427919 Accuracy 1.0 Time 3.118s\n",
            " correct:  tensor(29824) Total:  29824\n",
            "Epoch 3, Batch 466, Loss 15.395529478916282 Accuracy 1.0 Time 3.101s\n",
            " correct:  tensor(29888) Total:  29888\n",
            "Epoch 3, Batch 467, Loss 15.394828861734851 Accuracy 1.0 Time 3.177s\n",
            " correct:  tensor(29952) Total:  29952\n",
            "Epoch 3, Batch 468, Loss 15.395132242104946 Accuracy 1.0 Time 3.194s\n",
            " correct:  tensor(30016) Total:  30016\n",
            "Epoch 3, Batch 469, Loss 15.397057301454199 Accuracy 1.0 Time 3.175s\n",
            " correct:  tensor(30080) Total:  30080\n",
            "Epoch 3, Batch 470, Loss 15.398862378140713 Accuracy 1.0 Time 3.218s\n",
            " correct:  tensor(30144) Total:  30144\n",
            "Epoch 3, Batch 471, Loss 15.400728199385787 Accuracy 1.0 Time 3.16s\n",
            " correct:  tensor(30208) Total:  30208\n",
            "Epoch 3, Batch 472, Loss 15.401246693174718 Accuracy 1.0 Time 3.101s\n",
            " correct:  tensor(30272) Total:  30272\n",
            "Epoch 3, Batch 473, Loss 15.39944694510978 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(30336) Total:  30336\n",
            "Epoch 3, Batch 474, Loss 15.40078147654795 Accuracy 1.0 Time 3.18s\n",
            " correct:  tensor(30400) Total:  30400\n",
            "Epoch 3, Batch 475, Loss 15.401943698682283 Accuracy 1.0 Time 3.113s\n",
            " correct:  tensor(30464) Total:  30464\n",
            "Epoch 3, Batch 476, Loss 15.400849803155209 Accuracy 1.0 Time 3.126s\n",
            " correct:  tensor(30528) Total:  30528\n",
            "Epoch 3, Batch 477, Loss 15.401268163317155 Accuracy 1.0 Time 3.188s\n",
            " correct:  tensor(30592) Total:  30592\n",
            "Epoch 3, Batch 478, Loss 15.40004595553027 Accuracy 1.0 Time 3.205s\n",
            " correct:  tensor(30656) Total:  30656\n",
            "Epoch 3, Batch 479, Loss 15.401273233656594 Accuracy 1.0 Time 3.165s\n",
            " correct:  tensor(30720) Total:  30720\n",
            "Epoch 3, Batch 480, Loss 15.402720137437184 Accuracy 1.0 Time 3.176s\n",
            " correct:  tensor(30784) Total:  30784\n",
            "Epoch 3, Batch 481, Loss 15.403508535293929 Accuracy 1.0 Time 3.095s\n",
            " correct:  tensor(30848) Total:  30848\n",
            "Epoch 3, Batch 482, Loss 15.40148776200797 Accuracy 1.0 Time 3.077s\n",
            " correct:  tensor(30912) Total:  30912\n",
            "Epoch 3, Batch 483, Loss 15.40094256154252 Accuracy 1.0 Time 3.131s\n",
            " correct:  tensor(30976) Total:  30976\n",
            "Epoch 3, Batch 484, Loss 15.401972863299788 Accuracy 1.0 Time 3.23s\n",
            " correct:  tensor(31040) Total:  31040\n",
            "Epoch 3, Batch 485, Loss 15.401897998691835 Accuracy 1.0 Time 3.144s\n",
            " correct:  tensor(31104) Total:  31104\n",
            "Epoch 3, Batch 486, Loss 15.400504730365894 Accuracy 1.0 Time 3.168s\n",
            " correct:  tensor(31168) Total:  31168\n",
            "Epoch 3, Batch 487, Loss 15.398886764808356 Accuracy 1.0 Time 3.136s\n",
            " correct:  tensor(31232) Total:  31232\n",
            "Epoch 3, Batch 488, Loss 15.39985128699756 Accuracy 1.0 Time 3.154s\n",
            " correct:  tensor(31296) Total:  31296\n",
            "Epoch 3, Batch 489, Loss 15.401520438477062 Accuracy 1.0 Time 3.196s\n",
            " correct:  tensor(31360) Total:  31360\n",
            "Epoch 3, Batch 490, Loss 15.403242484890685 Accuracy 1.0 Time 3.219s\n",
            " correct:  tensor(31424) Total:  31424\n",
            "Epoch 3, Batch 491, Loss 15.401743366373532 Accuracy 1.0 Time 3.171s\n",
            " correct:  tensor(31488) Total:  31488\n",
            "Epoch 3, Batch 492, Loss 15.401863173740667 Accuracy 1.0 Time 3.216s\n",
            " correct:  tensor(31552) Total:  31552\n",
            "Epoch 3, Batch 493, Loss 15.40348100758953 Accuracy 1.0 Time 3.213s\n",
            " correct:  tensor(31616) Total:  31616\n",
            "Epoch 3, Batch 494, Loss 15.40438364198816 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(31680) Total:  31680\n",
            "Epoch 3, Batch 495, Loss 15.402150980631511 Accuracy 1.0 Time 3.175s\n",
            " correct:  tensor(31744) Total:  31744\n",
            "Epoch 3, Batch 496, Loss 15.403536723506067 Accuracy 1.0 Time 3.135s\n",
            " correct:  tensor(31808) Total:  31808\n",
            "Epoch 3, Batch 497, Loss 15.403021931408396 Accuracy 1.0 Time 3.268s\n",
            " correct:  tensor(31872) Total:  31872\n",
            "Epoch 3, Batch 498, Loss 15.403871361989093 Accuracy 1.0 Time 3.135s\n",
            " correct:  tensor(31936) Total:  31936\n",
            "Epoch 3, Batch 499, Loss 15.40462246399843 Accuracy 1.0 Time 3.111s\n",
            " correct:  tensor(32000) Total:  32000\n",
            "Epoch 3, Batch 500, Loss 15.402656847000122 Accuracy 1.0 Time 3.138s\n",
            " correct:  tensor(32064) Total:  32064\n",
            "Epoch 3, Batch 501, Loss 15.404127796728929 Accuracy 1.0 Time 3.062s\n",
            " correct:  tensor(32128) Total:  32128\n",
            "Epoch 3, Batch 502, Loss 15.404681281739498 Accuracy 1.0 Time 3.137s\n",
            " correct:  tensor(32192) Total:  32192\n",
            "Epoch 3, Batch 503, Loss 15.40353278658736 Accuracy 1.0 Time 3.175s\n",
            " correct:  tensor(32256) Total:  32256\n",
            "Epoch 3, Batch 504, Loss 15.403886516888937 Accuracy 1.0 Time 3.197s\n",
            " correct:  tensor(32320) Total:  32320\n",
            "Epoch 3, Batch 505, Loss 15.404707682014692 Accuracy 1.0 Time 3.102s\n",
            " correct:  tensor(32384) Total:  32384\n",
            "Epoch 3, Batch 506, Loss 15.404123163034793 Accuracy 1.0 Time 3.086s\n",
            " correct:  tensor(32448) Total:  32448\n",
            "Epoch 3, Batch 507, Loss 15.405234891752285 Accuracy 1.0 Time 3.155s\n",
            " correct:  tensor(32512) Total:  32512\n",
            "Epoch 3, Batch 508, Loss 15.40477157577755 Accuracy 1.0 Time 3.169s\n",
            " correct:  tensor(32576) Total:  32576\n",
            "Epoch 3, Batch 509, Loss 15.404994855929452 Accuracy 1.0 Time 3.117s\n",
            " correct:  tensor(32640) Total:  32640\n",
            "Epoch 3, Batch 510, Loss 15.40544584498686 Accuracy 1.0 Time 3.105s\n",
            " correct:  tensor(32704) Total:  32704\n",
            "Epoch 3, Batch 511, Loss 15.405735624279762 Accuracy 1.0 Time 3.074s\n",
            " correct:  tensor(32768) Total:  32768\n",
            "Epoch 3, Batch 512, Loss 15.406835298985243 Accuracy 1.0 Time 3.117s\n",
            " correct:  tensor(32832) Total:  32832\n",
            "Epoch 3, Batch 513, Loss 15.405676756221183 Accuracy 1.0 Time 3.1s\n",
            " correct:  tensor(32896) Total:  32896\n",
            "Epoch 3, Batch 514, Loss 15.404607654081708 Accuracy 1.0 Time 3.113s\n",
            " correct:  tensor(32960) Total:  32960\n",
            "Epoch 3, Batch 515, Loss 15.40381070109247 Accuracy 1.0 Time 3.127s\n",
            " correct:  tensor(33024) Total:  33024\n",
            "Epoch 3, Batch 516, Loss 15.40372631531353 Accuracy 1.0 Time 3.095s\n",
            " correct:  tensor(33088) Total:  33088\n",
            "Epoch 3, Batch 517, Loss 15.403028074961798 Accuracy 1.0 Time 3.15s\n",
            " correct:  tensor(33152) Total:  33152\n",
            "Epoch 3, Batch 518, Loss 15.403776590428297 Accuracy 1.0 Time 3.107s\n",
            " correct:  tensor(33216) Total:  33216\n",
            "Epoch 3, Batch 519, Loss 15.405060485148935 Accuracy 1.0 Time 3.102s\n",
            " correct:  tensor(33280) Total:  33280\n",
            "Epoch 3, Batch 520, Loss 15.405068584588857 Accuracy 1.0 Time 3.119s\n",
            " correct:  tensor(33344) Total:  33344\n",
            "Epoch 3, Batch 521, Loss 15.40661309181843 Accuracy 1.0 Time 3.111s\n",
            " correct:  tensor(33408) Total:  33408\n",
            "Epoch 3, Batch 522, Loss 15.406140426109577 Accuracy 1.0 Time 3.138s\n",
            " correct:  tensor(33472) Total:  33472\n",
            "Epoch 3, Batch 523, Loss 15.40667909873831 Accuracy 1.0 Time 3.109s\n",
            " correct:  tensor(33536) Total:  33536\n",
            "Epoch 3, Batch 524, Loss 15.404423488005428 Accuracy 1.0 Time 3.169s\n",
            " correct:  tensor(33600) Total:  33600\n",
            "Epoch 3, Batch 525, Loss 15.402886417933873 Accuracy 1.0 Time 3.087s\n",
            " correct:  tensor(33664) Total:  33664\n",
            "Epoch 3, Batch 526, Loss 15.404690419766386 Accuracy 1.0 Time 3.355s\n",
            " correct:  tensor(33728) Total:  33728\n",
            "Epoch 3, Batch 527, Loss 15.404518516511573 Accuracy 1.0 Time 3.384s\n",
            " correct:  tensor(33792) Total:  33792\n",
            "Epoch 3, Batch 528, Loss 15.403621834335905 Accuracy 1.0 Time 3.345s\n",
            " correct:  tensor(33856) Total:  33856\n",
            "Epoch 3, Batch 529, Loss 15.40310162362169 Accuracy 1.0 Time 3.184s\n",
            " correct:  tensor(33920) Total:  33920\n",
            "Epoch 3, Batch 530, Loss 15.403406033425961 Accuracy 1.0 Time 3.143s\n",
            " correct:  tensor(33984) Total:  33984\n",
            "Epoch 3, Batch 531, Loss 15.40304420672343 Accuracy 1.0 Time 3.157s\n",
            " correct:  tensor(34048) Total:  34048\n",
            "Epoch 3, Batch 532, Loss 15.40234603379902 Accuracy 1.0 Time 3.147s\n",
            " correct:  tensor(34112) Total:  34112\n",
            "Epoch 3, Batch 533, Loss 15.401578468408639 Accuracy 1.0 Time 3.155s\n",
            " correct:  tensor(34176) Total:  34176\n",
            "Epoch 3, Batch 534, Loss 15.401946423205544 Accuracy 1.0 Time 3.344s\n",
            " correct:  tensor(34240) Total:  34240\n",
            "Epoch 3, Batch 535, Loss 15.40287563644837 Accuracy 1.0 Time 3.209s\n",
            " correct:  tensor(34304) Total:  34304\n",
            "Epoch 3, Batch 536, Loss 15.404053065314221 Accuracy 1.0 Time 3.152s\n",
            " correct:  tensor(34368) Total:  34368\n",
            "Epoch 3, Batch 537, Loss 15.403102123537543 Accuracy 1.0 Time 3.12s\n",
            " correct:  tensor(34432) Total:  34432\n",
            "Epoch 3, Batch 538, Loss 15.403323825850363 Accuracy 1.0 Time 3.104s\n",
            " correct:  tensor(34496) Total:  34496\n",
            "Epoch 3, Batch 539, Loss 15.404444413193966 Accuracy 1.0 Time 3.104s\n",
            " correct:  tensor(34560) Total:  34560\n",
            "Epoch 3, Batch 540, Loss 15.404837689576325 Accuracy 1.0 Time 3.097s\n",
            " correct:  tensor(34624) Total:  34624\n",
            "Epoch 3, Batch 541, Loss 15.404037357478398 Accuracy 1.0 Time 3.084s\n",
            " correct:  tensor(34688) Total:  34688\n",
            "Epoch 3, Batch 542, Loss 15.402816422311142 Accuracy 1.0 Time 3.111s\n",
            " correct:  tensor(34752) Total:  34752\n",
            "Epoch 3, Batch 543, Loss 15.403425452019848 Accuracy 1.0 Time 3.154s\n",
            " correct:  tensor(34816) Total:  34816\n",
            "Epoch 3, Batch 544, Loss 15.404061654034782 Accuracy 1.0 Time 3.1s\n",
            " correct:  tensor(34880) Total:  34880\n",
            "Epoch 3, Batch 545, Loss 15.401904746589311 Accuracy 1.0 Time 3.117s\n",
            " correct:  tensor(34944) Total:  34944\n",
            "Epoch 3, Batch 546, Loss 15.402895468058604 Accuracy 1.0 Time 3.149s\n",
            " correct:  tensor(35008) Total:  35008\n",
            "Epoch 3, Batch 547, Loss 15.4038220410809 Accuracy 1.0 Time 3.16s\n",
            " correct:  tensor(35072) Total:  35072\n",
            "Epoch 3, Batch 548, Loss 15.404177199314981 Accuracy 1.0 Time 3.11s\n",
            " correct:  tensor(35136) Total:  35136\n",
            "Epoch 3, Batch 549, Loss 15.403404706597112 Accuracy 1.0 Time 3.098s\n",
            " correct:  tensor(35200) Total:  35200\n",
            "Epoch 3, Batch 550, Loss 15.403341816988858 Accuracy 1.0 Time 3.112s\n",
            " correct:  tensor(35264) Total:  35264\n",
            "Epoch 3, Batch 551, Loss 15.40406702391249 Accuracy 1.0 Time 3.134s\n",
            " correct:  tensor(35328) Total:  35328\n",
            "Epoch 3, Batch 552, Loss 15.404341016990552 Accuracy 1.0 Time 3.073s\n",
            " correct:  tensor(35392) Total:  35392\n",
            "Epoch 3, Batch 553, Loss 15.403407853938788 Accuracy 1.0 Time 3.173s\n",
            " correct:  tensor(35456) Total:  35456\n",
            "Epoch 3, Batch 554, Loss 15.404089674622574 Accuracy 1.0 Time 3.195s\n",
            " correct:  tensor(35520) Total:  35520\n",
            "Epoch 3, Batch 555, Loss 15.406177020717312 Accuracy 1.0 Time 3.165s\n",
            " correct:  tensor(35584) Total:  35584\n",
            "Epoch 3, Batch 556, Loss 15.406317498186509 Accuracy 1.0 Time 3.08s\n",
            " correct:  tensor(35648) Total:  35648\n",
            "Epoch 3, Batch 557, Loss 15.40508670567182 Accuracy 1.0 Time 3.117s\n",
            " correct:  tensor(35712) Total:  35712\n",
            "Epoch 3, Batch 558, Loss 15.405688384955075 Accuracy 1.0 Time 3.13s\n",
            " correct:  tensor(35776) Total:  35776\n",
            "Epoch 3, Batch 559, Loss 15.404561699609637 Accuracy 1.0 Time 3.086s\n",
            " correct:  tensor(35840) Total:  35840\n",
            "Epoch 3, Batch 560, Loss 15.405053802898951 Accuracy 1.0 Time 3.121s\n",
            " correct:  tensor(35904) Total:  35904\n",
            "Epoch 3, Batch 561, Loss 15.404850879879984 Accuracy 1.0 Time 3.119s\n",
            " correct:  tensor(35968) Total:  35968\n",
            "Epoch 3, Batch 562, Loss 15.404623749417341 Accuracy 1.0 Time 3.156s\n",
            " correct:  tensor(36032) Total:  36032\n",
            "Epoch 3, Batch 563, Loss 15.40332050730026 Accuracy 1.0 Time 3.193s\n",
            " correct:  tensor(36096) Total:  36096\n",
            "Epoch 3, Batch 564, Loss 15.403035640716553 Accuracy 1.0 Time 3.137s\n",
            " correct:  tensor(36160) Total:  36160\n",
            "Epoch 3, Batch 565, Loss 15.402370793840527 Accuracy 1.0 Time 3.134s\n",
            " correct:  tensor(36224) Total:  36224\n",
            "Epoch 3, Batch 566, Loss 15.402970800972659 Accuracy 1.0 Time 3.146s\n",
            " correct:  tensor(36288) Total:  36288\n",
            "Epoch 3, Batch 567, Loss 15.401602526400666 Accuracy 1.0 Time 3.196s\n",
            " correct:  tensor(36352) Total:  36352\n",
            "Epoch 3, Batch 568, Loss 15.400830628166736 Accuracy 1.0 Time 3.17s\n",
            " correct:  tensor(36416) Total:  36416\n",
            "Epoch 3, Batch 569, Loss 15.400955880672944 Accuracy 1.0 Time 3.16s\n",
            " correct:  tensor(36480) Total:  36480\n",
            "Epoch 3, Batch 570, Loss 15.400150680541993 Accuracy 1.0 Time 3.138s\n",
            " correct:  tensor(36544) Total:  36544\n",
            "Epoch 3, Batch 571, Loss 15.39915382632441 Accuracy 1.0 Time 3.159s\n",
            " correct:  tensor(36608) Total:  36608\n",
            "Epoch 3, Batch 572, Loss 15.39930664576017 Accuracy 1.0 Time 3.093s\n",
            " correct:  tensor(36672) Total:  36672\n",
            "Epoch 3, Batch 573, Loss 15.397785230040759 Accuracy 1.0 Time 3.122s\n",
            " correct:  tensor(36736) Total:  36736\n",
            "Epoch 3, Batch 574, Loss 15.39630720092029 Accuracy 1.0 Time 3.17s\n",
            " correct:  tensor(36800) Total:  36800\n",
            "Epoch 3, Batch 575, Loss 15.397275029058042 Accuracy 1.0 Time 3.116s\n",
            " correct:  tensor(36864) Total:  36864\n",
            "Epoch 3, Batch 576, Loss 15.39851486351755 Accuracy 1.0 Time 3.106s\n",
            " correct:  tensor(36928) Total:  36928\n",
            "Epoch 3, Batch 577, Loss 15.39823894236282 Accuracy 1.0 Time 3.149s\n",
            " correct:  tensor(36992) Total:  36992\n",
            "Epoch 3, Batch 578, Loss 15.39923013535338 Accuracy 1.0 Time 3.125s\n",
            " correct:  tensor(37056) Total:  37056\n",
            "Epoch 3, Batch 579, Loss 15.400761237004467 Accuracy 1.0 Time 3.098s\n",
            " correct:  tensor(37120) Total:  37120\n",
            "Epoch 3, Batch 580, Loss 15.40095290808842 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(37184) Total:  37184\n",
            "Epoch 3, Batch 581, Loss 15.402379270675056 Accuracy 1.0 Time 3.129s\n",
            " correct:  tensor(37248) Total:  37248\n",
            "Epoch 3, Batch 582, Loss 15.400947869028832 Accuracy 1.0 Time 3.146s\n",
            " correct:  tensor(37312) Total:  37312\n",
            "Epoch 3, Batch 583, Loss 15.40134995102269 Accuracy 1.0 Time 3.121s\n",
            " correct:  tensor(37376) Total:  37376\n",
            "Epoch 3, Batch 584, Loss 15.400985614894187 Accuracy 1.0 Time 3.135s\n",
            " correct:  tensor(37440) Total:  37440\n",
            "Epoch 3, Batch 585, Loss 15.399877760145399 Accuracy 1.0 Time 3.195s\n",
            " correct:  tensor(37504) Total:  37504\n",
            "Epoch 3, Batch 586, Loss 15.398893489772549 Accuracy 1.0 Time 3.192s\n",
            " correct:  tensor(37568) Total:  37568\n",
            "Epoch 3, Batch 587, Loss 15.399723155331976 Accuracy 1.0 Time 3.219s\n",
            " correct:  tensor(37632) Total:  37632\n",
            "Epoch 3, Batch 588, Loss 15.398978127914221 Accuracy 1.0 Time 3.103s\n",
            " correct:  tensor(37696) Total:  37696\n",
            "Epoch 3, Batch 589, Loss 15.398512815983874 Accuracy 1.0 Time 3.079s\n",
            " correct:  tensor(37760) Total:  37760\n",
            "Epoch 3, Batch 590, Loss 15.398946107444116 Accuracy 1.0 Time 3.161s\n",
            " correct:  tensor(37824) Total:  37824\n",
            "Epoch 3, Batch 591, Loss 15.398630621469566 Accuracy 1.0 Time 3.147s\n",
            " correct:  tensor(37888) Total:  37888\n",
            "Epoch 3, Batch 592, Loss 15.398055276355228 Accuracy 1.0 Time 3.103s\n",
            " correct:  tensor(37952) Total:  37952\n",
            "Epoch 3, Batch 593, Loss 15.396167943288665 Accuracy 1.0 Time 3.128s\n",
            " correct:  tensor(38016) Total:  38016\n",
            "Epoch 3, Batch 594, Loss 15.394391348867705 Accuracy 1.0 Time 3.042s\n",
            " correct:  tensor(38080) Total:  38080\n",
            "Epoch 3, Batch 595, Loss 15.392258208138601 Accuracy 1.0 Time 3.157s\n",
            " correct:  tensor(38144) Total:  38144\n",
            "Epoch 3, Batch 596, Loss 15.392719243196833 Accuracy 1.0 Time 3.132s\n",
            " correct:  tensor(38208) Total:  38208\n",
            "Epoch 3, Batch 597, Loss 15.392693650582727 Accuracy 1.0 Time 3.143s\n",
            " correct:  tensor(38272) Total:  38272\n",
            "Epoch 3, Batch 598, Loss 15.394819264427873 Accuracy 1.0 Time 3.136s\n",
            " correct:  tensor(38336) Total:  38336\n",
            "Epoch 3, Batch 599, Loss 15.394793792240607 Accuracy 1.0 Time 3.153s\n",
            " correct:  tensor(38400) Total:  38400\n",
            "Epoch 3, Batch 600, Loss 15.39412654876709 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(38464) Total:  38464\n",
            "Epoch 3, Batch 601, Loss 15.393280919498691 Accuracy 1.0 Time 3.165s\n",
            " correct:  tensor(38528) Total:  38528\n",
            "Epoch 3, Batch 602, Loss 15.39431865191539 Accuracy 1.0 Time 3.1s\n",
            " correct:  tensor(38592) Total:  38592\n",
            "Epoch 3, Batch 603, Loss 15.39420606445515 Accuracy 1.0 Time 3.102s\n",
            " correct:  tensor(38656) Total:  38656\n",
            "Epoch 3, Batch 604, Loss 15.39420303129992 Accuracy 1.0 Time 3.108s\n",
            " correct:  tensor(38720) Total:  38720\n",
            "Epoch 3, Batch 605, Loss 15.39401761047111 Accuracy 1.0 Time 3.199s\n",
            " correct:  tensor(38784) Total:  38784\n",
            "Epoch 3, Batch 606, Loss 15.39330243277471 Accuracy 1.0 Time 3.147s\n",
            " correct:  tensor(38848) Total:  38848\n",
            "Epoch 3, Batch 607, Loss 15.39290367201879 Accuracy 1.0 Time 3.159s\n",
            " correct:  tensor(38912) Total:  38912\n",
            "Epoch 3, Batch 608, Loss 15.39220149579801 Accuracy 1.0 Time 3.122s\n",
            " correct:  tensor(38976) Total:  38976\n",
            "Epoch 3, Batch 609, Loss 15.391874637509801 Accuracy 1.0 Time 3.171s\n",
            " correct:  tensor(39040) Total:  39040\n",
            "Epoch 3, Batch 610, Loss 15.391662078607277 Accuracy 1.0 Time 3.199s\n",
            " correct:  tensor(39104) Total:  39104\n",
            "Epoch 3, Batch 611, Loss 15.391167682828764 Accuracy 1.0 Time 3.144s\n",
            " correct:  tensor(39168) Total:  39168\n",
            "Epoch 3, Batch 612, Loss 15.38940883617775 Accuracy 1.0 Time 3.159s\n",
            " correct:  tensor(39232) Total:  39232\n",
            "Epoch 3, Batch 613, Loss 15.388774636908028 Accuracy 1.0 Time 3.112s\n",
            " correct:  tensor(39296) Total:  39296\n",
            "Epoch 3, Batch 614, Loss 15.390248085854497 Accuracy 1.0 Time 3.161s\n",
            " correct:  tensor(39360) Total:  39360\n",
            "Epoch 3, Batch 615, Loss 15.387751275349439 Accuracy 1.0 Time 3.184s\n",
            " correct:  tensor(39424) Total:  39424\n",
            "Epoch 3, Batch 616, Loss 15.386548997519853 Accuracy 1.0 Time 3.151s\n",
            " correct:  tensor(39488) Total:  39488\n",
            "Epoch 3, Batch 617, Loss 15.385694867015077 Accuracy 1.0 Time 3.137s\n",
            " correct:  tensor(39552) Total:  39552\n",
            "Epoch 3, Batch 618, Loss 15.386597746012667 Accuracy 1.0 Time 3.112s\n",
            " correct:  tensor(39616) Total:  39616\n",
            "Epoch 3, Batch 619, Loss 15.386974539625818 Accuracy 1.0 Time 3.14s\n",
            " correct:  tensor(39680) Total:  39680\n",
            "Epoch 3, Batch 620, Loss 15.386725010410432 Accuracy 1.0 Time 3.125s\n",
            " correct:  tensor(39744) Total:  39744\n",
            "Epoch 3, Batch 621, Loss 15.386191659887439 Accuracy 1.0 Time 3.201s\n",
            " correct:  tensor(39808) Total:  39808\n",
            "Epoch 3, Batch 622, Loss 15.385607083127429 Accuracy 1.0 Time 3.166s\n",
            " correct:  tensor(39872) Total:  39872\n",
            "Epoch 3, Batch 623, Loss 15.38340385002271 Accuracy 1.0 Time 3.087s\n",
            " correct:  tensor(39936) Total:  39936\n",
            "Epoch 3, Batch 624, Loss 15.38363184225865 Accuracy 1.0 Time 3.29s\n",
            " correct:  tensor(40000) Total:  40000\n",
            "Epoch 3, Batch 625, Loss 15.382598710632324 Accuracy 1.0 Time 3.349s\n",
            " correct:  tensor(40064) Total:  40064\n",
            "Epoch 3, Batch 626, Loss 15.38308738214901 Accuracy 1.0 Time 3.366s\n",
            " correct:  tensor(40128) Total:  40128\n",
            "Epoch 3, Batch 627, Loss 15.382222291195031 Accuracy 1.0 Time 3.212s\n",
            " correct:  tensor(40192) Total:  40192\n",
            "Epoch 3, Batch 628, Loss 15.381567309616477 Accuracy 1.0 Time 3.125s\n",
            " correct:  tensor(40256) Total:  40256\n",
            "Epoch 3, Batch 629, Loss 15.382274714107544 Accuracy 1.0 Time 3.124s\n",
            " correct:  tensor(40320) Total:  40320\n",
            "Epoch 3, Batch 630, Loss 15.382690706707182 Accuracy 1.0 Time 3.206s\n",
            " correct:  tensor(40384) Total:  40384\n",
            "Epoch 3, Batch 631, Loss 15.383132178885434 Accuracy 1.0 Time 3.35s\n",
            " correct:  tensor(40448) Total:  40448\n",
            "Epoch 3, Batch 632, Loss 15.381753233414662 Accuracy 1.0 Time 3.15s\n",
            " correct:  tensor(40512) Total:  40512\n",
            "Epoch 3, Batch 633, Loss 15.381595723060258 Accuracy 1.0 Time 3.082s\n",
            " correct:  tensor(40576) Total:  40576\n",
            "Epoch 3, Batch 634, Loss 15.380654162036883 Accuracy 1.0 Time 3.086s\n",
            " correct:  tensor(40640) Total:  40640\n",
            "Epoch 3, Batch 635, Loss 15.380818973751518 Accuracy 1.0 Time 3.082s\n",
            " correct:  tensor(40704) Total:  40704\n",
            "Epoch 3, Batch 636, Loss 15.379757062444147 Accuracy 1.0 Time 3.092s\n",
            " correct:  tensor(40768) Total:  40768\n",
            "Epoch 3, Batch 637, Loss 15.377828033602967 Accuracy 1.0 Time 3.108s\n",
            " correct:  tensor(40832) Total:  40832\n",
            "Epoch 3, Batch 638, Loss 15.378165567930216 Accuracy 1.0 Time 3.061s\n",
            " correct:  tensor(40896) Total:  40896\n",
            "Epoch 3, Batch 639, Loss 15.376265989969221 Accuracy 1.0 Time 3.06s\n",
            " correct:  tensor(40960) Total:  40960\n",
            "Epoch 3, Batch 640, Loss 15.375297425687313 Accuracy 1.0 Time 3.128s\n",
            " correct:  tensor(41024) Total:  41024\n",
            "Epoch 3, Batch 641, Loss 15.375665135019098 Accuracy 1.0 Time 3.127s\n",
            " correct:  tensor(41088) Total:  41088\n",
            "Epoch 3, Batch 642, Loss 15.375204084818236 Accuracy 1.0 Time 3.137s\n",
            " correct:  tensor(41152) Total:  41152\n",
            "Epoch 3, Batch 643, Loss 15.375382294365629 Accuracy 1.0 Time 3.094s\n",
            " correct:  tensor(41216) Total:  41216\n",
            "Epoch 3, Batch 644, Loss 15.37564846595622 Accuracy 1.0 Time 3.178s\n",
            " correct:  tensor(41280) Total:  41280\n",
            "Epoch 3, Batch 645, Loss 15.374225294127944 Accuracy 1.0 Time 3.139s\n",
            " correct:  tensor(41344) Total:  41344\n",
            "Epoch 3, Batch 646, Loss 15.374200693963113 Accuracy 1.0 Time 3.168s\n",
            " correct:  tensor(41408) Total:  41408\n",
            "Epoch 3, Batch 647, Loss 15.375311502164815 Accuracy 1.0 Time 3.147s\n",
            " correct:  tensor(41472) Total:  41472\n",
            "Epoch 3, Batch 648, Loss 15.37627508610855 Accuracy 1.0 Time 3.233s\n",
            " correct:  tensor(41536) Total:  41536\n",
            "Epoch 3, Batch 649, Loss 15.375294129910932 Accuracy 1.0 Time 3.178s\n",
            " correct:  tensor(41600) Total:  41600\n",
            "Epoch 3, Batch 650, Loss 15.374593997368446 Accuracy 1.0 Time 3.137s\n",
            " correct:  tensor(41664) Total:  41664\n",
            "Epoch 3, Batch 651, Loss 15.373141668176139 Accuracy 1.0 Time 3.148s\n",
            " correct:  tensor(41728) Total:  41728\n",
            "Epoch 3, Batch 652, Loss 15.371929003417126 Accuracy 1.0 Time 3.17s\n",
            " correct:  tensor(41792) Total:  41792\n",
            "Epoch 3, Batch 653, Loss 15.372199038086405 Accuracy 1.0 Time 3.208s\n",
            " correct:  tensor(41856) Total:  41856\n",
            "Epoch 3, Batch 654, Loss 15.373208184490146 Accuracy 1.0 Time 3.106s\n",
            " correct:  tensor(41920) Total:  41920\n",
            "Epoch 3, Batch 655, Loss 15.372109114668751 Accuracy 1.0 Time 3.134s\n",
            " correct:  tensor(41984) Total:  41984\n",
            "Epoch 3, Batch 656, Loss 15.37224395536795 Accuracy 1.0 Time 3.076s\n",
            " correct:  tensor(42048) Total:  42048\n",
            "Epoch 3, Batch 657, Loss 15.371885952884204 Accuracy 1.0 Time 3.179s\n",
            " correct:  tensor(42112) Total:  42112\n",
            "Epoch 3, Batch 658, Loss 15.370627171362788 Accuracy 1.0 Time 3.099s\n",
            " correct:  tensor(42176) Total:  42176\n",
            "Epoch 3, Batch 659, Loss 15.371871004697947 Accuracy 1.0 Time 3.17s\n",
            " correct:  tensor(42240) Total:  42240\n",
            "Epoch 3, Batch 660, Loss 15.373209227937641 Accuracy 1.0 Time 3.168s\n",
            " correct:  tensor(42304) Total:  42304\n",
            "Epoch 3, Batch 661, Loss 15.372715984638809 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(42368) Total:  42368\n",
            "Epoch 3, Batch 662, Loss 15.372619716782584 Accuracy 1.0 Time 3.09s\n",
            " correct:  tensor(42432) Total:  42432\n",
            "Epoch 3, Batch 663, Loss 15.374052523847439 Accuracy 1.0 Time 3.155s\n",
            " correct:  tensor(42496) Total:  42496\n",
            "Epoch 3, Batch 664, Loss 15.37414037319551 Accuracy 1.0 Time 3.169s\n",
            " correct:  tensor(42560) Total:  42560\n",
            "Epoch 3, Batch 665, Loss 15.37482823680218 Accuracy 1.0 Time 3.092s\n",
            " correct:  tensor(42624) Total:  42624\n",
            "Epoch 3, Batch 666, Loss 15.373600067677083 Accuracy 1.0 Time 3.132s\n",
            " correct:  tensor(42688) Total:  42688\n",
            "Epoch 3, Batch 667, Loss 15.372807271119537 Accuracy 1.0 Time 3.176s\n",
            " correct:  tensor(42752) Total:  42752\n",
            "Epoch 3, Batch 668, Loss 15.372312420142626 Accuracy 1.0 Time 3.122s\n",
            " correct:  tensor(42816) Total:  42816\n",
            "Epoch 3, Batch 669, Loss 15.372211844040852 Accuracy 1.0 Time 3.187s\n",
            " correct:  tensor(42880) Total:  42880\n",
            "Epoch 3, Batch 670, Loss 15.372721766002142 Accuracy 1.0 Time 3.136s\n",
            " correct:  tensor(42944) Total:  42944\n",
            "Epoch 3, Batch 671, Loss 15.372807578074061 Accuracy 1.0 Time 3.148s\n",
            " correct:  tensor(43008) Total:  43008\n",
            "Epoch 3, Batch 672, Loss 15.373721004951568 Accuracy 1.0 Time 3.18s\n",
            " correct:  tensor(43072) Total:  43072\n",
            "Epoch 3, Batch 673, Loss 15.373321679159224 Accuracy 1.0 Time 3.198s\n",
            " correct:  tensor(43136) Total:  43136\n",
            "Epoch 3, Batch 674, Loss 15.372841835021973 Accuracy 1.0 Time 3.17s\n",
            " correct:  tensor(43200) Total:  43200\n",
            "Epoch 3, Batch 675, Loss 15.373646130032009 Accuracy 1.0 Time 3.152s\n",
            " correct:  tensor(43264) Total:  43264\n",
            "Epoch 3, Batch 676, Loss 15.373681351983336 Accuracy 1.0 Time 3.135s\n",
            " correct:  tensor(43328) Total:  43328\n",
            "Epoch 3, Batch 677, Loss 15.373529366587606 Accuracy 1.0 Time 3.188s\n",
            " correct:  tensor(43392) Total:  43392\n",
            "Epoch 3, Batch 678, Loss 15.373883390848615 Accuracy 1.0 Time 3.166s\n",
            " correct:  tensor(43456) Total:  43456\n",
            "Epoch 3, Batch 679, Loss 15.372763885138488 Accuracy 1.0 Time 3.151s\n",
            " correct:  tensor(43520) Total:  43520\n",
            "Epoch 3, Batch 680, Loss 15.373256492614747 Accuracy 1.0 Time 3.173s\n",
            " correct:  tensor(43584) Total:  43584\n",
            "Epoch 3, Batch 681, Loss 15.373032585289685 Accuracy 1.0 Time 3.124s\n",
            " correct:  tensor(43648) Total:  43648\n",
            "Epoch 3, Batch 682, Loss 15.371855749762304 Accuracy 1.0 Time 3.203s\n",
            " correct:  tensor(43712) Total:  43712\n",
            "Epoch 3, Batch 683, Loss 15.371987168296714 Accuracy 1.0 Time 3.15s\n",
            " correct:  tensor(43776) Total:  43776\n",
            "Epoch 3, Batch 684, Loss 15.371816886098761 Accuracy 1.0 Time 3.134s\n",
            " correct:  tensor(43840) Total:  43840\n",
            "Epoch 3, Batch 685, Loss 15.372033666346194 Accuracy 1.0 Time 3.171s\n",
            " correct:  tensor(43904) Total:  43904\n",
            "Epoch 3, Batch 686, Loss 15.36996752180094 Accuracy 1.0 Time 3.172s\n",
            " correct:  tensor(43968) Total:  43968\n",
            "Epoch 3, Batch 687, Loss 15.370207347897637 Accuracy 1.0 Time 3.099s\n",
            " correct:  tensor(44032) Total:  44032\n",
            "Epoch 3, Batch 688, Loss 15.370178323845531 Accuracy 1.0 Time 3.126s\n",
            " correct:  tensor(44096) Total:  44096\n",
            "Epoch 3, Batch 689, Loss 15.369652013128139 Accuracy 1.0 Time 3.17s\n",
            " correct:  tensor(44160) Total:  44160\n",
            "Epoch 3, Batch 690, Loss 15.368274210501408 Accuracy 1.0 Time 3.085s\n",
            " correct:  tensor(44224) Total:  44224\n",
            "Epoch 3, Batch 691, Loss 15.367317663432893 Accuracy 1.0 Time 3.092s\n",
            " correct:  tensor(44288) Total:  44288\n",
            "Epoch 3, Batch 692, Loss 15.36703230604271 Accuracy 1.0 Time 3.104s\n",
            " correct:  tensor(44352) Total:  44352\n",
            "Epoch 3, Batch 693, Loss 15.366904312398011 Accuracy 1.0 Time 3.122s\n",
            " correct:  tensor(44416) Total:  44416\n",
            "Epoch 3, Batch 694, Loss 15.366086019906255 Accuracy 1.0 Time 3.174s\n",
            " correct:  tensor(44480) Total:  44480\n",
            "Epoch 3, Batch 695, Loss 15.36504177669827 Accuracy 1.0 Time 3.137s\n",
            " correct:  tensor(44544) Total:  44544\n",
            "Epoch 3, Batch 696, Loss 15.364867481692084 Accuracy 1.0 Time 3.157s\n",
            " correct:  tensor(44608) Total:  44608\n",
            "Epoch 3, Batch 697, Loss 15.363686133320396 Accuracy 1.0 Time 3.174s\n",
            " correct:  tensor(44672) Total:  44672\n",
            "Epoch 3, Batch 698, Loss 15.364106383228028 Accuracy 1.0 Time 3.137s\n",
            " correct:  tensor(44736) Total:  44736\n",
            "Epoch 3, Batch 699, Loss 15.364346028056438 Accuracy 1.0 Time 3.159s\n",
            " correct:  tensor(44800) Total:  44800\n",
            "Epoch 3, Batch 700, Loss 15.364245963777815 Accuracy 1.0 Time 3.124s\n",
            " correct:  tensor(44864) Total:  44864\n",
            "Epoch 3, Batch 701, Loss 15.364571199947688 Accuracy 1.0 Time 3.128s\n",
            " correct:  tensor(44928) Total:  44928\n",
            "Epoch 3, Batch 702, Loss 15.36438380347358 Accuracy 1.0 Time 3.151s\n",
            " correct:  tensor(44992) Total:  44992\n",
            "Epoch 3, Batch 703, Loss 15.363274807611196 Accuracy 1.0 Time 3.124s\n",
            " correct:  tensor(45056) Total:  45056\n",
            "Epoch 3, Batch 704, Loss 15.363032179799946 Accuracy 1.0 Time 3.094s\n",
            " correct:  tensor(45120) Total:  45120\n",
            "Epoch 3, Batch 705, Loss 15.362062546209241 Accuracy 1.0 Time 3.16s\n",
            " correct:  tensor(45184) Total:  45184\n",
            "Epoch 3, Batch 706, Loss 15.360275441458812 Accuracy 1.0 Time 3.197s\n",
            " correct:  tensor(45248) Total:  45248\n",
            "Epoch 3, Batch 707, Loss 15.359377881256492 Accuracy 1.0 Time 3.176s\n",
            " correct:  tensor(45312) Total:  45312\n",
            "Epoch 3, Batch 708, Loss 15.359651667923577 Accuracy 1.0 Time 3.182s\n",
            " correct:  tensor(45376) Total:  45376\n",
            "Epoch 3, Batch 709, Loss 15.358965050518428 Accuracy 1.0 Time 3.103s\n",
            " correct:  tensor(45440) Total:  45440\n",
            "Epoch 3, Batch 710, Loss 15.359503171141718 Accuracy 1.0 Time 3.156s\n",
            " correct:  tensor(45504) Total:  45504\n",
            "Epoch 3, Batch 711, Loss 15.359944717793525 Accuracy 1.0 Time 3.163s\n",
            " correct:  tensor(45568) Total:  45568\n",
            "Epoch 3, Batch 712, Loss 15.35879933834076 Accuracy 1.0 Time 3.138s\n",
            " correct:  tensor(45632) Total:  45632\n",
            "Epoch 3, Batch 713, Loss 15.359982651045748 Accuracy 1.0 Time 3.155s\n",
            " correct:  tensor(45696) Total:  45696\n",
            "Epoch 3, Batch 714, Loss 15.360665838257605 Accuracy 1.0 Time 3.15s\n",
            " correct:  tensor(45760) Total:  45760\n",
            "Epoch 3, Batch 715, Loss 15.361577684895975 Accuracy 1.0 Time 3.171s\n",
            " correct:  tensor(45824) Total:  45824\n",
            "Epoch 3, Batch 716, Loss 15.36131604423736 Accuracy 1.0 Time 3.161s\n",
            " correct:  tensor(45888) Total:  45888\n",
            "Epoch 3, Batch 717, Loss 15.362193277856463 Accuracy 1.0 Time 3.171s\n",
            " correct:  tensor(45952) Total:  45952\n",
            "Epoch 3, Batch 718, Loss 15.362811953244435 Accuracy 1.0 Time 3.176s\n",
            " correct:  tensor(46016) Total:  46016\n",
            "Epoch 3, Batch 719, Loss 15.362814921828736 Accuracy 1.0 Time 3.135s\n",
            " correct:  tensor(46080) Total:  46080\n",
            "Epoch 3, Batch 720, Loss 15.362688288423751 Accuracy 1.0 Time 3.145s\n",
            " correct:  tensor(46144) Total:  46144\n",
            "Epoch 3, Batch 721, Loss 15.362946519573915 Accuracy 1.0 Time 3.155s\n",
            " correct:  tensor(46208) Total:  46208\n",
            "Epoch 3, Batch 722, Loss 15.362840318283546 Accuracy 1.0 Time 3.194s\n",
            " correct:  tensor(46272) Total:  46272\n",
            "Epoch 3, Batch 723, Loss 15.361679516392625 Accuracy 1.0 Time 3.359s\n",
            " correct:  tensor(46336) Total:  46336\n",
            "Epoch 3, Batch 724, Loss 15.362074437062384 Accuracy 1.0 Time 3.368s\n",
            " correct:  tensor(46400) Total:  46400\n",
            "Epoch 3, Batch 725, Loss 15.36272926593649 Accuracy 1.0 Time 3.332s\n",
            " correct:  tensor(46464) Total:  46464\n",
            "Epoch 3, Batch 726, Loss 15.362441047164035 Accuracy 1.0 Time 3.15s\n",
            " correct:  tensor(46528) Total:  46528\n",
            "Epoch 3, Batch 727, Loss 15.362615848997764 Accuracy 1.0 Time 3.325s\n",
            " correct:  tensor(46592) Total:  46592\n",
            "Epoch 3, Batch 728, Loss 15.361177409088219 Accuracy 1.0 Time 3.237s\n",
            " correct:  tensor(46656) Total:  46656\n",
            "Epoch 3, Batch 729, Loss 15.361387129837267 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(46720) Total:  46720\n",
            "Epoch 3, Batch 730, Loss 15.362049445060835 Accuracy 1.0 Time 3.18s\n",
            " correct:  tensor(46784) Total:  46784\n",
            "Epoch 3, Batch 731, Loss 15.362454540915431 Accuracy 1.0 Time 3.167s\n",
            " correct:  tensor(46848) Total:  46848\n",
            "Epoch 3, Batch 732, Loss 15.362107695126143 Accuracy 1.0 Time 3.17s\n",
            " correct:  tensor(46912) Total:  46912\n",
            "Epoch 3, Batch 733, Loss 15.362067710491353 Accuracy 1.0 Time 3.167s\n",
            " correct:  tensor(46976) Total:  46976\n",
            "Epoch 3, Batch 734, Loss 15.360978203183624 Accuracy 1.0 Time 3.175s\n",
            " correct:  tensor(47040) Total:  47040\n",
            "Epoch 3, Batch 735, Loss 15.360894388730834 Accuracy 1.0 Time 3.088s\n",
            " correct:  tensor(47104) Total:  47104\n",
            "Epoch 3, Batch 736, Loss 15.36072244203609 Accuracy 1.0 Time 3.171s\n",
            " correct:  tensor(47168) Total:  47168\n",
            "Epoch 3, Batch 737, Loss 15.360688750423602 Accuracy 1.0 Time 3.157s\n",
            " correct:  tensor(47232) Total:  47232\n",
            "Epoch 3, Batch 738, Loss 15.360880885343887 Accuracy 1.0 Time 3.142s\n",
            " correct:  tensor(47296) Total:  47296\n",
            "Epoch 3, Batch 739, Loss 15.361014314530182 Accuracy 1.0 Time 3.222s\n",
            " correct:  tensor(47360) Total:  47360\n",
            "Epoch 3, Batch 740, Loss 15.361320602571642 Accuracy 1.0 Time 3.144s\n",
            " correct:  tensor(47424) Total:  47424\n",
            "Epoch 3, Batch 741, Loss 15.36089741041464 Accuracy 1.0 Time 3.11s\n",
            " correct:  tensor(47488) Total:  47488\n",
            "Epoch 3, Batch 742, Loss 15.361752803113582 Accuracy 1.0 Time 3.178s\n",
            " correct:  tensor(47552) Total:  47552\n",
            "Epoch 3, Batch 743, Loss 15.362147812567361 Accuracy 1.0 Time 3.122s\n",
            " correct:  tensor(47616) Total:  47616\n",
            "Epoch 3, Batch 744, Loss 15.361419182951732 Accuracy 1.0 Time 3.179s\n",
            " correct:  tensor(47680) Total:  47680\n",
            "Epoch 3, Batch 745, Loss 15.361021489905031 Accuracy 1.0 Time 3.148s\n",
            " correct:  tensor(47744) Total:  47744\n",
            "Epoch 3, Batch 746, Loss 15.361210897205343 Accuracy 1.0 Time 3.178s\n",
            " correct:  tensor(47808) Total:  47808\n",
            "Epoch 3, Batch 747, Loss 15.361702130022776 Accuracy 1.0 Time 3.177s\n",
            " correct:  tensor(47872) Total:  47872\n",
            "Epoch 3, Batch 748, Loss 15.360860755736816 Accuracy 1.0 Time 3.15s\n",
            " correct:  tensor(47936) Total:  47936\n",
            "Epoch 3, Batch 749, Loss 15.359373906266706 Accuracy 1.0 Time 3.165s\n",
            " correct:  tensor(48000) Total:  48000\n",
            "Epoch 3, Batch 750, Loss 15.359247158050538 Accuracy 1.0 Time 3.11s\n",
            " correct:  tensor(48064) Total:  48064\n",
            "Epoch 3, Batch 751, Loss 15.359036835468562 Accuracy 1.0 Time 3.112s\n",
            " correct:  tensor(48128) Total:  48128\n",
            "Epoch 3, Batch 752, Loss 15.358140852857144 Accuracy 1.0 Time 3.117s\n",
            " correct:  tensor(48192) Total:  48192\n",
            "Epoch 3, Batch 753, Loss 15.357230536016335 Accuracy 1.0 Time 3.166s\n",
            " correct:  tensor(48256) Total:  48256\n",
            "Epoch 3, Batch 754, Loss 15.358902770581233 Accuracy 1.0 Time 3.16s\n",
            " correct:  tensor(48320) Total:  48320\n",
            "Epoch 3, Batch 755, Loss 15.358719899007026 Accuracy 1.0 Time 3.174s\n",
            " correct:  tensor(48384) Total:  48384\n",
            "Epoch 3, Batch 756, Loss 15.358676513036093 Accuracy 1.0 Time 3.173s\n",
            " correct:  tensor(48448) Total:  48448\n",
            "Epoch 3, Batch 757, Loss 15.357948866356631 Accuracy 1.0 Time 3.142s\n",
            " correct:  tensor(48512) Total:  48512\n",
            "Epoch 3, Batch 758, Loss 15.357110373262994 Accuracy 1.0 Time 3.139s\n",
            " correct:  tensor(48576) Total:  48576\n",
            "Epoch 3, Batch 759, Loss 15.356101135334322 Accuracy 1.0 Time 3.189s\n",
            " correct:  tensor(48640) Total:  48640\n",
            "Epoch 3, Batch 760, Loss 15.355191471702174 Accuracy 1.0 Time 3.079s\n",
            " correct:  tensor(48704) Total:  48704\n",
            "Epoch 3, Batch 761, Loss 15.355156062623674 Accuracy 1.0 Time 3.071s\n",
            " correct:  tensor(48768) Total:  48768\n",
            "Epoch 3, Batch 762, Loss 15.358096678425946 Accuracy 1.0 Time 3.159s\n",
            " correct:  tensor(48832) Total:  48832\n",
            "Epoch 3, Batch 763, Loss 15.356941884342058 Accuracy 1.0 Time 3.163s\n",
            " correct:  tensor(48896) Total:  48896\n",
            "Epoch 3, Batch 764, Loss 15.3589376916436 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(48960) Total:  48960\n",
            "Epoch 3, Batch 765, Loss 15.359746971629024 Accuracy 1.0 Time 3.162s\n",
            " correct:  tensor(49024) Total:  49024\n",
            "Epoch 3, Batch 766, Loss 15.35873639427962 Accuracy 1.0 Time 3.188s\n",
            " correct:  tensor(49088) Total:  49088\n",
            "Epoch 3, Batch 767, Loss 15.359312955217225 Accuracy 1.0 Time 3.167s\n",
            " correct:  tensor(49152) Total:  49152\n",
            "Epoch 3, Batch 768, Loss 15.358021457990011 Accuracy 1.0 Time 3.207s\n",
            " correct:  tensor(49216) Total:  49216\n",
            "Epoch 3, Batch 769, Loss 15.357745187955343 Accuracy 1.0 Time 3.165s\n",
            " correct:  tensor(49280) Total:  49280\n",
            "Epoch 3, Batch 770, Loss 15.357739021251728 Accuracy 1.0 Time 3.174s\n",
            " correct:  tensor(49344) Total:  49344\n",
            "Epoch 3, Batch 771, Loss 15.357781739241108 Accuracy 1.0 Time 3.134s\n",
            " correct:  tensor(49408) Total:  49408\n",
            "Epoch 3, Batch 772, Loss 15.357968304441384 Accuracy 1.0 Time 3.146s\n",
            " correct:  tensor(49472) Total:  49472\n",
            "Epoch 3, Batch 773, Loss 15.358215802082887 Accuracy 1.0 Time 3.104s\n",
            " correct:  tensor(49536) Total:  49536\n",
            "Epoch 3, Batch 774, Loss 15.359137684183835 Accuracy 1.0 Time 3.2s\n",
            " correct:  tensor(49600) Total:  49600\n",
            "Epoch 3, Batch 775, Loss 15.358871651926348 Accuracy 1.0 Time 3.106s\n",
            " correct:  tensor(49664) Total:  49664\n",
            "Epoch 3, Batch 776, Loss 15.359074540973939 Accuracy 1.0 Time 3.239s\n",
            " correct:  tensor(49728) Total:  49728\n",
            "Epoch 3, Batch 777, Loss 15.36008169844344 Accuracy 1.0 Time 3.136s\n",
            " correct:  tensor(49792) Total:  49792\n",
            "Epoch 3, Batch 778, Loss 15.358881874378666 Accuracy 1.0 Time 3.126s\n",
            " correct:  tensor(49856) Total:  49856\n",
            "Epoch 3, Batch 779, Loss 15.358064029580975 Accuracy 1.0 Time 3.14s\n",
            " correct:  tensor(49920) Total:  49920\n",
            "Epoch 3, Batch 780, Loss 15.357829164847349 Accuracy 1.0 Time 3.107s\n",
            " correct:  tensor(49984) Total:  49984\n",
            "Epoch 3, Batch 781, Loss 15.3571154732405 Accuracy 1.0 Time 3.174s\n",
            " correct:  tensor(50048) Total:  50048\n",
            "Epoch 3, Batch 782, Loss 15.356656275746767 Accuracy 1.0 Time 3.175s\n",
            " correct:  tensor(50112) Total:  50112\n",
            "Epoch 3, Batch 783, Loss 15.356659920888537 Accuracy 1.0 Time 3.132s\n",
            " correct:  tensor(50176) Total:  50176\n",
            "Epoch 3, Batch 784, Loss 15.355331045024249 Accuracy 1.0 Time 3.167s\n",
            " correct:  tensor(50240) Total:  50240\n",
            "Epoch 3, Batch 785, Loss 15.355558441550869 Accuracy 1.0 Time 3.161s\n",
            " correct:  tensor(50304) Total:  50304\n",
            "Epoch 3, Batch 786, Loss 15.355304464432423 Accuracy 1.0 Time 3.095s\n",
            " correct:  tensor(50368) Total:  50368\n",
            "Epoch 3, Batch 787, Loss 15.356015486517279 Accuracy 1.0 Time 3.06s\n",
            " correct:  tensor(50432) Total:  50432\n",
            "Epoch 3, Batch 788, Loss 15.354951507548995 Accuracy 1.0 Time 3.117s\n",
            " correct:  tensor(50496) Total:  50496\n",
            "Epoch 3, Batch 789, Loss 15.354631978749323 Accuracy 1.0 Time 3.121s\n",
            " correct:  tensor(50560) Total:  50560\n",
            "Epoch 3, Batch 790, Loss 15.354185712790187 Accuracy 1.0 Time 3.157s\n",
            " correct:  tensor(50624) Total:  50624\n",
            "Epoch 3, Batch 791, Loss 15.354628237399982 Accuracy 1.0 Time 3.168s\n",
            " correct:  tensor(50688) Total:  50688\n",
            "Epoch 3, Batch 792, Loss 15.353905262369098 Accuracy 1.0 Time 3.144s\n",
            " correct:  tensor(50752) Total:  50752\n",
            "Epoch 3, Batch 793, Loss 15.355440037548167 Accuracy 1.0 Time 3.145s\n",
            " correct:  tensor(50816) Total:  50816\n",
            "Epoch 3, Batch 794, Loss 15.35640578786432 Accuracy 1.0 Time 3.149s\n",
            " correct:  tensor(50880) Total:  50880\n",
            "Epoch 3, Batch 795, Loss 15.356358119076903 Accuracy 1.0 Time 3.244s\n",
            " correct:  tensor(50944) Total:  50944\n",
            "Epoch 3, Batch 796, Loss 15.357417844647738 Accuracy 1.0 Time 3.13s\n",
            " correct:  tensor(51008) Total:  51008\n",
            "Epoch 3, Batch 797, Loss 15.355981430714223 Accuracy 1.0 Time 3.168s\n",
            " correct:  tensor(51072) Total:  51072\n",
            "Epoch 3, Batch 798, Loss 15.355542901165801 Accuracy 1.0 Time 3.149s\n",
            " correct:  tensor(51136) Total:  51136\n",
            "Epoch 3, Batch 799, Loss 15.354804220426367 Accuracy 1.0 Time 3.158s\n",
            " correct:  tensor(51200) Total:  51200\n",
            "Epoch 3, Batch 800, Loss 15.354002394676208 Accuracy 1.0 Time 3.176s\n",
            " correct:  tensor(51264) Total:  51264\n",
            "Epoch 3, Batch 801, Loss 15.35322163256813 Accuracy 1.0 Time 3.181s\n",
            " correct:  tensor(51328) Total:  51328\n",
            "Epoch 3, Batch 802, Loss 15.353102925412376 Accuracy 1.0 Time 3.147s\n",
            " correct:  tensor(51392) Total:  51392\n",
            "Epoch 3, Batch 803, Loss 15.353399770791325 Accuracy 1.0 Time 3.087s\n",
            " correct:  tensor(51456) Total:  51456\n",
            "Epoch 3, Batch 804, Loss 15.352745174768552 Accuracy 1.0 Time 3.17s\n",
            " correct:  tensor(51520) Total:  51520\n",
            "Epoch 3, Batch 805, Loss 15.352699477628152 Accuracy 1.0 Time 3.167s\n",
            " correct:  tensor(51584) Total:  51584\n",
            "Epoch 3, Batch 806, Loss 15.352847455453045 Accuracy 1.0 Time 3.173s\n",
            " correct:  tensor(51648) Total:  51648\n",
            "Epoch 3, Batch 807, Loss 15.352866017980942 Accuracy 1.0 Time 3.21s\n",
            " correct:  tensor(51712) Total:  51712\n",
            "Epoch 3, Batch 808, Loss 15.352243665421364 Accuracy 1.0 Time 3.131s\n",
            " correct:  tensor(51776) Total:  51776\n",
            "Epoch 3, Batch 809, Loss 15.352436899107378 Accuracy 1.0 Time 3.093s\n",
            " correct:  tensor(51840) Total:  51840\n",
            "Epoch 3, Batch 810, Loss 15.352059598616611 Accuracy 1.0 Time 3.157s\n",
            " correct:  tensor(51904) Total:  51904\n",
            "Epoch 3, Batch 811, Loss 15.352061896083093 Accuracy 1.0 Time 3.162s\n",
            " correct:  tensor(51968) Total:  51968\n",
            "Epoch 3, Batch 812, Loss 15.351811050781475 Accuracy 1.0 Time 3.17s\n",
            " correct:  tensor(52032) Total:  52032\n",
            "Epoch 3, Batch 813, Loss 15.351888350455084 Accuracy 1.0 Time 3.161s\n",
            " correct:  tensor(52096) Total:  52096\n",
            "Epoch 3, Batch 814, Loss 15.351579485712824 Accuracy 1.0 Time 3.125s\n",
            " correct:  tensor(52160) Total:  52160\n",
            "Epoch 3, Batch 815, Loss 15.352524100929681 Accuracy 1.0 Time 3.154s\n",
            " correct:  tensor(52224) Total:  52224\n",
            "Epoch 3, Batch 816, Loss 15.352085452453762 Accuracy 1.0 Time 3.131s\n",
            " correct:  tensor(52288) Total:  52288\n",
            "Epoch 3, Batch 817, Loss 15.352659952684307 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(52352) Total:  52352\n",
            "Epoch 3, Batch 818, Loss 15.352084264778567 Accuracy 1.0 Time 3.157s\n",
            " correct:  tensor(52416) Total:  52416\n",
            "Epoch 3, Batch 819, Loss 15.352126679286561 Accuracy 1.0 Time 3.187s\n",
            " correct:  tensor(52480) Total:  52480\n",
            "Epoch 3, Batch 820, Loss 15.35326835818407 Accuracy 1.0 Time 3.222s\n",
            " correct:  tensor(52544) Total:  52544\n",
            "Epoch 3, Batch 821, Loss 15.353663242400492 Accuracy 1.0 Time 3.326s\n",
            " correct:  tensor(52608) Total:  52608\n",
            "Epoch 3, Batch 822, Loss 15.354345995724346 Accuracy 1.0 Time 3.347s\n",
            " correct:  tensor(52672) Total:  52672\n",
            "Epoch 3, Batch 823, Loss 15.355484047028737 Accuracy 1.0 Time 3.403s\n",
            " correct:  tensor(52736) Total:  52736\n",
            "Epoch 3, Batch 824, Loss 15.354844534281389 Accuracy 1.0 Time 3.306s\n",
            " correct:  tensor(52800) Total:  52800\n",
            "Epoch 3, Batch 825, Loss 15.354770378343988 Accuracy 1.0 Time 3.131s\n",
            " correct:  tensor(52864) Total:  52864\n",
            "Epoch 3, Batch 826, Loss 15.35442916417526 Accuracy 1.0 Time 3.162s\n",
            " correct:  tensor(52928) Total:  52928\n",
            "Epoch 3, Batch 827, Loss 15.353534910670215 Accuracy 1.0 Time 3.164s\n",
            " correct:  tensor(52992) Total:  52992\n",
            "Epoch 3, Batch 828, Loss 15.353879381493094 Accuracy 1.0 Time 3.161s\n",
            " correct:  tensor(53056) Total:  53056\n",
            "Epoch 3, Batch 829, Loss 15.35457726468223 Accuracy 1.0 Time 3.099s\n",
            " correct:  tensor(53120) Total:  53120\n",
            "Epoch 3, Batch 830, Loss 15.353832190869802 Accuracy 1.0 Time 3.106s\n",
            " correct:  tensor(53184) Total:  53184\n",
            "Epoch 3, Batch 831, Loss 15.352695668217077 Accuracy 1.0 Time 3.184s\n",
            " correct:  tensor(53248) Total:  53248\n",
            "Epoch 3, Batch 832, Loss 15.353028861375956 Accuracy 1.0 Time 3.167s\n",
            " correct:  tensor(53312) Total:  53312\n",
            "Epoch 3, Batch 833, Loss 15.35264172159037 Accuracy 1.0 Time 3.115s\n",
            " correct:  tensor(53376) Total:  53376\n",
            "Epoch 3, Batch 834, Loss 15.352707946329094 Accuracy 1.0 Time 3.18s\n",
            " correct:  tensor(53440) Total:  53440\n",
            "Epoch 3, Batch 835, Loss 15.353368603826283 Accuracy 1.0 Time 3.154s\n",
            " correct:  tensor(53504) Total:  53504\n",
            "Epoch 3, Batch 836, Loss 15.354482094066565 Accuracy 1.0 Time 3.087s\n",
            " correct:  tensor(53568) Total:  53568\n",
            "Epoch 3, Batch 837, Loss 15.353926307696453 Accuracy 1.0 Time 3.176s\n",
            " correct:  tensor(53632) Total:  53632\n",
            "Epoch 3, Batch 838, Loss 15.353149645083844 Accuracy 1.0 Time 3.179s\n",
            " correct:  tensor(53696) Total:  53696\n",
            "Epoch 3, Batch 839, Loss 15.352945117473034 Accuracy 1.0 Time 3.25s\n",
            " correct:  tensor(53760) Total:  53760\n",
            "Epoch 3, Batch 840, Loss 15.352382670130048 Accuracy 1.0 Time 3.197s\n",
            " correct:  tensor(53824) Total:  53824\n",
            "Epoch 3, Batch 841, Loss 15.353302558735633 Accuracy 1.0 Time 3.12s\n",
            " correct:  tensor(53888) Total:  53888\n",
            "Epoch 3, Batch 842, Loss 15.35446403372033 Accuracy 1.0 Time 3.11s\n",
            " correct:  tensor(53952) Total:  53952\n",
            "Epoch 3, Batch 843, Loss 15.354159416253033 Accuracy 1.0 Time 3.13s\n",
            " correct:  tensor(54016) Total:  54016\n",
            "Epoch 3, Batch 844, Loss 15.355169814909804 Accuracy 1.0 Time 3.139s\n",
            " correct:  tensor(54080) Total:  54080\n",
            "Epoch 3, Batch 845, Loss 15.355167129336024 Accuracy 1.0 Time 3.135s\n",
            " correct:  tensor(54144) Total:  54144\n",
            "Epoch 3, Batch 846, Loss 15.354427970047539 Accuracy 1.0 Time 3.164s\n",
            " correct:  tensor(54208) Total:  54208\n",
            "Epoch 3, Batch 847, Loss 15.354046075093564 Accuracy 1.0 Time 3.131s\n",
            " correct:  tensor(54272) Total:  54272\n",
            "Epoch 3, Batch 848, Loss 15.352503624727142 Accuracy 1.0 Time 3.186s\n",
            " correct:  tensor(54336) Total:  54336\n",
            "Epoch 3, Batch 849, Loss 15.352326878389285 Accuracy 1.0 Time 3.102s\n",
            " correct:  tensor(54400) Total:  54400\n",
            "Epoch 3, Batch 850, Loss 15.352605144276339 Accuracy 1.0 Time 3.119s\n",
            " correct:  tensor(54464) Total:  54464\n",
            "Epoch 3, Batch 851, Loss 15.35177887536664 Accuracy 1.0 Time 3.174s\n",
            " correct:  tensor(54528) Total:  54528\n",
            "Epoch 3, Batch 852, Loss 15.351812575344749 Accuracy 1.0 Time 3.1s\n",
            " correct:  tensor(54592) Total:  54592\n",
            "Epoch 3, Batch 853, Loss 15.351366910677303 Accuracy 1.0 Time 3.152s\n",
            " correct:  tensor(54656) Total:  54656\n",
            "Epoch 3, Batch 854, Loss 15.350288417914433 Accuracy 1.0 Time 3.124s\n",
            " correct:  tensor(54720) Total:  54720\n",
            "Epoch 3, Batch 855, Loss 15.349970612330743 Accuracy 1.0 Time 3.11s\n",
            " correct:  tensor(54784) Total:  54784\n",
            "Epoch 3, Batch 856, Loss 15.350161205942385 Accuracy 1.0 Time 3.133s\n",
            " correct:  tensor(54848) Total:  54848\n",
            "Epoch 3, Batch 857, Loss 15.349667186358706 Accuracy 1.0 Time 3.132s\n",
            " correct:  tensor(54912) Total:  54912\n",
            "Epoch 3, Batch 858, Loss 15.350347764564283 Accuracy 1.0 Time 3.165s\n",
            " correct:  tensor(54976) Total:  54976\n",
            "Epoch 3, Batch 859, Loss 15.349414323622467 Accuracy 1.0 Time 3.135s\n",
            " correct:  tensor(55040) Total:  55040\n",
            "Epoch 3, Batch 860, Loss 15.349035833048266 Accuracy 1.0 Time 3.144s\n",
            " correct:  tensor(55104) Total:  55104\n",
            "Epoch 3, Batch 861, Loss 15.348839599219486 Accuracy 1.0 Time 3.09s\n",
            " correct:  tensor(55168) Total:  55168\n",
            "Epoch 3, Batch 862, Loss 15.347940607579844 Accuracy 1.0 Time 3.159s\n",
            " correct:  tensor(55232) Total:  55232\n",
            "Epoch 3, Batch 863, Loss 15.34743517547466 Accuracy 1.0 Time 3.103s\n",
            " correct:  tensor(55296) Total:  55296\n",
            "Epoch 3, Batch 864, Loss 15.346740157515914 Accuracy 1.0 Time 3.12s\n",
            " correct:  tensor(55360) Total:  55360\n",
            "Epoch 3, Batch 865, Loss 15.346396988664749 Accuracy 1.0 Time 3.154s\n",
            " correct:  tensor(55424) Total:  55424\n",
            "Epoch 3, Batch 866, Loss 15.34573032894531 Accuracy 1.0 Time 3.132s\n",
            " correct:  tensor(55488) Total:  55488\n",
            "Epoch 3, Batch 867, Loss 15.345288169975237 Accuracy 1.0 Time 3.171s\n",
            " correct:  tensor(55552) Total:  55552\n",
            "Epoch 3, Batch 868, Loss 15.34571586556149 Accuracy 1.0 Time 3.116s\n",
            " correct:  tensor(55616) Total:  55616\n",
            "Epoch 3, Batch 869, Loss 15.346921944920021 Accuracy 1.0 Time 3.162s\n",
            " correct:  tensor(55680) Total:  55680\n",
            "Epoch 3, Batch 870, Loss 15.346404769502836 Accuracy 1.0 Time 3.127s\n",
            " correct:  tensor(55744) Total:  55744\n",
            "Epoch 3, Batch 871, Loss 15.346316001445363 Accuracy 1.0 Time 3.196s\n",
            " correct:  tensor(55808) Total:  55808\n",
            "Epoch 3, Batch 872, Loss 15.34658813695295 Accuracy 1.0 Time 3.214s\n",
            " correct:  tensor(55872) Total:  55872\n",
            "Epoch 3, Batch 873, Loss 15.346431179418051 Accuracy 1.0 Time 3.144s\n",
            " correct:  tensor(55936) Total:  55936\n",
            "Epoch 3, Batch 874, Loss 15.345937533564099 Accuracy 1.0 Time 3.144s\n",
            " correct:  tensor(56000) Total:  56000\n",
            "Epoch 3, Batch 875, Loss 15.345733831133161 Accuracy 1.0 Time 3.135s\n",
            " correct:  tensor(56064) Total:  56064\n",
            "Epoch 3, Batch 876, Loss 15.345674563760626 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(56128) Total:  56128\n",
            "Epoch 3, Batch 877, Loss 15.344772870619458 Accuracy 1.0 Time 3.243s\n",
            " correct:  tensor(56192) Total:  56192\n",
            "Epoch 3, Batch 878, Loss 15.34478697092495 Accuracy 1.0 Time 3.119s\n",
            " correct:  tensor(56256) Total:  56256\n",
            "Epoch 3, Batch 879, Loss 15.344682773768021 Accuracy 1.0 Time 3.136s\n",
            " correct:  tensor(56320) Total:  56320\n",
            "Epoch 3, Batch 880, Loss 15.344508067044345 Accuracy 1.0 Time 3.149s\n",
            " correct:  tensor(56384) Total:  56384\n",
            "Epoch 3, Batch 881, Loss 15.343184845672157 Accuracy 1.0 Time 3.121s\n",
            " correct:  tensor(56448) Total:  56448\n",
            "Epoch 3, Batch 882, Loss 15.342492517700542 Accuracy 1.0 Time 3.124s\n",
            " correct:  tensor(56512) Total:  56512\n",
            "Epoch 3, Batch 883, Loss 15.344697249165208 Accuracy 1.0 Time 3.118s\n",
            " correct:  tensor(56576) Total:  56576\n",
            "Epoch 3, Batch 884, Loss 15.343944451388191 Accuracy 1.0 Time 3.151s\n",
            " correct:  tensor(56640) Total:  56640\n",
            "Epoch 3, Batch 885, Loss 15.343778122077554 Accuracy 1.0 Time 3.136s\n",
            " correct:  tensor(56704) Total:  56704\n",
            "Epoch 3, Batch 886, Loss 15.342802532101324 Accuracy 1.0 Time 3.144s\n",
            " correct:  tensor(56768) Total:  56768\n",
            "Epoch 3, Batch 887, Loss 15.34213881616259 Accuracy 1.0 Time 3.156s\n",
            " correct:  tensor(56832) Total:  56832\n",
            "Epoch 3, Batch 888, Loss 15.341322233011057 Accuracy 1.0 Time 3.14s\n",
            " correct:  tensor(56896) Total:  56896\n",
            "Epoch 3, Batch 889, Loss 15.341301750680861 Accuracy 1.0 Time 3.153s\n",
            " correct:  tensor(56960) Total:  56960\n",
            "Epoch 3, Batch 890, Loss 15.341724585415273 Accuracy 1.0 Time 3.155s\n",
            " correct:  tensor(57024) Total:  57024\n",
            "Epoch 3, Batch 891, Loss 15.341081107639438 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(57088) Total:  57088\n",
            "Epoch 3, Batch 892, Loss 15.341038903847938 Accuracy 1.0 Time 3.145s\n",
            " correct:  tensor(57152) Total:  57152\n",
            "Epoch 3, Batch 893, Loss 15.341339613261976 Accuracy 1.0 Time 3.146s\n",
            " correct:  tensor(57216) Total:  57216\n",
            "Epoch 3, Batch 894, Loss 15.340495124492602 Accuracy 1.0 Time 3.13s\n",
            " correct:  tensor(57280) Total:  57280\n",
            "Epoch 3, Batch 895, Loss 15.34070106271925 Accuracy 1.0 Time 3.118s\n",
            " correct:  tensor(57344) Total:  57344\n",
            "Epoch 3, Batch 896, Loss 15.340779483318329 Accuracy 1.0 Time 3.207s\n",
            " correct:  tensor(57408) Total:  57408\n",
            "Epoch 3, Batch 897, Loss 15.341844318436673 Accuracy 1.0 Time 3.072s\n",
            " correct:  tensor(57472) Total:  57472\n",
            "Epoch 3, Batch 898, Loss 15.340969326766933 Accuracy 1.0 Time 3.085s\n",
            " correct:  tensor(57536) Total:  57536\n",
            "Epoch 3, Batch 899, Loss 15.340645571571834 Accuracy 1.0 Time 3.097s\n",
            " correct:  tensor(57600) Total:  57600\n",
            "Epoch 3, Batch 900, Loss 15.341281983057657 Accuracy 1.0 Time 3.154s\n",
            " correct:  tensor(57664) Total:  57664\n",
            "Epoch 3, Batch 901, Loss 15.34123518834765 Accuracy 1.0 Time 3.173s\n",
            " correct:  tensor(57728) Total:  57728\n",
            "Epoch 3, Batch 902, Loss 15.340595150205354 Accuracy 1.0 Time 3.151s\n",
            " correct:  tensor(57792) Total:  57792\n",
            "Epoch 3, Batch 903, Loss 15.339901255610773 Accuracy 1.0 Time 3.151s\n",
            " correct:  tensor(57856) Total:  57856\n",
            "Epoch 3, Batch 904, Loss 15.340485231011314 Accuracy 1.0 Time 3.161s\n",
            " correct:  tensor(57920) Total:  57920\n",
            "Epoch 3, Batch 905, Loss 15.34025005741014 Accuracy 1.0 Time 3.157s\n",
            " correct:  tensor(57984) Total:  57984\n",
            "Epoch 3, Batch 906, Loss 15.339979434908095 Accuracy 1.0 Time 3.143s\n",
            " correct:  tensor(58048) Total:  58048\n",
            "Epoch 3, Batch 907, Loss 15.340196787193829 Accuracy 1.0 Time 3.153s\n",
            " correct:  tensor(58112) Total:  58112\n",
            "Epoch 3, Batch 908, Loss 15.340830556096485 Accuracy 1.0 Time 3.159s\n",
            " correct:  tensor(58176) Total:  58176\n",
            "Epoch 3, Batch 909, Loss 15.340880663612626 Accuracy 1.0 Time 3.22s\n",
            " correct:  tensor(58240) Total:  58240\n",
            "Epoch 3, Batch 910, Loss 15.341478726104066 Accuracy 1.0 Time 3.198s\n",
            " correct:  tensor(58304) Total:  58304\n",
            "Epoch 3, Batch 911, Loss 15.341419191705931 Accuracy 1.0 Time 3.144s\n",
            " correct:  tensor(58368) Total:  58368\n",
            "Epoch 3, Batch 912, Loss 15.341087392547674 Accuracy 1.0 Time 3.179s\n",
            " correct:  tensor(58432) Total:  58432\n",
            "Epoch 3, Batch 913, Loss 15.341755429337868 Accuracy 1.0 Time 3.161s\n",
            " correct:  tensor(58496) Total:  58496\n",
            "Epoch 3, Batch 914, Loss 15.3419020483739 Accuracy 1.0 Time 3.193s\n",
            " correct:  tensor(58560) Total:  58560\n",
            "Epoch 3, Batch 915, Loss 15.342440504063674 Accuracy 1.0 Time 3.238s\n",
            " correct:  tensor(58624) Total:  58624\n",
            "Epoch 3, Batch 916, Loss 15.341650594269865 Accuracy 1.0 Time 3.18s\n",
            " correct:  tensor(58688) Total:  58688\n",
            "Epoch 3, Batch 917, Loss 15.341220798513232 Accuracy 1.0 Time 3.142s\n",
            " correct:  tensor(58752) Total:  58752\n",
            "Epoch 3, Batch 918, Loss 15.341809265494087 Accuracy 1.0 Time 3.19s\n",
            " correct:  tensor(58816) Total:  58816\n",
            "Epoch 3, Batch 919, Loss 15.341551735040543 Accuracy 1.0 Time 3.389s\n",
            " correct:  tensor(58880) Total:  58880\n",
            "Epoch 3, Batch 920, Loss 15.340898348974145 Accuracy 1.0 Time 3.402s\n",
            " correct:  tensor(58944) Total:  58944\n",
            "Epoch 3, Batch 921, Loss 15.341250315033522 Accuracy 1.0 Time 3.334s\n",
            " correct:  tensor(59008) Total:  59008\n",
            "Epoch 3, Batch 922, Loss 15.34163598952221 Accuracy 1.0 Time 3.144s\n",
            " correct:  tensor(59072) Total:  59072\n",
            "Epoch 3, Batch 923, Loss 15.341739391066529 Accuracy 1.0 Time 3.168s\n",
            " correct:  tensor(59136) Total:  59136\n",
            "Epoch 3, Batch 924, Loss 15.340379030673535 Accuracy 1.0 Time 3.15s\n",
            " correct:  tensor(59200) Total:  59200\n",
            "Epoch 3, Batch 925, Loss 15.339879388551454 Accuracy 1.0 Time 3.12s\n",
            " correct:  tensor(59264) Total:  59264\n",
            "Epoch 3, Batch 926, Loss 15.339557528238316 Accuracy 1.0 Time 3.158s\n",
            " correct:  tensor(59328) Total:  59328\n",
            "Epoch 3, Batch 927, Loss 15.340141922910622 Accuracy 1.0 Time 3.164s\n",
            " correct:  tensor(59392) Total:  59392\n",
            "Epoch 3, Batch 928, Loss 15.340258108130817 Accuracy 1.0 Time 3.19s\n",
            " correct:  tensor(59456) Total:  59456\n",
            "Epoch 3, Batch 929, Loss 15.340137310253919 Accuracy 1.0 Time 3.238s\n",
            " correct:  tensor(59520) Total:  59520\n",
            "Epoch 3, Batch 930, Loss 15.338996691344887 Accuracy 1.0 Time 3.189s\n",
            " correct:  tensor(59584) Total:  59584\n",
            "Epoch 3, Batch 931, Loss 15.339152116908679 Accuracy 1.0 Time 3.206s\n",
            " correct:  tensor(59648) Total:  59648\n",
            "Epoch 3, Batch 932, Loss 15.338273593796169 Accuracy 1.0 Time 3.17s\n",
            " correct:  tensor(59712) Total:  59712\n",
            "Epoch 3, Batch 933, Loss 15.337936116184835 Accuracy 1.0 Time 3.129s\n",
            " correct:  tensor(59776) Total:  59776\n",
            "Epoch 3, Batch 934, Loss 15.33765680529576 Accuracy 1.0 Time 3.225s\n",
            " correct:  tensor(59840) Total:  59840\n",
            "Epoch 3, Batch 935, Loss 15.338218436011656 Accuracy 1.0 Time 3.175s\n",
            " correct:  tensor(59904) Total:  59904\n",
            "Epoch 3, Batch 936, Loss 15.337982193017618 Accuracy 1.0 Time 3.164s\n",
            " correct:  tensor(59968) Total:  59968\n",
            "Epoch 3, Batch 937, Loss 15.338233473461237 Accuracy 1.0 Time 3.16s\n",
            " correct:  tensor(60000) Total:  60000\n",
            "Epoch 3, Batch 938, Loss 15.329976522592085 Accuracy 1.0 Time 1.823s\n",
            "TRAIN Epoch 3, Loss 15.329976522592085 Accuracy 1.0 Time 2969.51s\n",
            "TESTING...\n",
            " correct:  tensor(64) Total:  64\n",
            " correct:  tensor(128) Total:  128\n",
            " correct:  tensor(192) Total:  192\n",
            " correct:  tensor(256) Total:  256\n",
            " correct:  tensor(320) Total:  320\n",
            " correct:  tensor(384) Total:  384\n",
            " correct:  tensor(448) Total:  448\n",
            " correct:  tensor(512) Total:  512\n",
            " correct:  tensor(576) Total:  576\n",
            " correct:  tensor(640) Total:  640\n",
            " correct:  tensor(704) Total:  704\n",
            " correct:  tensor(768) Total:  768\n",
            " correct:  tensor(832) Total:  832\n",
            " correct:  tensor(896) Total:  896\n",
            " correct:  tensor(960) Total:  960\n",
            " correct:  tensor(1024) Total:  1024\n",
            " correct:  tensor(1088) Total:  1088\n",
            " correct:  tensor(1152) Total:  1152\n",
            " correct:  tensor(1216) Total:  1216\n",
            " correct:  tensor(1280) Total:  1280\n",
            " correct:  tensor(1344) Total:  1344\n",
            " correct:  tensor(1408) Total:  1408\n",
            " correct:  tensor(1472) Total:  1472\n",
            " correct:  tensor(1536) Total:  1536\n",
            " correct:  tensor(1600) Total:  1600\n",
            " correct:  tensor(1664) Total:  1664\n",
            " correct:  tensor(1728) Total:  1728\n",
            " correct:  tensor(1792) Total:  1792\n",
            " correct:  tensor(1856) Total:  1856\n",
            " correct:  tensor(1920) Total:  1920\n",
            " correct:  tensor(1984) Total:  1984\n",
            " correct:  tensor(2048) Total:  2048\n",
            " correct:  tensor(2112) Total:  2112\n",
            " correct:  tensor(2176) Total:  2176\n",
            " correct:  tensor(2240) Total:  2240\n",
            " correct:  tensor(2304) Total:  2304\n",
            " correct:  tensor(2368) Total:  2368\n",
            " correct:  tensor(2432) Total:  2432\n",
            " correct:  tensor(2496) Total:  2496\n",
            " correct:  tensor(2560) Total:  2560\n",
            " correct:  tensor(2624) Total:  2624\n",
            " correct:  tensor(2688) Total:  2688\n",
            " correct:  tensor(2752) Total:  2752\n",
            " correct:  tensor(2816) Total:  2816\n",
            " correct:  tensor(2880) Total:  2880\n",
            " correct:  tensor(2944) Total:  2944\n",
            " correct:  tensor(3008) Total:  3008\n",
            " correct:  tensor(3072) Total:  3072\n",
            " correct:  tensor(3136) Total:  3136\n",
            " correct:  tensor(3200) Total:  3200\n",
            " correct:  tensor(3264) Total:  3264\n",
            " correct:  tensor(3328) Total:  3328\n",
            " correct:  tensor(3392) Total:  3392\n",
            " correct:  tensor(3456) Total:  3456\n",
            " correct:  tensor(3520) Total:  3520\n",
            " correct:  tensor(3584) Total:  3584\n",
            " correct:  tensor(3648) Total:  3648\n",
            " correct:  tensor(3712) Total:  3712\n",
            " correct:  tensor(3776) Total:  3776\n",
            " correct:  tensor(3840) Total:  3840\n",
            " correct:  tensor(3904) Total:  3904\n",
            " correct:  tensor(3968) Total:  3968\n",
            " correct:  tensor(4032) Total:  4032\n",
            " correct:  tensor(4096) Total:  4096\n",
            " correct:  tensor(4160) Total:  4160\n",
            " correct:  tensor(4224) Total:  4224\n",
            " correct:  tensor(4288) Total:  4288\n",
            " correct:  tensor(4352) Total:  4352\n",
            " correct:  tensor(4416) Total:  4416\n",
            " correct:  tensor(4480) Total:  4480\n",
            " correct:  tensor(4544) Total:  4544\n",
            " correct:  tensor(4608) Total:  4608\n",
            " correct:  tensor(4672) Total:  4672\n",
            " correct:  tensor(4736) Total:  4736\n",
            " correct:  tensor(4800) Total:  4800\n",
            " correct:  tensor(4864) Total:  4864\n",
            " correct:  tensor(4928) Total:  4928\n",
            " correct:  tensor(4992) Total:  4992\n",
            " correct:  tensor(5056) Total:  5056\n",
            " correct:  tensor(5120) Total:  5120\n",
            " correct:  tensor(5184) Total:  5184\n",
            " correct:  tensor(5248) Total:  5248\n",
            " correct:  tensor(5312) Total:  5312\n",
            " correct:  tensor(5376) Total:  5376\n",
            " correct:  tensor(5440) Total:  5440\n",
            " correct:  tensor(5504) Total:  5504\n",
            " correct:  tensor(5568) Total:  5568\n",
            " correct:  tensor(5632) Total:  5632\n",
            " correct:  tensor(5696) Total:  5696\n",
            " correct:  tensor(5760) Total:  5760\n",
            " correct:  tensor(5824) Total:  5824\n",
            " correct:  tensor(5888) Total:  5888\n",
            " correct:  tensor(5952) Total:  5952\n",
            " correct:  tensor(6016) Total:  6016\n",
            " correct:  tensor(6080) Total:  6080\n",
            " correct:  tensor(6144) Total:  6144\n",
            " correct:  tensor(6208) Total:  6208\n",
            " correct:  tensor(6272) Total:  6272\n",
            " correct:  tensor(6336) Total:  6336\n",
            " correct:  tensor(6400) Total:  6400\n",
            " correct:  tensor(6464) Total:  6464\n",
            " correct:  tensor(6528) Total:  6528\n",
            " correct:  tensor(6592) Total:  6592\n",
            " correct:  tensor(6656) Total:  6656\n",
            " correct:  tensor(6720) Total:  6720\n",
            " correct:  tensor(6784) Total:  6784\n",
            " correct:  tensor(6848) Total:  6848\n",
            " correct:  tensor(6912) Total:  6912\n",
            " correct:  tensor(6976) Total:  6976\n",
            " correct:  tensor(7040) Total:  7040\n",
            " correct:  tensor(7104) Total:  7104\n",
            " correct:  tensor(7168) Total:  7168\n",
            " correct:  tensor(7232) Total:  7232\n",
            " correct:  tensor(7296) Total:  7296\n",
            " correct:  tensor(7360) Total:  7360\n",
            " correct:  tensor(7424) Total:  7424\n",
            " correct:  tensor(7488) Total:  7488\n",
            " correct:  tensor(7552) Total:  7552\n",
            " correct:  tensor(7616) Total:  7616\n",
            " correct:  tensor(7680) Total:  7680\n",
            " correct:  tensor(7744) Total:  7744\n",
            " correct:  tensor(7808) Total:  7808\n",
            " correct:  tensor(7872) Total:  7872\n",
            " correct:  tensor(7936) Total:  7936\n",
            " correct:  tensor(8000) Total:  8000\n",
            " correct:  tensor(8064) Total:  8064\n",
            " correct:  tensor(8128) Total:  8128\n",
            " correct:  tensor(8192) Total:  8192\n",
            " correct:  tensor(8256) Total:  8256\n",
            " correct:  tensor(8320) Total:  8320\n",
            " correct:  tensor(8384) Total:  8384\n",
            " correct:  tensor(8448) Total:  8448\n",
            " correct:  tensor(8512) Total:  8512\n",
            " correct:  tensor(8576) Total:  8576\n",
            " correct:  tensor(8640) Total:  8640\n",
            " correct:  tensor(8704) Total:  8704\n",
            " correct:  tensor(8768) Total:  8768\n",
            " correct:  tensor(8832) Total:  8832\n",
            " correct:  tensor(8896) Total:  8896\n",
            " correct:  tensor(8960) Total:  8960\n",
            " correct:  tensor(9024) Total:  9024\n",
            " correct:  tensor(9088) Total:  9088\n",
            " correct:  tensor(9152) Total:  9152\n",
            " correct:  tensor(9216) Total:  9216\n",
            " correct:  tensor(9280) Total:  9280\n",
            " correct:  tensor(9344) Total:  9344\n",
            " correct:  tensor(9408) Total:  9408\n",
            " correct:  tensor(9472) Total:  9472\n",
            " correct:  tensor(9536) Total:  9536\n",
            " correct:  tensor(9600) Total:  9600\n",
            " correct:  tensor(9664) Total:  9664\n",
            " correct:  tensor(9728) Total:  9728\n",
            " correct:  tensor(9792) Total:  9792\n",
            " correct:  tensor(9856) Total:  9856\n",
            " correct:  tensor(9920) Total:  9920\n",
            " correct:  tensor(9984) Total:  9984\n",
            " correct:  tensor(10000) Total:  10000\n",
            "TEST Epoch 3, Loss 15.254893779754639 Accuracy 1.0 Time 165.861s\n",
            "TRAINING...\n",
            " correct:  tensor(64) Total:  64\n",
            "Epoch 4, Batch 1, Loss 15.039655685424805 Accuracy 1.0 Time 3.233s\n",
            " correct:  tensor(128) Total:  128\n",
            "Epoch 4, Batch 2, Loss 14.902204990386963 Accuracy 1.0 Time 3.142s\n",
            " correct:  tensor(192) Total:  192\n",
            "Epoch 4, Batch 3, Loss 15.07279904683431 Accuracy 1.0 Time 3.168s\n",
            " correct:  tensor(256) Total:  256\n",
            "Epoch 4, Batch 4, Loss 15.247405529022217 Accuracy 1.0 Time 3.144s\n",
            " correct:  tensor(320) Total:  320\n",
            "Epoch 4, Batch 5, Loss 15.211347579956055 Accuracy 1.0 Time 3.203s\n",
            " correct:  tensor(384) Total:  384\n",
            "Epoch 4, Batch 6, Loss 15.23164415359497 Accuracy 1.0 Time 3.166s\n",
            " correct:  tensor(448) Total:  448\n",
            "Epoch 4, Batch 7, Loss 15.241961479187012 Accuracy 1.0 Time 3.206s\n",
            " correct:  tensor(512) Total:  512\n",
            "Epoch 4, Batch 8, Loss 15.209985852241516 Accuracy 1.0 Time 3.161s\n",
            " correct:  tensor(576) Total:  576\n",
            "Epoch 4, Batch 9, Loss 15.214938163757324 Accuracy 1.0 Time 3.201s\n",
            " correct:  tensor(640) Total:  640\n",
            "Epoch 4, Batch 10, Loss 15.128703689575195 Accuracy 1.0 Time 3.227s\n",
            " correct:  tensor(704) Total:  704\n",
            "Epoch 4, Batch 11, Loss 15.109199437228115 Accuracy 1.0 Time 3.197s\n",
            " correct:  tensor(768) Total:  768\n",
            "Epoch 4, Batch 12, Loss 15.107849756876627 Accuracy 1.0 Time 3.149s\n",
            " correct:  tensor(832) Total:  832\n",
            "Epoch 4, Batch 13, Loss 15.06924240405743 Accuracy 1.0 Time 3.204s\n",
            " correct:  tensor(896) Total:  896\n",
            "Epoch 4, Batch 14, Loss 15.0473039490836 Accuracy 1.0 Time 3.18s\n",
            " correct:  tensor(960) Total:  960\n",
            "Epoch 4, Batch 15, Loss 15.01260363260905 Accuracy 1.0 Time 3.182s\n",
            " correct:  tensor(1024) Total:  1024\n",
            "Epoch 4, Batch 16, Loss 14.98227596282959 Accuracy 1.0 Time 3.175s\n",
            " correct:  tensor(1088) Total:  1088\n",
            "Epoch 4, Batch 17, Loss 14.97979180953082 Accuracy 1.0 Time 3.153s\n",
            " correct:  tensor(1152) Total:  1152\n",
            "Epoch 4, Batch 18, Loss 14.998723559909397 Accuracy 1.0 Time 3.224s\n",
            " correct:  tensor(1216) Total:  1216\n",
            "Epoch 4, Batch 19, Loss 14.981991617303146 Accuracy 1.0 Time 3.231s\n",
            " correct:  tensor(1280) Total:  1280\n",
            "Epoch 4, Batch 20, Loss 14.954211902618407 Accuracy 1.0 Time 3.193s\n",
            " correct:  tensor(1344) Total:  1344\n",
            "Epoch 4, Batch 21, Loss 14.94431241353353 Accuracy 1.0 Time 3.172s\n",
            " correct:  tensor(1408) Total:  1408\n",
            "Epoch 4, Batch 22, Loss 14.903524485501377 Accuracy 1.0 Time 3.17s\n",
            " correct:  tensor(1472) Total:  1472\n",
            "Epoch 4, Batch 23, Loss 14.881852606068486 Accuracy 1.0 Time 3.163s\n",
            " correct:  tensor(1536) Total:  1536\n",
            "Epoch 4, Batch 24, Loss 14.862138907114664 Accuracy 1.0 Time 3.204s\n",
            " correct:  tensor(1600) Total:  1600\n",
            "Epoch 4, Batch 25, Loss 14.831747131347656 Accuracy 1.0 Time 3.202s\n",
            " correct:  tensor(1664) Total:  1664\n",
            "Epoch 4, Batch 26, Loss 14.840555337759165 Accuracy 1.0 Time 3.38s\n",
            " correct:  tensor(1728) Total:  1728\n",
            "Epoch 4, Batch 27, Loss 14.857210123980487 Accuracy 1.0 Time 3.361s\n",
            " correct:  tensor(1792) Total:  1792\n",
            "Epoch 4, Batch 28, Loss 14.846228837966919 Accuracy 1.0 Time 3.426s\n",
            " correct:  tensor(1856) Total:  1856\n",
            "Epoch 4, Batch 29, Loss 14.838745544696677 Accuracy 1.0 Time 3.297s\n",
            " correct:  tensor(1920) Total:  1920\n",
            "Epoch 4, Batch 30, Loss 14.821365038553873 Accuracy 1.0 Time 3.133s\n",
            " correct:  tensor(1984) Total:  1984\n",
            "Epoch 4, Batch 31, Loss 14.831810674359721 Accuracy 1.0 Time 3.165s\n",
            " correct:  tensor(2048) Total:  2048\n",
            "Epoch 4, Batch 32, Loss 14.819598197937012 Accuracy 1.0 Time 3.15s\n",
            " correct:  tensor(2112) Total:  2112\n",
            "Epoch 4, Batch 33, Loss 14.813003395542953 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(2176) Total:  2176\n",
            "Epoch 4, Batch 34, Loss 14.80526839985567 Accuracy 1.0 Time 3.202s\n",
            " correct:  tensor(2240) Total:  2240\n",
            "Epoch 4, Batch 35, Loss 14.809719576154436 Accuracy 1.0 Time 3.239s\n",
            " correct:  tensor(2304) Total:  2304\n",
            "Epoch 4, Batch 36, Loss 14.818202892939249 Accuracy 1.0 Time 3.187s\n",
            " correct:  tensor(2368) Total:  2368\n",
            "Epoch 4, Batch 37, Loss 14.807648890727275 Accuracy 1.0 Time 3.154s\n",
            " correct:  tensor(2432) Total:  2432\n",
            "Epoch 4, Batch 38, Loss 14.808997430299458 Accuracy 1.0 Time 3.186s\n",
            " correct:  tensor(2496) Total:  2496\n",
            "Epoch 4, Batch 39, Loss 14.826840156163925 Accuracy 1.0 Time 3.156s\n",
            " correct:  tensor(2560) Total:  2560\n",
            "Epoch 4, Batch 40, Loss 14.83515386581421 Accuracy 1.0 Time 3.13s\n",
            " correct:  tensor(2624) Total:  2624\n",
            "Epoch 4, Batch 41, Loss 14.834757642048162 Accuracy 1.0 Time 3.186s\n",
            " correct:  tensor(2688) Total:  2688\n",
            "Epoch 4, Batch 42, Loss 14.834347429729643 Accuracy 1.0 Time 3.13s\n",
            " correct:  tensor(2752) Total:  2752\n",
            "Epoch 4, Batch 43, Loss 14.82337519179943 Accuracy 1.0 Time 3.153s\n",
            " correct:  tensor(2816) Total:  2816\n",
            "Epoch 4, Batch 44, Loss 14.830785989761353 Accuracy 1.0 Time 3.166s\n",
            " correct:  tensor(2880) Total:  2880\n",
            "Epoch 4, Batch 45, Loss 14.847225824991861 Accuracy 1.0 Time 3.172s\n",
            " correct:  tensor(2944) Total:  2944\n",
            "Epoch 4, Batch 46, Loss 14.853131667427395 Accuracy 1.0 Time 3.157s\n",
            " correct:  tensor(3008) Total:  3008\n",
            "Epoch 4, Batch 47, Loss 14.838358148615411 Accuracy 1.0 Time 3.173s\n",
            " correct:  tensor(3072) Total:  3072\n",
            "Epoch 4, Batch 48, Loss 14.847561736901602 Accuracy 1.0 Time 3.125s\n",
            " correct:  tensor(3136) Total:  3136\n",
            "Epoch 4, Batch 49, Loss 14.853316560083506 Accuracy 1.0 Time 3.119s\n",
            " correct:  tensor(3200) Total:  3200\n",
            "Epoch 4, Batch 50, Loss 14.843210754394532 Accuracy 1.0 Time 3.09s\n",
            " correct:  tensor(3264) Total:  3264\n",
            "Epoch 4, Batch 51, Loss 14.843385322421204 Accuracy 1.0 Time 3.158s\n",
            " correct:  tensor(3328) Total:  3328\n",
            "Epoch 4, Batch 52, Loss 14.842141499886147 Accuracy 1.0 Time 3.127s\n",
            " correct:  tensor(3392) Total:  3392\n",
            "Epoch 4, Batch 53, Loss 14.84206474951978 Accuracy 1.0 Time 3.136s\n",
            " correct:  tensor(3456) Total:  3456\n",
            "Epoch 4, Batch 54, Loss 14.843463279582837 Accuracy 1.0 Time 3.197s\n",
            " correct:  tensor(3520) Total:  3520\n",
            "Epoch 4, Batch 55, Loss 14.83514645316384 Accuracy 1.0 Time 3.155s\n",
            " correct:  tensor(3584) Total:  3584\n",
            "Epoch 4, Batch 56, Loss 14.834202783448356 Accuracy 1.0 Time 3.092s\n",
            " correct:  tensor(3648) Total:  3648\n",
            "Epoch 4, Batch 57, Loss 14.827389850951077 Accuracy 1.0 Time 3.136s\n",
            " correct:  tensor(3712) Total:  3712\n",
            "Epoch 4, Batch 58, Loss 14.82648441709321 Accuracy 1.0 Time 3.076s\n",
            " correct:  tensor(3776) Total:  3776\n",
            "Epoch 4, Batch 59, Loss 14.832196332640567 Accuracy 1.0 Time 3.108s\n",
            " correct:  tensor(3840) Total:  3840\n",
            "Epoch 4, Batch 60, Loss 14.821784782409669 Accuracy 1.0 Time 3.107s\n",
            " correct:  tensor(3904) Total:  3904\n",
            "Epoch 4, Batch 61, Loss 14.822647282334625 Accuracy 1.0 Time 3.112s\n",
            " correct:  tensor(3968) Total:  3968\n",
            "Epoch 4, Batch 62, Loss 14.819318663689398 Accuracy 1.0 Time 3.139s\n",
            " correct:  tensor(4032) Total:  4032\n",
            "Epoch 4, Batch 63, Loss 14.822538496955993 Accuracy 1.0 Time 3.143s\n",
            " correct:  tensor(4096) Total:  4096\n",
            "Epoch 4, Batch 64, Loss 14.82502506673336 Accuracy 1.0 Time 3.135s\n",
            " correct:  tensor(4160) Total:  4160\n",
            "Epoch 4, Batch 65, Loss 14.82616643172044 Accuracy 1.0 Time 3.098s\n",
            " correct:  tensor(4224) Total:  4224\n",
            "Epoch 4, Batch 66, Loss 14.835017796718713 Accuracy 1.0 Time 3.155s\n",
            " correct:  tensor(4288) Total:  4288\n",
            "Epoch 4, Batch 67, Loss 14.825381464032985 Accuracy 1.0 Time 3.173s\n",
            " correct:  tensor(4352) Total:  4352\n",
            "Epoch 4, Batch 68, Loss 14.813503433676328 Accuracy 1.0 Time 3.172s\n",
            " correct:  tensor(4416) Total:  4416\n",
            "Epoch 4, Batch 69, Loss 14.809083206066187 Accuracy 1.0 Time 3.149s\n",
            " correct:  tensor(4480) Total:  4480\n",
            "Epoch 4, Batch 70, Loss 14.819674478258406 Accuracy 1.0 Time 3.23s\n",
            " correct:  tensor(4544) Total:  4544\n",
            "Epoch 4, Batch 71, Loss 14.832842746251066 Accuracy 1.0 Time 3.199s\n",
            " correct:  tensor(4608) Total:  4608\n",
            "Epoch 4, Batch 72, Loss 14.823280586136711 Accuracy 1.0 Time 3.148s\n",
            " correct:  tensor(4672) Total:  4672\n",
            "Epoch 4, Batch 73, Loss 14.822086412612705 Accuracy 1.0 Time 3.216s\n",
            " correct:  tensor(4736) Total:  4736\n",
            "Epoch 4, Batch 74, Loss 14.810918460021147 Accuracy 1.0 Time 3.186s\n",
            " correct:  tensor(4800) Total:  4800\n",
            "Epoch 4, Batch 75, Loss 14.809487533569335 Accuracy 1.0 Time 3.118s\n",
            " correct:  tensor(4864) Total:  4864\n",
            "Epoch 4, Batch 76, Loss 14.8031039488943 Accuracy 1.0 Time 3.197s\n",
            " correct:  tensor(4928) Total:  4928\n",
            "Epoch 4, Batch 77, Loss 14.797542188074681 Accuracy 1.0 Time 3.127s\n",
            " correct:  tensor(4992) Total:  4992\n",
            "Epoch 4, Batch 78, Loss 14.79753802372859 Accuracy 1.0 Time 3.143s\n",
            " correct:  tensor(5056) Total:  5056\n",
            "Epoch 4, Batch 79, Loss 14.806129419350926 Accuracy 1.0 Time 3.171s\n",
            " correct:  tensor(5120) Total:  5120\n",
            "Epoch 4, Batch 80, Loss 14.803388690948486 Accuracy 1.0 Time 3.082s\n",
            " correct:  tensor(5184) Total:  5184\n",
            "Epoch 4, Batch 81, Loss 14.802631484137642 Accuracy 1.0 Time 3.127s\n",
            " correct:  tensor(5248) Total:  5248\n",
            "Epoch 4, Batch 82, Loss 14.806818345697915 Accuracy 1.0 Time 3.127s\n",
            " correct:  tensor(5312) Total:  5312\n",
            "Epoch 4, Batch 83, Loss 14.808734089495188 Accuracy 1.0 Time 3.122s\n",
            " correct:  tensor(5376) Total:  5376\n",
            "Epoch 4, Batch 84, Loss 14.800741468157087 Accuracy 1.0 Time 3.143s\n",
            " correct:  tensor(5440) Total:  5440\n",
            "Epoch 4, Batch 85, Loss 14.804451325360466 Accuracy 1.0 Time 3.123s\n",
            " correct:  tensor(5504) Total:  5504\n",
            "Epoch 4, Batch 86, Loss 14.804573247599047 Accuracy 1.0 Time 3.094s\n",
            " correct:  tensor(5568) Total:  5568\n",
            "Epoch 4, Batch 87, Loss 14.809605181902304 Accuracy 1.0 Time 3.123s\n",
            " correct:  tensor(5632) Total:  5632\n",
            "Epoch 4, Batch 88, Loss 14.803415634415366 Accuracy 1.0 Time 3.075s\n",
            " correct:  tensor(5696) Total:  5696\n",
            "Epoch 4, Batch 89, Loss 14.802774750784542 Accuracy 1.0 Time 3.101s\n",
            " correct:  tensor(5760) Total:  5760\n",
            "Epoch 4, Batch 90, Loss 14.800148857964409 Accuracy 1.0 Time 3.126s\n",
            " correct:  tensor(5824) Total:  5824\n",
            "Epoch 4, Batch 91, Loss 14.79480073740194 Accuracy 1.0 Time 3.097s\n",
            " correct:  tensor(5888) Total:  5888\n",
            "Epoch 4, Batch 92, Loss 14.788061587706856 Accuracy 1.0 Time 3.084s\n",
            " correct:  tensor(5952) Total:  5952\n",
            "Epoch 4, Batch 93, Loss 14.789031623512185 Accuracy 1.0 Time 3.164s\n",
            " correct:  tensor(6016) Total:  6016\n",
            "Epoch 4, Batch 94, Loss 14.796479265740578 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(6080) Total:  6080\n",
            "Epoch 4, Batch 95, Loss 14.79549184096487 Accuracy 1.0 Time 3.212s\n",
            " correct:  tensor(6144) Total:  6144\n",
            "Epoch 4, Batch 96, Loss 14.801069567600885 Accuracy 1.0 Time 3.224s\n",
            " correct:  tensor(6208) Total:  6208\n",
            "Epoch 4, Batch 97, Loss 14.798427493301864 Accuracy 1.0 Time 3.114s\n",
            " correct:  tensor(6272) Total:  6272\n",
            "Epoch 4, Batch 98, Loss 14.808068820408412 Accuracy 1.0 Time 3.19s\n",
            " correct:  tensor(6336) Total:  6336\n",
            "Epoch 4, Batch 99, Loss 14.814835182344071 Accuracy 1.0 Time 3.194s\n",
            " correct:  tensor(6400) Total:  6400\n",
            "Epoch 4, Batch 100, Loss 14.815496397018432 Accuracy 1.0 Time 3.144s\n",
            " correct:  tensor(6464) Total:  6464\n",
            "Epoch 4, Batch 101, Loss 14.816571802195936 Accuracy 1.0 Time 3.164s\n",
            " correct:  tensor(6528) Total:  6528\n",
            "Epoch 4, Batch 102, Loss 14.822141479043399 Accuracy 1.0 Time 3.239s\n",
            " correct:  tensor(6592) Total:  6592\n",
            "Epoch 4, Batch 103, Loss 14.818167195736784 Accuracy 1.0 Time 3.192s\n",
            " correct:  tensor(6656) Total:  6656\n",
            "Epoch 4, Batch 104, Loss 14.81870355972877 Accuracy 1.0 Time 3.143s\n",
            " correct:  tensor(6720) Total:  6720\n",
            "Epoch 4, Batch 105, Loss 14.814879708063035 Accuracy 1.0 Time 3.172s\n",
            " correct:  tensor(6784) Total:  6784\n",
            "Epoch 4, Batch 106, Loss 14.818793287817037 Accuracy 1.0 Time 3.103s\n",
            " correct:  tensor(6848) Total:  6848\n",
            "Epoch 4, Batch 107, Loss 14.818476605638166 Accuracy 1.0 Time 3.206s\n",
            " correct:  tensor(6912) Total:  6912\n",
            "Epoch 4, Batch 108, Loss 14.819900344919276 Accuracy 1.0 Time 3.122s\n",
            " correct:  tensor(6976) Total:  6976\n",
            "Epoch 4, Batch 109, Loss 14.818384800482233 Accuracy 1.0 Time 3.169s\n",
            " correct:  tensor(7040) Total:  7040\n",
            "Epoch 4, Batch 110, Loss 14.810552631724965 Accuracy 1.0 Time 3.17s\n",
            " correct:  tensor(7104) Total:  7104\n",
            "Epoch 4, Batch 111, Loss 14.81249408034591 Accuracy 1.0 Time 3.118s\n",
            " correct:  tensor(7168) Total:  7168\n",
            "Epoch 4, Batch 112, Loss 14.81168268408094 Accuracy 1.0 Time 3.14s\n",
            " correct:  tensor(7232) Total:  7232\n",
            "Epoch 4, Batch 113, Loss 14.813974996583651 Accuracy 1.0 Time 3.158s\n",
            " correct:  tensor(7296) Total:  7296\n",
            "Epoch 4, Batch 114, Loss 14.809465952086867 Accuracy 1.0 Time 3.206s\n",
            " correct:  tensor(7360) Total:  7360\n",
            "Epoch 4, Batch 115, Loss 14.810026226872983 Accuracy 1.0 Time 3.108s\n",
            " correct:  tensor(7424) Total:  7424\n",
            "Epoch 4, Batch 116, Loss 14.799143700764096 Accuracy 1.0 Time 3.152s\n",
            " correct:  tensor(7488) Total:  7488\n",
            "Epoch 4, Batch 117, Loss 14.802398844661875 Accuracy 1.0 Time 3.229s\n",
            " correct:  tensor(7552) Total:  7552\n",
            "Epoch 4, Batch 118, Loss 14.810398214954441 Accuracy 1.0 Time 3.125s\n",
            " correct:  tensor(7616) Total:  7616\n",
            "Epoch 4, Batch 119, Loss 14.816050232959395 Accuracy 1.0 Time 3.152s\n",
            " correct:  tensor(7680) Total:  7680\n",
            "Epoch 4, Batch 120, Loss 14.816958634058635 Accuracy 1.0 Time 3.17s\n",
            " correct:  tensor(7744) Total:  7744\n",
            "Epoch 4, Batch 121, Loss 14.816513802394393 Accuracy 1.0 Time 3.19s\n",
            " correct:  tensor(7808) Total:  7808\n",
            "Epoch 4, Batch 122, Loss 14.817006220583055 Accuracy 1.0 Time 3.271s\n",
            " correct:  tensor(7872) Total:  7872\n",
            "Epoch 4, Batch 123, Loss 14.81407445426879 Accuracy 1.0 Time 3.362s\n",
            " correct:  tensor(7936) Total:  7936\n",
            "Epoch 4, Batch 124, Loss 14.818659636282153 Accuracy 1.0 Time 3.234s\n",
            " correct:  tensor(8000) Total:  8000\n",
            "Epoch 4, Batch 125, Loss 14.818386169433595 Accuracy 1.0 Time 3.298s\n",
            " correct:  tensor(8064) Total:  8064\n",
            "Epoch 4, Batch 126, Loss 14.818640277499245 Accuracy 1.0 Time 3.349s\n",
            " correct:  tensor(8128) Total:  8128\n",
            "Epoch 4, Batch 127, Loss 14.820183438578928 Accuracy 1.0 Time 3.252s\n",
            " correct:  tensor(8192) Total:  8192\n",
            "Epoch 4, Batch 128, Loss 14.822298124432564 Accuracy 1.0 Time 3.176s\n",
            " correct:  tensor(8256) Total:  8256\n",
            "Epoch 4, Batch 129, Loss 14.818021463793377 Accuracy 1.0 Time 3.179s\n",
            " correct:  tensor(8320) Total:  8320\n",
            "Epoch 4, Batch 130, Loss 14.814272374373216 Accuracy 1.0 Time 3.161s\n",
            " correct:  tensor(8384) Total:  8384\n",
            "Epoch 4, Batch 131, Loss 14.812105440911447 Accuracy 1.0 Time 3.22s\n",
            " correct:  tensor(8448) Total:  8448\n",
            "Epoch 4, Batch 132, Loss 14.81405343431415 Accuracy 1.0 Time 3.126s\n",
            " correct:  tensor(8512) Total:  8512\n",
            "Epoch 4, Batch 133, Loss 14.812592506408691 Accuracy 1.0 Time 3.198s\n",
            " correct:  tensor(8576) Total:  8576\n",
            "Epoch 4, Batch 134, Loss 14.817269602818275 Accuracy 1.0 Time 3.204s\n",
            " correct:  tensor(8640) Total:  8640\n",
            "Epoch 4, Batch 135, Loss 14.820738241407607 Accuracy 1.0 Time 3.218s\n",
            " correct:  tensor(8704) Total:  8704\n",
            "Epoch 4, Batch 136, Loss 14.816936710301567 Accuracy 1.0 Time 3.153s\n",
            " correct:  tensor(8768) Total:  8768\n",
            "Epoch 4, Batch 137, Loss 14.820770451622288 Accuracy 1.0 Time 3.171s\n",
            " correct:  tensor(8832) Total:  8832\n",
            "Epoch 4, Batch 138, Loss 14.820362450419992 Accuracy 1.0 Time 3.11s\n",
            " correct:  tensor(8896) Total:  8896\n",
            "Epoch 4, Batch 139, Loss 14.816920808750949 Accuracy 1.0 Time 3.125s\n",
            " correct:  tensor(8960) Total:  8960\n",
            "Epoch 4, Batch 140, Loss 14.812265763963971 Accuracy 1.0 Time 3.157s\n",
            " correct:  tensor(9024) Total:  9024\n",
            "Epoch 4, Batch 141, Loss 14.813708772050573 Accuracy 1.0 Time 3.149s\n",
            " correct:  tensor(9088) Total:  9088\n",
            "Epoch 4, Batch 142, Loss 14.81366278420032 Accuracy 1.0 Time 3.163s\n",
            " correct:  tensor(9152) Total:  9152\n",
            "Epoch 4, Batch 143, Loss 14.808113058130225 Accuracy 1.0 Time 3.096s\n",
            " correct:  tensor(9216) Total:  9216\n",
            "Epoch 4, Batch 144, Loss 14.802202767795986 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(9280) Total:  9280\n",
            "Epoch 4, Batch 145, Loss 14.800014581351444 Accuracy 1.0 Time 3.107s\n",
            " correct:  tensor(9344) Total:  9344\n",
            "Epoch 4, Batch 146, Loss 14.800696915143156 Accuracy 1.0 Time 3.087s\n",
            " correct:  tensor(9408) Total:  9408\n",
            "Epoch 4, Batch 147, Loss 14.80218200943097 Accuracy 1.0 Time 3.096s\n",
            " correct:  tensor(9472) Total:  9472\n",
            "Epoch 4, Batch 148, Loss 14.798161345559198 Accuracy 1.0 Time 3.12s\n",
            " correct:  tensor(9536) Total:  9536\n",
            "Epoch 4, Batch 149, Loss 14.79838316872616 Accuracy 1.0 Time 3.132s\n",
            " correct:  tensor(9600) Total:  9600\n",
            "Epoch 4, Batch 150, Loss 14.803146101633708 Accuracy 1.0 Time 3.175s\n",
            " correct:  tensor(9664) Total:  9664\n",
            "Epoch 4, Batch 151, Loss 14.80814632516823 Accuracy 1.0 Time 3.136s\n",
            " correct:  tensor(9728) Total:  9728\n",
            "Epoch 4, Batch 152, Loss 14.807074007235075 Accuracy 1.0 Time 3.215s\n",
            " correct:  tensor(9792) Total:  9792\n",
            "Epoch 4, Batch 153, Loss 14.806782928167605 Accuracy 1.0 Time 3.107s\n",
            " correct:  tensor(9856) Total:  9856\n",
            "Epoch 4, Batch 154, Loss 14.808689061697427 Accuracy 1.0 Time 3.106s\n",
            " correct:  tensor(9920) Total:  9920\n",
            "Epoch 4, Batch 155, Loss 14.81077933157644 Accuracy 1.0 Time 3.109s\n",
            " correct:  tensor(9984) Total:  9984\n",
            "Epoch 4, Batch 156, Loss 14.814215635642027 Accuracy 1.0 Time 3.163s\n",
            " correct:  tensor(10048) Total:  10048\n",
            "Epoch 4, Batch 157, Loss 14.8145538378673 Accuracy 1.0 Time 3.094s\n",
            " correct:  tensor(10112) Total:  10112\n",
            "Epoch 4, Batch 158, Loss 14.8182916037644 Accuracy 1.0 Time 3.137s\n",
            " correct:  tensor(10176) Total:  10176\n",
            "Epoch 4, Batch 159, Loss 14.827228666101611 Accuracy 1.0 Time 3.13s\n",
            " correct:  tensor(10240) Total:  10240\n",
            "Epoch 4, Batch 160, Loss 14.825240409374237 Accuracy 1.0 Time 3.17s\n",
            " correct:  tensor(10304) Total:  10304\n",
            "Epoch 4, Batch 161, Loss 14.828553383394798 Accuracy 1.0 Time 3.15s\n",
            " correct:  tensor(10368) Total:  10368\n",
            "Epoch 4, Batch 162, Loss 14.828463236490885 Accuracy 1.0 Time 3.137s\n",
            " correct:  tensor(10432) Total:  10432\n",
            "Epoch 4, Batch 163, Loss 14.82477897222788 Accuracy 1.0 Time 3.142s\n",
            " correct:  tensor(10496) Total:  10496\n",
            "Epoch 4, Batch 164, Loss 14.824138804179865 Accuracy 1.0 Time 3.126s\n",
            " correct:  tensor(10560) Total:  10560\n",
            "Epoch 4, Batch 165, Loss 14.822399659590287 Accuracy 1.0 Time 3.199s\n",
            " correct:  tensor(10624) Total:  10624\n",
            "Epoch 4, Batch 166, Loss 14.825720459581857 Accuracy 1.0 Time 3.224s\n",
            " correct:  tensor(10688) Total:  10688\n",
            "Epoch 4, Batch 167, Loss 14.8246126232033 Accuracy 1.0 Time 3.15s\n",
            " correct:  tensor(10752) Total:  10752\n",
            "Epoch 4, Batch 168, Loss 14.82728472210112 Accuracy 1.0 Time 3.167s\n",
            " correct:  tensor(10816) Total:  10816\n",
            "Epoch 4, Batch 169, Loss 14.827372511463052 Accuracy 1.0 Time 3.208s\n",
            " correct:  tensor(10880) Total:  10880\n",
            "Epoch 4, Batch 170, Loss 14.825078554714427 Accuracy 1.0 Time 3.116s\n",
            " correct:  tensor(10944) Total:  10944\n",
            "Epoch 4, Batch 171, Loss 14.825253230089332 Accuracy 1.0 Time 3.212s\n",
            " correct:  tensor(11008) Total:  11008\n",
            "Epoch 4, Batch 172, Loss 14.8260769178701 Accuracy 1.0 Time 3.156s\n",
            " correct:  tensor(11072) Total:  11072\n",
            "Epoch 4, Batch 173, Loss 14.82518390699618 Accuracy 1.0 Time 3.176s\n",
            " correct:  tensor(11136) Total:  11136\n",
            "Epoch 4, Batch 174, Loss 14.821248794424124 Accuracy 1.0 Time 3.154s\n",
            " correct:  tensor(11200) Total:  11200\n",
            "Epoch 4, Batch 175, Loss 14.824648072378976 Accuracy 1.0 Time 3.118s\n",
            " correct:  tensor(11264) Total:  11264\n",
            "Epoch 4, Batch 176, Loss 14.823299711400812 Accuracy 1.0 Time 3.129s\n",
            " correct:  tensor(11328) Total:  11328\n",
            "Epoch 4, Batch 177, Loss 14.824710689695541 Accuracy 1.0 Time 3.106s\n",
            " correct:  tensor(11392) Total:  11392\n",
            "Epoch 4, Batch 178, Loss 14.82624949230237 Accuracy 1.0 Time 3.129s\n",
            " correct:  tensor(11456) Total:  11456\n",
            "Epoch 4, Batch 179, Loss 14.826375838764553 Accuracy 1.0 Time 3.152s\n",
            " correct:  tensor(11520) Total:  11520\n",
            "Epoch 4, Batch 180, Loss 14.830439440409343 Accuracy 1.0 Time 3.192s\n",
            " correct:  tensor(11584) Total:  11584\n",
            "Epoch 4, Batch 181, Loss 14.828300117787736 Accuracy 1.0 Time 3.208s\n",
            " correct:  tensor(11648) Total:  11648\n",
            "Epoch 4, Batch 182, Loss 14.829666645972283 Accuracy 1.0 Time 3.181s\n",
            " correct:  tensor(11712) Total:  11712\n",
            "Epoch 4, Batch 183, Loss 14.831361317243733 Accuracy 1.0 Time 3.176s\n",
            " correct:  tensor(11776) Total:  11776\n",
            "Epoch 4, Batch 184, Loss 14.833612265794173 Accuracy 1.0 Time 3.148s\n",
            " correct:  tensor(11840) Total:  11840\n",
            "Epoch 4, Batch 185, Loss 14.83112492947965 Accuracy 1.0 Time 3.144s\n",
            " correct:  tensor(11904) Total:  11904\n",
            "Epoch 4, Batch 186, Loss 14.834162101950696 Accuracy 1.0 Time 3.169s\n",
            " correct:  tensor(11968) Total:  11968\n",
            "Epoch 4, Batch 187, Loss 14.835142533409405 Accuracy 1.0 Time 3.143s\n",
            " correct:  tensor(12032) Total:  12032\n",
            "Epoch 4, Batch 188, Loss 14.83382574040839 Accuracy 1.0 Time 3.167s\n",
            " correct:  tensor(12096) Total:  12096\n",
            "Epoch 4, Batch 189, Loss 14.83509854917173 Accuracy 1.0 Time 3.149s\n",
            " correct:  tensor(12160) Total:  12160\n",
            "Epoch 4, Batch 190, Loss 14.838541954442075 Accuracy 1.0 Time 3.28s\n",
            " correct:  tensor(12224) Total:  12224\n",
            "Epoch 4, Batch 191, Loss 14.83448141906898 Accuracy 1.0 Time 3.188s\n",
            " correct:  tensor(12288) Total:  12288\n",
            "Epoch 4, Batch 192, Loss 14.835136592388153 Accuracy 1.0 Time 3.193s\n",
            " correct:  tensor(12352) Total:  12352\n",
            "Epoch 4, Batch 193, Loss 14.840737308245249 Accuracy 1.0 Time 3.11s\n",
            " correct:  tensor(12416) Total:  12416\n",
            "Epoch 4, Batch 194, Loss 14.841889297839293 Accuracy 1.0 Time 3.23s\n",
            " correct:  tensor(12480) Total:  12480\n",
            "Epoch 4, Batch 195, Loss 14.838899348332331 Accuracy 1.0 Time 3.183s\n",
            " correct:  tensor(12544) Total:  12544\n",
            "Epoch 4, Batch 196, Loss 14.839909986573822 Accuracy 1.0 Time 3.142s\n",
            " correct:  tensor(12608) Total:  12608\n",
            "Epoch 4, Batch 197, Loss 14.840127407596801 Accuracy 1.0 Time 3.171s\n",
            " correct:  tensor(12672) Total:  12672\n",
            "Epoch 4, Batch 198, Loss 14.838100693442605 Accuracy 1.0 Time 3.168s\n",
            " correct:  tensor(12736) Total:  12736\n",
            "Epoch 4, Batch 199, Loss 14.840273023250713 Accuracy 1.0 Time 3.176s\n",
            " correct:  tensor(12800) Total:  12800\n",
            "Epoch 4, Batch 200, Loss 14.845625491142274 Accuracy 1.0 Time 3.185s\n",
            " correct:  tensor(12864) Total:  12864\n",
            "Epoch 4, Batch 201, Loss 14.844982759276434 Accuracy 1.0 Time 3.163s\n",
            " correct:  tensor(12928) Total:  12928\n",
            "Epoch 4, Batch 202, Loss 14.843629704843654 Accuracy 1.0 Time 3.149s\n",
            " correct:  tensor(12992) Total:  12992\n",
            "Epoch 4, Batch 203, Loss 14.844765559793105 Accuracy 1.0 Time 3.152s\n",
            " correct:  tensor(13056) Total:  13056\n",
            "Epoch 4, Batch 204, Loss 14.84195198732264 Accuracy 1.0 Time 3.121s\n",
            " correct:  tensor(13120) Total:  13120\n",
            "Epoch 4, Batch 205, Loss 14.84157489218363 Accuracy 1.0 Time 3.129s\n",
            " correct:  tensor(13184) Total:  13184\n",
            "Epoch 4, Batch 206, Loss 14.848283383452777 Accuracy 1.0 Time 3.119s\n",
            " correct:  tensor(13248) Total:  13248\n",
            "Epoch 4, Batch 207, Loss 14.849715481633725 Accuracy 1.0 Time 3.181s\n",
            " correct:  tensor(13312) Total:  13312\n",
            "Epoch 4, Batch 208, Loss 14.851096382507912 Accuracy 1.0 Time 3.252s\n",
            " correct:  tensor(13376) Total:  13376\n",
            "Epoch 4, Batch 209, Loss 14.851525968341736 Accuracy 1.0 Time 3.149s\n",
            " correct:  tensor(13440) Total:  13440\n",
            "Epoch 4, Batch 210, Loss 14.853014469146729 Accuracy 1.0 Time 3.091s\n",
            " correct:  tensor(13504) Total:  13504\n",
            "Epoch 4, Batch 211, Loss 14.852682312517935 Accuracy 1.0 Time 3.149s\n",
            " correct:  tensor(13568) Total:  13568\n",
            "Epoch 4, Batch 212, Loss 14.854066169486856 Accuracy 1.0 Time 3.166s\n",
            " correct:  tensor(13632) Total:  13632\n",
            "Epoch 4, Batch 213, Loss 14.852504148169862 Accuracy 1.0 Time 3.172s\n",
            " correct:  tensor(13696) Total:  13696\n",
            "Epoch 4, Batch 214, Loss 14.85350502762839 Accuracy 1.0 Time 3.109s\n",
            " correct:  tensor(13760) Total:  13760\n",
            "Epoch 4, Batch 215, Loss 14.85536047469738 Accuracy 1.0 Time 3.21s\n",
            " correct:  tensor(13824) Total:  13824\n",
            "Epoch 4, Batch 216, Loss 14.85649761888716 Accuracy 1.0 Time 3.167s\n",
            " correct:  tensor(13888) Total:  13888\n",
            "Epoch 4, Batch 217, Loss 14.859233816647858 Accuracy 1.0 Time 3.112s\n",
            " correct:  tensor(13952) Total:  13952\n",
            "Epoch 4, Batch 218, Loss 14.856796382764063 Accuracy 1.0 Time 3.238s\n",
            " correct:  tensor(14016) Total:  14016\n",
            "Epoch 4, Batch 219, Loss 14.856409713013532 Accuracy 1.0 Time 3.313s\n",
            " correct:  tensor(14080) Total:  14080\n",
            "Epoch 4, Batch 220, Loss 14.858135097677058 Accuracy 1.0 Time 3.175s\n",
            " correct:  tensor(14144) Total:  14144\n",
            "Epoch 4, Batch 221, Loss 14.860852422757386 Accuracy 1.0 Time 3.12s\n",
            " correct:  tensor(14208) Total:  14208\n",
            "Epoch 4, Batch 222, Loss 14.861000434772388 Accuracy 1.0 Time 3.336s\n",
            " correct:  tensor(14272) Total:  14272\n",
            "Epoch 4, Batch 223, Loss 14.860706757002347 Accuracy 1.0 Time 3.358s\n",
            " correct:  tensor(14336) Total:  14336\n",
            "Epoch 4, Batch 224, Loss 14.860877373388835 Accuracy 1.0 Time 3.404s\n",
            " correct:  tensor(14400) Total:  14400\n",
            "Epoch 4, Batch 225, Loss 14.859621022542317 Accuracy 1.0 Time 3.181s\n",
            " correct:  tensor(14464) Total:  14464\n",
            "Epoch 4, Batch 226, Loss 14.861192222190114 Accuracy 1.0 Time 3.19s\n",
            " correct:  tensor(14528) Total:  14528\n",
            "Epoch 4, Batch 227, Loss 14.861394378057135 Accuracy 1.0 Time 3.193s\n",
            " correct:  tensor(14592) Total:  14592\n",
            "Epoch 4, Batch 228, Loss 14.863904610014798 Accuracy 1.0 Time 3.164s\n",
            " correct:  tensor(14656) Total:  14656\n",
            "Epoch 4, Batch 229, Loss 14.859923012912533 Accuracy 1.0 Time 3.129s\n",
            " correct:  tensor(14720) Total:  14720\n",
            "Epoch 4, Batch 230, Loss 14.859286345606264 Accuracy 1.0 Time 3.167s\n",
            " correct:  tensor(14784) Total:  14784\n",
            "Epoch 4, Batch 231, Loss 14.857072140747334 Accuracy 1.0 Time 3.169s\n",
            " correct:  tensor(14848) Total:  14848\n",
            "Epoch 4, Batch 232, Loss 14.85908190957431 Accuracy 1.0 Time 3.112s\n",
            " correct:  tensor(14912) Total:  14912\n",
            "Epoch 4, Batch 233, Loss 14.857358027937074 Accuracy 1.0 Time 3.131s\n",
            " correct:  tensor(14976) Total:  14976\n",
            "Epoch 4, Batch 234, Loss 14.85734732538207 Accuracy 1.0 Time 3.149s\n",
            " correct:  tensor(15040) Total:  15040\n",
            "Epoch 4, Batch 235, Loss 14.857394518750779 Accuracy 1.0 Time 3.135s\n",
            " correct:  tensor(15104) Total:  15104\n",
            "Epoch 4, Batch 236, Loss 14.863175064830457 Accuracy 1.0 Time 3.119s\n",
            " correct:  tensor(15168) Total:  15168\n",
            "Epoch 4, Batch 237, Loss 14.86763750551119 Accuracy 1.0 Time 3.143s\n",
            " correct:  tensor(15232) Total:  15232\n",
            "Epoch 4, Batch 238, Loss 14.872973554274616 Accuracy 1.0 Time 3.101s\n",
            " correct:  tensor(15296) Total:  15296\n",
            "Epoch 4, Batch 239, Loss 14.873124282230393 Accuracy 1.0 Time 3.151s\n",
            " correct:  tensor(15360) Total:  15360\n",
            "Epoch 4, Batch 240, Loss 14.871285871664684 Accuracy 1.0 Time 3.177s\n",
            " correct:  tensor(15424) Total:  15424\n",
            "Epoch 4, Batch 241, Loss 14.871326731448352 Accuracy 1.0 Time 3.181s\n",
            " correct:  tensor(15488) Total:  15488\n",
            "Epoch 4, Batch 242, Loss 14.87102940062846 Accuracy 1.0 Time 3.13s\n",
            " correct:  tensor(15552) Total:  15552\n",
            "Epoch 4, Batch 243, Loss 14.87224075333081 Accuracy 1.0 Time 3.095s\n",
            " correct:  tensor(15616) Total:  15616\n",
            "Epoch 4, Batch 244, Loss 14.871847293416007 Accuracy 1.0 Time 3.114s\n",
            " correct:  tensor(15680) Total:  15680\n",
            "Epoch 4, Batch 245, Loss 14.872755766888053 Accuracy 1.0 Time 3.11s\n",
            " correct:  tensor(15744) Total:  15744\n",
            "Epoch 4, Batch 246, Loss 14.87182081424124 Accuracy 1.0 Time 3.105s\n",
            " correct:  tensor(15808) Total:  15808\n",
            "Epoch 4, Batch 247, Loss 14.87006653851343 Accuracy 1.0 Time 3.164s\n",
            " correct:  tensor(15872) Total:  15872\n",
            "Epoch 4, Batch 248, Loss 14.871215778012429 Accuracy 1.0 Time 3.153s\n",
            " correct:  tensor(15936) Total:  15936\n",
            "Epoch 4, Batch 249, Loss 14.872097750744187 Accuracy 1.0 Time 3.192s\n",
            " correct:  tensor(16000) Total:  16000\n",
            "Epoch 4, Batch 250, Loss 14.872038394927978 Accuracy 1.0 Time 3.149s\n",
            " correct:  tensor(16064) Total:  16064\n",
            "Epoch 4, Batch 251, Loss 14.870194959450528 Accuracy 1.0 Time 3.168s\n",
            " correct:  tensor(16128) Total:  16128\n",
            "Epoch 4, Batch 252, Loss 14.870959826878138 Accuracy 1.0 Time 3.174s\n",
            " correct:  tensor(16192) Total:  16192\n",
            "Epoch 4, Batch 253, Loss 14.870912585805057 Accuracy 1.0 Time 3.135s\n",
            " correct:  tensor(16256) Total:  16256\n",
            "Epoch 4, Batch 254, Loss 14.869524925712525 Accuracy 1.0 Time 3.153s\n",
            " correct:  tensor(16320) Total:  16320\n",
            "Epoch 4, Batch 255, Loss 14.871406738430846 Accuracy 1.0 Time 3.139s\n",
            " correct:  tensor(16384) Total:  16384\n",
            "Epoch 4, Batch 256, Loss 14.871220827102661 Accuracy 1.0 Time 3.154s\n",
            " correct:  tensor(16448) Total:  16448\n",
            "Epoch 4, Batch 257, Loss 14.877609750176218 Accuracy 1.0 Time 3.187s\n",
            " correct:  tensor(16512) Total:  16512\n",
            "Epoch 4, Batch 258, Loss 14.874967190646386 Accuracy 1.0 Time 3.11s\n",
            " correct:  tensor(16576) Total:  16576\n",
            "Epoch 4, Batch 259, Loss 14.875786792357456 Accuracy 1.0 Time 3.169s\n",
            " correct:  tensor(16640) Total:  16640\n",
            "Epoch 4, Batch 260, Loss 14.875092829190768 Accuracy 1.0 Time 3.205s\n",
            " correct:  tensor(16704) Total:  16704\n",
            "Epoch 4, Batch 261, Loss 14.874044136982768 Accuracy 1.0 Time 3.129s\n",
            " correct:  tensor(16768) Total:  16768\n",
            "Epoch 4, Batch 262, Loss 14.874576204605685 Accuracy 1.0 Time 3.203s\n",
            " correct:  tensor(16832) Total:  16832\n",
            "Epoch 4, Batch 263, Loss 14.874557183269312 Accuracy 1.0 Time 3.226s\n",
            " correct:  tensor(16896) Total:  16896\n",
            "Epoch 4, Batch 264, Loss 14.875098304315047 Accuracy 1.0 Time 3.175s\n",
            " correct:  tensor(16960) Total:  16960\n",
            "Epoch 4, Batch 265, Loss 14.87510588663929 Accuracy 1.0 Time 3.214s\n",
            " correct:  tensor(17024) Total:  17024\n",
            "Epoch 4, Batch 266, Loss 14.881250922841238 Accuracy 1.0 Time 3.174s\n",
            " correct:  tensor(17088) Total:  17088\n",
            "Epoch 4, Batch 267, Loss 14.881778163409859 Accuracy 1.0 Time 3.26s\n",
            " correct:  tensor(17152) Total:  17152\n",
            "Epoch 4, Batch 268, Loss 14.882301056562964 Accuracy 1.0 Time 3.21s\n",
            " correct:  tensor(17216) Total:  17216\n",
            "Epoch 4, Batch 269, Loss 14.878924118098716 Accuracy 1.0 Time 3.172s\n",
            " correct:  tensor(17280) Total:  17280\n",
            "Epoch 4, Batch 270, Loss 14.877965294873272 Accuracy 1.0 Time 3.168s\n",
            " correct:  tensor(17344) Total:  17344\n",
            "Epoch 4, Batch 271, Loss 14.877751258906404 Accuracy 1.0 Time 3.121s\n",
            " correct:  tensor(17408) Total:  17408\n",
            "Epoch 4, Batch 272, Loss 14.876767814159393 Accuracy 1.0 Time 3.144s\n",
            " correct:  tensor(17472) Total:  17472\n",
            "Epoch 4, Batch 273, Loss 14.876188777741932 Accuracy 1.0 Time 3.191s\n",
            " correct:  tensor(17536) Total:  17536\n",
            "Epoch 4, Batch 274, Loss 14.879588412542413 Accuracy 1.0 Time 3.13s\n",
            " correct:  tensor(17600) Total:  17600\n",
            "Epoch 4, Batch 275, Loss 14.879347638216885 Accuracy 1.0 Time 3.192s\n",
            " correct:  tensor(17664) Total:  17664\n",
            "Epoch 4, Batch 276, Loss 14.877800043078436 Accuracy 1.0 Time 3.182s\n",
            " correct:  tensor(17728) Total:  17728\n",
            "Epoch 4, Batch 277, Loss 14.880226675784115 Accuracy 1.0 Time 3.113s\n",
            " correct:  tensor(17792) Total:  17792\n",
            "Epoch 4, Batch 278, Loss 14.88067130905261 Accuracy 1.0 Time 3.153s\n",
            " correct:  tensor(17856) Total:  17856\n",
            "Epoch 4, Batch 279, Loss 14.883363846809633 Accuracy 1.0 Time 3.132s\n",
            " correct:  tensor(17920) Total:  17920\n",
            "Epoch 4, Batch 280, Loss 14.886725068092346 Accuracy 1.0 Time 3.116s\n",
            " correct:  tensor(17984) Total:  17984\n",
            "Epoch 4, Batch 281, Loss 14.885965242080417 Accuracy 1.0 Time 3.145s\n",
            " correct:  tensor(18048) Total:  18048\n",
            "Epoch 4, Batch 282, Loss 14.886705067140836 Accuracy 1.0 Time 3.204s\n",
            " correct:  tensor(18112) Total:  18112\n",
            "Epoch 4, Batch 283, Loss 14.888995601937122 Accuracy 1.0 Time 3.192s\n",
            " correct:  tensor(18176) Total:  18176\n",
            "Epoch 4, Batch 284, Loss 14.892346318338959 Accuracy 1.0 Time 3.249s\n",
            " correct:  tensor(18240) Total:  18240\n",
            "Epoch 4, Batch 285, Loss 14.894055758024518 Accuracy 1.0 Time 3.198s\n",
            " correct:  tensor(18304) Total:  18304\n",
            "Epoch 4, Batch 286, Loss 14.899739165406126 Accuracy 1.0 Time 3.223s\n",
            " correct:  tensor(18368) Total:  18368\n",
            "Epoch 4, Batch 287, Loss 14.899906228228312 Accuracy 1.0 Time 3.197s\n",
            " correct:  tensor(18432) Total:  18432\n",
            "Epoch 4, Batch 288, Loss 14.901078634791904 Accuracy 1.0 Time 3.124s\n",
            " correct:  tensor(18496) Total:  18496\n",
            "Epoch 4, Batch 289, Loss 14.899845968068265 Accuracy 1.0 Time 3.14s\n",
            " correct:  tensor(18560) Total:  18560\n",
            "Epoch 4, Batch 290, Loss 14.901246705548516 Accuracy 1.0 Time 3.188s\n",
            " correct:  tensor(18624) Total:  18624\n",
            "Epoch 4, Batch 291, Loss 14.897841391284851 Accuracy 1.0 Time 3.2s\n",
            " correct:  tensor(18688) Total:  18688\n",
            "Epoch 4, Batch 292, Loss 14.900696754455566 Accuracy 1.0 Time 3.164s\n",
            " correct:  tensor(18752) Total:  18752\n",
            "Epoch 4, Batch 293, Loss 14.902515993997099 Accuracy 1.0 Time 3.174s\n",
            " correct:  tensor(18816) Total:  18816\n",
            "Epoch 4, Batch 294, Loss 14.904078460875011 Accuracy 1.0 Time 3.151s\n",
            " correct:  tensor(18880) Total:  18880\n",
            "Epoch 4, Batch 295, Loss 14.900768250934147 Accuracy 1.0 Time 3.229s\n",
            " correct:  tensor(18944) Total:  18944\n",
            "Epoch 4, Batch 296, Loss 14.904300496384904 Accuracy 1.0 Time 3.109s\n",
            " correct:  tensor(19008) Total:  19008\n",
            "Epoch 4, Batch 297, Loss 14.902652608826505 Accuracy 1.0 Time 3.13s\n",
            " correct:  tensor(19072) Total:  19072\n",
            "Epoch 4, Batch 298, Loss 14.90503117222114 Accuracy 1.0 Time 3.195s\n",
            " correct:  tensor(19136) Total:  19136\n",
            "Epoch 4, Batch 299, Loss 14.909591024137262 Accuracy 1.0 Time 3.177s\n",
            " correct:  tensor(19200) Total:  19200\n",
            "Epoch 4, Batch 300, Loss 14.907987626393636 Accuracy 1.0 Time 3.099s\n",
            " correct:  tensor(19264) Total:  19264\n",
            "Epoch 4, Batch 301, Loss 14.906426971536934 Accuracy 1.0 Time 3.162s\n",
            " correct:  tensor(19328) Total:  19328\n",
            "Epoch 4, Batch 302, Loss 14.904064393201411 Accuracy 1.0 Time 3.132s\n",
            " correct:  tensor(19392) Total:  19392\n",
            "Epoch 4, Batch 303, Loss 14.904346711564772 Accuracy 1.0 Time 3.268s\n",
            " correct:  tensor(19456) Total:  19456\n",
            "Epoch 4, Batch 304, Loss 14.903222658132252 Accuracy 1.0 Time 3.221s\n",
            " correct:  tensor(19520) Total:  19520\n",
            "Epoch 4, Batch 305, Loss 14.902575880582216 Accuracy 1.0 Time 3.2s\n",
            " correct:  tensor(19584) Total:  19584\n",
            "Epoch 4, Batch 306, Loss 14.907604538537319 Accuracy 1.0 Time 3.098s\n",
            " correct:  tensor(19648) Total:  19648\n",
            "Epoch 4, Batch 307, Loss 14.907927485164679 Accuracy 1.0 Time 3.12s\n",
            " correct:  tensor(19712) Total:  19712\n",
            "Epoch 4, Batch 308, Loss 14.909252971797795 Accuracy 1.0 Time 3.155s\n",
            " correct:  tensor(19776) Total:  19776\n",
            "Epoch 4, Batch 309, Loss 14.911057552473444 Accuracy 1.0 Time 3.121s\n",
            " correct:  tensor(19840) Total:  19840\n",
            "Epoch 4, Batch 310, Loss 14.909514513323384 Accuracy 1.0 Time 3.202s\n",
            " correct:  tensor(19904) Total:  19904\n",
            "Epoch 4, Batch 311, Loss 14.90927262014898 Accuracy 1.0 Time 3.168s\n",
            " correct:  tensor(19968) Total:  19968\n",
            "Epoch 4, Batch 312, Loss 14.91315724910834 Accuracy 1.0 Time 3.127s\n",
            " correct:  tensor(20032) Total:  20032\n",
            "Epoch 4, Batch 313, Loss 14.914640877574396 Accuracy 1.0 Time 3.183s\n",
            " correct:  tensor(20096) Total:  20096\n",
            "Epoch 4, Batch 314, Loss 14.917773872424084 Accuracy 1.0 Time 3.254s\n",
            " correct:  tensor(20160) Total:  20160\n",
            "Epoch 4, Batch 315, Loss 14.917661645677354 Accuracy 1.0 Time 3.352s\n",
            " correct:  tensor(20224) Total:  20224\n",
            "Epoch 4, Batch 316, Loss 14.916484048094931 Accuracy 1.0 Time 3.169s\n",
            " correct:  tensor(20288) Total:  20288\n",
            "Epoch 4, Batch 317, Loss 14.915780380321227 Accuracy 1.0 Time 3.137s\n",
            " correct:  tensor(20352) Total:  20352\n",
            "Epoch 4, Batch 318, Loss 14.91616057150019 Accuracy 1.0 Time 3.143s\n",
            " correct:  tensor(20416) Total:  20416\n",
            "Epoch 4, Batch 319, Loss 14.919806011044493 Accuracy 1.0 Time 3.155s\n",
            " correct:  tensor(20480) Total:  20480\n",
            "Epoch 4, Batch 320, Loss 14.917968600988388 Accuracy 1.0 Time 3.409s\n",
            " correct:  tensor(20544) Total:  20544\n",
            "Epoch 4, Batch 321, Loss 14.91727412452579 Accuracy 1.0 Time 3.36s\n",
            " correct:  tensor(20608) Total:  20608\n",
            "Epoch 4, Batch 322, Loss 14.916878602519539 Accuracy 1.0 Time 3.391s\n",
            " correct:  tensor(20672) Total:  20672\n",
            "Epoch 4, Batch 323, Loss 14.917121405941044 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(20736) Total:  20736\n",
            "Epoch 4, Batch 324, Loss 14.919662019352854 Accuracy 1.0 Time 3.117s\n",
            " correct:  tensor(20800) Total:  20800\n",
            "Epoch 4, Batch 325, Loss 14.921097608713003 Accuracy 1.0 Time 3.175s\n",
            " correct:  tensor(20864) Total:  20864\n",
            "Epoch 4, Batch 326, Loss 14.919299763404519 Accuracy 1.0 Time 3.137s\n",
            " correct:  tensor(20928) Total:  20928\n",
            "Epoch 4, Batch 327, Loss 14.918400414493107 Accuracy 1.0 Time 3.1s\n",
            " correct:  tensor(20992) Total:  20992\n",
            "Epoch 4, Batch 328, Loss 14.916935478768698 Accuracy 1.0 Time 3.124s\n",
            " correct:  tensor(21056) Total:  21056\n",
            "Epoch 4, Batch 329, Loss 14.918198634788258 Accuracy 1.0 Time 3.129s\n",
            " correct:  tensor(21120) Total:  21120\n",
            "Epoch 4, Batch 330, Loss 14.918360499179725 Accuracy 1.0 Time 3.098s\n",
            " correct:  tensor(21184) Total:  21184\n",
            "Epoch 4, Batch 331, Loss 14.917480419769747 Accuracy 1.0 Time 3.197s\n",
            " correct:  tensor(21248) Total:  21248\n",
            "Epoch 4, Batch 332, Loss 14.915261759815445 Accuracy 1.0 Time 3.159s\n",
            " correct:  tensor(21312) Total:  21312\n",
            "Epoch 4, Batch 333, Loss 14.914881517221263 Accuracy 1.0 Time 3.229s\n",
            " correct:  tensor(21376) Total:  21376\n",
            "Epoch 4, Batch 334, Loss 14.917485705392803 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(21440) Total:  21440\n",
            "Epoch 4, Batch 335, Loss 14.915922665951857 Accuracy 1.0 Time 3.159s\n",
            " correct:  tensor(21504) Total:  21504\n",
            "Epoch 4, Batch 336, Loss 14.916803799924397 Accuracy 1.0 Time 3.174s\n",
            " correct:  tensor(21568) Total:  21568\n",
            "Epoch 4, Batch 337, Loss 14.91673450300177 Accuracy 1.0 Time 3.154s\n",
            " correct:  tensor(21632) Total:  21632\n",
            "Epoch 4, Batch 338, Loss 14.916697936650564 Accuracy 1.0 Time 3.115s\n",
            " correct:  tensor(21696) Total:  21696\n",
            "Epoch 4, Batch 339, Loss 14.915683242775346 Accuracy 1.0 Time 3.162s\n",
            " correct:  tensor(21760) Total:  21760\n",
            "Epoch 4, Batch 340, Loss 14.914717068391688 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(21824) Total:  21824\n",
            "Epoch 4, Batch 341, Loss 14.914491700986265 Accuracy 1.0 Time 3.222s\n",
            " correct:  tensor(21888) Total:  21888\n",
            "Epoch 4, Batch 342, Loss 14.914407869528608 Accuracy 1.0 Time 3.169s\n",
            " correct:  tensor(21952) Total:  21952\n",
            "Epoch 4, Batch 343, Loss 14.91497469365423 Accuracy 1.0 Time 3.189s\n",
            " correct:  tensor(22016) Total:  22016\n",
            "Epoch 4, Batch 344, Loss 14.914820271869038 Accuracy 1.0 Time 3.137s\n",
            " correct:  tensor(22080) Total:  22080\n",
            "Epoch 4, Batch 345, Loss 14.911655464725218 Accuracy 1.0 Time 3.089s\n",
            " correct:  tensor(22144) Total:  22144\n",
            "Epoch 4, Batch 346, Loss 14.911215057262796 Accuracy 1.0 Time 3.189s\n",
            " correct:  tensor(22208) Total:  22208\n",
            "Epoch 4, Batch 347, Loss 14.908789788611685 Accuracy 1.0 Time 3.127s\n",
            " correct:  tensor(22272) Total:  22272\n",
            "Epoch 4, Batch 348, Loss 14.907816840314316 Accuracy 1.0 Time 3.095s\n",
            " correct:  tensor(22336) Total:  22336\n",
            "Epoch 4, Batch 349, Loss 14.909977442897151 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(22400) Total:  22400\n",
            "Epoch 4, Batch 350, Loss 14.908952473231725 Accuracy 1.0 Time 3.17s\n",
            " correct:  tensor(22464) Total:  22464\n",
            "Epoch 4, Batch 351, Loss 14.907835484909535 Accuracy 1.0 Time 3.228s\n",
            " correct:  tensor(22528) Total:  22528\n",
            "Epoch 4, Batch 352, Loss 14.90586986325004 Accuracy 1.0 Time 3.208s\n",
            " correct:  tensor(22592) Total:  22592\n",
            "Epoch 4, Batch 353, Loss 14.904746739114648 Accuracy 1.0 Time 3.209s\n",
            " correct:  tensor(22656) Total:  22656\n",
            "Epoch 4, Batch 354, Loss 14.904591264024292 Accuracy 1.0 Time 3.248s\n",
            " correct:  tensor(22720) Total:  22720\n",
            "Epoch 4, Batch 355, Loss 14.90374939878222 Accuracy 1.0 Time 3.165s\n",
            " correct:  tensor(22784) Total:  22784\n",
            "Epoch 4, Batch 356, Loss 14.904012905077987 Accuracy 1.0 Time 3.18s\n",
            " correct:  tensor(22848) Total:  22848\n",
            "Epoch 4, Batch 357, Loss 14.902128337144184 Accuracy 1.0 Time 3.208s\n",
            " correct:  tensor(22912) Total:  22912\n",
            "Epoch 4, Batch 358, Loss 14.900956476200893 Accuracy 1.0 Time 3.163s\n",
            " correct:  tensor(22976) Total:  22976\n",
            "Epoch 4, Batch 359, Loss 14.90197625332888 Accuracy 1.0 Time 3.211s\n",
            " correct:  tensor(23040) Total:  23040\n",
            "Epoch 4, Batch 360, Loss 14.902005585034688 Accuracy 1.0 Time 3.207s\n",
            " correct:  tensor(23104) Total:  23104\n",
            "Epoch 4, Batch 361, Loss 14.903472654707214 Accuracy 1.0 Time 3.169s\n",
            " correct:  tensor(23168) Total:  23168\n",
            "Epoch 4, Batch 362, Loss 14.904438543056257 Accuracy 1.0 Time 3.143s\n",
            " correct:  tensor(23232) Total:  23232\n",
            "Epoch 4, Batch 363, Loss 14.904854041485747 Accuracy 1.0 Time 3.167s\n",
            " correct:  tensor(23296) Total:  23296\n",
            "Epoch 4, Batch 364, Loss 14.903842399408529 Accuracy 1.0 Time 3.163s\n",
            " correct:  tensor(23360) Total:  23360\n",
            "Epoch 4, Batch 365, Loss 14.901676091755906 Accuracy 1.0 Time 3.135s\n",
            " correct:  tensor(23424) Total:  23424\n",
            "Epoch 4, Batch 366, Loss 14.899558580638281 Accuracy 1.0 Time 3.173s\n",
            " correct:  tensor(23488) Total:  23488\n",
            "Epoch 4, Batch 367, Loss 14.90068715534678 Accuracy 1.0 Time 3.178s\n",
            " correct:  tensor(23552) Total:  23552\n",
            "Epoch 4, Batch 368, Loss 14.899979285571886 Accuracy 1.0 Time 3.102s\n",
            " correct:  tensor(23616) Total:  23616\n",
            "Epoch 4, Batch 369, Loss 14.899295998136525 Accuracy 1.0 Time 3.174s\n",
            " correct:  tensor(23680) Total:  23680\n",
            "Epoch 4, Batch 370, Loss 14.89866697723801 Accuracy 1.0 Time 3.077s\n",
            " correct:  tensor(23744) Total:  23744\n",
            "Epoch 4, Batch 371, Loss 14.897621748582372 Accuracy 1.0 Time 3.135s\n",
            " correct:  tensor(23808) Total:  23808\n",
            "Epoch 4, Batch 372, Loss 14.89651560270658 Accuracy 1.0 Time 3.13s\n",
            " correct:  tensor(23872) Total:  23872\n",
            "Epoch 4, Batch 373, Loss 14.892904381330148 Accuracy 1.0 Time 3.095s\n",
            " correct:  tensor(23936) Total:  23936\n",
            "Epoch 4, Batch 374, Loss 14.890625838927408 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(24000) Total:  24000\n",
            "Epoch 4, Batch 375, Loss 14.890724403381348 Accuracy 1.0 Time 3.124s\n",
            " correct:  tensor(24064) Total:  24064\n",
            "Epoch 4, Batch 376, Loss 14.889383425103857 Accuracy 1.0 Time 3.095s\n",
            " correct:  tensor(24128) Total:  24128\n",
            "Epoch 4, Batch 377, Loss 14.888922182887555 Accuracy 1.0 Time 3.118s\n",
            " correct:  tensor(24192) Total:  24192\n",
            "Epoch 4, Batch 378, Loss 14.888546095954048 Accuracy 1.0 Time 3.132s\n",
            " correct:  tensor(24256) Total:  24256\n",
            "Epoch 4, Batch 379, Loss 14.891080894067608 Accuracy 1.0 Time 3.209s\n",
            " correct:  tensor(24320) Total:  24320\n",
            "Epoch 4, Batch 380, Loss 14.890723574788947 Accuracy 1.0 Time 3.179s\n",
            " correct:  tensor(24384) Total:  24384\n",
            "Epoch 4, Batch 381, Loss 14.892522524035195 Accuracy 1.0 Time 3.204s\n",
            " correct:  tensor(24448) Total:  24448\n",
            "Epoch 4, Batch 382, Loss 14.892430667477752 Accuracy 1.0 Time 3.094s\n",
            " correct:  tensor(24512) Total:  24512\n",
            "Epoch 4, Batch 383, Loss 14.891912637117949 Accuracy 1.0 Time 3.13s\n",
            " correct:  tensor(24576) Total:  24576\n",
            "Epoch 4, Batch 384, Loss 14.892177748183409 Accuracy 1.0 Time 3.082s\n",
            " correct:  tensor(24640) Total:  24640\n",
            "Epoch 4, Batch 385, Loss 14.89225843355253 Accuracy 1.0 Time 3.139s\n",
            " correct:  tensor(24704) Total:  24704\n",
            "Epoch 4, Batch 386, Loss 14.891792598783661 Accuracy 1.0 Time 3.115s\n",
            " correct:  tensor(24768) Total:  24768\n",
            "Epoch 4, Batch 387, Loss 14.891610180068694 Accuracy 1.0 Time 3.125s\n",
            " correct:  tensor(24832) Total:  24832\n",
            "Epoch 4, Batch 388, Loss 14.890362117708344 Accuracy 1.0 Time 3.172s\n",
            " correct:  tensor(24896) Total:  24896\n",
            "Epoch 4, Batch 389, Loss 14.893979278505615 Accuracy 1.0 Time 3.089s\n",
            " correct:  tensor(24960) Total:  24960\n",
            "Epoch 4, Batch 390, Loss 14.89213206706903 Accuracy 1.0 Time 3.156s\n",
            " correct:  tensor(25024) Total:  25024\n",
            "Epoch 4, Batch 391, Loss 14.892632381995316 Accuracy 1.0 Time 3.137s\n",
            " correct:  tensor(25088) Total:  25088\n",
            "Epoch 4, Batch 392, Loss 14.892644894366361 Accuracy 1.0 Time 3.16s\n",
            " correct:  tensor(25152) Total:  25152\n",
            "Epoch 4, Batch 393, Loss 14.894077160279563 Accuracy 1.0 Time 3.128s\n",
            " correct:  tensor(25216) Total:  25216\n",
            "Epoch 4, Batch 394, Loss 14.892350828587102 Accuracy 1.0 Time 3.089s\n",
            " correct:  tensor(25280) Total:  25280\n",
            "Epoch 4, Batch 395, Loss 14.892916971520533 Accuracy 1.0 Time 3.114s\n",
            " correct:  tensor(25344) Total:  25344\n",
            "Epoch 4, Batch 396, Loss 14.89101634844385 Accuracy 1.0 Time 3.151s\n",
            " correct:  tensor(25408) Total:  25408\n",
            "Epoch 4, Batch 397, Loss 14.890184700338907 Accuracy 1.0 Time 3.08s\n",
            " correct:  tensor(25472) Total:  25472\n",
            "Epoch 4, Batch 398, Loss 14.889811963891264 Accuracy 1.0 Time 3.078s\n",
            " correct:  tensor(25536) Total:  25536\n",
            "Epoch 4, Batch 399, Loss 14.889992776072413 Accuracy 1.0 Time 3.118s\n",
            " correct:  tensor(25600) Total:  25600\n",
            "Epoch 4, Batch 400, Loss 14.88805645942688 Accuracy 1.0 Time 3.138s\n",
            " correct:  tensor(25664) Total:  25664\n",
            "Epoch 4, Batch 401, Loss 14.888570714175255 Accuracy 1.0 Time 3.218s\n",
            " correct:  tensor(25728) Total:  25728\n",
            "Epoch 4, Batch 402, Loss 14.887792404611313 Accuracy 1.0 Time 3.074s\n",
            " correct:  tensor(25792) Total:  25792\n",
            "Epoch 4, Batch 403, Loss 14.888991767656123 Accuracy 1.0 Time 3.126s\n",
            " correct:  tensor(25856) Total:  25856\n",
            "Epoch 4, Batch 404, Loss 14.889275628741425 Accuracy 1.0 Time 3.092s\n",
            " correct:  tensor(25920) Total:  25920\n",
            "Epoch 4, Batch 405, Loss 14.887121224109038 Accuracy 1.0 Time 3.13s\n",
            " correct:  tensor(25984) Total:  25984\n",
            "Epoch 4, Batch 406, Loss 14.886991049855801 Accuracy 1.0 Time 3.147s\n",
            " correct:  tensor(26048) Total:  26048\n",
            "Epoch 4, Batch 407, Loss 14.8889511961316 Accuracy 1.0 Time 3.121s\n",
            " correct:  tensor(26112) Total:  26112\n",
            "Epoch 4, Batch 408, Loss 14.889051645409827 Accuracy 1.0 Time 3.1s\n",
            " correct:  tensor(26176) Total:  26176\n",
            "Epoch 4, Batch 409, Loss 14.888441591799113 Accuracy 1.0 Time 3.068s\n",
            " correct:  tensor(26240) Total:  26240\n",
            "Epoch 4, Batch 410, Loss 14.889086593069681 Accuracy 1.0 Time 3.109s\n",
            " correct:  tensor(26304) Total:  26304\n",
            "Epoch 4, Batch 411, Loss 14.888389896012281 Accuracy 1.0 Time 3.324s\n",
            " correct:  tensor(26368) Total:  26368\n",
            "Epoch 4, Batch 412, Loss 14.886526894800872 Accuracy 1.0 Time 3.333s\n",
            " correct:  tensor(26432) Total:  26432\n",
            "Epoch 4, Batch 413, Loss 14.886708395821708 Accuracy 1.0 Time 3.15s\n",
            " correct:  tensor(26496) Total:  26496\n",
            "Epoch 4, Batch 414, Loss 14.885766285053197 Accuracy 1.0 Time 3.121s\n",
            " correct:  tensor(26560) Total:  26560\n",
            "Epoch 4, Batch 415, Loss 14.884730028818888 Accuracy 1.0 Time 3.173s\n",
            " correct:  tensor(26624) Total:  26624\n",
            "Epoch 4, Batch 416, Loss 14.88456975038235 Accuracy 1.0 Time 3.113s\n",
            " correct:  tensor(26688) Total:  26688\n",
            "Epoch 4, Batch 417, Loss 14.883826633151486 Accuracy 1.0 Time 3.242s\n",
            " correct:  tensor(26752) Total:  26752\n",
            "Epoch 4, Batch 418, Loss 14.88552298614283 Accuracy 1.0 Time 3.329s\n",
            " correct:  tensor(26816) Total:  26816\n",
            "Epoch 4, Batch 419, Loss 14.887712002938573 Accuracy 1.0 Time 3.336s\n",
            " correct:  tensor(26880) Total:  26880\n",
            "Epoch 4, Batch 420, Loss 14.88715600059146 Accuracy 1.0 Time 3.409s\n",
            " correct:  tensor(26944) Total:  26944\n",
            "Epoch 4, Batch 421, Loss 14.88875295770423 Accuracy 1.0 Time 3.291s\n",
            " correct:  tensor(27008) Total:  27008\n",
            "Epoch 4, Batch 422, Loss 14.890350212983046 Accuracy 1.0 Time 3.165s\n",
            " correct:  tensor(27072) Total:  27072\n",
            "Epoch 4, Batch 423, Loss 14.89406955664885 Accuracy 1.0 Time 3.112s\n",
            " correct:  tensor(27136) Total:  27136\n",
            "Epoch 4, Batch 424, Loss 14.893788931504735 Accuracy 1.0 Time 3.228s\n",
            " correct:  tensor(27200) Total:  27200\n",
            "Epoch 4, Batch 425, Loss 14.8936238748887 Accuracy 1.0 Time 3.16s\n",
            " correct:  tensor(27264) Total:  27264\n",
            "Epoch 4, Batch 426, Loss 14.892959529805072 Accuracy 1.0 Time 3.164s\n",
            " correct:  tensor(27328) Total:  27328\n",
            "Epoch 4, Batch 427, Loss 14.891795406296884 Accuracy 1.0 Time 3.131s\n",
            " correct:  tensor(27392) Total:  27392\n",
            "Epoch 4, Batch 428, Loss 14.891355523439211 Accuracy 1.0 Time 3.192s\n",
            " correct:  tensor(27456) Total:  27456\n",
            "Epoch 4, Batch 429, Loss 14.890556259866639 Accuracy 1.0 Time 3.152s\n",
            " correct:  tensor(27520) Total:  27520\n",
            "Epoch 4, Batch 430, Loss 14.890649516083473 Accuracy 1.0 Time 3.201s\n",
            " correct:  tensor(27584) Total:  27584\n",
            "Epoch 4, Batch 431, Loss 14.89224384776954 Accuracy 1.0 Time 3.169s\n",
            " correct:  tensor(27648) Total:  27648\n",
            "Epoch 4, Batch 432, Loss 14.89245080947876 Accuracy 1.0 Time 3.133s\n",
            " correct:  tensor(27712) Total:  27712\n",
            "Epoch 4, Batch 433, Loss 14.891804882340288 Accuracy 1.0 Time 3.122s\n",
            " correct:  tensor(27776) Total:  27776\n",
            "Epoch 4, Batch 434, Loss 14.89124681762836 Accuracy 1.0 Time 3.188s\n",
            " correct:  tensor(27840) Total:  27840\n",
            "Epoch 4, Batch 435, Loss 14.890078156569908 Accuracy 1.0 Time 3.182s\n",
            " correct:  tensor(27904) Total:  27904\n",
            "Epoch 4, Batch 436, Loss 14.89084927532651 Accuracy 1.0 Time 3.148s\n",
            " correct:  tensor(27968) Total:  27968\n",
            "Epoch 4, Batch 437, Loss 14.887787847278865 Accuracy 1.0 Time 3.204s\n",
            " correct:  tensor(28032) Total:  28032\n",
            "Epoch 4, Batch 438, Loss 14.887107435426756 Accuracy 1.0 Time 3.203s\n",
            " correct:  tensor(28096) Total:  28096\n",
            "Epoch 4, Batch 439, Loss 14.886226228266478 Accuracy 1.0 Time 3.136s\n",
            " correct:  tensor(28160) Total:  28160\n",
            "Epoch 4, Batch 440, Loss 14.88408710523085 Accuracy 1.0 Time 3.21s\n",
            " correct:  tensor(28224) Total:  28224\n",
            "Epoch 4, Batch 441, Loss 14.884488901583786 Accuracy 1.0 Time 3.147s\n",
            " correct:  tensor(28288) Total:  28288\n",
            "Epoch 4, Batch 442, Loss 14.882493785064145 Accuracy 1.0 Time 3.175s\n",
            " correct:  tensor(28352) Total:  28352\n",
            "Epoch 4, Batch 443, Loss 14.88166842665145 Accuracy 1.0 Time 3.15s\n",
            " correct:  tensor(28416) Total:  28416\n",
            "Epoch 4, Batch 444, Loss 14.881644367097735 Accuracy 1.0 Time 3.194s\n",
            " correct:  tensor(28480) Total:  28480\n",
            "Epoch 4, Batch 445, Loss 14.88031611710452 Accuracy 1.0 Time 3.109s\n",
            " correct:  tensor(28544) Total:  28544\n",
            "Epoch 4, Batch 446, Loss 14.880356786496971 Accuracy 1.0 Time 3.115s\n",
            " correct:  tensor(28608) Total:  28608\n",
            "Epoch 4, Batch 447, Loss 14.88120900864569 Accuracy 1.0 Time 3.098s\n",
            " correct:  tensor(28672) Total:  28672\n",
            "Epoch 4, Batch 448, Loss 14.879821432488304 Accuracy 1.0 Time 3.145s\n",
            " correct:  tensor(28736) Total:  28736\n",
            "Epoch 4, Batch 449, Loss 14.879203906834524 Accuracy 1.0 Time 3.194s\n",
            " correct:  tensor(28800) Total:  28800\n",
            "Epoch 4, Batch 450, Loss 14.880479397243924 Accuracy 1.0 Time 3.171s\n",
            " correct:  tensor(28864) Total:  28864\n",
            "Epoch 4, Batch 451, Loss 14.878397730131635 Accuracy 1.0 Time 3.105s\n",
            " correct:  tensor(28928) Total:  28928\n",
            "Epoch 4, Batch 452, Loss 14.876461199954548 Accuracy 1.0 Time 3.14s\n",
            " correct:  tensor(28992) Total:  28992\n",
            "Epoch 4, Batch 453, Loss 14.875447662724039 Accuracy 1.0 Time 3.112s\n",
            " correct:  tensor(29056) Total:  29056\n",
            "Epoch 4, Batch 454, Loss 14.875211885847184 Accuracy 1.0 Time 3.165s\n",
            " correct:  tensor(29120) Total:  29120\n",
            "Epoch 4, Batch 455, Loss 14.875911081754245 Accuracy 1.0 Time 3.195s\n",
            " correct:  tensor(29184) Total:  29184\n",
            "Epoch 4, Batch 456, Loss 14.875535151414704 Accuracy 1.0 Time 3.136s\n",
            " correct:  tensor(29248) Total:  29248\n",
            "Epoch 4, Batch 457, Loss 14.874009491317382 Accuracy 1.0 Time 3.127s\n",
            " correct:  tensor(29312) Total:  29312\n",
            "Epoch 4, Batch 458, Loss 14.874064474647222 Accuracy 1.0 Time 3.183s\n",
            " correct:  tensor(29376) Total:  29376\n",
            "Epoch 4, Batch 459, Loss 14.874551428169466 Accuracy 1.0 Time 3.075s\n",
            " correct:  tensor(29440) Total:  29440\n",
            "Epoch 4, Batch 460, Loss 14.874788889677628 Accuracy 1.0 Time 3.125s\n",
            " correct:  tensor(29504) Total:  29504\n",
            "Epoch 4, Batch 461, Loss 14.874048022024027 Accuracy 1.0 Time 3.122s\n",
            " correct:  tensor(29568) Total:  29568\n",
            "Epoch 4, Batch 462, Loss 14.874474744260054 Accuracy 1.0 Time 3.121s\n",
            " correct:  tensor(29632) Total:  29632\n",
            "Epoch 4, Batch 463, Loss 14.872729851411691 Accuracy 1.0 Time 3.09s\n",
            " correct:  tensor(29696) Total:  29696\n",
            "Epoch 4, Batch 464, Loss 14.872110763500476 Accuracy 1.0 Time 3.153s\n",
            " correct:  tensor(29760) Total:  29760\n",
            "Epoch 4, Batch 465, Loss 14.87185845939062 Accuracy 1.0 Time 3.105s\n",
            " correct:  tensor(29824) Total:  29824\n",
            "Epoch 4, Batch 466, Loss 14.870996548894137 Accuracy 1.0 Time 3.14s\n",
            " correct:  tensor(29888) Total:  29888\n",
            "Epoch 4, Batch 467, Loss 14.870478313535898 Accuracy 1.0 Time 3.155s\n",
            " correct:  tensor(29952) Total:  29952\n",
            "Epoch 4, Batch 468, Loss 14.869382817521055 Accuracy 1.0 Time 3.148s\n",
            " correct:  tensor(30016) Total:  30016\n",
            "Epoch 4, Batch 469, Loss 14.867830813312327 Accuracy 1.0 Time 3.064s\n",
            " correct:  tensor(30080) Total:  30080\n",
            "Epoch 4, Batch 470, Loss 14.867858079139223 Accuracy 1.0 Time 3.12s\n",
            " correct:  tensor(30144) Total:  30144\n",
            "Epoch 4, Batch 471, Loss 14.868107483898251 Accuracy 1.0 Time 3.075s\n",
            " correct:  tensor(30208) Total:  30208\n",
            "Epoch 4, Batch 472, Loss 14.868030705694425 Accuracy 1.0 Time 3.125s\n",
            " correct:  tensor(30272) Total:  30272\n",
            "Epoch 4, Batch 473, Loss 14.86726575371579 Accuracy 1.0 Time 3.168s\n",
            " correct:  tensor(30336) Total:  30336\n",
            "Epoch 4, Batch 474, Loss 14.867068974780635 Accuracy 1.0 Time 3.19s\n",
            " correct:  tensor(30400) Total:  30400\n",
            "Epoch 4, Batch 475, Loss 14.867524590743216 Accuracy 1.0 Time 3.153s\n",
            " correct:  tensor(30464) Total:  30464\n",
            "Epoch 4, Batch 476, Loss 14.868160372020817 Accuracy 1.0 Time 3.168s\n",
            " correct:  tensor(30528) Total:  30528\n",
            "Epoch 4, Batch 477, Loss 14.86858681212931 Accuracy 1.0 Time 3.121s\n",
            " correct:  tensor(30592) Total:  30592\n",
            "Epoch 4, Batch 478, Loss 14.868987620126253 Accuracy 1.0 Time 3.113s\n",
            " correct:  tensor(30656) Total:  30656\n",
            "Epoch 4, Batch 479, Loss 14.86953873923029 Accuracy 1.0 Time 3.131s\n",
            " correct:  tensor(30720) Total:  30720\n",
            "Epoch 4, Batch 480, Loss 14.868441659212113 Accuracy 1.0 Time 3.131s\n",
            " correct:  tensor(30784) Total:  30784\n",
            "Epoch 4, Batch 481, Loss 14.868837542940327 Accuracy 1.0 Time 3.074s\n",
            " correct:  tensor(30848) Total:  30848\n",
            "Epoch 4, Batch 482, Loss 14.869780239722541 Accuracy 1.0 Time 3.137s\n",
            " correct:  tensor(30912) Total:  30912\n",
            "Epoch 4, Batch 483, Loss 14.869540192819283 Accuracy 1.0 Time 3.126s\n",
            " correct:  tensor(30976) Total:  30976\n",
            "Epoch 4, Batch 484, Loss 14.867590488481127 Accuracy 1.0 Time 3.187s\n",
            " correct:  tensor(31040) Total:  31040\n",
            "Epoch 4, Batch 485, Loss 14.866242564093206 Accuracy 1.0 Time 3.113s\n",
            " correct:  tensor(31104) Total:  31104\n",
            "Epoch 4, Batch 486, Loss 14.865105532814937 Accuracy 1.0 Time 3.172s\n",
            " correct:  tensor(31168) Total:  31168\n",
            "Epoch 4, Batch 487, Loss 14.864849151282339 Accuracy 1.0 Time 3.117s\n",
            " correct:  tensor(31232) Total:  31232\n",
            "Epoch 4, Batch 488, Loss 14.864249239202406 Accuracy 1.0 Time 3.091s\n",
            " correct:  tensor(31296) Total:  31296\n",
            "Epoch 4, Batch 489, Loss 14.865563412141702 Accuracy 1.0 Time 3.191s\n",
            " correct:  tensor(31360) Total:  31360\n",
            "Epoch 4, Batch 490, Loss 14.86636625212066 Accuracy 1.0 Time 3.149s\n",
            " correct:  tensor(31424) Total:  31424\n",
            "Epoch 4, Batch 491, Loss 14.866353668166274 Accuracy 1.0 Time 3.156s\n",
            " correct:  tensor(31488) Total:  31488\n",
            "Epoch 4, Batch 492, Loss 14.866073426192369 Accuracy 1.0 Time 3.136s\n",
            " correct:  tensor(31552) Total:  31552\n",
            "Epoch 4, Batch 493, Loss 14.865452242187748 Accuracy 1.0 Time 3.174s\n",
            " correct:  tensor(31616) Total:  31616\n",
            "Epoch 4, Batch 494, Loss 14.866175062743276 Accuracy 1.0 Time 3.129s\n",
            " correct:  tensor(31680) Total:  31680\n",
            "Epoch 4, Batch 495, Loss 14.865988041656186 Accuracy 1.0 Time 3.118s\n",
            " correct:  tensor(31744) Total:  31744\n",
            "Epoch 4, Batch 496, Loss 14.86722364541023 Accuracy 1.0 Time 3.154s\n",
            " correct:  tensor(31808) Total:  31808\n",
            "Epoch 4, Batch 497, Loss 14.8674487261705 Accuracy 1.0 Time 3.2s\n",
            " correct:  tensor(31872) Total:  31872\n",
            "Epoch 4, Batch 498, Loss 14.866105213701486 Accuracy 1.0 Time 3.197s\n",
            " correct:  tensor(31936) Total:  31936\n",
            "Epoch 4, Batch 499, Loss 14.865912575043275 Accuracy 1.0 Time 3.139s\n",
            " correct:  tensor(32000) Total:  32000\n",
            "Epoch 4, Batch 500, Loss 14.864587842941285 Accuracy 1.0 Time 3.187s\n",
            " correct:  tensor(32064) Total:  32064\n",
            "Epoch 4, Batch 501, Loss 14.86279926947253 Accuracy 1.0 Time 3.127s\n",
            " correct:  tensor(32128) Total:  32128\n",
            "Epoch 4, Batch 502, Loss 14.863045696243347 Accuracy 1.0 Time 3.155s\n",
            " correct:  tensor(32192) Total:  32192\n",
            "Epoch 4, Batch 503, Loss 14.862552955657778 Accuracy 1.0 Time 3.089s\n",
            " correct:  tensor(32256) Total:  32256\n",
            "Epoch 4, Batch 504, Loss 14.862616917443654 Accuracy 1.0 Time 3.135s\n",
            " correct:  tensor(32320) Total:  32320\n",
            "Epoch 4, Batch 505, Loss 14.863334034457065 Accuracy 1.0 Time 3.146s\n",
            " correct:  tensor(32384) Total:  32384\n",
            "Epoch 4, Batch 506, Loss 14.862070132621191 Accuracy 1.0 Time 3.16s\n",
            " correct:  tensor(32448) Total:  32448\n",
            "Epoch 4, Batch 507, Loss 14.861561918164616 Accuracy 1.0 Time 3.249s\n",
            " correct:  tensor(32512) Total:  32512\n",
            "Epoch 4, Batch 508, Loss 14.86104527420885 Accuracy 1.0 Time 3.306s\n",
            " correct:  tensor(32576) Total:  32576\n",
            "Epoch 4, Batch 509, Loss 14.861096998563216 Accuracy 1.0 Time 3.217s\n",
            " correct:  tensor(32640) Total:  32640\n",
            "Epoch 4, Batch 510, Loss 14.86080706540276 Accuracy 1.0 Time 3.097s\n",
            " correct:  tensor(32704) Total:  32704\n",
            "Epoch 4, Batch 511, Loss 14.85876849299541 Accuracy 1.0 Time 3.195s\n",
            " correct:  tensor(32768) Total:  32768\n",
            "Epoch 4, Batch 512, Loss 14.858028948307037 Accuracy 1.0 Time 3.235s\n",
            " correct:  tensor(32832) Total:  32832\n",
            "Epoch 4, Batch 513, Loss 14.857013888294004 Accuracy 1.0 Time 3.165s\n",
            " correct:  tensor(32896) Total:  32896\n",
            "Epoch 4, Batch 514, Loss 14.857260344093412 Accuracy 1.0 Time 3.114s\n",
            " correct:  tensor(32960) Total:  32960\n",
            "Epoch 4, Batch 515, Loss 14.857286032889654 Accuracy 1.0 Time 3.164s\n",
            " correct:  tensor(33024) Total:  33024\n",
            "Epoch 4, Batch 516, Loss 14.857136680174243 Accuracy 1.0 Time 3.363s\n",
            " correct:  tensor(33088) Total:  33088\n",
            "Epoch 4, Batch 517, Loss 14.858259795021043 Accuracy 1.0 Time 3.313s\n",
            " correct:  tensor(33152) Total:  33152\n",
            "Epoch 4, Batch 518, Loss 14.859313789942089 Accuracy 1.0 Time 3.327s\n",
            " correct:  tensor(33216) Total:  33216\n",
            "Epoch 4, Batch 519, Loss 14.858247722045075 Accuracy 1.0 Time 3.245s\n",
            " correct:  tensor(33280) Total:  33280\n",
            "Epoch 4, Batch 520, Loss 14.85792372410114 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(33344) Total:  33344\n",
            "Epoch 4, Batch 521, Loss 14.858745865995733 Accuracy 1.0 Time 3.136s\n",
            " correct:  tensor(33408) Total:  33408\n",
            "Epoch 4, Batch 522, Loss 14.859738271355173 Accuracy 1.0 Time 3.098s\n",
            " correct:  tensor(33472) Total:  33472\n",
            "Epoch 4, Batch 523, Loss 14.860468866264387 Accuracy 1.0 Time 3.146s\n",
            " correct:  tensor(33536) Total:  33536\n",
            "Epoch 4, Batch 524, Loss 14.860284674258633 Accuracy 1.0 Time 3.154s\n",
            " correct:  tensor(33600) Total:  33600\n",
            "Epoch 4, Batch 525, Loss 14.85891576312837 Accuracy 1.0 Time 3.135s\n",
            " correct:  tensor(33664) Total:  33664\n",
            "Epoch 4, Batch 526, Loss 14.859368405867892 Accuracy 1.0 Time 3.151s\n",
            " correct:  tensor(33728) Total:  33728\n",
            "Epoch 4, Batch 527, Loss 14.859250177242272 Accuracy 1.0 Time 3.162s\n",
            " correct:  tensor(33792) Total:  33792\n",
            "Epoch 4, Batch 528, Loss 14.860030846162276 Accuracy 1.0 Time 3.115s\n",
            " correct:  tensor(33856) Total:  33856\n",
            "Epoch 4, Batch 529, Loss 14.858878052752951 Accuracy 1.0 Time 3.171s\n",
            " correct:  tensor(33920) Total:  33920\n",
            "Epoch 4, Batch 530, Loss 14.858151961272618 Accuracy 1.0 Time 3.248s\n",
            " correct:  tensor(33984) Total:  33984\n",
            "Epoch 4, Batch 531, Loss 14.859705968091717 Accuracy 1.0 Time 4.165s\n",
            " correct:  tensor(34048) Total:  34048\n",
            "Epoch 4, Batch 532, Loss 14.858082281915765 Accuracy 1.0 Time 3.229s\n",
            " correct:  tensor(34112) Total:  34112\n",
            "Epoch 4, Batch 533, Loss 14.857256078809556 Accuracy 1.0 Time 3.164s\n",
            " correct:  tensor(34176) Total:  34176\n",
            "Epoch 4, Batch 534, Loss 14.857690439688579 Accuracy 1.0 Time 3.159s\n",
            " correct:  tensor(34240) Total:  34240\n",
            "Epoch 4, Batch 535, Loss 14.857785363954918 Accuracy 1.0 Time 3.13s\n",
            " correct:  tensor(34304) Total:  34304\n",
            "Epoch 4, Batch 536, Loss 14.857109352723876 Accuracy 1.0 Time 3.19s\n",
            " correct:  tensor(34368) Total:  34368\n",
            "Epoch 4, Batch 537, Loss 14.856137552740854 Accuracy 1.0 Time 3.21s\n",
            " correct:  tensor(34432) Total:  34432\n",
            "Epoch 4, Batch 538, Loss 14.854863186308 Accuracy 1.0 Time 3.167s\n",
            " correct:  tensor(34496) Total:  34496\n",
            "Epoch 4, Batch 539, Loss 14.854765550546169 Accuracy 1.0 Time 3.177s\n",
            " correct:  tensor(34560) Total:  34560\n",
            "Epoch 4, Batch 540, Loss 14.85404887729221 Accuracy 1.0 Time 3.146s\n",
            " correct:  tensor(34624) Total:  34624\n",
            "Epoch 4, Batch 541, Loss 14.85388836358259 Accuracy 1.0 Time 3.174s\n",
            " correct:  tensor(34688) Total:  34688\n",
            "Epoch 4, Batch 542, Loss 14.853346719073194 Accuracy 1.0 Time 3.149s\n",
            " correct:  tensor(34752) Total:  34752\n",
            "Epoch 4, Batch 543, Loss 14.853919917926824 Accuracy 1.0 Time 3.269s\n",
            " correct:  tensor(34816) Total:  34816\n",
            "Epoch 4, Batch 544, Loss 14.853603759232689 Accuracy 1.0 Time 3.153s\n",
            " correct:  tensor(34880) Total:  34880\n",
            "Epoch 4, Batch 545, Loss 14.853459839427144 Accuracy 1.0 Time 3.108s\n",
            " correct:  tensor(34944) Total:  34944\n",
            "Epoch 4, Batch 546, Loss 14.854783489590599 Accuracy 1.0 Time 3.117s\n",
            " correct:  tensor(35008) Total:  35008\n",
            "Epoch 4, Batch 547, Loss 14.855633915233438 Accuracy 1.0 Time 3.096s\n",
            " correct:  tensor(35072) Total:  35072\n",
            "Epoch 4, Batch 548, Loss 14.856848433069938 Accuracy 1.0 Time 3.112s\n",
            " correct:  tensor(35136) Total:  35136\n",
            "Epoch 4, Batch 549, Loss 14.85561419836159 Accuracy 1.0 Time 3.2s\n",
            " correct:  tensor(35200) Total:  35200\n",
            "Epoch 4, Batch 550, Loss 14.855700829245828 Accuracy 1.0 Time 3.129s\n",
            " correct:  tensor(35264) Total:  35264\n",
            "Epoch 4, Batch 551, Loss 14.85620649583543 Accuracy 1.0 Time 3.119s\n",
            " correct:  tensor(35328) Total:  35328\n",
            "Epoch 4, Batch 552, Loss 14.856264223223148 Accuracy 1.0 Time 3.129s\n",
            " correct:  tensor(35392) Total:  35392\n",
            "Epoch 4, Batch 553, Loss 14.855869386338412 Accuracy 1.0 Time 3.147s\n",
            " correct:  tensor(35456) Total:  35456\n",
            "Epoch 4, Batch 554, Loss 14.854861206096002 Accuracy 1.0 Time 3.139s\n",
            " correct:  tensor(35520) Total:  35520\n",
            "Epoch 4, Batch 555, Loss 14.853666307260324 Accuracy 1.0 Time 3.089s\n",
            " correct:  tensor(35584) Total:  35584\n",
            "Epoch 4, Batch 556, Loss 14.853983244449973 Accuracy 1.0 Time 3.13s\n",
            " correct:  tensor(35648) Total:  35648\n",
            "Epoch 4, Batch 557, Loss 14.854027253293049 Accuracy 1.0 Time 3.152s\n",
            " correct:  tensor(35712) Total:  35712\n",
            "Epoch 4, Batch 558, Loss 14.85309537935428 Accuracy 1.0 Time 3.108s\n",
            " correct:  tensor(35776) Total:  35776\n",
            "Epoch 4, Batch 559, Loss 14.853255921911469 Accuracy 1.0 Time 3.158s\n",
            " correct:  tensor(35840) Total:  35840\n",
            "Epoch 4, Batch 560, Loss 14.854561001913888 Accuracy 1.0 Time 3.127s\n",
            " correct:  tensor(35904) Total:  35904\n",
            "Epoch 4, Batch 561, Loss 14.853352854383608 Accuracy 1.0 Time 3.183s\n",
            " correct:  tensor(35968) Total:  35968\n",
            "Epoch 4, Batch 562, Loss 14.85293797156989 Accuracy 1.0 Time 3.178s\n",
            " correct:  tensor(36032) Total:  36032\n",
            "Epoch 4, Batch 563, Loss 14.854046806349 Accuracy 1.0 Time 3.189s\n",
            " correct:  tensor(36096) Total:  36096\n",
            "Epoch 4, Batch 564, Loss 14.854035169520277 Accuracy 1.0 Time 3.151s\n",
            " correct:  tensor(36160) Total:  36160\n",
            "Epoch 4, Batch 565, Loss 14.854862513584374 Accuracy 1.0 Time 3.196s\n",
            " correct:  tensor(36224) Total:  36224\n",
            "Epoch 4, Batch 566, Loss 14.853712248717938 Accuracy 1.0 Time 3.149s\n",
            " correct:  tensor(36288) Total:  36288\n",
            "Epoch 4, Batch 567, Loss 14.85105663380295 Accuracy 1.0 Time 3.162s\n",
            " correct:  tensor(36352) Total:  36352\n",
            "Epoch 4, Batch 568, Loss 14.850887332164065 Accuracy 1.0 Time 3.189s\n",
            " correct:  tensor(36416) Total:  36416\n",
            "Epoch 4, Batch 569, Loss 14.848831300785756 Accuracy 1.0 Time 3.173s\n",
            " correct:  tensor(36480) Total:  36480\n",
            "Epoch 4, Batch 570, Loss 14.847510973612467 Accuracy 1.0 Time 3.197s\n",
            " correct:  tensor(36544) Total:  36544\n",
            "Epoch 4, Batch 571, Loss 14.847783591409073 Accuracy 1.0 Time 3.17s\n",
            " correct:  tensor(36608) Total:  36608\n",
            "Epoch 4, Batch 572, Loss 14.847598257598344 Accuracy 1.0 Time 3.185s\n",
            " correct:  tensor(36672) Total:  36672\n",
            "Epoch 4, Batch 573, Loss 14.847234569502959 Accuracy 1.0 Time 3.186s\n",
            " correct:  tensor(36736) Total:  36736\n",
            "Epoch 4, Batch 574, Loss 14.846812814785629 Accuracy 1.0 Time 3.144s\n",
            " correct:  tensor(36800) Total:  36800\n",
            "Epoch 4, Batch 575, Loss 14.847630689869757 Accuracy 1.0 Time 3.232s\n",
            " correct:  tensor(36864) Total:  36864\n",
            "Epoch 4, Batch 576, Loss 14.846597111887402 Accuracy 1.0 Time 3.188s\n",
            " correct:  tensor(36928) Total:  36928\n",
            "Epoch 4, Batch 577, Loss 14.845605628742502 Accuracy 1.0 Time 3.188s\n",
            " correct:  tensor(36992) Total:  36992\n",
            "Epoch 4, Batch 578, Loss 14.845374394453108 Accuracy 1.0 Time 3.205s\n",
            " correct:  tensor(37056) Total:  37056\n",
            "Epoch 4, Batch 579, Loss 14.845668440243946 Accuracy 1.0 Time 3.212s\n",
            " correct:  tensor(37120) Total:  37120\n",
            "Epoch 4, Batch 580, Loss 14.844614116076766 Accuracy 1.0 Time 3.19s\n",
            " correct:  tensor(37184) Total:  37184\n",
            "Epoch 4, Batch 581, Loss 14.844381859298418 Accuracy 1.0 Time 3.217s\n",
            " correct:  tensor(37248) Total:  37248\n",
            "Epoch 4, Batch 582, Loss 14.845594737128295 Accuracy 1.0 Time 3.236s\n",
            " correct:  tensor(37312) Total:  37312\n",
            "Epoch 4, Batch 583, Loss 14.845570842356869 Accuracy 1.0 Time 3.154s\n",
            " correct:  tensor(37376) Total:  37376\n",
            "Epoch 4, Batch 584, Loss 14.844571247492752 Accuracy 1.0 Time 3.21s\n",
            " correct:  tensor(37440) Total:  37440\n",
            "Epoch 4, Batch 585, Loss 14.845238220997345 Accuracy 1.0 Time 3.278s\n",
            " correct:  tensor(37504) Total:  37504\n",
            "Epoch 4, Batch 586, Loss 14.845523863522265 Accuracy 1.0 Time 3.213s\n",
            " correct:  tensor(37568) Total:  37568\n",
            "Epoch 4, Batch 587, Loss 14.845322935504036 Accuracy 1.0 Time 3.246s\n",
            " correct:  tensor(37632) Total:  37632\n",
            "Epoch 4, Batch 588, Loss 14.845008702505202 Accuracy 1.0 Time 3.218s\n",
            " correct:  tensor(37696) Total:  37696\n",
            "Epoch 4, Batch 589, Loss 14.846270375016994 Accuracy 1.0 Time 3.164s\n",
            " correct:  tensor(37760) Total:  37760\n",
            "Epoch 4, Batch 590, Loss 14.844508508908547 Accuracy 1.0 Time 3.2s\n",
            " correct:  tensor(37824) Total:  37824\n",
            "Epoch 4, Batch 591, Loss 14.845340525438338 Accuracy 1.0 Time 3.233s\n",
            " correct:  tensor(37888) Total:  37888\n",
            "Epoch 4, Batch 592, Loss 14.845833723609513 Accuracy 1.0 Time 3.207s\n",
            " correct:  tensor(37952) Total:  37952\n",
            "Epoch 4, Batch 593, Loss 14.84751904633958 Accuracy 1.0 Time 3.177s\n",
            " correct:  tensor(38016) Total:  38016\n",
            "Epoch 4, Batch 594, Loss 14.846846769955825 Accuracy 1.0 Time 3.187s\n",
            " correct:  tensor(38080) Total:  38080\n",
            "Epoch 4, Batch 595, Loss 14.845372260919138 Accuracy 1.0 Time 3.187s\n",
            " correct:  tensor(38144) Total:  38144\n",
            "Epoch 4, Batch 596, Loss 14.845379866209607 Accuracy 1.0 Time 3.114s\n",
            " correct:  tensor(38208) Total:  38208\n",
            "Epoch 4, Batch 597, Loss 14.846101834346701 Accuracy 1.0 Time 3.24s\n",
            " correct:  tensor(38272) Total:  38272\n",
            "Epoch 4, Batch 598, Loss 14.844972087388054 Accuracy 1.0 Time 3.129s\n",
            " correct:  tensor(38336) Total:  38336\n",
            "Epoch 4, Batch 599, Loss 14.84507955573437 Accuracy 1.0 Time 3.165s\n",
            " correct:  tensor(38400) Total:  38400\n",
            "Epoch 4, Batch 600, Loss 14.84548234462738 Accuracy 1.0 Time 3.221s\n",
            " correct:  tensor(38464) Total:  38464\n",
            "Epoch 4, Batch 601, Loss 14.84536184447379 Accuracy 1.0 Time 3.177s\n",
            " correct:  tensor(38528) Total:  38528\n",
            "Epoch 4, Batch 602, Loss 14.844466605455773 Accuracy 1.0 Time 3.202s\n",
            " correct:  tensor(38592) Total:  38592\n",
            "Epoch 4, Batch 603, Loss 14.844484776802126 Accuracy 1.0 Time 3.358s\n",
            " correct:  tensor(38656) Total:  38656\n",
            "Epoch 4, Batch 604, Loss 14.843978147633027 Accuracy 1.0 Time 3.301s\n",
            " correct:  tensor(38720) Total:  38720\n",
            "Epoch 4, Batch 605, Loss 14.844710351612942 Accuracy 1.0 Time 3.26s\n",
            " correct:  tensor(38784) Total:  38784\n",
            "Epoch 4, Batch 606, Loss 14.845718124125263 Accuracy 1.0 Time 3.178s\n",
            " correct:  tensor(38848) Total:  38848\n",
            "Epoch 4, Batch 607, Loss 14.846306392349168 Accuracy 1.0 Time 3.176s\n",
            " correct:  tensor(38912) Total:  38912\n",
            "Epoch 4, Batch 608, Loss 14.845937981417304 Accuracy 1.0 Time 3.2s\n",
            " correct:  tensor(38976) Total:  38976\n",
            "Epoch 4, Batch 609, Loss 14.844017702174696 Accuracy 1.0 Time 3.178s\n",
            " correct:  tensor(39040) Total:  39040\n",
            "Epoch 4, Batch 610, Loss 14.844062759837167 Accuracy 1.0 Time 3.29s\n",
            " correct:  tensor(39104) Total:  39104\n",
            "Epoch 4, Batch 611, Loss 14.843268976664191 Accuracy 1.0 Time 3.153s\n",
            " correct:  tensor(39168) Total:  39168\n",
            "Epoch 4, Batch 612, Loss 14.843420843672908 Accuracy 1.0 Time 3.167s\n",
            " correct:  tensor(39232) Total:  39232\n",
            "Epoch 4, Batch 613, Loss 14.842430324772248 Accuracy 1.0 Time 3.393s\n",
            " correct:  tensor(39296) Total:  39296\n",
            "Epoch 4, Batch 614, Loss 14.841227478623779 Accuracy 1.0 Time 3.419s\n",
            " correct:  tensor(39360) Total:  39360\n",
            "Epoch 4, Batch 615, Loss 14.842170324558165 Accuracy 1.0 Time 3.343s\n",
            " correct:  tensor(39424) Total:  39424\n",
            "Epoch 4, Batch 616, Loss 14.842395254543849 Accuracy 1.0 Time 3.243s\n",
            " correct:  tensor(39488) Total:  39488\n",
            "Epoch 4, Batch 617, Loss 14.842822345381231 Accuracy 1.0 Time 3.148s\n",
            " correct:  tensor(39552) Total:  39552\n",
            "Epoch 4, Batch 618, Loss 14.842540793434315 Accuracy 1.0 Time 3.157s\n",
            " correct:  tensor(39616) Total:  39616\n",
            "Epoch 4, Batch 619, Loss 14.841818228676939 Accuracy 1.0 Time 3.142s\n",
            " correct:  tensor(39680) Total:  39680\n",
            "Epoch 4, Batch 620, Loss 14.84211534223249 Accuracy 1.0 Time 3.16s\n",
            " correct:  tensor(39744) Total:  39744\n",
            "Epoch 4, Batch 621, Loss 14.841627325220768 Accuracy 1.0 Time 3.139s\n",
            " correct:  tensor(39808) Total:  39808\n",
            "Epoch 4, Batch 622, Loss 14.840117607852653 Accuracy 1.0 Time 3.132s\n",
            " correct:  tensor(39872) Total:  39872\n",
            "Epoch 4, Batch 623, Loss 14.84029667641531 Accuracy 1.0 Time 3.138s\n",
            " correct:  tensor(39936) Total:  39936\n",
            "Epoch 4, Batch 624, Loss 14.840596133317703 Accuracy 1.0 Time 3.206s\n",
            " correct:  tensor(40000) Total:  40000\n",
            "Epoch 4, Batch 625, Loss 14.840594694519043 Accuracy 1.0 Time 3.205s\n",
            " correct:  tensor(40064) Total:  40064\n",
            "Epoch 4, Batch 626, Loss 14.840785923857277 Accuracy 1.0 Time 3.153s\n",
            " correct:  tensor(40128) Total:  40128\n",
            "Epoch 4, Batch 627, Loss 14.84040864156574 Accuracy 1.0 Time 3.183s\n",
            " correct:  tensor(40192) Total:  40192\n",
            "Epoch 4, Batch 628, Loss 14.840693574042836 Accuracy 1.0 Time 3.151s\n",
            " correct:  tensor(40256) Total:  40256\n",
            "Epoch 4, Batch 629, Loss 14.840974959356796 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(40320) Total:  40320\n",
            "Epoch 4, Batch 630, Loss 14.840825963398768 Accuracy 1.0 Time 3.144s\n",
            " correct:  tensor(40384) Total:  40384\n",
            "Epoch 4, Batch 631, Loss 14.839759852353442 Accuracy 1.0 Time 3.105s\n",
            " correct:  tensor(40448) Total:  40448\n",
            "Epoch 4, Batch 632, Loss 14.837818931929673 Accuracy 1.0 Time 3.133s\n",
            " correct:  tensor(40512) Total:  40512\n",
            "Epoch 4, Batch 633, Loss 14.837832728080087 Accuracy 1.0 Time 3.193s\n",
            " correct:  tensor(40576) Total:  40576\n",
            "Epoch 4, Batch 634, Loss 14.837492987936603 Accuracy 1.0 Time 3.147s\n",
            " correct:  tensor(40640) Total:  40640\n",
            "Epoch 4, Batch 635, Loss 14.838924996683916 Accuracy 1.0 Time 3.153s\n",
            " correct:  tensor(40704) Total:  40704\n",
            "Epoch 4, Batch 636, Loss 14.8386166050749 Accuracy 1.0 Time 3.135s\n",
            " correct:  tensor(40768) Total:  40768\n",
            "Epoch 4, Batch 637, Loss 14.837568607959119 Accuracy 1.0 Time 3.267s\n",
            " correct:  tensor(40832) Total:  40832\n",
            "Epoch 4, Batch 638, Loss 14.836287737640093 Accuracy 1.0 Time 3.148s\n",
            " correct:  tensor(40896) Total:  40896\n",
            "Epoch 4, Batch 639, Loss 14.83559180574611 Accuracy 1.0 Time 3.165s\n",
            " correct:  tensor(40960) Total:  40960\n",
            "Epoch 4, Batch 640, Loss 14.836591257154941 Accuracy 1.0 Time 3.134s\n",
            " correct:  tensor(41024) Total:  41024\n",
            "Epoch 4, Batch 641, Loss 14.837368187034 Accuracy 1.0 Time 3.194s\n",
            " correct:  tensor(41088) Total:  41088\n",
            "Epoch 4, Batch 642, Loss 14.836305245432156 Accuracy 1.0 Time 3.153s\n",
            " correct:  tensor(41152) Total:  41152\n",
            "Epoch 4, Batch 643, Loss 14.836311834225572 Accuracy 1.0 Time 3.228s\n",
            " correct:  tensor(41216) Total:  41216\n",
            "Epoch 4, Batch 644, Loss 14.835709069826589 Accuracy 1.0 Time 3.172s\n",
            " correct:  tensor(41280) Total:  41280\n",
            "Epoch 4, Batch 645, Loss 14.834696280309396 Accuracy 1.0 Time 3.092s\n",
            " correct:  tensor(41344) Total:  41344\n",
            "Epoch 4, Batch 646, Loss 14.834514082031722 Accuracy 1.0 Time 3.186s\n",
            " correct:  tensor(41408) Total:  41408\n",
            "Epoch 4, Batch 647, Loss 14.833832873443178 Accuracy 1.0 Time 3.12s\n",
            " correct:  tensor(41472) Total:  41472\n",
            "Epoch 4, Batch 648, Loss 14.833373459768884 Accuracy 1.0 Time 3.094s\n",
            " correct:  tensor(41536) Total:  41536\n",
            "Epoch 4, Batch 649, Loss 14.833653001829362 Accuracy 1.0 Time 3.191s\n",
            " correct:  tensor(41600) Total:  41600\n",
            "Epoch 4, Batch 650, Loss 14.83341031881479 Accuracy 1.0 Time 3.246s\n",
            " correct:  tensor(41664) Total:  41664\n",
            "Epoch 4, Batch 651, Loss 14.83375041422573 Accuracy 1.0 Time 3.159s\n",
            " correct:  tensor(41728) Total:  41728\n",
            "Epoch 4, Batch 652, Loss 14.834180883103354 Accuracy 1.0 Time 3.194s\n",
            " correct:  tensor(41792) Total:  41792\n",
            "Epoch 4, Batch 653, Loss 14.833883481222857 Accuracy 1.0 Time 3.157s\n",
            " correct:  tensor(41856) Total:  41856\n",
            "Epoch 4, Batch 654, Loss 14.833149096287718 Accuracy 1.0 Time 3.179s\n",
            " correct:  tensor(41920) Total:  41920\n",
            "Epoch 4, Batch 655, Loss 14.833246153911562 Accuracy 1.0 Time 3.16s\n",
            " correct:  tensor(41984) Total:  41984\n",
            "Epoch 4, Batch 656, Loss 14.83358164822183 Accuracy 1.0 Time 3.183s\n",
            " correct:  tensor(42048) Total:  42048\n",
            "Epoch 4, Batch 657, Loss 14.83426847414339 Accuracy 1.0 Time 3.164s\n",
            " correct:  tensor(42112) Total:  42112\n",
            "Epoch 4, Batch 658, Loss 14.83435866897954 Accuracy 1.0 Time 3.244s\n",
            " correct:  tensor(42176) Total:  42176\n",
            "Epoch 4, Batch 659, Loss 14.833879653166568 Accuracy 1.0 Time 3.199s\n",
            " correct:  tensor(42240) Total:  42240\n",
            "Epoch 4, Batch 660, Loss 14.834323679317128 Accuracy 1.0 Time 3.156s\n",
            " correct:  tensor(42304) Total:  42304\n",
            "Epoch 4, Batch 661, Loss 14.83420549182055 Accuracy 1.0 Time 3.201s\n",
            " correct:  tensor(42368) Total:  42368\n",
            "Epoch 4, Batch 662, Loss 14.83404771510928 Accuracy 1.0 Time 3.178s\n",
            " correct:  tensor(42432) Total:  42432\n",
            "Epoch 4, Batch 663, Loss 14.833640607773448 Accuracy 1.0 Time 3.205s\n",
            " correct:  tensor(42496) Total:  42496\n",
            "Epoch 4, Batch 664, Loss 14.83287531352905 Accuracy 1.0 Time 3.151s\n",
            " correct:  tensor(42560) Total:  42560\n",
            "Epoch 4, Batch 665, Loss 14.832087166865069 Accuracy 1.0 Time 3.201s\n",
            " correct:  tensor(42624) Total:  42624\n",
            "Epoch 4, Batch 666, Loss 14.831859329441288 Accuracy 1.0 Time 3.151s\n",
            " correct:  tensor(42688) Total:  42688\n",
            "Epoch 4, Batch 667, Loss 14.832607957019263 Accuracy 1.0 Time 3.142s\n",
            " correct:  tensor(42752) Total:  42752\n",
            "Epoch 4, Batch 668, Loss 14.833029297297585 Accuracy 1.0 Time 3.197s\n",
            " correct:  tensor(42816) Total:  42816\n",
            "Epoch 4, Batch 669, Loss 14.832343074416544 Accuracy 1.0 Time 3.195s\n",
            " correct:  tensor(42880) Total:  42880\n",
            "Epoch 4, Batch 670, Loss 14.831901762378749 Accuracy 1.0 Time 3.179s\n",
            " correct:  tensor(42944) Total:  42944\n",
            "Epoch 4, Batch 671, Loss 14.832651252007519 Accuracy 1.0 Time 3.228s\n",
            " correct:  tensor(43008) Total:  43008\n",
            "Epoch 4, Batch 672, Loss 14.832464096092043 Accuracy 1.0 Time 3.177s\n",
            " correct:  tensor(43072) Total:  43072\n",
            "Epoch 4, Batch 673, Loss 14.83215920957024 Accuracy 1.0 Time 3.184s\n",
            " correct:  tensor(43136) Total:  43136\n",
            "Epoch 4, Batch 674, Loss 14.831932141448341 Accuracy 1.0 Time 3.131s\n",
            " correct:  tensor(43200) Total:  43200\n",
            "Epoch 4, Batch 675, Loss 14.831831892508047 Accuracy 1.0 Time 3.145s\n",
            " correct:  tensor(43264) Total:  43264\n",
            "Epoch 4, Batch 676, Loss 14.832257469730264 Accuracy 1.0 Time 3.17s\n",
            " correct:  tensor(43328) Total:  43328\n",
            "Epoch 4, Batch 677, Loss 14.831363644170269 Accuracy 1.0 Time 3.152s\n",
            " correct:  tensor(43392) Total:  43392\n",
            "Epoch 4, Batch 678, Loss 14.83197415894815 Accuracy 1.0 Time 3.138s\n",
            " correct:  tensor(43456) Total:  43456\n",
            "Epoch 4, Batch 679, Loss 14.831567703887535 Accuracy 1.0 Time 3.109s\n",
            " correct:  tensor(43520) Total:  43520\n",
            "Epoch 4, Batch 680, Loss 14.831048147818622 Accuracy 1.0 Time 3.167s\n",
            " correct:  tensor(43584) Total:  43584\n",
            "Epoch 4, Batch 681, Loss 14.830801945600916 Accuracy 1.0 Time 3.27s\n",
            " correct:  tensor(43648) Total:  43648\n",
            "Epoch 4, Batch 682, Loss 14.830640528558636 Accuracy 1.0 Time 3.118s\n",
            " correct:  tensor(43712) Total:  43712\n",
            "Epoch 4, Batch 683, Loss 14.830878041324643 Accuracy 1.0 Time 3.224s\n",
            " correct:  tensor(43776) Total:  43776\n",
            "Epoch 4, Batch 684, Loss 14.831031569263391 Accuracy 1.0 Time 3.16s\n",
            " correct:  tensor(43840) Total:  43840\n",
            "Epoch 4, Batch 685, Loss 14.8303272497915 Accuracy 1.0 Time 3.173s\n",
            " correct:  tensor(43904) Total:  43904\n",
            "Epoch 4, Batch 686, Loss 14.830351787822934 Accuracy 1.0 Time 3.216s\n",
            " correct:  tensor(43968) Total:  43968\n",
            "Epoch 4, Batch 687, Loss 14.830413662884085 Accuracy 1.0 Time 3.185s\n",
            " correct:  tensor(44032) Total:  44032\n",
            "Epoch 4, Batch 688, Loss 14.829953053662944 Accuracy 1.0 Time 3.211s\n",
            " correct:  tensor(44096) Total:  44096\n",
            "Epoch 4, Batch 689, Loss 14.829476175183654 Accuracy 1.0 Time 3.217s\n",
            " correct:  tensor(44160) Total:  44160\n",
            "Epoch 4, Batch 690, Loss 14.82907638411591 Accuracy 1.0 Time 3.208s\n",
            " correct:  tensor(44224) Total:  44224\n",
            "Epoch 4, Batch 691, Loss 14.829447707633033 Accuracy 1.0 Time 3.161s\n",
            " correct:  tensor(44288) Total:  44288\n",
            "Epoch 4, Batch 692, Loss 14.82934801426926 Accuracy 1.0 Time 3.145s\n",
            " correct:  tensor(44352) Total:  44352\n",
            "Epoch 4, Batch 693, Loss 14.829840944958972 Accuracy 1.0 Time 3.139s\n",
            " correct:  tensor(44416) Total:  44416\n",
            "Epoch 4, Batch 694, Loss 14.83097879893498 Accuracy 1.0 Time 3.181s\n",
            " correct:  tensor(44480) Total:  44480\n",
            "Epoch 4, Batch 695, Loss 14.830536093128671 Accuracy 1.0 Time 3.169s\n",
            " correct:  tensor(44544) Total:  44544\n",
            "Epoch 4, Batch 696, Loss 14.830455740292868 Accuracy 1.0 Time 3.193s\n",
            " correct:  tensor(44608) Total:  44608\n",
            "Epoch 4, Batch 697, Loss 14.830566995968265 Accuracy 1.0 Time 3.176s\n",
            " correct:  tensor(44672) Total:  44672\n",
            "Epoch 4, Batch 698, Loss 14.830211066926449 Accuracy 1.0 Time 3.123s\n",
            " correct:  tensor(44736) Total:  44736\n",
            "Epoch 4, Batch 699, Loss 14.829813408749297 Accuracy 1.0 Time 3.331s\n",
            " correct:  tensor(44800) Total:  44800\n",
            "Epoch 4, Batch 700, Loss 14.828576612472535 Accuracy 1.0 Time 3.303s\n",
            " correct:  tensor(44864) Total:  44864\n",
            "Epoch 4, Batch 701, Loss 14.828030744054688 Accuracy 1.0 Time 3.175s\n",
            " correct:  tensor(44928) Total:  44928\n",
            "Epoch 4, Batch 702, Loss 14.82926661472375 Accuracy 1.0 Time 3.194s\n",
            " correct:  tensor(44992) Total:  44992\n",
            "Epoch 4, Batch 703, Loss 14.828378581051128 Accuracy 1.0 Time 3.128s\n",
            " correct:  tensor(45056) Total:  45056\n",
            "Epoch 4, Batch 704, Loss 14.826803876595063 Accuracy 1.0 Time 3.146s\n",
            " correct:  tensor(45120) Total:  45120\n",
            "Epoch 4, Batch 705, Loss 14.827961089763235 Accuracy 1.0 Time 3.154s\n",
            " correct:  tensor(45184) Total:  45184\n",
            "Epoch 4, Batch 706, Loss 14.828203318814719 Accuracy 1.0 Time 3.136s\n",
            " correct:  tensor(45248) Total:  45248\n",
            "Epoch 4, Batch 707, Loss 14.829723973253998 Accuracy 1.0 Time 3.177s\n",
            " correct:  tensor(45312) Total:  45312\n",
            "Epoch 4, Batch 708, Loss 14.829779914543453 Accuracy 1.0 Time 3.107s\n",
            " correct:  tensor(45376) Total:  45376\n",
            "Epoch 4, Batch 709, Loss 14.830272596544877 Accuracy 1.0 Time 3.173s\n",
            " correct:  tensor(45440) Total:  45440\n",
            "Epoch 4, Batch 710, Loss 14.830181414644484 Accuracy 1.0 Time 3.299s\n",
            " correct:  tensor(45504) Total:  45504\n",
            "Epoch 4, Batch 711, Loss 14.830724048212108 Accuracy 1.0 Time 3.396s\n",
            " correct:  tensor(45568) Total:  45568\n",
            "Epoch 4, Batch 712, Loss 14.830682159809584 Accuracy 1.0 Time 3.397s\n",
            " correct:  tensor(45632) Total:  45632\n",
            "Epoch 4, Batch 713, Loss 14.831161634116153 Accuracy 1.0 Time 3.312s\n",
            " correct:  tensor(45696) Total:  45696\n",
            "Epoch 4, Batch 714, Loss 14.82988395076506 Accuracy 1.0 Time 3.199s\n",
            " correct:  tensor(45760) Total:  45760\n",
            "Epoch 4, Batch 715, Loss 14.829003726185618 Accuracy 1.0 Time 3.189s\n",
            " correct:  tensor(45824) Total:  45824\n",
            "Epoch 4, Batch 716, Loss 14.829494007472885 Accuracy 1.0 Time 3.039s\n",
            " correct:  tensor(45888) Total:  45888\n",
            "Epoch 4, Batch 717, Loss 14.829775554864476 Accuracy 1.0 Time 3.081s\n",
            " correct:  tensor(45952) Total:  45952\n",
            "Epoch 4, Batch 718, Loss 14.829481601715088 Accuracy 1.0 Time 3.176s\n",
            " correct:  tensor(46016) Total:  46016\n",
            "Epoch 4, Batch 719, Loss 14.829221604762388 Accuracy 1.0 Time 3.191s\n",
            " correct:  tensor(46080) Total:  46080\n",
            "Epoch 4, Batch 720, Loss 14.830118355486128 Accuracy 1.0 Time 3.116s\n",
            " correct:  tensor(46144) Total:  46144\n",
            "Epoch 4, Batch 721, Loss 14.83084154459706 Accuracy 1.0 Time 3.193s\n",
            " correct:  tensor(46208) Total:  46208\n",
            "Epoch 4, Batch 722, Loss 14.831377137730987 Accuracy 1.0 Time 3.157s\n",
            " correct:  tensor(46272) Total:  46272\n",
            "Epoch 4, Batch 723, Loss 14.829951808660358 Accuracy 1.0 Time 3.098s\n",
            " correct:  tensor(46336) Total:  46336\n",
            "Epoch 4, Batch 724, Loss 14.828654480243914 Accuracy 1.0 Time 3.066s\n",
            " correct:  tensor(46400) Total:  46400\n",
            "Epoch 4, Batch 725, Loss 14.827224626212285 Accuracy 1.0 Time 3.16s\n",
            " correct:  tensor(46464) Total:  46464\n",
            "Epoch 4, Batch 726, Loss 14.827443012521286 Accuracy 1.0 Time 3.089s\n",
            " correct:  tensor(46528) Total:  46528\n",
            "Epoch 4, Batch 727, Loss 14.826385185839877 Accuracy 1.0 Time 3.173s\n",
            " correct:  tensor(46592) Total:  46592\n",
            "Epoch 4, Batch 728, Loss 14.825855393986126 Accuracy 1.0 Time 3.119s\n",
            " correct:  tensor(46656) Total:  46656\n",
            "Epoch 4, Batch 729, Loss 14.825132110972463 Accuracy 1.0 Time 3.196s\n",
            " correct:  tensor(46720) Total:  46720\n",
            "Epoch 4, Batch 730, Loss 14.824097541913595 Accuracy 1.0 Time 3.193s\n",
            " correct:  tensor(46784) Total:  46784\n",
            "Epoch 4, Batch 731, Loss 14.822537397392471 Accuracy 1.0 Time 3.257s\n",
            " correct:  tensor(46848) Total:  46848\n",
            "Epoch 4, Batch 732, Loss 14.82258423560304 Accuracy 1.0 Time 3.206s\n",
            " correct:  tensor(46912) Total:  46912\n",
            "Epoch 4, Batch 733, Loss 14.822826744589603 Accuracy 1.0 Time 3.133s\n",
            " correct:  tensor(46976) Total:  46976\n",
            "Epoch 4, Batch 734, Loss 14.823459927977268 Accuracy 1.0 Time 3.17s\n",
            " correct:  tensor(47040) Total:  47040\n",
            "Epoch 4, Batch 735, Loss 14.823767284471161 Accuracy 1.0 Time 3.153s\n",
            " correct:  tensor(47104) Total:  47104\n",
            "Epoch 4, Batch 736, Loss 14.823159001443697 Accuracy 1.0 Time 3.135s\n",
            " correct:  tensor(47168) Total:  47168\n",
            "Epoch 4, Batch 737, Loss 14.823648495460624 Accuracy 1.0 Time 3.192s\n",
            " correct:  tensor(47232) Total:  47232\n",
            "Epoch 4, Batch 738, Loss 14.823258601553071 Accuracy 1.0 Time 3.126s\n",
            " correct:  tensor(47296) Total:  47296\n",
            "Epoch 4, Batch 739, Loss 14.823104805488224 Accuracy 1.0 Time 3.162s\n",
            " correct:  tensor(47360) Total:  47360\n",
            "Epoch 4, Batch 740, Loss 14.82307689640973 Accuracy 1.0 Time 3.173s\n",
            " correct:  tensor(47424) Total:  47424\n",
            "Epoch 4, Batch 741, Loss 14.823525285913878 Accuracy 1.0 Time 3.18s\n",
            " correct:  tensor(47488) Total:  47488\n",
            "Epoch 4, Batch 742, Loss 14.82291433303504 Accuracy 1.0 Time 3.187s\n",
            " correct:  tensor(47552) Total:  47552\n",
            "Epoch 4, Batch 743, Loss 14.823578354487669 Accuracy 1.0 Time 3.166s\n",
            " correct:  tensor(47616) Total:  47616\n",
            "Epoch 4, Batch 744, Loss 14.823054112413878 Accuracy 1.0 Time 3.216s\n",
            " correct:  tensor(47680) Total:  47680\n",
            "Epoch 4, Batch 745, Loss 14.82249507135993 Accuracy 1.0 Time 3.143s\n",
            " correct:  tensor(47744) Total:  47744\n",
            "Epoch 4, Batch 746, Loss 14.823105512930944 Accuracy 1.0 Time 3.163s\n",
            " correct:  tensor(47808) Total:  47808\n",
            "Epoch 4, Batch 747, Loss 14.822116749672846 Accuracy 1.0 Time 3.161s\n",
            " correct:  tensor(47872) Total:  47872\n",
            "Epoch 4, Batch 748, Loss 14.821758577530396 Accuracy 1.0 Time 3.157s\n",
            " correct:  tensor(47936) Total:  47936\n",
            "Epoch 4, Batch 749, Loss 14.821820529026088 Accuracy 1.0 Time 3.143s\n",
            " correct:  tensor(48000) Total:  48000\n",
            "Epoch 4, Batch 750, Loss 14.823918471018473 Accuracy 1.0 Time 3.093s\n",
            " correct:  tensor(48064) Total:  48064\n",
            "Epoch 4, Batch 751, Loss 14.823528697424024 Accuracy 1.0 Time 3.191s\n",
            " correct:  tensor(48128) Total:  48128\n",
            "Epoch 4, Batch 752, Loss 14.822991251945496 Accuracy 1.0 Time 3.165s\n",
            " correct:  tensor(48192) Total:  48192\n",
            "Epoch 4, Batch 753, Loss 14.822814652644306 Accuracy 1.0 Time 3.161s\n",
            " correct:  tensor(48256) Total:  48256\n",
            "Epoch 4, Batch 754, Loss 14.822135380155528 Accuracy 1.0 Time 3.186s\n",
            " correct:  tensor(48320) Total:  48320\n",
            "Epoch 4, Batch 755, Loss 14.822553616959528 Accuracy 1.0 Time 3.194s\n",
            " correct:  tensor(48384) Total:  48384\n",
            "Epoch 4, Batch 756, Loss 14.822874258434961 Accuracy 1.0 Time 3.281s\n",
            " correct:  tensor(48448) Total:  48448\n",
            "Epoch 4, Batch 757, Loss 14.82295091369508 Accuracy 1.0 Time 3.167s\n",
            " correct:  tensor(48512) Total:  48512\n",
            "Epoch 4, Batch 758, Loss 14.82303298809283 Accuracy 1.0 Time 3.135s\n",
            " correct:  tensor(48576) Total:  48576\n",
            "Epoch 4, Batch 759, Loss 14.822363477764707 Accuracy 1.0 Time 3.172s\n",
            " correct:  tensor(48640) Total:  48640\n",
            "Epoch 4, Batch 760, Loss 14.821646438146892 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(48704) Total:  48704\n",
            "Epoch 4, Batch 761, Loss 14.821158901620631 Accuracy 1.0 Time 3.214s\n",
            " correct:  tensor(48768) Total:  48768\n",
            "Epoch 4, Batch 762, Loss 14.82119373011151 Accuracy 1.0 Time 3.17s\n",
            " correct:  tensor(48832) Total:  48832\n",
            "Epoch 4, Batch 763, Loss 14.820838793233369 Accuracy 1.0 Time 3.134s\n",
            " correct:  tensor(48896) Total:  48896\n",
            "Epoch 4, Batch 764, Loss 14.820944928373965 Accuracy 1.0 Time 3.09s\n",
            " correct:  tensor(48960) Total:  48960\n",
            "Epoch 4, Batch 765, Loss 14.820077231662726 Accuracy 1.0 Time 3.159s\n",
            " correct:  tensor(49024) Total:  49024\n",
            "Epoch 4, Batch 766, Loss 14.820622789019392 Accuracy 1.0 Time 3.15s\n",
            " correct:  tensor(49088) Total:  49088\n",
            "Epoch 4, Batch 767, Loss 14.820807956809947 Accuracy 1.0 Time 3.148s\n",
            " correct:  tensor(49152) Total:  49152\n",
            "Epoch 4, Batch 768, Loss 14.821199089288712 Accuracy 1.0 Time 3.164s\n",
            " correct:  tensor(49216) Total:  49216\n",
            "Epoch 4, Batch 769, Loss 14.8221491824821 Accuracy 1.0 Time 3.111s\n",
            " correct:  tensor(49280) Total:  49280\n",
            "Epoch 4, Batch 770, Loss 14.822087512078223 Accuracy 1.0 Time 3.166s\n",
            " correct:  tensor(49344) Total:  49344\n",
            "Epoch 4, Batch 771, Loss 14.822501991509464 Accuracy 1.0 Time 3.119s\n",
            " correct:  tensor(49408) Total:  49408\n",
            "Epoch 4, Batch 772, Loss 14.82244073171072 Accuracy 1.0 Time 3.167s\n",
            " correct:  tensor(49472) Total:  49472\n",
            "Epoch 4, Batch 773, Loss 14.823569926993505 Accuracy 1.0 Time 3.152s\n",
            " correct:  tensor(49536) Total:  49536\n",
            "Epoch 4, Batch 774, Loss 14.823313975518989 Accuracy 1.0 Time 3.124s\n",
            " correct:  tensor(49600) Total:  49600\n",
            "Epoch 4, Batch 775, Loss 14.824033764254663 Accuracy 1.0 Time 3.137s\n",
            " correct:  tensor(49664) Total:  49664\n",
            "Epoch 4, Batch 776, Loss 14.823588275417839 Accuracy 1.0 Time 3.102s\n",
            " correct:  tensor(49728) Total:  49728\n",
            "Epoch 4, Batch 777, Loss 14.825287606571939 Accuracy 1.0 Time 3.177s\n",
            " correct:  tensor(49792) Total:  49792\n",
            "Epoch 4, Batch 778, Loss 14.82538682329624 Accuracy 1.0 Time 3.137s\n",
            " correct:  tensor(49856) Total:  49856\n",
            "Epoch 4, Batch 779, Loss 14.82609937616429 Accuracy 1.0 Time 3.119s\n",
            " correct:  tensor(49920) Total:  49920\n",
            "Epoch 4, Batch 780, Loss 14.826369882241274 Accuracy 1.0 Time 3.144s\n",
            " correct:  tensor(49984) Total:  49984\n",
            "Epoch 4, Batch 781, Loss 14.827302130480582 Accuracy 1.0 Time 3.154s\n",
            " correct:  tensor(50048) Total:  50048\n",
            "Epoch 4, Batch 782, Loss 14.826370185598389 Accuracy 1.0 Time 3.225s\n",
            " correct:  tensor(50112) Total:  50112\n",
            "Epoch 4, Batch 783, Loss 14.827437828327048 Accuracy 1.0 Time 3.113s\n",
            " correct:  tensor(50176) Total:  50176\n",
            "Epoch 4, Batch 784, Loss 14.82798713932232 Accuracy 1.0 Time 3.163s\n",
            " correct:  tensor(50240) Total:  50240\n",
            "Epoch 4, Batch 785, Loss 14.827506576829656 Accuracy 1.0 Time 3.134s\n",
            " correct:  tensor(50304) Total:  50304\n",
            "Epoch 4, Batch 786, Loss 14.828271994457294 Accuracy 1.0 Time 3.114s\n",
            " correct:  tensor(50368) Total:  50368\n",
            "Epoch 4, Batch 787, Loss 14.827249812958655 Accuracy 1.0 Time 3.144s\n",
            " correct:  tensor(50432) Total:  50432\n",
            "Epoch 4, Batch 788, Loss 14.82685784640046 Accuracy 1.0 Time 3.181s\n",
            " correct:  tensor(50496) Total:  50496\n",
            "Epoch 4, Batch 789, Loss 14.827183178320281 Accuracy 1.0 Time 3.16s\n",
            " correct:  tensor(50560) Total:  50560\n",
            "Epoch 4, Batch 790, Loss 14.826920516581476 Accuracy 1.0 Time 3.132s\n",
            " correct:  tensor(50624) Total:  50624\n",
            "Epoch 4, Batch 791, Loss 14.826603709822509 Accuracy 1.0 Time 3.195s\n",
            " correct:  tensor(50688) Total:  50688\n",
            "Epoch 4, Batch 792, Loss 14.82688740768818 Accuracy 1.0 Time 3.214s\n",
            " correct:  tensor(50752) Total:  50752\n",
            "Epoch 4, Batch 793, Loss 14.827395716952195 Accuracy 1.0 Time 3.143s\n",
            " correct:  tensor(50816) Total:  50816\n",
            "Epoch 4, Batch 794, Loss 14.826769524917795 Accuracy 1.0 Time 3.183s\n",
            " correct:  tensor(50880) Total:  50880\n",
            "Epoch 4, Batch 795, Loss 14.826299123943977 Accuracy 1.0 Time 3.36s\n",
            " correct:  tensor(50944) Total:  50944\n",
            "Epoch 4, Batch 796, Loss 14.826228335874164 Accuracy 1.0 Time 3.269s\n",
            " correct:  tensor(51008) Total:  51008\n",
            "Epoch 4, Batch 797, Loss 14.827306425556484 Accuracy 1.0 Time 3.175s\n",
            " correct:  tensor(51072) Total:  51072\n",
            "Epoch 4, Batch 798, Loss 14.827547049462646 Accuracy 1.0 Time 3.187s\n",
            " correct:  tensor(51136) Total:  51136\n",
            "Epoch 4, Batch 799, Loss 14.827446961432733 Accuracy 1.0 Time 3.174s\n",
            " correct:  tensor(51200) Total:  51200\n",
            "Epoch 4, Batch 800, Loss 14.829082218408585 Accuracy 1.0 Time 3.149s\n",
            " correct:  tensor(51264) Total:  51264\n",
            "Epoch 4, Batch 801, Loss 14.828396693597572 Accuracy 1.0 Time 3.141s\n",
            " correct:  tensor(51328) Total:  51328\n",
            "Epoch 4, Batch 802, Loss 14.828081649437808 Accuracy 1.0 Time 3.096s\n",
            " correct:  tensor(51392) Total:  51392\n",
            "Epoch 4, Batch 803, Loss 14.82865709326187 Accuracy 1.0 Time 3.167s\n",
            " correct:  tensor(51456) Total:  51456\n",
            "Epoch 4, Batch 804, Loss 14.828849998872672 Accuracy 1.0 Time 3.175s\n",
            " correct:  tensor(51520) Total:  51520\n",
            "Epoch 4, Batch 805, Loss 14.828276289471928 Accuracy 1.0 Time 3.217s\n",
            " correct:  tensor(51584) Total:  51584\n",
            "Epoch 4, Batch 806, Loss 14.828923527123614 Accuracy 1.0 Time 3.173s\n",
            " correct:  tensor(51648) Total:  51648\n",
            "Epoch 4, Batch 807, Loss 14.82813325690514 Accuracy 1.0 Time 3.197s\n",
            " correct:  tensor(51712) Total:  51712\n",
            "Epoch 4, Batch 808, Loss 14.82812518412524 Accuracy 1.0 Time 3.286s\n",
            " correct:  tensor(51776) Total:  51776\n",
            "Epoch 4, Batch 809, Loss 14.828550413011474 Accuracy 1.0 Time 3.375s\n",
            " correct:  tensor(51840) Total:  51840\n",
            "Epoch 4, Batch 810, Loss 14.827743358376585 Accuracy 1.0 Time 3.389s\n",
            " correct:  tensor(51904) Total:  51904\n",
            "Epoch 4, Batch 811, Loss 14.828069223575616 Accuracy 1.0 Time 3.28s\n",
            " correct:  tensor(51968) Total:  51968\n",
            "Epoch 4, Batch 812, Loss 14.828132013969233 Accuracy 1.0 Time 3.177s\n",
            " correct:  tensor(52032) Total:  52032\n",
            "Epoch 4, Batch 813, Loss 14.82916483228057 Accuracy 1.0 Time 3.225s\n",
            " correct:  tensor(52096) Total:  52096\n",
            "Epoch 4, Batch 814, Loss 14.830093312322068 Accuracy 1.0 Time 3.163s\n",
            " correct:  tensor(52160) Total:  52160\n",
            "Epoch 4, Batch 815, Loss 14.830819760656064 Accuracy 1.0 Time 3.118s\n",
            " correct:  tensor(52224) Total:  52224\n",
            "Epoch 4, Batch 816, Loss 14.832084048028086 Accuracy 1.0 Time 3.203s\n",
            " correct:  tensor(52288) Total:  52288\n",
            "Epoch 4, Batch 817, Loss 14.832293697203095 Accuracy 1.0 Time 3.159s\n",
            " correct:  tensor(52352) Total:  52352\n",
            "Epoch 4, Batch 818, Loss 14.832643218611738 Accuracy 1.0 Time 3.173s\n",
            " correct:  tensor(52416) Total:  52416\n",
            "Epoch 4, Batch 819, Loss 14.832487109816554 Accuracy 1.0 Time 3.149s\n",
            " correct:  tensor(52480) Total:  52480\n",
            "Epoch 4, Batch 820, Loss 14.832892423722802 Accuracy 1.0 Time 3.139s\n",
            " correct:  tensor(52544) Total:  52544\n",
            "Epoch 4, Batch 821, Loss 14.832109584878046 Accuracy 1.0 Time 3.12s\n",
            " correct:  tensor(52608) Total:  52608\n",
            "Epoch 4, Batch 822, Loss 14.831984597691074 Accuracy 1.0 Time 3.154s\n",
            " correct:  tensor(52672) Total:  52672\n",
            "Epoch 4, Batch 823, Loss 14.832431537122947 Accuracy 1.0 Time 3.172s\n",
            " correct:  tensor(52736) Total:  52736\n",
            "Epoch 4, Batch 824, Loss 14.83294005185655 Accuracy 1.0 Time 3.159s\n",
            " correct:  tensor(52800) Total:  52800\n",
            "Epoch 4, Batch 825, Loss 14.834515555410674 Accuracy 1.0 Time 3.206s\n",
            " correct:  tensor(52864) Total:  52864\n",
            "Epoch 4, Batch 826, Loss 14.835264802844993 Accuracy 1.0 Time 3.199s\n",
            " correct:  tensor(52928) Total:  52928\n",
            "Epoch 4, Batch 827, Loss 14.836618887037579 Accuracy 1.0 Time 3.136s\n",
            " correct:  tensor(52992) Total:  52992\n",
            "Epoch 4, Batch 828, Loss 14.835689487088706 Accuracy 1.0 Time 3.275s\n",
            " correct:  tensor(53056) Total:  53056\n",
            "Epoch 4, Batch 829, Loss 14.835168816643543 Accuracy 1.0 Time 3.202s\n",
            " correct:  tensor(53120) Total:  53120\n",
            "Epoch 4, Batch 830, Loss 14.83514562055289 Accuracy 1.0 Time 3.159s\n",
            " correct:  tensor(53184) Total:  53184\n",
            "Epoch 4, Batch 831, Loss 14.834949648337243 Accuracy 1.0 Time 3.172s\n",
            " correct:  tensor(53248) Total:  53248\n",
            "Epoch 4, Batch 832, Loss 14.834340472634022 Accuracy 1.0 Time 3.201s\n",
            " correct:  tensor(53312) Total:  53312\n",
            "Epoch 4, Batch 833, Loss 14.835397905805388 Accuracy 1.0 Time 3.172s\n",
            " correct:  tensor(53376) Total:  53376\n",
            "Epoch 4, Batch 834, Loss 14.834433808315286 Accuracy 1.0 Time 3.231s\n",
            " correct:  tensor(53440) Total:  53440\n",
            "Epoch 4, Batch 835, Loss 14.834359973062298 Accuracy 1.0 Time 3.176s\n",
            " correct:  tensor(53504) Total:  53504\n",
            "Epoch 4, Batch 836, Loss 14.834841719084379 Accuracy 1.0 Time 3.147s\n",
            " correct:  tensor(53568) Total:  53568\n",
            "Epoch 4, Batch 837, Loss 14.834251965530742 Accuracy 1.0 Time 3.174s\n",
            " correct:  tensor(53632) Total:  53632\n",
            "Epoch 4, Batch 838, Loss 14.834080972648747 Accuracy 1.0 Time 3.187s\n",
            " correct:  tensor(53696) Total:  53696\n",
            "Epoch 4, Batch 839, Loss 14.834260915544803 Accuracy 1.0 Time 3.148s\n",
            " correct:  tensor(53760) Total:  53760\n",
            "Epoch 4, Batch 840, Loss 14.833852112860907 Accuracy 1.0 Time 3.176s\n",
            " correct:  tensor(53824) Total:  53824\n",
            "Epoch 4, Batch 841, Loss 14.833674060036822 Accuracy 1.0 Time 3.165s\n",
            " correct:  tensor(53888) Total:  53888\n",
            "Epoch 4, Batch 842, Loss 14.83343598281969 Accuracy 1.0 Time 3.199s\n",
            " correct:  tensor(53952) Total:  53952\n",
            "Epoch 4, Batch 843, Loss 14.83277626761621 Accuracy 1.0 Time 3.161s\n",
            " correct:  tensor(54016) Total:  54016\n",
            "Epoch 4, Batch 844, Loss 14.832340794151992 Accuracy 1.0 Time 3.189s\n",
            " correct:  tensor(54080) Total:  54080\n",
            "Epoch 4, Batch 845, Loss 14.832042214433118 Accuracy 1.0 Time 3.169s\n",
            " correct:  tensor(54144) Total:  54144\n",
            "Epoch 4, Batch 846, Loss 14.831849702424755 Accuracy 1.0 Time 3.228s\n",
            " correct:  tensor(54208) Total:  54208\n",
            "Epoch 4, Batch 847, Loss 14.831583079368475 Accuracy 1.0 Time 3.156s\n",
            " correct:  tensor(54272) Total:  54272\n",
            "Epoch 4, Batch 848, Loss 14.83118194791506 Accuracy 1.0 Time 3.138s\n",
            " correct:  tensor(54336) Total:  54336\n",
            "Epoch 4, Batch 849, Loss 14.831484534013116 Accuracy 1.0 Time 3.175s\n",
            " correct:  tensor(54400) Total:  54400\n",
            "Epoch 4, Batch 850, Loss 14.831316682030172 Accuracy 1.0 Time 3.212s\n",
            " correct:  tensor(54464) Total:  54464\n",
            "Epoch 4, Batch 851, Loss 14.8308649993251 Accuracy 1.0 Time 3.155s\n",
            " correct:  tensor(54528) Total:  54528\n",
            "Epoch 4, Batch 852, Loss 14.831167770663338 Accuracy 1.0 Time 3.115s\n",
            " correct:  tensor(54592) Total:  54592\n",
            "Epoch 4, Batch 853, Loss 14.830644937360972 Accuracy 1.0 Time 3.057s\n",
            " correct:  tensor(54656) Total:  54656\n",
            "Epoch 4, Batch 854, Loss 14.831994243193007 Accuracy 1.0 Time 3.183s\n",
            " correct:  tensor(54720) Total:  54720\n",
            "Epoch 4, Batch 855, Loss 14.831144659700449 Accuracy 1.0 Time 3.091s\n",
            " correct:  tensor(54784) Total:  54784\n",
            "Epoch 4, Batch 856, Loss 14.83078257168565 Accuracy 1.0 Time 3.168s\n",
            " correct:  tensor(54848) Total:  54848\n",
            "Epoch 4, Batch 857, Loss 14.831897964833637 Accuracy 1.0 Time 3.12s\n",
            " correct:  tensor(54912) Total:  54912\n",
            "Epoch 4, Batch 858, Loss 14.831885643494434 Accuracy 1.0 Time 3.201s\n",
            " correct:  tensor(54976) Total:  54976\n",
            "Epoch 4, Batch 859, Loss 14.831851368594364 Accuracy 1.0 Time 3.132s\n",
            " correct:  tensor(55040) Total:  55040\n",
            "Epoch 4, Batch 860, Loss 14.83261186577553 Accuracy 1.0 Time 3.143s\n",
            " correct:  tensor(55104) Total:  55104\n",
            "Epoch 4, Batch 861, Loss 14.83212197422289 Accuracy 1.0 Time 3.097s\n",
            " correct:  tensor(55168) Total:  55168\n",
            "Epoch 4, Batch 862, Loss 14.83294400704156 Accuracy 1.0 Time 3.108s\n",
            " correct:  tensor(55232) Total:  55232\n",
            "Epoch 4, Batch 863, Loss 14.832760351143458 Accuracy 1.0 Time 3.161s\n",
            " correct:  tensor(55296) Total:  55296\n",
            "Epoch 4, Batch 864, Loss 14.83235999058794 Accuracy 1.0 Time 3.052s\n",
            " correct:  tensor(55360) Total:  55360\n",
            "Epoch 4, Batch 865, Loss 14.832201751532581 Accuracy 1.0 Time 3.135s\n",
            " correct:  tensor(55424) Total:  55424\n",
            "Epoch 4, Batch 866, Loss 14.831691110932525 Accuracy 1.0 Time 3.168s\n",
            " correct:  tensor(55488) Total:  55488\n",
            "Epoch 4, Batch 867, Loss 14.831137618666563 Accuracy 1.0 Time 3.124s\n",
            " correct:  tensor(55552) Total:  55552\n",
            "Epoch 4, Batch 868, Loss 14.830832467101137 Accuracy 1.0 Time 3.125s\n",
            " correct:  tensor(55616) Total:  55616\n",
            "Epoch 4, Batch 869, Loss 14.830564209177524 Accuracy 1.0 Time 3.169s\n",
            " correct:  tensor(55680) Total:  55680\n",
            "Epoch 4, Batch 870, Loss 14.830979221168606 Accuracy 1.0 Time 3.161s\n",
            " correct:  tensor(55744) Total:  55744\n",
            "Epoch 4, Batch 871, Loss 14.830086310613305 Accuracy 1.0 Time 3.168s\n",
            " correct:  tensor(55808) Total:  55808\n",
            "Epoch 4, Batch 872, Loss 14.83016955961875 Accuracy 1.0 Time 3.147s\n",
            " correct:  tensor(55872) Total:  55872\n",
            "Epoch 4, Batch 873, Loss 14.830036827515627 Accuracy 1.0 Time 3.173s\n",
            " correct:  tensor(55936) Total:  55936\n",
            "Epoch 4, Batch 874, Loss 14.831141958520396 Accuracy 1.0 Time 3.123s\n",
            " correct:  tensor(56000) Total:  56000\n",
            "Epoch 4, Batch 875, Loss 14.831055782863071 Accuracy 1.0 Time 3.18s\n",
            " correct:  tensor(56064) Total:  56064\n",
            "Epoch 4, Batch 876, Loss 14.830901804579991 Accuracy 1.0 Time 3.152s\n",
            " correct:  tensor(56128) Total:  56128\n",
            "Epoch 4, Batch 877, Loss 14.830315298474094 Accuracy 1.0 Time 3.095s\n",
            " correct:  tensor(56192) Total:  56192\n",
            "Epoch 4, Batch 878, Loss 14.830219857513498 Accuracy 1.0 Time 3.151s\n",
            " correct:  tensor(56256) Total:  56256\n",
            "Epoch 4, Batch 879, Loss 14.82995736856645 Accuracy 1.0 Time 3.079s\n",
            " correct:  tensor(56320) Total:  56320\n",
            "Epoch 4, Batch 880, Loss 14.830039336464623 Accuracy 1.0 Time 3.115s\n",
            " correct:  tensor(56384) Total:  56384\n",
            "Epoch 4, Batch 881, Loss 14.830605801334446 Accuracy 1.0 Time 3.159s\n",
            " correct:  tensor(56448) Total:  56448\n",
            "Epoch 4, Batch 882, Loss 14.830766625955802 Accuracy 1.0 Time 3.179s\n",
            " correct:  tensor(56512) Total:  56512\n",
            "Epoch 4, Batch 883, Loss 14.831221015645045 Accuracy 1.0 Time 3.167s\n",
            " correct:  tensor(56576) Total:  56576\n",
            "Epoch 4, Batch 884, Loss 14.832184561776899 Accuracy 1.0 Time 3.122s\n",
            " correct:  tensor(56640) Total:  56640\n",
            "Epoch 4, Batch 885, Loss 14.832254892553987 Accuracy 1.0 Time 3.099s\n",
            " correct:  tensor(56704) Total:  56704\n",
            "Epoch 4, Batch 886, Loss 14.83130610661905 Accuracy 1.0 Time 3.113s\n",
            " correct:  tensor(56768) Total:  56768\n",
            "Epoch 4, Batch 887, Loss 14.831758306690375 Accuracy 1.0 Time 3.166s\n",
            " correct:  tensor(56832) Total:  56832\n",
            "Epoch 4, Batch 888, Loss 14.832264237575703 Accuracy 1.0 Time 3.089s\n",
            " correct:  tensor(56896) Total:  56896\n",
            "Epoch 4, Batch 889, Loss 14.832045354510587 Accuracy 1.0 Time 3.137s\n",
            " correct:  tensor(56960) Total:  56960\n",
            "Epoch 4, Batch 890, Loss 14.831371851717488 Accuracy 1.0 Time 3.177s\n",
            " correct:  tensor(57024) Total:  57024\n",
            "Epoch 4, Batch 891, Loss 14.831661389197832 Accuracy 1.0 Time 3.311s\n",
            " correct:  tensor(57088) Total:  57088\n",
            "Epoch 4, Batch 892, Loss 14.831864457493941 Accuracy 1.0 Time 3.355s\n",
            " correct:  tensor(57152) Total:  57152\n",
            "Epoch 4, Batch 893, Loss 14.831293444617465 Accuracy 1.0 Time 3.138s\n",
            " correct:  tensor(57216) Total:  57216\n",
            "Epoch 4, Batch 894, Loss 14.831671294483296 Accuracy 1.0 Time 3.17s\n",
            " correct:  tensor(57280) Total:  57280\n",
            "Epoch 4, Batch 895, Loss 14.831275753468775 Accuracy 1.0 Time 3.111s\n",
            " correct:  tensor(57344) Total:  57344\n",
            "Epoch 4, Batch 896, Loss 14.83154471218586 Accuracy 1.0 Time 3.186s\n",
            " correct:  tensor(57408) Total:  57408\n",
            "Epoch 4, Batch 897, Loss 14.831667269618482 Accuracy 1.0 Time 3.174s\n",
            " correct:  tensor(57472) Total:  57472\n",
            "Epoch 4, Batch 898, Loss 14.8320049666084 Accuracy 1.0 Time 3.153s\n",
            " correct:  tensor(57536) Total:  57536\n",
            "Epoch 4, Batch 899, Loss 14.832035427496617 Accuracy 1.0 Time 3.193s\n",
            " correct:  tensor(57600) Total:  57600\n",
            "Epoch 4, Batch 900, Loss 14.831252093844943 Accuracy 1.0 Time 3.161s\n",
            " correct:  tensor(57664) Total:  57664\n",
            "Epoch 4, Batch 901, Loss 14.831991424306516 Accuracy 1.0 Time 3.182s\n",
            " correct:  tensor(57728) Total:  57728\n",
            "Epoch 4, Batch 902, Loss 14.831772985056604 Accuracy 1.0 Time 3.19s\n",
            " correct:  tensor(57792) Total:  57792\n",
            "Epoch 4, Batch 903, Loss 14.831582177120982 Accuracy 1.0 Time 3.177s\n",
            " correct:  tensor(57856) Total:  57856\n",
            "Epoch 4, Batch 904, Loss 14.831676408252886 Accuracy 1.0 Time 3.133s\n",
            " correct:  tensor(57920) Total:  57920\n",
            "Epoch 4, Batch 905, Loss 14.831234682199046 Accuracy 1.0 Time 3.171s\n",
            " correct:  tensor(57984) Total:  57984\n",
            "Epoch 4, Batch 906, Loss 14.830871734661246 Accuracy 1.0 Time 3.324s\n",
            " correct:  tensor(58048) Total:  58048\n",
            "Epoch 4, Batch 907, Loss 14.831365604253431 Accuracy 1.0 Time 3.388s\n",
            " correct:  tensor(58112) Total:  58112\n",
            "Epoch 4, Batch 908, Loss 14.831935938234372 Accuracy 1.0 Time 3.381s\n",
            " correct:  tensor(58176) Total:  58176\n",
            "Epoch 4, Batch 909, Loss 14.831970436213696 Accuracy 1.0 Time 3.277s\n",
            " correct:  tensor(58240) Total:  58240\n",
            "Epoch 4, Batch 910, Loss 14.83161527298309 Accuracy 1.0 Time 3.203s\n",
            " correct:  tensor(58304) Total:  58304\n",
            "Epoch 4, Batch 911, Loss 14.831825320205102 Accuracy 1.0 Time 3.146s\n",
            " correct:  tensor(58368) Total:  58368\n",
            "Epoch 4, Batch 912, Loss 14.831833199450845 Accuracy 1.0 Time 3.146s\n",
            " correct:  tensor(58432) Total:  58432\n",
            "Epoch 4, Batch 913, Loss 14.83134799614663 Accuracy 1.0 Time 3.153s\n",
            " correct:  tensor(58496) Total:  58496\n",
            "Epoch 4, Batch 914, Loss 14.831101192165516 Accuracy 1.0 Time 3.126s\n",
            " correct:  tensor(58560) Total:  58560\n",
            "Epoch 4, Batch 915, Loss 14.83002350603948 Accuracy 1.0 Time 3.18s\n",
            " correct:  tensor(58624) Total:  58624\n",
            "Epoch 4, Batch 916, Loss 14.830135948272773 Accuracy 1.0 Time 3.176s\n",
            " correct:  tensor(58688) Total:  58688\n",
            "Epoch 4, Batch 917, Loss 14.8302315896054 Accuracy 1.0 Time 3.173s\n",
            " correct:  tensor(58752) Total:  58752\n",
            "Epoch 4, Batch 918, Loss 14.829976448566047 Accuracy 1.0 Time 3.202s\n",
            " correct:  tensor(58816) Total:  58816\n",
            "Epoch 4, Batch 919, Loss 14.829945707476826 Accuracy 1.0 Time 3.164s\n",
            " correct:  tensor(58880) Total:  58880\n",
            "Epoch 4, Batch 920, Loss 14.829449540635814 Accuracy 1.0 Time 3.228s\n",
            " correct:  tensor(58944) Total:  58944\n",
            "Epoch 4, Batch 921, Loss 14.829155335856054 Accuracy 1.0 Time 3.186s\n",
            " correct:  tensor(59008) Total:  59008\n",
            "Epoch 4, Batch 922, Loss 14.82889830988555 Accuracy 1.0 Time 3.139s\n",
            " correct:  tensor(59072) Total:  59072\n",
            "Epoch 4, Batch 923, Loss 14.82918328195326 Accuracy 1.0 Time 3.195s\n",
            " correct:  tensor(59136) Total:  59136\n",
            "Epoch 4, Batch 924, Loss 14.828789119596605 Accuracy 1.0 Time 3.181s\n",
            " correct:  tensor(59200) Total:  59200\n",
            "Epoch 4, Batch 925, Loss 14.828959966092496 Accuracy 1.0 Time 3.237s\n",
            " correct:  tensor(59264) Total:  59264\n",
            "Epoch 4, Batch 926, Loss 14.829181451776888 Accuracy 1.0 Time 3.276s\n",
            " correct:  tensor(59328) Total:  59328\n",
            "Epoch 4, Batch 927, Loss 14.829533811526005 Accuracy 1.0 Time 3.151s\n",
            " correct:  tensor(59392) Total:  59392\n",
            "Epoch 4, Batch 928, Loss 14.829916332302423 Accuracy 1.0 Time 3.212s\n",
            " correct:  tensor(59456) Total:  59456\n",
            "Epoch 4, Batch 929, Loss 14.829556493892609 Accuracy 1.0 Time 3.213s\n",
            " correct:  tensor(59520) Total:  59520\n",
            "Epoch 4, Batch 930, Loss 14.829483793115102 Accuracy 1.0 Time 3.182s\n",
            " correct:  tensor(59584) Total:  59584\n",
            "Epoch 4, Batch 931, Loss 14.828739127334284 Accuracy 1.0 Time 3.226s\n",
            " correct:  tensor(59648) Total:  59648\n",
            "Epoch 4, Batch 932, Loss 14.827953937227633 Accuracy 1.0 Time 3.171s\n",
            " correct:  tensor(59712) Total:  59712\n",
            "Epoch 4, Batch 933, Loss 14.82777056535596 Accuracy 1.0 Time 3.147s\n",
            " correct:  tensor(59776) Total:  59776\n",
            "Epoch 4, Batch 934, Loss 14.82982643564457 Accuracy 1.0 Time 3.201s\n",
            " correct:  tensor(59840) Total:  59840\n",
            "Epoch 4, Batch 935, Loss 14.830207673751097 Accuracy 1.0 Time 3.117s\n",
            " correct:  tensor(59904) Total:  59904\n",
            "Epoch 4, Batch 936, Loss 14.82974830040565 Accuracy 1.0 Time 3.173s\n",
            " correct:  tensor(59968) Total:  59968\n",
            "Epoch 4, Batch 937, Loss 14.829663883278439 Accuracy 1.0 Time 3.135s\n",
            " correct:  tensor(60000) Total:  60000\n",
            "Epoch 4, Batch 938, Loss 14.821820550890111 Accuracy 1.0 Time 1.676s\n",
            "TRAIN Epoch 4, Loss 14.821820550890111 Accuracy 1.0 Time 2979.165s\n",
            "TESTING...\n",
            " correct:  tensor(64) Total:  64\n",
            " correct:  tensor(128) Total:  128\n",
            " correct:  tensor(192) Total:  192\n",
            " correct:  tensor(256) Total:  256\n",
            " correct:  tensor(320) Total:  320\n",
            " correct:  tensor(384) Total:  384\n",
            " correct:  tensor(448) Total:  448\n",
            " correct:  tensor(512) Total:  512\n",
            " correct:  tensor(576) Total:  576\n",
            " correct:  tensor(640) Total:  640\n",
            " correct:  tensor(704) Total:  704\n",
            " correct:  tensor(768) Total:  768\n",
            " correct:  tensor(832) Total:  832\n",
            " correct:  tensor(896) Total:  896\n",
            " correct:  tensor(960) Total:  960\n",
            " correct:  tensor(1024) Total:  1024\n",
            " correct:  tensor(1088) Total:  1088\n",
            " correct:  tensor(1152) Total:  1152\n",
            " correct:  tensor(1216) Total:  1216\n",
            " correct:  tensor(1280) Total:  1280\n",
            " correct:  tensor(1344) Total:  1344\n",
            " correct:  tensor(1408) Total:  1408\n",
            " correct:  tensor(1472) Total:  1472\n",
            " correct:  tensor(1536) Total:  1536\n",
            " correct:  tensor(1600) Total:  1600\n",
            " correct:  tensor(1664) Total:  1664\n",
            " correct:  tensor(1728) Total:  1728\n",
            " correct:  tensor(1792) Total:  1792\n",
            " correct:  tensor(1856) Total:  1856\n",
            " correct:  tensor(1920) Total:  1920\n",
            " correct:  tensor(1984) Total:  1984\n",
            " correct:  tensor(2048) Total:  2048\n",
            " correct:  tensor(2112) Total:  2112\n",
            " correct:  tensor(2176) Total:  2176\n",
            " correct:  tensor(2240) Total:  2240\n",
            " correct:  tensor(2304) Total:  2304\n",
            " correct:  tensor(2368) Total:  2368\n",
            " correct:  tensor(2432) Total:  2432\n",
            " correct:  tensor(2496) Total:  2496\n",
            " correct:  tensor(2560) Total:  2560\n",
            " correct:  tensor(2624) Total:  2624\n",
            " correct:  tensor(2688) Total:  2688\n",
            " correct:  tensor(2752) Total:  2752\n",
            " correct:  tensor(2816) Total:  2816\n",
            " correct:  tensor(2880) Total:  2880\n",
            " correct:  tensor(2944) Total:  2944\n",
            " correct:  tensor(3008) Total:  3008\n",
            " correct:  tensor(3072) Total:  3072\n",
            " correct:  tensor(3136) Total:  3136\n",
            " correct:  tensor(3200) Total:  3200\n",
            " correct:  tensor(3264) Total:  3264\n",
            " correct:  tensor(3328) Total:  3328\n",
            " correct:  tensor(3392) Total:  3392\n",
            " correct:  tensor(3456) Total:  3456\n",
            " correct:  tensor(3520) Total:  3520\n",
            " correct:  tensor(3584) Total:  3584\n",
            " correct:  tensor(3648) Total:  3648\n",
            " correct:  tensor(3712) Total:  3712\n",
            " correct:  tensor(3776) Total:  3776\n",
            " correct:  tensor(3840) Total:  3840\n",
            " correct:  tensor(3904) Total:  3904\n",
            " correct:  tensor(3968) Total:  3968\n",
            " correct:  tensor(4032) Total:  4032\n",
            " correct:  tensor(4096) Total:  4096\n",
            " correct:  tensor(4160) Total:  4160\n",
            " correct:  tensor(4224) Total:  4224\n",
            " correct:  tensor(4288) Total:  4288\n",
            " correct:  tensor(4352) Total:  4352\n",
            " correct:  tensor(4416) Total:  4416\n",
            " correct:  tensor(4480) Total:  4480\n",
            " correct:  tensor(4544) Total:  4544\n",
            " correct:  tensor(4608) Total:  4608\n",
            " correct:  tensor(4672) Total:  4672\n",
            " correct:  tensor(4736) Total:  4736\n",
            " correct:  tensor(4800) Total:  4800\n",
            " correct:  tensor(4864) Total:  4864\n",
            " correct:  tensor(4928) Total:  4928\n",
            " correct:  tensor(4992) Total:  4992\n",
            " correct:  tensor(5056) Total:  5056\n",
            " correct:  tensor(5120) Total:  5120\n",
            " correct:  tensor(5184) Total:  5184\n",
            " correct:  tensor(5248) Total:  5248\n",
            " correct:  tensor(5312) Total:  5312\n",
            " correct:  tensor(5376) Total:  5376\n",
            " correct:  tensor(5440) Total:  5440\n",
            " correct:  tensor(5504) Total:  5504\n",
            " correct:  tensor(5568) Total:  5568\n",
            " correct:  tensor(5632) Total:  5632\n",
            " correct:  tensor(5696) Total:  5696\n",
            " correct:  tensor(5760) Total:  5760\n",
            " correct:  tensor(5824) Total:  5824\n",
            " correct:  tensor(5888) Total:  5888\n",
            " correct:  tensor(5952) Total:  5952\n",
            " correct:  tensor(6016) Total:  6016\n",
            " correct:  tensor(6080) Total:  6080\n",
            " correct:  tensor(6144) Total:  6144\n",
            " correct:  tensor(6208) Total:  6208\n",
            " correct:  tensor(6272) Total:  6272\n",
            " correct:  tensor(6336) Total:  6336\n",
            " correct:  tensor(6400) Total:  6400\n",
            " correct:  tensor(6464) Total:  6464\n",
            " correct:  tensor(6528) Total:  6528\n",
            " correct:  tensor(6592) Total:  6592\n",
            " correct:  tensor(6656) Total:  6656\n",
            " correct:  tensor(6720) Total:  6720\n",
            " correct:  tensor(6784) Total:  6784\n",
            " correct:  tensor(6848) Total:  6848\n",
            " correct:  tensor(6912) Total:  6912\n",
            " correct:  tensor(6976) Total:  6976\n",
            " correct:  tensor(7040) Total:  7040\n",
            " correct:  tensor(7104) Total:  7104\n",
            " correct:  tensor(7168) Total:  7168\n",
            " correct:  tensor(7232) Total:  7232\n",
            " correct:  tensor(7296) Total:  7296\n",
            " correct:  tensor(7360) Total:  7360\n",
            " correct:  tensor(7424) Total:  7424\n",
            " correct:  tensor(7488) Total:  7488\n",
            " correct:  tensor(7552) Total:  7552\n",
            " correct:  tensor(7616) Total:  7616\n",
            " correct:  tensor(7680) Total:  7680\n",
            " correct:  tensor(7744) Total:  7744\n",
            " correct:  tensor(7808) Total:  7808\n",
            " correct:  tensor(7872) Total:  7872\n",
            " correct:  tensor(7936) Total:  7936\n",
            " correct:  tensor(8000) Total:  8000\n",
            " correct:  tensor(8064) Total:  8064\n",
            " correct:  tensor(8128) Total:  8128\n",
            " correct:  tensor(8192) Total:  8192\n",
            " correct:  tensor(8256) Total:  8256\n",
            " correct:  tensor(8320) Total:  8320\n",
            " correct:  tensor(8384) Total:  8384\n",
            " correct:  tensor(8448) Total:  8448\n",
            " correct:  tensor(8512) Total:  8512\n",
            " correct:  tensor(8576) Total:  8576\n",
            " correct:  tensor(8640) Total:  8640\n",
            " correct:  tensor(8704) Total:  8704\n",
            " correct:  tensor(8768) Total:  8768\n",
            " correct:  tensor(8832) Total:  8832\n",
            " correct:  tensor(8896) Total:  8896\n",
            " correct:  tensor(8960) Total:  8960\n",
            " correct:  tensor(9024) Total:  9024\n",
            " correct:  tensor(9088) Total:  9088\n",
            " correct:  tensor(9152) Total:  9152\n",
            " correct:  tensor(9216) Total:  9216\n",
            " correct:  tensor(9280) Total:  9280\n",
            " correct:  tensor(9344) Total:  9344\n",
            " correct:  tensor(9408) Total:  9408\n",
            " correct:  tensor(9472) Total:  9472\n",
            " correct:  tensor(9536) Total:  9536\n",
            " correct:  tensor(9600) Total:  9600\n",
            " correct:  tensor(9664) Total:  9664\n",
            " correct:  tensor(9728) Total:  9728\n",
            " correct:  tensor(9792) Total:  9792\n",
            " correct:  tensor(9856) Total:  9856\n",
            " correct:  tensor(9920) Total:  9920\n",
            " correct:  tensor(9984) Total:  9984\n",
            " correct:  tensor(10000) Total:  10000\n",
            "TEST Epoch 4, Loss 14.960331324559109 Accuracy 1.0 Time 166.447s\n",
            "Accuracy of     0 : 99 %\n",
            "Accuracy of     1 : 99 %\n",
            "Accuracy of     2 : 99 %\n",
            "Accuracy of     3 : 99 %\n",
            "Accuracy of     4 : 99 %\n",
            "Accuracy of     5 : 98 %\n",
            "Accuracy of     6 : 99 %\n",
            "Accuracy of     7 : 98 %\n",
            "Accuracy of     8 : 99 %\n",
            "Accuracy of     9 : 98 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "on76f7U8ws3F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Saving the Complete Model**"
      ]
    },
    {
      "metadata": {
        "id": "myfTXpzs8kLs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "7c0decd5-3334-4543-a4f5-d42186aa24c1"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "os.mkdir('./Model')\n",
        "PATH='/Model'\n",
        "torch.save(net,PATH)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:250: UserWarning: Couldn't retrieve source code for container of type CapsuleNet. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:250: UserWarning: Couldn't retrieve source code for container of type PrimaryCapsules. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:250: UserWarning: Couldn't retrieve source code for container of type Router. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "0TaNjYrwwbe7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "outputId": "d932a3b0-a0bd-4150-a71b-7e8673daec90"
      },
      "cell_type": "code",
      "source": [
        "model=torch.load(PATH)\n",
        "print(model)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CapsuleNet(\n",
            "  (conv1): Conv2d(1, 256, kernel_size=(9, 9), stride=(1, 1))\n",
            "  (relu): ReLU(inplace)\n",
            "  (primary): PrimaryCapsules(\n",
            "    (conv): Conv2d(256, 256, kernel_size=(9, 9), stride=(2, 2))\n",
            "  )\n",
            "  (digits): Router(\n",
            "    (0): CapsuleLinear(8, 16)\n",
            "    (1): Routing(Routing No =3)\n",
            "  )\n",
            "  (decoder): Sequential(\n",
            "    (0): Linear(in_features=160, out_features=512, bias=True)\n",
            "    (1): ReLU(inplace)\n",
            "    (2): Linear(in_features=512, out_features=1024, bias=True)\n",
            "    (3): ReLU(inplace)\n",
            "    (4): Linear(in_features=1024, out_features=784, bias=True)\n",
            "    (5): Sigmoid()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BugYyRna83xI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MTzjlIbPEOD7",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "b810cb3a-527b-47b6-966b-0ed1961d191e"
      },
      "cell_type": "code",
      "source": [
        "files.upload()\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7258eaad-aa08-486f-b5d2-60fbf9b7e94a\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-7258eaad-aa08-486f-b5d2-60fbf9b7e94a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving test.csv to test.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HAjK3nr1Fkoh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy\n",
        "testdata=pd.read_csv('test.csv')\n",
        "dataset=testdata.iloc[:,0:]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BlwBdui5IKvP",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 113
        },
        "outputId": "656c8782-d9f5-4cdf-9b0f-49eb185205a5"
      },
      "cell_type": "code",
      "source": [
        "files.upload()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8ed67cfc-66c1-45db-bd7a-c8f41960b8df\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-8ed67cfc-66c1-45db-bd7a-c8f41960b8df\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving sample_submission.csv to sample_submission.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sample_submission.csv': b'ImageId,Label\\r\\n1,0\\r\\n2,0\\r\\n3,0\\r\\n4,0\\r\\n5,0\\r\\n6,0\\r\\n7,0\\r\\n8,0\\r\\n9,0\\r\\n10,0\\r\\n11,0\\r\\n12,0\\r\\n13,0\\r\\n14,0\\r\\n15,0\\r\\n16,0\\r\\n17,0\\r\\n18,0\\r\\n19,0\\r\\n20,0\\r\\n21,0\\r\\n22,0\\r\\n23,0\\r\\n24,0\\r\\n25,0\\r\\n26,0\\r\\n27,0\\r\\n28,0\\r\\n29,0\\r\\n30,0\\r\\n31,0\\r\\n32,0\\r\\n33,0\\r\\n34,0\\r\\n35,0\\r\\n36,0\\r\\n37,0\\r\\n38,0\\r\\n39,0\\r\\n40,0\\r\\n41,0\\r\\n42,0\\r\\n43,0\\r\\n44,0\\r\\n45,0\\r\\n46,0\\r\\n47,0\\r\\n48,0\\r\\n49,0\\r\\n50,0\\r\\n51,0\\r\\n52,0\\r\\n53,0\\r\\n54,0\\r\\n55,0\\r\\n56,0\\r\\n57,0\\r\\n58,0\\r\\n59,0\\r\\n60,0\\r\\n61,0\\r\\n62,0\\r\\n63,0\\r\\n64,0\\r\\n65,0\\r\\n66,0\\r\\n67,0\\r\\n68,0\\r\\n69,0\\r\\n70,0\\r\\n71,0\\r\\n72,0\\r\\n73,0\\r\\n74,0\\r\\n75,0\\r\\n76,0\\r\\n77,0\\r\\n78,0\\r\\n79,0\\r\\n80,0\\r\\n81,0\\r\\n82,0\\r\\n83,0\\r\\n84,0\\r\\n85,0\\r\\n86,0\\r\\n87,0\\r\\n88,0\\r\\n89,0\\r\\n90,0\\r\\n91,0\\r\\n92,0\\r\\n93,0\\r\\n94,0\\r\\n95,0\\r\\n96,0\\r\\n97,0\\r\\n98,0\\r\\n99,0\\r\\n100,0\\r\\n101,0\\r\\n102,0\\r\\n103,0\\r\\n104,0\\r\\n105,0\\r\\n106,0\\r\\n107,0\\r\\n108,0\\r\\n109,0\\r\\n110,0\\r\\n111,0\\r\\n112,0\\r\\n113,0\\r\\n114,0\\r\\n115,0\\r\\n116,0\\r\\n117,0\\r\\n118,0\\r\\n119,0\\r\\n120,0\\r\\n121,0\\r\\n122,0\\r\\n123,0\\r\\n124,0\\r\\n125,0\\r\\n126,0\\r\\n127,0\\r\\n128,0\\r\\n129,0\\r\\n130,0\\r\\n131,0\\r\\n132,0\\r\\n133,0\\r\\n134,0\\r\\n135,0\\r\\n136,0\\r\\n137,0\\r\\n138,0\\r\\n139,0\\r\\n140,0\\r\\n141,0\\r\\n142,0\\r\\n143,0\\r\\n144,0\\r\\n145,0\\r\\n146,0\\r\\n147,0\\r\\n148,0\\r\\n149,0\\r\\n150,0\\r\\n151,0\\r\\n152,0\\r\\n153,0\\r\\n154,0\\r\\n155,0\\r\\n156,0\\r\\n157,0\\r\\n158,0\\r\\n159,0\\r\\n160,0\\r\\n161,0\\r\\n162,0\\r\\n163,0\\r\\n164,0\\r\\n165,0\\r\\n166,0\\r\\n167,0\\r\\n168,0\\r\\n169,0\\r\\n170,0\\r\\n171,0\\r\\n172,0\\r\\n173,0\\r\\n174,0\\r\\n175,0\\r\\n176,0\\r\\n177,0\\r\\n178,0\\r\\n179,0\\r\\n180,0\\r\\n181,0\\r\\n182,0\\r\\n183,0\\r\\n184,0\\r\\n185,0\\r\\n186,0\\r\\n187,0\\r\\n188,0\\r\\n189,0\\r\\n190,0\\r\\n191,0\\r\\n192,0\\r\\n193,0\\r\\n194,0\\r\\n195,0\\r\\n196,0\\r\\n197,0\\r\\n198,0\\r\\n199,0\\r\\n200,0\\r\\n201,0\\r\\n202,0\\r\\n203,0\\r\\n204,0\\r\\n205,0\\r\\n206,0\\r\\n207,0\\r\\n208,0\\r\\n209,0\\r\\n210,0\\r\\n211,0\\r\\n212,0\\r\\n213,0\\r\\n214,0\\r\\n215,0\\r\\n216,0\\r\\n217,0\\r\\n218,0\\r\\n219,0\\r\\n220,0\\r\\n221,0\\r\\n222,0\\r\\n223,0\\r\\n224,0\\r\\n225,0\\r\\n226,0\\r\\n227,0\\r\\n228,0\\r\\n229,0\\r\\n230,0\\r\\n231,0\\r\\n232,0\\r\\n233,0\\r\\n234,0\\r\\n235,0\\r\\n236,0\\r\\n237,0\\r\\n238,0\\r\\n239,0\\r\\n240,0\\r\\n241,0\\r\\n242,0\\r\\n243,0\\r\\n244,0\\r\\n245,0\\r\\n246,0\\r\\n247,0\\r\\n248,0\\r\\n249,0\\r\\n250,0\\r\\n251,0\\r\\n252,0\\r\\n253,0\\r\\n254,0\\r\\n255,0\\r\\n256,0\\r\\n257,0\\r\\n258,0\\r\\n259,0\\r\\n260,0\\r\\n261,0\\r\\n262,0\\r\\n263,0\\r\\n264,0\\r\\n265,0\\r\\n266,0\\r\\n267,0\\r\\n268,0\\r\\n269,0\\r\\n270,0\\r\\n271,0\\r\\n272,0\\r\\n273,0\\r\\n274,0\\r\\n275,0\\r\\n276,0\\r\\n277,0\\r\\n278,0\\r\\n279,0\\r\\n280,0\\r\\n281,0\\r\\n282,0\\r\\n283,0\\r\\n284,0\\r\\n285,0\\r\\n286,0\\r\\n287,0\\r\\n288,0\\r\\n289,0\\r\\n290,0\\r\\n291,0\\r\\n292,0\\r\\n293,0\\r\\n294,0\\r\\n295,0\\r\\n296,0\\r\\n297,0\\r\\n298,0\\r\\n299,0\\r\\n300,0\\r\\n301,0\\r\\n302,0\\r\\n303,0\\r\\n304,0\\r\\n305,0\\r\\n306,0\\r\\n307,0\\r\\n308,0\\r\\n309,0\\r\\n310,0\\r\\n311,0\\r\\n312,0\\r\\n313,0\\r\\n314,0\\r\\n315,0\\r\\n316,0\\r\\n317,0\\r\\n318,0\\r\\n319,0\\r\\n320,0\\r\\n321,0\\r\\n322,0\\r\\n323,0\\r\\n324,0\\r\\n325,0\\r\\n326,0\\r\\n327,0\\r\\n328,0\\r\\n329,0\\r\\n330,0\\r\\n331,0\\r\\n332,0\\r\\n333,0\\r\\n334,0\\r\\n335,0\\r\\n336,0\\r\\n337,0\\r\\n338,0\\r\\n339,0\\r\\n340,0\\r\\n341,0\\r\\n342,0\\r\\n343,0\\r\\n344,0\\r\\n345,0\\r\\n346,0\\r\\n347,0\\r\\n348,0\\r\\n349,0\\r\\n350,0\\r\\n351,0\\r\\n352,0\\r\\n353,0\\r\\n354,0\\r\\n355,0\\r\\n356,0\\r\\n357,0\\r\\n358,0\\r\\n359,0\\r\\n360,0\\r\\n361,0\\r\\n362,0\\r\\n363,0\\r\\n364,0\\r\\n365,0\\r\\n366,0\\r\\n367,0\\r\\n368,0\\r\\n369,0\\r\\n370,0\\r\\n371,0\\r\\n372,0\\r\\n373,0\\r\\n374,0\\r\\n375,0\\r\\n376,0\\r\\n377,0\\r\\n378,0\\r\\n379,0\\r\\n380,0\\r\\n381,0\\r\\n382,0\\r\\n383,0\\r\\n384,0\\r\\n385,0\\r\\n386,0\\r\\n387,0\\r\\n388,0\\r\\n389,0\\r\\n390,0\\r\\n391,0\\r\\n392,0\\r\\n393,0\\r\\n394,0\\r\\n395,0\\r\\n396,0\\r\\n397,0\\r\\n398,0\\r\\n399,0\\r\\n400,0\\r\\n401,0\\r\\n402,0\\r\\n403,0\\r\\n404,0\\r\\n405,0\\r\\n406,0\\r\\n407,0\\r\\n408,0\\r\\n409,0\\r\\n410,0\\r\\n411,0\\r\\n412,0\\r\\n413,0\\r\\n414,0\\r\\n415,0\\r\\n416,0\\r\\n417,0\\r\\n418,0\\r\\n419,0\\r\\n420,0\\r\\n421,0\\r\\n422,0\\r\\n423,0\\r\\n424,0\\r\\n425,0\\r\\n426,0\\r\\n427,0\\r\\n428,0\\r\\n429,0\\r\\n430,0\\r\\n431,0\\r\\n432,0\\r\\n433,0\\r\\n434,0\\r\\n435,0\\r\\n436,0\\r\\n437,0\\r\\n438,0\\r\\n439,0\\r\\n440,0\\r\\n441,0\\r\\n442,0\\r\\n443,0\\r\\n444,0\\r\\n445,0\\r\\n446,0\\r\\n447,0\\r\\n448,0\\r\\n449,0\\r\\n450,0\\r\\n451,0\\r\\n452,0\\r\\n453,0\\r\\n454,0\\r\\n455,0\\r\\n456,0\\r\\n457,0\\r\\n458,0\\r\\n459,0\\r\\n460,0\\r\\n461,0\\r\\n462,0\\r\\n463,0\\r\\n464,0\\r\\n465,0\\r\\n466,0\\r\\n467,0\\r\\n468,0\\r\\n469,0\\r\\n470,0\\r\\n471,0\\r\\n472,0\\r\\n473,0\\r\\n474,0\\r\\n475,0\\r\\n476,0\\r\\n477,0\\r\\n478,0\\r\\n479,0\\r\\n480,0\\r\\n481,0\\r\\n482,0\\r\\n483,0\\r\\n484,0\\r\\n485,0\\r\\n486,0\\r\\n487,0\\r\\n488,0\\r\\n489,0\\r\\n490,0\\r\\n491,0\\r\\n492,0\\r\\n493,0\\r\\n494,0\\r\\n495,0\\r\\n496,0\\r\\n497,0\\r\\n498,0\\r\\n499,0\\r\\n500,0\\r\\n501,0\\r\\n502,0\\r\\n503,0\\r\\n504,0\\r\\n505,0\\r\\n506,0\\r\\n507,0\\r\\n508,0\\r\\n509,0\\r\\n510,0\\r\\n511,0\\r\\n512,0\\r\\n513,0\\r\\n514,0\\r\\n515,0\\r\\n516,0\\r\\n517,0\\r\\n518,0\\r\\n519,0\\r\\n520,0\\r\\n521,0\\r\\n522,0\\r\\n523,0\\r\\n524,0\\r\\n525,0\\r\\n526,0\\r\\n527,0\\r\\n528,0\\r\\n529,0\\r\\n530,0\\r\\n531,0\\r\\n532,0\\r\\n533,0\\r\\n534,0\\r\\n535,0\\r\\n536,0\\r\\n537,0\\r\\n538,0\\r\\n539,0\\r\\n540,0\\r\\n541,0\\r\\n542,0\\r\\n543,0\\r\\n544,0\\r\\n545,0\\r\\n546,0\\r\\n547,0\\r\\n548,0\\r\\n549,0\\r\\n550,0\\r\\n551,0\\r\\n552,0\\r\\n553,0\\r\\n554,0\\r\\n555,0\\r\\n556,0\\r\\n557,0\\r\\n558,0\\r\\n559,0\\r\\n560,0\\r\\n561,0\\r\\n562,0\\r\\n563,0\\r\\n564,0\\r\\n565,0\\r\\n566,0\\r\\n567,0\\r\\n568,0\\r\\n569,0\\r\\n570,0\\r\\n571,0\\r\\n572,0\\r\\n573,0\\r\\n574,0\\r\\n575,0\\r\\n576,0\\r\\n577,0\\r\\n578,0\\r\\n579,0\\r\\n580,0\\r\\n581,0\\r\\n582,0\\r\\n583,0\\r\\n584,0\\r\\n585,0\\r\\n586,0\\r\\n587,0\\r\\n588,0\\r\\n589,0\\r\\n590,0\\r\\n591,0\\r\\n592,0\\r\\n593,0\\r\\n594,0\\r\\n595,0\\r\\n596,0\\r\\n597,0\\r\\n598,0\\r\\n599,0\\r\\n600,0\\r\\n601,0\\r\\n602,0\\r\\n603,0\\r\\n604,0\\r\\n605,0\\r\\n606,0\\r\\n607,0\\r\\n608,0\\r\\n609,0\\r\\n610,0\\r\\n611,0\\r\\n612,0\\r\\n613,0\\r\\n614,0\\r\\n615,0\\r\\n616,0\\r\\n617,0\\r\\n618,0\\r\\n619,0\\r\\n620,0\\r\\n621,0\\r\\n622,0\\r\\n623,0\\r\\n624,0\\r\\n625,0\\r\\n626,0\\r\\n627,0\\r\\n628,0\\r\\n629,0\\r\\n630,0\\r\\n631,0\\r\\n632,0\\r\\n633,0\\r\\n634,0\\r\\n635,0\\r\\n636,0\\r\\n637,0\\r\\n638,0\\r\\n639,0\\r\\n640,0\\r\\n641,0\\r\\n642,0\\r\\n643,0\\r\\n644,0\\r\\n645,0\\r\\n646,0\\r\\n647,0\\r\\n648,0\\r\\n649,0\\r\\n650,0\\r\\n651,0\\r\\n652,0\\r\\n653,0\\r\\n654,0\\r\\n655,0\\r\\n656,0\\r\\n657,0\\r\\n658,0\\r\\n659,0\\r\\n660,0\\r\\n661,0\\r\\n662,0\\r\\n663,0\\r\\n664,0\\r\\n665,0\\r\\n666,0\\r\\n667,0\\r\\n668,0\\r\\n669,0\\r\\n670,0\\r\\n671,0\\r\\n672,0\\r\\n673,0\\r\\n674,0\\r\\n675,0\\r\\n676,0\\r\\n677,0\\r\\n678,0\\r\\n679,0\\r\\n680,0\\r\\n681,0\\r\\n682,0\\r\\n683,0\\r\\n684,0\\r\\n685,0\\r\\n686,0\\r\\n687,0\\r\\n688,0\\r\\n689,0\\r\\n690,0\\r\\n691,0\\r\\n692,0\\r\\n693,0\\r\\n694,0\\r\\n695,0\\r\\n696,0\\r\\n697,0\\r\\n698,0\\r\\n699,0\\r\\n700,0\\r\\n701,0\\r\\n702,0\\r\\n703,0\\r\\n704,0\\r\\n705,0\\r\\n706,0\\r\\n707,0\\r\\n708,0\\r\\n709,0\\r\\n710,0\\r\\n711,0\\r\\n712,0\\r\\n713,0\\r\\n714,0\\r\\n715,0\\r\\n716,0\\r\\n717,0\\r\\n718,0\\r\\n719,0\\r\\n720,0\\r\\n721,0\\r\\n722,0\\r\\n723,0\\r\\n724,0\\r\\n725,0\\r\\n726,0\\r\\n727,0\\r\\n728,0\\r\\n729,0\\r\\n730,0\\r\\n731,0\\r\\n732,0\\r\\n733,0\\r\\n734,0\\r\\n735,0\\r\\n736,0\\r\\n737,0\\r\\n738,0\\r\\n739,0\\r\\n740,0\\r\\n741,0\\r\\n742,0\\r\\n743,0\\r\\n744,0\\r\\n745,0\\r\\n746,0\\r\\n747,0\\r\\n748,0\\r\\n749,0\\r\\n750,0\\r\\n751,0\\r\\n752,0\\r\\n753,0\\r\\n754,0\\r\\n755,0\\r\\n756,0\\r\\n757,0\\r\\n758,0\\r\\n759,0\\r\\n760,0\\r\\n761,0\\r\\n762,0\\r\\n763,0\\r\\n764,0\\r\\n765,0\\r\\n766,0\\r\\n767,0\\r\\n768,0\\r\\n769,0\\r\\n770,0\\r\\n771,0\\r\\n772,0\\r\\n773,0\\r\\n774,0\\r\\n775,0\\r\\n776,0\\r\\n777,0\\r\\n778,0\\r\\n779,0\\r\\n780,0\\r\\n781,0\\r\\n782,0\\r\\n783,0\\r\\n784,0\\r\\n785,0\\r\\n786,0\\r\\n787,0\\r\\n788,0\\r\\n789,0\\r\\n790,0\\r\\n791,0\\r\\n792,0\\r\\n793,0\\r\\n794,0\\r\\n795,0\\r\\n796,0\\r\\n797,0\\r\\n798,0\\r\\n799,0\\r\\n800,0\\r\\n801,0\\r\\n802,0\\r\\n803,0\\r\\n804,0\\r\\n805,0\\r\\n806,0\\r\\n807,0\\r\\n808,0\\r\\n809,0\\r\\n810,0\\r\\n811,0\\r\\n812,0\\r\\n813,0\\r\\n814,0\\r\\n815,0\\r\\n816,0\\r\\n817,0\\r\\n818,0\\r\\n819,0\\r\\n820,0\\r\\n821,0\\r\\n822,0\\r\\n823,0\\r\\n824,0\\r\\n825,0\\r\\n826,0\\r\\n827,0\\r\\n828,0\\r\\n829,0\\r\\n830,0\\r\\n831,0\\r\\n832,0\\r\\n833,0\\r\\n834,0\\r\\n835,0\\r\\n836,0\\r\\n837,0\\r\\n838,0\\r\\n839,0\\r\\n840,0\\r\\n841,0\\r\\n842,0\\r\\n843,0\\r\\n844,0\\r\\n845,0\\r\\n846,0\\r\\n847,0\\r\\n848,0\\r\\n849,0\\r\\n850,0\\r\\n851,0\\r\\n852,0\\r\\n853,0\\r\\n854,0\\r\\n855,0\\r\\n856,0\\r\\n857,0\\r\\n858,0\\r\\n859,0\\r\\n860,0\\r\\n861,0\\r\\n862,0\\r\\n863,0\\r\\n864,0\\r\\n865,0\\r\\n866,0\\r\\n867,0\\r\\n868,0\\r\\n869,0\\r\\n870,0\\r\\n871,0\\r\\n872,0\\r\\n873,0\\r\\n874,0\\r\\n875,0\\r\\n876,0\\r\\n877,0\\r\\n878,0\\r\\n879,0\\r\\n880,0\\r\\n881,0\\r\\n882,0\\r\\n883,0\\r\\n884,0\\r\\n885,0\\r\\n886,0\\r\\n887,0\\r\\n888,0\\r\\n889,0\\r\\n890,0\\r\\n891,0\\r\\n892,0\\r\\n893,0\\r\\n894,0\\r\\n895,0\\r\\n896,0\\r\\n897,0\\r\\n898,0\\r\\n899,0\\r\\n900,0\\r\\n901,0\\r\\n902,0\\r\\n903,0\\r\\n904,0\\r\\n905,0\\r\\n906,0\\r\\n907,0\\r\\n908,0\\r\\n909,0\\r\\n910,0\\r\\n911,0\\r\\n912,0\\r\\n913,0\\r\\n914,0\\r\\n915,0\\r\\n916,0\\r\\n917,0\\r\\n918,0\\r\\n919,0\\r\\n920,0\\r\\n921,0\\r\\n922,0\\r\\n923,0\\r\\n924,0\\r\\n925,0\\r\\n926,0\\r\\n927,0\\r\\n928,0\\r\\n929,0\\r\\n930,0\\r\\n931,0\\r\\n932,0\\r\\n933,0\\r\\n934,0\\r\\n935,0\\r\\n936,0\\r\\n937,0\\r\\n938,0\\r\\n939,0\\r\\n940,0\\r\\n941,0\\r\\n942,0\\r\\n943,0\\r\\n944,0\\r\\n945,0\\r\\n946,0\\r\\n947,0\\r\\n948,0\\r\\n949,0\\r\\n950,0\\r\\n951,0\\r\\n952,0\\r\\n953,0\\r\\n954,0\\r\\n955,0\\r\\n956,0\\r\\n957,0\\r\\n958,0\\r\\n959,0\\r\\n960,0\\r\\n961,0\\r\\n962,0\\r\\n963,0\\r\\n964,0\\r\\n965,0\\r\\n966,0\\r\\n967,0\\r\\n968,0\\r\\n969,0\\r\\n970,0\\r\\n971,0\\r\\n972,0\\r\\n973,0\\r\\n974,0\\r\\n975,0\\r\\n976,0\\r\\n977,0\\r\\n978,0\\r\\n979,0\\r\\n980,0\\r\\n981,0\\r\\n982,0\\r\\n983,0\\r\\n984,0\\r\\n985,0\\r\\n986,0\\r\\n987,0\\r\\n988,0\\r\\n989,0\\r\\n990,0\\r\\n991,0\\r\\n992,0\\r\\n993,0\\r\\n994,0\\r\\n995,0\\r\\n996,0\\r\\n997,0\\r\\n998,0\\r\\n999,0\\r\\n1000,0\\r\\n1001,0\\r\\n1002,0\\r\\n1003,0\\r\\n1004,0\\r\\n1005,0\\r\\n1006,0\\r\\n1007,0\\r\\n1008,0\\r\\n1009,0\\r\\n1010,0\\r\\n1011,0\\r\\n1012,0\\r\\n1013,0\\r\\n1014,0\\r\\n1015,0\\r\\n1016,0\\r\\n1017,0\\r\\n1018,0\\r\\n1019,0\\r\\n1020,0\\r\\n1021,0\\r\\n1022,0\\r\\n1023,0\\r\\n1024,0\\r\\n1025,0\\r\\n1026,0\\r\\n1027,0\\r\\n1028,0\\r\\n1029,0\\r\\n1030,0\\r\\n1031,0\\r\\n1032,0\\r\\n1033,0\\r\\n1034,0\\r\\n1035,0\\r\\n1036,0\\r\\n1037,0\\r\\n1038,0\\r\\n1039,0\\r\\n1040,0\\r\\n1041,0\\r\\n1042,0\\r\\n1043,0\\r\\n1044,0\\r\\n1045,0\\r\\n1046,0\\r\\n1047,0\\r\\n1048,0\\r\\n1049,0\\r\\n1050,0\\r\\n1051,0\\r\\n1052,0\\r\\n1053,0\\r\\n1054,0\\r\\n1055,0\\r\\n1056,0\\r\\n1057,0\\r\\n1058,0\\r\\n1059,0\\r\\n1060,0\\r\\n1061,0\\r\\n1062,0\\r\\n1063,0\\r\\n1064,0\\r\\n1065,0\\r\\n1066,0\\r\\n1067,0\\r\\n1068,0\\r\\n1069,0\\r\\n1070,0\\r\\n1071,0\\r\\n1072,0\\r\\n1073,0\\r\\n1074,0\\r\\n1075,0\\r\\n1076,0\\r\\n1077,0\\r\\n1078,0\\r\\n1079,0\\r\\n1080,0\\r\\n1081,0\\r\\n1082,0\\r\\n1083,0\\r\\n1084,0\\r\\n1085,0\\r\\n1086,0\\r\\n1087,0\\r\\n1088,0\\r\\n1089,0\\r\\n1090,0\\r\\n1091,0\\r\\n1092,0\\r\\n1093,0\\r\\n1094,0\\r\\n1095,0\\r\\n1096,0\\r\\n1097,0\\r\\n1098,0\\r\\n1099,0\\r\\n1100,0\\r\\n1101,0\\r\\n1102,0\\r\\n1103,0\\r\\n1104,0\\r\\n1105,0\\r\\n1106,0\\r\\n1107,0\\r\\n1108,0\\r\\n1109,0\\r\\n1110,0\\r\\n1111,0\\r\\n1112,0\\r\\n1113,0\\r\\n1114,0\\r\\n1115,0\\r\\n1116,0\\r\\n1117,0\\r\\n1118,0\\r\\n1119,0\\r\\n1120,0\\r\\n1121,0\\r\\n1122,0\\r\\n1123,0\\r\\n1124,0\\r\\n1125,0\\r\\n1126,0\\r\\n1127,0\\r\\n1128,0\\r\\n1129,0\\r\\n1130,0\\r\\n1131,0\\r\\n1132,0\\r\\n1133,0\\r\\n1134,0\\r\\n1135,0\\r\\n1136,0\\r\\n1137,0\\r\\n1138,0\\r\\n1139,0\\r\\n1140,0\\r\\n1141,0\\r\\n1142,0\\r\\n1143,0\\r\\n1144,0\\r\\n1145,0\\r\\n1146,0\\r\\n1147,0\\r\\n1148,0\\r\\n1149,0\\r\\n1150,0\\r\\n1151,0\\r\\n1152,0\\r\\n1153,0\\r\\n1154,0\\r\\n1155,0\\r\\n1156,0\\r\\n1157,0\\r\\n1158,0\\r\\n1159,0\\r\\n1160,0\\r\\n1161,0\\r\\n1162,0\\r\\n1163,0\\r\\n1164,0\\r\\n1165,0\\r\\n1166,0\\r\\n1167,0\\r\\n1168,0\\r\\n1169,0\\r\\n1170,0\\r\\n1171,0\\r\\n1172,0\\r\\n1173,0\\r\\n1174,0\\r\\n1175,0\\r\\n1176,0\\r\\n1177,0\\r\\n1178,0\\r\\n1179,0\\r\\n1180,0\\r\\n1181,0\\r\\n1182,0\\r\\n1183,0\\r\\n1184,0\\r\\n1185,0\\r\\n1186,0\\r\\n1187,0\\r\\n1188,0\\r\\n1189,0\\r\\n1190,0\\r\\n1191,0\\r\\n1192,0\\r\\n1193,0\\r\\n1194,0\\r\\n1195,0\\r\\n1196,0\\r\\n1197,0\\r\\n1198,0\\r\\n1199,0\\r\\n1200,0\\r\\n1201,0\\r\\n1202,0\\r\\n1203,0\\r\\n1204,0\\r\\n1205,0\\r\\n1206,0\\r\\n1207,0\\r\\n1208,0\\r\\n1209,0\\r\\n1210,0\\r\\n1211,0\\r\\n1212,0\\r\\n1213,0\\r\\n1214,0\\r\\n1215,0\\r\\n1216,0\\r\\n1217,0\\r\\n1218,0\\r\\n1219,0\\r\\n1220,0\\r\\n1221,0\\r\\n1222,0\\r\\n1223,0\\r\\n1224,0\\r\\n1225,0\\r\\n1226,0\\r\\n1227,0\\r\\n1228,0\\r\\n1229,0\\r\\n1230,0\\r\\n1231,0\\r\\n1232,0\\r\\n1233,0\\r\\n1234,0\\r\\n1235,0\\r\\n1236,0\\r\\n1237,0\\r\\n1238,0\\r\\n1239,0\\r\\n1240,0\\r\\n1241,0\\r\\n1242,0\\r\\n1243,0\\r\\n1244,0\\r\\n1245,0\\r\\n1246,0\\r\\n1247,0\\r\\n1248,0\\r\\n1249,0\\r\\n1250,0\\r\\n1251,0\\r\\n1252,0\\r\\n1253,0\\r\\n1254,0\\r\\n1255,0\\r\\n1256,0\\r\\n1257,0\\r\\n1258,0\\r\\n1259,0\\r\\n1260,0\\r\\n1261,0\\r\\n1262,0\\r\\n1263,0\\r\\n1264,0\\r\\n1265,0\\r\\n1266,0\\r\\n1267,0\\r\\n1268,0\\r\\n1269,0\\r\\n1270,0\\r\\n1271,0\\r\\n1272,0\\r\\n1273,0\\r\\n1274,0\\r\\n1275,0\\r\\n1276,0\\r\\n1277,0\\r\\n1278,0\\r\\n1279,0\\r\\n1280,0\\r\\n1281,0\\r\\n1282,0\\r\\n1283,0\\r\\n1284,0\\r\\n1285,0\\r\\n1286,0\\r\\n1287,0\\r\\n1288,0\\r\\n1289,0\\r\\n1290,0\\r\\n1291,0\\r\\n1292,0\\r\\n1293,0\\r\\n1294,0\\r\\n1295,0\\r\\n1296,0\\r\\n1297,0\\r\\n1298,0\\r\\n1299,0\\r\\n1300,0\\r\\n1301,0\\r\\n1302,0\\r\\n1303,0\\r\\n1304,0\\r\\n1305,0\\r\\n1306,0\\r\\n1307,0\\r\\n1308,0\\r\\n1309,0\\r\\n1310,0\\r\\n1311,0\\r\\n1312,0\\r\\n1313,0\\r\\n1314,0\\r\\n1315,0\\r\\n1316,0\\r\\n1317,0\\r\\n1318,0\\r\\n1319,0\\r\\n1320,0\\r\\n1321,0\\r\\n1322,0\\r\\n1323,0\\r\\n1324,0\\r\\n1325,0\\r\\n1326,0\\r\\n1327,0\\r\\n1328,0\\r\\n1329,0\\r\\n1330,0\\r\\n1331,0\\r\\n1332,0\\r\\n1333,0\\r\\n1334,0\\r\\n1335,0\\r\\n1336,0\\r\\n1337,0\\r\\n1338,0\\r\\n1339,0\\r\\n1340,0\\r\\n1341,0\\r\\n1342,0\\r\\n1343,0\\r\\n1344,0\\r\\n1345,0\\r\\n1346,0\\r\\n1347,0\\r\\n1348,0\\r\\n1349,0\\r\\n1350,0\\r\\n1351,0\\r\\n1352,0\\r\\n1353,0\\r\\n1354,0\\r\\n1355,0\\r\\n1356,0\\r\\n1357,0\\r\\n1358,0\\r\\n1359,0\\r\\n1360,0\\r\\n1361,0\\r\\n1362,0\\r\\n1363,0\\r\\n1364,0\\r\\n1365,0\\r\\n1366,0\\r\\n1367,0\\r\\n1368,0\\r\\n1369,0\\r\\n1370,0\\r\\n1371,0\\r\\n1372,0\\r\\n1373,0\\r\\n1374,0\\r\\n1375,0\\r\\n1376,0\\r\\n1377,0\\r\\n1378,0\\r\\n1379,0\\r\\n1380,0\\r\\n1381,0\\r\\n1382,0\\r\\n1383,0\\r\\n1384,0\\r\\n1385,0\\r\\n1386,0\\r\\n1387,0\\r\\n1388,0\\r\\n1389,0\\r\\n1390,0\\r\\n1391,0\\r\\n1392,0\\r\\n1393,0\\r\\n1394,0\\r\\n1395,0\\r\\n1396,0\\r\\n1397,0\\r\\n1398,0\\r\\n1399,0\\r\\n1400,0\\r\\n1401,0\\r\\n1402,0\\r\\n1403,0\\r\\n1404,0\\r\\n1405,0\\r\\n1406,0\\r\\n1407,0\\r\\n1408,0\\r\\n1409,0\\r\\n1410,0\\r\\n1411,0\\r\\n1412,0\\r\\n1413,0\\r\\n1414,0\\r\\n1415,0\\r\\n1416,0\\r\\n1417,0\\r\\n1418,0\\r\\n1419,0\\r\\n1420,0\\r\\n1421,0\\r\\n1422,0\\r\\n1423,0\\r\\n1424,0\\r\\n1425,0\\r\\n1426,0\\r\\n1427,0\\r\\n1428,0\\r\\n1429,0\\r\\n1430,0\\r\\n1431,0\\r\\n1432,0\\r\\n1433,0\\r\\n1434,0\\r\\n1435,0\\r\\n1436,0\\r\\n1437,0\\r\\n1438,0\\r\\n1439,0\\r\\n1440,0\\r\\n1441,0\\r\\n1442,0\\r\\n1443,0\\r\\n1444,0\\r\\n1445,0\\r\\n1446,0\\r\\n1447,0\\r\\n1448,0\\r\\n1449,0\\r\\n1450,0\\r\\n1451,0\\r\\n1452,0\\r\\n1453,0\\r\\n1454,0\\r\\n1455,0\\r\\n1456,0\\r\\n1457,0\\r\\n1458,0\\r\\n1459,0\\r\\n1460,0\\r\\n1461,0\\r\\n1462,0\\r\\n1463,0\\r\\n1464,0\\r\\n1465,0\\r\\n1466,0\\r\\n1467,0\\r\\n1468,0\\r\\n1469,0\\r\\n1470,0\\r\\n1471,0\\r\\n1472,0\\r\\n1473,0\\r\\n1474,0\\r\\n1475,0\\r\\n1476,0\\r\\n1477,0\\r\\n1478,0\\r\\n1479,0\\r\\n1480,0\\r\\n1481,0\\r\\n1482,0\\r\\n1483,0\\r\\n1484,0\\r\\n1485,0\\r\\n1486,0\\r\\n1487,0\\r\\n1488,0\\r\\n1489,0\\r\\n1490,0\\r\\n1491,0\\r\\n1492,0\\r\\n1493,0\\r\\n1494,0\\r\\n1495,0\\r\\n1496,0\\r\\n1497,0\\r\\n1498,0\\r\\n1499,0\\r\\n1500,0\\r\\n1501,0\\r\\n1502,0\\r\\n1503,0\\r\\n1504,0\\r\\n1505,0\\r\\n1506,0\\r\\n1507,0\\r\\n1508,0\\r\\n1509,0\\r\\n1510,0\\r\\n1511,0\\r\\n1512,0\\r\\n1513,0\\r\\n1514,0\\r\\n1515,0\\r\\n1516,0\\r\\n1517,0\\r\\n1518,0\\r\\n1519,0\\r\\n1520,0\\r\\n1521,0\\r\\n1522,0\\r\\n1523,0\\r\\n1524,0\\r\\n1525,0\\r\\n1526,0\\r\\n1527,0\\r\\n1528,0\\r\\n1529,0\\r\\n1530,0\\r\\n1531,0\\r\\n1532,0\\r\\n1533,0\\r\\n1534,0\\r\\n1535,0\\r\\n1536,0\\r\\n1537,0\\r\\n1538,0\\r\\n1539,0\\r\\n1540,0\\r\\n1541,0\\r\\n1542,0\\r\\n1543,0\\r\\n1544,0\\r\\n1545,0\\r\\n1546,0\\r\\n1547,0\\r\\n1548,0\\r\\n1549,0\\r\\n1550,0\\r\\n1551,0\\r\\n1552,0\\r\\n1553,0\\r\\n1554,0\\r\\n1555,0\\r\\n1556,0\\r\\n1557,0\\r\\n1558,0\\r\\n1559,0\\r\\n1560,0\\r\\n1561,0\\r\\n1562,0\\r\\n1563,0\\r\\n1564,0\\r\\n1565,0\\r\\n1566,0\\r\\n1567,0\\r\\n1568,0\\r\\n1569,0\\r\\n1570,0\\r\\n1571,0\\r\\n1572,0\\r\\n1573,0\\r\\n1574,0\\r\\n1575,0\\r\\n1576,0\\r\\n1577,0\\r\\n1578,0\\r\\n1579,0\\r\\n1580,0\\r\\n1581,0\\r\\n1582,0\\r\\n1583,0\\r\\n1584,0\\r\\n1585,0\\r\\n1586,0\\r\\n1587,0\\r\\n1588,0\\r\\n1589,0\\r\\n1590,0\\r\\n1591,0\\r\\n1592,0\\r\\n1593,0\\r\\n1594,0\\r\\n1595,0\\r\\n1596,0\\r\\n1597,0\\r\\n1598,0\\r\\n1599,0\\r\\n1600,0\\r\\n1601,0\\r\\n1602,0\\r\\n1603,0\\r\\n1604,0\\r\\n1605,0\\r\\n1606,0\\r\\n1607,0\\r\\n1608,0\\r\\n1609,0\\r\\n1610,0\\r\\n1611,0\\r\\n1612,0\\r\\n1613,0\\r\\n1614,0\\r\\n1615,0\\r\\n1616,0\\r\\n1617,0\\r\\n1618,0\\r\\n1619,0\\r\\n1620,0\\r\\n1621,0\\r\\n1622,0\\r\\n1623,0\\r\\n1624,0\\r\\n1625,0\\r\\n1626,0\\r\\n1627,0\\r\\n1628,0\\r\\n1629,0\\r\\n1630,0\\r\\n1631,0\\r\\n1632,0\\r\\n1633,0\\r\\n1634,0\\r\\n1635,0\\r\\n1636,0\\r\\n1637,0\\r\\n1638,0\\r\\n1639,0\\r\\n1640,0\\r\\n1641,0\\r\\n1642,0\\r\\n1643,0\\r\\n1644,0\\r\\n1645,0\\r\\n1646,0\\r\\n1647,0\\r\\n1648,0\\r\\n1649,0\\r\\n1650,0\\r\\n1651,0\\r\\n1652,0\\r\\n1653,0\\r\\n1654,0\\r\\n1655,0\\r\\n1656,0\\r\\n1657,0\\r\\n1658,0\\r\\n1659,0\\r\\n1660,0\\r\\n1661,0\\r\\n1662,0\\r\\n1663,0\\r\\n1664,0\\r\\n1665,0\\r\\n1666,0\\r\\n1667,0\\r\\n1668,0\\r\\n1669,0\\r\\n1670,0\\r\\n1671,0\\r\\n1672,0\\r\\n1673,0\\r\\n1674,0\\r\\n1675,0\\r\\n1676,0\\r\\n1677,0\\r\\n1678,0\\r\\n1679,0\\r\\n1680,0\\r\\n1681,0\\r\\n1682,0\\r\\n1683,0\\r\\n1684,0\\r\\n1685,0\\r\\n1686,0\\r\\n1687,0\\r\\n1688,0\\r\\n1689,0\\r\\n1690,0\\r\\n1691,0\\r\\n1692,0\\r\\n1693,0\\r\\n1694,0\\r\\n1695,0\\r\\n1696,0\\r\\n1697,0\\r\\n1698,0\\r\\n1699,0\\r\\n1700,0\\r\\n1701,0\\r\\n1702,0\\r\\n1703,0\\r\\n1704,0\\r\\n1705,0\\r\\n1706,0\\r\\n1707,0\\r\\n1708,0\\r\\n1709,0\\r\\n1710,0\\r\\n1711,0\\r\\n1712,0\\r\\n1713,0\\r\\n1714,0\\r\\n1715,0\\r\\n1716,0\\r\\n1717,0\\r\\n1718,0\\r\\n1719,0\\r\\n1720,0\\r\\n1721,0\\r\\n1722,0\\r\\n1723,0\\r\\n1724,0\\r\\n1725,0\\r\\n1726,0\\r\\n1727,0\\r\\n1728,0\\r\\n1729,0\\r\\n1730,0\\r\\n1731,0\\r\\n1732,0\\r\\n1733,0\\r\\n1734,0\\r\\n1735,0\\r\\n1736,0\\r\\n1737,0\\r\\n1738,0\\r\\n1739,0\\r\\n1740,0\\r\\n1741,0\\r\\n1742,0\\r\\n1743,0\\r\\n1744,0\\r\\n1745,0\\r\\n1746,0\\r\\n1747,0\\r\\n1748,0\\r\\n1749,0\\r\\n1750,0\\r\\n1751,0\\r\\n1752,0\\r\\n1753,0\\r\\n1754,0\\r\\n1755,0\\r\\n1756,0\\r\\n1757,0\\r\\n1758,0\\r\\n1759,0\\r\\n1760,0\\r\\n1761,0\\r\\n1762,0\\r\\n1763,0\\r\\n1764,0\\r\\n1765,0\\r\\n1766,0\\r\\n1767,0\\r\\n1768,0\\r\\n1769,0\\r\\n1770,0\\r\\n1771,0\\r\\n1772,0\\r\\n1773,0\\r\\n1774,0\\r\\n1775,0\\r\\n1776,0\\r\\n1777,0\\r\\n1778,0\\r\\n1779,0\\r\\n1780,0\\r\\n1781,0\\r\\n1782,0\\r\\n1783,0\\r\\n1784,0\\r\\n1785,0\\r\\n1786,0\\r\\n1787,0\\r\\n1788,0\\r\\n1789,0\\r\\n1790,0\\r\\n1791,0\\r\\n1792,0\\r\\n1793,0\\r\\n1794,0\\r\\n1795,0\\r\\n1796,0\\r\\n1797,0\\r\\n1798,0\\r\\n1799,0\\r\\n1800,0\\r\\n1801,0\\r\\n1802,0\\r\\n1803,0\\r\\n1804,0\\r\\n1805,0\\r\\n1806,0\\r\\n1807,0\\r\\n1808,0\\r\\n1809,0\\r\\n1810,0\\r\\n1811,0\\r\\n1812,0\\r\\n1813,0\\r\\n1814,0\\r\\n1815,0\\r\\n1816,0\\r\\n1817,0\\r\\n1818,0\\r\\n1819,0\\r\\n1820,0\\r\\n1821,0\\r\\n1822,0\\r\\n1823,0\\r\\n1824,0\\r\\n1825,0\\r\\n1826,0\\r\\n1827,0\\r\\n1828,0\\r\\n1829,0\\r\\n1830,0\\r\\n1831,0\\r\\n1832,0\\r\\n1833,0\\r\\n1834,0\\r\\n1835,0\\r\\n1836,0\\r\\n1837,0\\r\\n1838,0\\r\\n1839,0\\r\\n1840,0\\r\\n1841,0\\r\\n1842,0\\r\\n1843,0\\r\\n1844,0\\r\\n1845,0\\r\\n1846,0\\r\\n1847,0\\r\\n1848,0\\r\\n1849,0\\r\\n1850,0\\r\\n1851,0\\r\\n1852,0\\r\\n1853,0\\r\\n1854,0\\r\\n1855,0\\r\\n1856,0\\r\\n1857,0\\r\\n1858,0\\r\\n1859,0\\r\\n1860,0\\r\\n1861,0\\r\\n1862,0\\r\\n1863,0\\r\\n1864,0\\r\\n1865,0\\r\\n1866,0\\r\\n1867,0\\r\\n1868,0\\r\\n1869,0\\r\\n1870,0\\r\\n1871,0\\r\\n1872,0\\r\\n1873,0\\r\\n1874,0\\r\\n1875,0\\r\\n1876,0\\r\\n1877,0\\r\\n1878,0\\r\\n1879,0\\r\\n1880,0\\r\\n1881,0\\r\\n1882,0\\r\\n1883,0\\r\\n1884,0\\r\\n1885,0\\r\\n1886,0\\r\\n1887,0\\r\\n1888,0\\r\\n1889,0\\r\\n1890,0\\r\\n1891,0\\r\\n1892,0\\r\\n1893,0\\r\\n1894,0\\r\\n1895,0\\r\\n1896,0\\r\\n1897,0\\r\\n1898,0\\r\\n1899,0\\r\\n1900,0\\r\\n1901,0\\r\\n1902,0\\r\\n1903,0\\r\\n1904,0\\r\\n1905,0\\r\\n1906,0\\r\\n1907,0\\r\\n1908,0\\r\\n1909,0\\r\\n1910,0\\r\\n1911,0\\r\\n1912,0\\r\\n1913,0\\r\\n1914,0\\r\\n1915,0\\r\\n1916,0\\r\\n1917,0\\r\\n1918,0\\r\\n1919,0\\r\\n1920,0\\r\\n1921,0\\r\\n1922,0\\r\\n1923,0\\r\\n1924,0\\r\\n1925,0\\r\\n1926,0\\r\\n1927,0\\r\\n1928,0\\r\\n1929,0\\r\\n1930,0\\r\\n1931,0\\r\\n1932,0\\r\\n1933,0\\r\\n1934,0\\r\\n1935,0\\r\\n1936,0\\r\\n1937,0\\r\\n1938,0\\r\\n1939,0\\r\\n1940,0\\r\\n1941,0\\r\\n1942,0\\r\\n1943,0\\r\\n1944,0\\r\\n1945,0\\r\\n1946,0\\r\\n1947,0\\r\\n1948,0\\r\\n1949,0\\r\\n1950,0\\r\\n1951,0\\r\\n1952,0\\r\\n1953,0\\r\\n1954,0\\r\\n1955,0\\r\\n1956,0\\r\\n1957,0\\r\\n1958,0\\r\\n1959,0\\r\\n1960,0\\r\\n1961,0\\r\\n1962,0\\r\\n1963,0\\r\\n1964,0\\r\\n1965,0\\r\\n1966,0\\r\\n1967,0\\r\\n1968,0\\r\\n1969,0\\r\\n1970,0\\r\\n1971,0\\r\\n1972,0\\r\\n1973,0\\r\\n1974,0\\r\\n1975,0\\r\\n1976,0\\r\\n1977,0\\r\\n1978,0\\r\\n1979,0\\r\\n1980,0\\r\\n1981,0\\r\\n1982,0\\r\\n1983,0\\r\\n1984,0\\r\\n1985,0\\r\\n1986,0\\r\\n1987,0\\r\\n1988,0\\r\\n1989,0\\r\\n1990,0\\r\\n1991,0\\r\\n1992,0\\r\\n1993,0\\r\\n1994,0\\r\\n1995,0\\r\\n1996,0\\r\\n1997,0\\r\\n1998,0\\r\\n1999,0\\r\\n2000,0\\r\\n2001,0\\r\\n2002,0\\r\\n2003,0\\r\\n2004,0\\r\\n2005,0\\r\\n2006,0\\r\\n2007,0\\r\\n2008,0\\r\\n2009,0\\r\\n2010,0\\r\\n2011,0\\r\\n2012,0\\r\\n2013,0\\r\\n2014,0\\r\\n2015,0\\r\\n2016,0\\r\\n2017,0\\r\\n2018,0\\r\\n2019,0\\r\\n2020,0\\r\\n2021,0\\r\\n2022,0\\r\\n2023,0\\r\\n2024,0\\r\\n2025,0\\r\\n2026,0\\r\\n2027,0\\r\\n2028,0\\r\\n2029,0\\r\\n2030,0\\r\\n2031,0\\r\\n2032,0\\r\\n2033,0\\r\\n2034,0\\r\\n2035,0\\r\\n2036,0\\r\\n2037,0\\r\\n2038,0\\r\\n2039,0\\r\\n2040,0\\r\\n2041,0\\r\\n2042,0\\r\\n2043,0\\r\\n2044,0\\r\\n2045,0\\r\\n2046,0\\r\\n2047,0\\r\\n2048,0\\r\\n2049,0\\r\\n2050,0\\r\\n2051,0\\r\\n2052,0\\r\\n2053,0\\r\\n2054,0\\r\\n2055,0\\r\\n2056,0\\r\\n2057,0\\r\\n2058,0\\r\\n2059,0\\r\\n2060,0\\r\\n2061,0\\r\\n2062,0\\r\\n2063,0\\r\\n2064,0\\r\\n2065,0\\r\\n2066,0\\r\\n2067,0\\r\\n2068,0\\r\\n2069,0\\r\\n2070,0\\r\\n2071,0\\r\\n2072,0\\r\\n2073,0\\r\\n2074,0\\r\\n2075,0\\r\\n2076,0\\r\\n2077,0\\r\\n2078,0\\r\\n2079,0\\r\\n2080,0\\r\\n2081,0\\r\\n2082,0\\r\\n2083,0\\r\\n2084,0\\r\\n2085,0\\r\\n2086,0\\r\\n2087,0\\r\\n2088,0\\r\\n2089,0\\r\\n2090,0\\r\\n2091,0\\r\\n2092,0\\r\\n2093,0\\r\\n2094,0\\r\\n2095,0\\r\\n2096,0\\r\\n2097,0\\r\\n2098,0\\r\\n2099,0\\r\\n2100,0\\r\\n2101,0\\r\\n2102,0\\r\\n2103,0\\r\\n2104,0\\r\\n2105,0\\r\\n2106,0\\r\\n2107,0\\r\\n2108,0\\r\\n2109,0\\r\\n2110,0\\r\\n2111,0\\r\\n2112,0\\r\\n2113,0\\r\\n2114,0\\r\\n2115,0\\r\\n2116,0\\r\\n2117,0\\r\\n2118,0\\r\\n2119,0\\r\\n2120,0\\r\\n2121,0\\r\\n2122,0\\r\\n2123,0\\r\\n2124,0\\r\\n2125,0\\r\\n2126,0\\r\\n2127,0\\r\\n2128,0\\r\\n2129,0\\r\\n2130,0\\r\\n2131,0\\r\\n2132,0\\r\\n2133,0\\r\\n2134,0\\r\\n2135,0\\r\\n2136,0\\r\\n2137,0\\r\\n2138,0\\r\\n2139,0\\r\\n2140,0\\r\\n2141,0\\r\\n2142,0\\r\\n2143,0\\r\\n2144,0\\r\\n2145,0\\r\\n2146,0\\r\\n2147,0\\r\\n2148,0\\r\\n2149,0\\r\\n2150,0\\r\\n2151,0\\r\\n2152,0\\r\\n2153,0\\r\\n2154,0\\r\\n2155,0\\r\\n2156,0\\r\\n2157,0\\r\\n2158,0\\r\\n2159,0\\r\\n2160,0\\r\\n2161,0\\r\\n2162,0\\r\\n2163,0\\r\\n2164,0\\r\\n2165,0\\r\\n2166,0\\r\\n2167,0\\r\\n2168,0\\r\\n2169,0\\r\\n2170,0\\r\\n2171,0\\r\\n2172,0\\r\\n2173,0\\r\\n2174,0\\r\\n2175,0\\r\\n2176,0\\r\\n2177,0\\r\\n2178,0\\r\\n2179,0\\r\\n2180,0\\r\\n2181,0\\r\\n2182,0\\r\\n2183,0\\r\\n2184,0\\r\\n2185,0\\r\\n2186,0\\r\\n2187,0\\r\\n2188,0\\r\\n2189,0\\r\\n2190,0\\r\\n2191,0\\r\\n2192,0\\r\\n2193,0\\r\\n2194,0\\r\\n2195,0\\r\\n2196,0\\r\\n2197,0\\r\\n2198,0\\r\\n2199,0\\r\\n2200,0\\r\\n2201,0\\r\\n2202,0\\r\\n2203,0\\r\\n2204,0\\r\\n2205,0\\r\\n2206,0\\r\\n2207,0\\r\\n2208,0\\r\\n2209,0\\r\\n2210,0\\r\\n2211,0\\r\\n2212,0\\r\\n2213,0\\r\\n2214,0\\r\\n2215,0\\r\\n2216,0\\r\\n2217,0\\r\\n2218,0\\r\\n2219,0\\r\\n2220,0\\r\\n2221,0\\r\\n2222,0\\r\\n2223,0\\r\\n2224,0\\r\\n2225,0\\r\\n2226,0\\r\\n2227,0\\r\\n2228,0\\r\\n2229,0\\r\\n2230,0\\r\\n2231,0\\r\\n2232,0\\r\\n2233,0\\r\\n2234,0\\r\\n2235,0\\r\\n2236,0\\r\\n2237,0\\r\\n2238,0\\r\\n2239,0\\r\\n2240,0\\r\\n2241,0\\r\\n2242,0\\r\\n2243,0\\r\\n2244,0\\r\\n2245,0\\r\\n2246,0\\r\\n2247,0\\r\\n2248,0\\r\\n2249,0\\r\\n2250,0\\r\\n2251,0\\r\\n2252,0\\r\\n2253,0\\r\\n2254,0\\r\\n2255,0\\r\\n2256,0\\r\\n2257,0\\r\\n2258,0\\r\\n2259,0\\r\\n2260,0\\r\\n2261,0\\r\\n2262,0\\r\\n2263,0\\r\\n2264,0\\r\\n2265,0\\r\\n2266,0\\r\\n2267,0\\r\\n2268,0\\r\\n2269,0\\r\\n2270,0\\r\\n2271,0\\r\\n2272,0\\r\\n2273,0\\r\\n2274,0\\r\\n2275,0\\r\\n2276,0\\r\\n2277,0\\r\\n2278,0\\r\\n2279,0\\r\\n2280,0\\r\\n2281,0\\r\\n2282,0\\r\\n2283,0\\r\\n2284,0\\r\\n2285,0\\r\\n2286,0\\r\\n2287,0\\r\\n2288,0\\r\\n2289,0\\r\\n2290,0\\r\\n2291,0\\r\\n2292,0\\r\\n2293,0\\r\\n2294,0\\r\\n2295,0\\r\\n2296,0\\r\\n2297,0\\r\\n2298,0\\r\\n2299,0\\r\\n2300,0\\r\\n2301,0\\r\\n2302,0\\r\\n2303,0\\r\\n2304,0\\r\\n2305,0\\r\\n2306,0\\r\\n2307,0\\r\\n2308,0\\r\\n2309,0\\r\\n2310,0\\r\\n2311,0\\r\\n2312,0\\r\\n2313,0\\r\\n2314,0\\r\\n2315,0\\r\\n2316,0\\r\\n2317,0\\r\\n2318,0\\r\\n2319,0\\r\\n2320,0\\r\\n2321,0\\r\\n2322,0\\r\\n2323,0\\r\\n2324,0\\r\\n2325,0\\r\\n2326,0\\r\\n2327,0\\r\\n2328,0\\r\\n2329,0\\r\\n2330,0\\r\\n2331,0\\r\\n2332,0\\r\\n2333,0\\r\\n2334,0\\r\\n2335,0\\r\\n2336,0\\r\\n2337,0\\r\\n2338,0\\r\\n2339,0\\r\\n2340,0\\r\\n2341,0\\r\\n2342,0\\r\\n2343,0\\r\\n2344,0\\r\\n2345,0\\r\\n2346,0\\r\\n2347,0\\r\\n2348,0\\r\\n2349,0\\r\\n2350,0\\r\\n2351,0\\r\\n2352,0\\r\\n2353,0\\r\\n2354,0\\r\\n2355,0\\r\\n2356,0\\r\\n2357,0\\r\\n2358,0\\r\\n2359,0\\r\\n2360,0\\r\\n2361,0\\r\\n2362,0\\r\\n2363,0\\r\\n2364,0\\r\\n2365,0\\r\\n2366,0\\r\\n2367,0\\r\\n2368,0\\r\\n2369,0\\r\\n2370,0\\r\\n2371,0\\r\\n2372,0\\r\\n2373,0\\r\\n2374,0\\r\\n2375,0\\r\\n2376,0\\r\\n2377,0\\r\\n2378,0\\r\\n2379,0\\r\\n2380,0\\r\\n2381,0\\r\\n2382,0\\r\\n2383,0\\r\\n2384,0\\r\\n2385,0\\r\\n2386,0\\r\\n2387,0\\r\\n2388,0\\r\\n2389,0\\r\\n2390,0\\r\\n2391,0\\r\\n2392,0\\r\\n2393,0\\r\\n2394,0\\r\\n2395,0\\r\\n2396,0\\r\\n2397,0\\r\\n2398,0\\r\\n2399,0\\r\\n2400,0\\r\\n2401,0\\r\\n2402,0\\r\\n2403,0\\r\\n2404,0\\r\\n2405,0\\r\\n2406,0\\r\\n2407,0\\r\\n2408,0\\r\\n2409,0\\r\\n2410,0\\r\\n2411,0\\r\\n2412,0\\r\\n2413,0\\r\\n2414,0\\r\\n2415,0\\r\\n2416,0\\r\\n2417,0\\r\\n2418,0\\r\\n2419,0\\r\\n2420,0\\r\\n2421,0\\r\\n2422,0\\r\\n2423,0\\r\\n2424,0\\r\\n2425,0\\r\\n2426,0\\r\\n2427,0\\r\\n2428,0\\r\\n2429,0\\r\\n2430,0\\r\\n2431,0\\r\\n2432,0\\r\\n2433,0\\r\\n2434,0\\r\\n2435,0\\r\\n2436,0\\r\\n2437,0\\r\\n2438,0\\r\\n2439,0\\r\\n2440,0\\r\\n2441,0\\r\\n2442,0\\r\\n2443,0\\r\\n2444,0\\r\\n2445,0\\r\\n2446,0\\r\\n2447,0\\r\\n2448,0\\r\\n2449,0\\r\\n2450,0\\r\\n2451,0\\r\\n2452,0\\r\\n2453,0\\r\\n2454,0\\r\\n2455,0\\r\\n2456,0\\r\\n2457,0\\r\\n2458,0\\r\\n2459,0\\r\\n2460,0\\r\\n2461,0\\r\\n2462,0\\r\\n2463,0\\r\\n2464,0\\r\\n2465,0\\r\\n2466,0\\r\\n2467,0\\r\\n2468,0\\r\\n2469,0\\r\\n2470,0\\r\\n2471,0\\r\\n2472,0\\r\\n2473,0\\r\\n2474,0\\r\\n2475,0\\r\\n2476,0\\r\\n2477,0\\r\\n2478,0\\r\\n2479,0\\r\\n2480,0\\r\\n2481,0\\r\\n2482,0\\r\\n2483,0\\r\\n2484,0\\r\\n2485,0\\r\\n2486,0\\r\\n2487,0\\r\\n2488,0\\r\\n2489,0\\r\\n2490,0\\r\\n2491,0\\r\\n2492,0\\r\\n2493,0\\r\\n2494,0\\r\\n2495,0\\r\\n2496,0\\r\\n2497,0\\r\\n2498,0\\r\\n2499,0\\r\\n2500,0\\r\\n2501,0\\r\\n2502,0\\r\\n2503,0\\r\\n2504,0\\r\\n2505,0\\r\\n2506,0\\r\\n2507,0\\r\\n2508,0\\r\\n2509,0\\r\\n2510,0\\r\\n2511,0\\r\\n2512,0\\r\\n2513,0\\r\\n2514,0\\r\\n2515,0\\r\\n2516,0\\r\\n2517,0\\r\\n2518,0\\r\\n2519,0\\r\\n2520,0\\r\\n2521,0\\r\\n2522,0\\r\\n2523,0\\r\\n2524,0\\r\\n2525,0\\r\\n2526,0\\r\\n2527,0\\r\\n2528,0\\r\\n2529,0\\r\\n2530,0\\r\\n2531,0\\r\\n2532,0\\r\\n2533,0\\r\\n2534,0\\r\\n2535,0\\r\\n2536,0\\r\\n2537,0\\r\\n2538,0\\r\\n2539,0\\r\\n2540,0\\r\\n2541,0\\r\\n2542,0\\r\\n2543,0\\r\\n2544,0\\r\\n2545,0\\r\\n2546,0\\r\\n2547,0\\r\\n2548,0\\r\\n2549,0\\r\\n2550,0\\r\\n2551,0\\r\\n2552,0\\r\\n2553,0\\r\\n2554,0\\r\\n2555,0\\r\\n2556,0\\r\\n2557,0\\r\\n2558,0\\r\\n2559,0\\r\\n2560,0\\r\\n2561,0\\r\\n2562,0\\r\\n2563,0\\r\\n2564,0\\r\\n2565,0\\r\\n2566,0\\r\\n2567,0\\r\\n2568,0\\r\\n2569,0\\r\\n2570,0\\r\\n2571,0\\r\\n2572,0\\r\\n2573,0\\r\\n2574,0\\r\\n2575,0\\r\\n2576,0\\r\\n2577,0\\r\\n2578,0\\r\\n2579,0\\r\\n2580,0\\r\\n2581,0\\r\\n2582,0\\r\\n2583,0\\r\\n2584,0\\r\\n2585,0\\r\\n2586,0\\r\\n2587,0\\r\\n2588,0\\r\\n2589,0\\r\\n2590,0\\r\\n2591,0\\r\\n2592,0\\r\\n2593,0\\r\\n2594,0\\r\\n2595,0\\r\\n2596,0\\r\\n2597,0\\r\\n2598,0\\r\\n2599,0\\r\\n2600,0\\r\\n2601,0\\r\\n2602,0\\r\\n2603,0\\r\\n2604,0\\r\\n2605,0\\r\\n2606,0\\r\\n2607,0\\r\\n2608,0\\r\\n2609,0\\r\\n2610,0\\r\\n2611,0\\r\\n2612,0\\r\\n2613,0\\r\\n2614,0\\r\\n2615,0\\r\\n2616,0\\r\\n2617,0\\r\\n2618,0\\r\\n2619,0\\r\\n2620,0\\r\\n2621,0\\r\\n2622,0\\r\\n2623,0\\r\\n2624,0\\r\\n2625,0\\r\\n2626,0\\r\\n2627,0\\r\\n2628,0\\r\\n2629,0\\r\\n2630,0\\r\\n2631,0\\r\\n2632,0\\r\\n2633,0\\r\\n2634,0\\r\\n2635,0\\r\\n2636,0\\r\\n2637,0\\r\\n2638,0\\r\\n2639,0\\r\\n2640,0\\r\\n2641,0\\r\\n2642,0\\r\\n2643,0\\r\\n2644,0\\r\\n2645,0\\r\\n2646,0\\r\\n2647,0\\r\\n2648,0\\r\\n2649,0\\r\\n2650,0\\r\\n2651,0\\r\\n2652,0\\r\\n2653,0\\r\\n2654,0\\r\\n2655,0\\r\\n2656,0\\r\\n2657,0\\r\\n2658,0\\r\\n2659,0\\r\\n2660,0\\r\\n2661,0\\r\\n2662,0\\r\\n2663,0\\r\\n2664,0\\r\\n2665,0\\r\\n2666,0\\r\\n2667,0\\r\\n2668,0\\r\\n2669,0\\r\\n2670,0\\r\\n2671,0\\r\\n2672,0\\r\\n2673,0\\r\\n2674,0\\r\\n2675,0\\r\\n2676,0\\r\\n2677,0\\r\\n2678,0\\r\\n2679,0\\r\\n2680,0\\r\\n2681,0\\r\\n2682,0\\r\\n2683,0\\r\\n2684,0\\r\\n2685,0\\r\\n2686,0\\r\\n2687,0\\r\\n2688,0\\r\\n2689,0\\r\\n2690,0\\r\\n2691,0\\r\\n2692,0\\r\\n2693,0\\r\\n2694,0\\r\\n2695,0\\r\\n2696,0\\r\\n2697,0\\r\\n2698,0\\r\\n2699,0\\r\\n2700,0\\r\\n2701,0\\r\\n2702,0\\r\\n2703,0\\r\\n2704,0\\r\\n2705,0\\r\\n2706,0\\r\\n2707,0\\r\\n2708,0\\r\\n2709,0\\r\\n2710,0\\r\\n2711,0\\r\\n2712,0\\r\\n2713,0\\r\\n2714,0\\r\\n2715,0\\r\\n2716,0\\r\\n2717,0\\r\\n2718,0\\r\\n2719,0\\r\\n2720,0\\r\\n2721,0\\r\\n2722,0\\r\\n2723,0\\r\\n2724,0\\r\\n2725,0\\r\\n2726,0\\r\\n2727,0\\r\\n2728,0\\r\\n2729,0\\r\\n2730,0\\r\\n2731,0\\r\\n2732,0\\r\\n2733,0\\r\\n2734,0\\r\\n2735,0\\r\\n2736,0\\r\\n2737,0\\r\\n2738,0\\r\\n2739,0\\r\\n2740,0\\r\\n2741,0\\r\\n2742,0\\r\\n2743,0\\r\\n2744,0\\r\\n2745,0\\r\\n2746,0\\r\\n2747,0\\r\\n2748,0\\r\\n2749,0\\r\\n2750,0\\r\\n2751,0\\r\\n2752,0\\r\\n2753,0\\r\\n2754,0\\r\\n2755,0\\r\\n2756,0\\r\\n2757,0\\r\\n2758,0\\r\\n2759,0\\r\\n2760,0\\r\\n2761,0\\r\\n2762,0\\r\\n2763,0\\r\\n2764,0\\r\\n2765,0\\r\\n2766,0\\r\\n2767,0\\r\\n2768,0\\r\\n2769,0\\r\\n2770,0\\r\\n2771,0\\r\\n2772,0\\r\\n2773,0\\r\\n2774,0\\r\\n2775,0\\r\\n2776,0\\r\\n2777,0\\r\\n2778,0\\r\\n2779,0\\r\\n2780,0\\r\\n2781,0\\r\\n2782,0\\r\\n2783,0\\r\\n2784,0\\r\\n2785,0\\r\\n2786,0\\r\\n2787,0\\r\\n2788,0\\r\\n2789,0\\r\\n2790,0\\r\\n2791,0\\r\\n2792,0\\r\\n2793,0\\r\\n2794,0\\r\\n2795,0\\r\\n2796,0\\r\\n2797,0\\r\\n2798,0\\r\\n2799,0\\r\\n2800,0\\r\\n2801,0\\r\\n2802,0\\r\\n2803,0\\r\\n2804,0\\r\\n2805,0\\r\\n2806,0\\r\\n2807,0\\r\\n2808,0\\r\\n2809,0\\r\\n2810,0\\r\\n2811,0\\r\\n2812,0\\r\\n2813,0\\r\\n2814,0\\r\\n2815,0\\r\\n2816,0\\r\\n2817,0\\r\\n2818,0\\r\\n2819,0\\r\\n2820,0\\r\\n2821,0\\r\\n2822,0\\r\\n2823,0\\r\\n2824,0\\r\\n2825,0\\r\\n2826,0\\r\\n2827,0\\r\\n2828,0\\r\\n2829,0\\r\\n2830,0\\r\\n2831,0\\r\\n2832,0\\r\\n2833,0\\r\\n2834,0\\r\\n2835,0\\r\\n2836,0\\r\\n2837,0\\r\\n2838,0\\r\\n2839,0\\r\\n2840,0\\r\\n2841,0\\r\\n2842,0\\r\\n2843,0\\r\\n2844,0\\r\\n2845,0\\r\\n2846,0\\r\\n2847,0\\r\\n2848,0\\r\\n2849,0\\r\\n2850,0\\r\\n2851,0\\r\\n2852,0\\r\\n2853,0\\r\\n2854,0\\r\\n2855,0\\r\\n2856,0\\r\\n2857,0\\r\\n2858,0\\r\\n2859,0\\r\\n2860,0\\r\\n2861,0\\r\\n2862,0\\r\\n2863,0\\r\\n2864,0\\r\\n2865,0\\r\\n2866,0\\r\\n2867,0\\r\\n2868,0\\r\\n2869,0\\r\\n2870,0\\r\\n2871,0\\r\\n2872,0\\r\\n2873,0\\r\\n2874,0\\r\\n2875,0\\r\\n2876,0\\r\\n2877,0\\r\\n2878,0\\r\\n2879,0\\r\\n2880,0\\r\\n2881,0\\r\\n2882,0\\r\\n2883,0\\r\\n2884,0\\r\\n2885,0\\r\\n2886,0\\r\\n2887,0\\r\\n2888,0\\r\\n2889,0\\r\\n2890,0\\r\\n2891,0\\r\\n2892,0\\r\\n2893,0\\r\\n2894,0\\r\\n2895,0\\r\\n2896,0\\r\\n2897,0\\r\\n2898,0\\r\\n2899,0\\r\\n2900,0\\r\\n2901,0\\r\\n2902,0\\r\\n2903,0\\r\\n2904,0\\r\\n2905,0\\r\\n2906,0\\r\\n2907,0\\r\\n2908,0\\r\\n2909,0\\r\\n2910,0\\r\\n2911,0\\r\\n2912,0\\r\\n2913,0\\r\\n2914,0\\r\\n2915,0\\r\\n2916,0\\r\\n2917,0\\r\\n2918,0\\r\\n2919,0\\r\\n2920,0\\r\\n2921,0\\r\\n2922,0\\r\\n2923,0\\r\\n2924,0\\r\\n2925,0\\r\\n2926,0\\r\\n2927,0\\r\\n2928,0\\r\\n2929,0\\r\\n2930,0\\r\\n2931,0\\r\\n2932,0\\r\\n2933,0\\r\\n2934,0\\r\\n2935,0\\r\\n2936,0\\r\\n2937,0\\r\\n2938,0\\r\\n2939,0\\r\\n2940,0\\r\\n2941,0\\r\\n2942,0\\r\\n2943,0\\r\\n2944,0\\r\\n2945,0\\r\\n2946,0\\r\\n2947,0\\r\\n2948,0\\r\\n2949,0\\r\\n2950,0\\r\\n2951,0\\r\\n2952,0\\r\\n2953,0\\r\\n2954,0\\r\\n2955,0\\r\\n2956,0\\r\\n2957,0\\r\\n2958,0\\r\\n2959,0\\r\\n2960,0\\r\\n2961,0\\r\\n2962,0\\r\\n2963,0\\r\\n2964,0\\r\\n2965,0\\r\\n2966,0\\r\\n2967,0\\r\\n2968,0\\r\\n2969,0\\r\\n2970,0\\r\\n2971,0\\r\\n2972,0\\r\\n2973,0\\r\\n2974,0\\r\\n2975,0\\r\\n2976,0\\r\\n2977,0\\r\\n2978,0\\r\\n2979,0\\r\\n2980,0\\r\\n2981,0\\r\\n2982,0\\r\\n2983,0\\r\\n2984,0\\r\\n2985,0\\r\\n2986,0\\r\\n2987,0\\r\\n2988,0\\r\\n2989,0\\r\\n2990,0\\r\\n2991,0\\r\\n2992,0\\r\\n2993,0\\r\\n2994,0\\r\\n2995,0\\r\\n2996,0\\r\\n2997,0\\r\\n2998,0\\r\\n2999,0\\r\\n3000,0\\r\\n3001,0\\r\\n3002,0\\r\\n3003,0\\r\\n3004,0\\r\\n3005,0\\r\\n3006,0\\r\\n3007,0\\r\\n3008,0\\r\\n3009,0\\r\\n3010,0\\r\\n3011,0\\r\\n3012,0\\r\\n3013,0\\r\\n3014,0\\r\\n3015,0\\r\\n3016,0\\r\\n3017,0\\r\\n3018,0\\r\\n3019,0\\r\\n3020,0\\r\\n3021,0\\r\\n3022,0\\r\\n3023,0\\r\\n3024,0\\r\\n3025,0\\r\\n3026,0\\r\\n3027,0\\r\\n3028,0\\r\\n3029,0\\r\\n3030,0\\r\\n3031,0\\r\\n3032,0\\r\\n3033,0\\r\\n3034,0\\r\\n3035,0\\r\\n3036,0\\r\\n3037,0\\r\\n3038,0\\r\\n3039,0\\r\\n3040,0\\r\\n3041,0\\r\\n3042,0\\r\\n3043,0\\r\\n3044,0\\r\\n3045,0\\r\\n3046,0\\r\\n3047,0\\r\\n3048,0\\r\\n3049,0\\r\\n3050,0\\r\\n3051,0\\r\\n3052,0\\r\\n3053,0\\r\\n3054,0\\r\\n3055,0\\r\\n3056,0\\r\\n3057,0\\r\\n3058,0\\r\\n3059,0\\r\\n3060,0\\r\\n3061,0\\r\\n3062,0\\r\\n3063,0\\r\\n3064,0\\r\\n3065,0\\r\\n3066,0\\r\\n3067,0\\r\\n3068,0\\r\\n3069,0\\r\\n3070,0\\r\\n3071,0\\r\\n3072,0\\r\\n3073,0\\r\\n3074,0\\r\\n3075,0\\r\\n3076,0\\r\\n3077,0\\r\\n3078,0\\r\\n3079,0\\r\\n3080,0\\r\\n3081,0\\r\\n3082,0\\r\\n3083,0\\r\\n3084,0\\r\\n3085,0\\r\\n3086,0\\r\\n3087,0\\r\\n3088,0\\r\\n3089,0\\r\\n3090,0\\r\\n3091,0\\r\\n3092,0\\r\\n3093,0\\r\\n3094,0\\r\\n3095,0\\r\\n3096,0\\r\\n3097,0\\r\\n3098,0\\r\\n3099,0\\r\\n3100,0\\r\\n3101,0\\r\\n3102,0\\r\\n3103,0\\r\\n3104,0\\r\\n3105,0\\r\\n3106,0\\r\\n3107,0\\r\\n3108,0\\r\\n3109,0\\r\\n3110,0\\r\\n3111,0\\r\\n3112,0\\r\\n3113,0\\r\\n3114,0\\r\\n3115,0\\r\\n3116,0\\r\\n3117,0\\r\\n3118,0\\r\\n3119,0\\r\\n3120,0\\r\\n3121,0\\r\\n3122,0\\r\\n3123,0\\r\\n3124,0\\r\\n3125,0\\r\\n3126,0\\r\\n3127,0\\r\\n3128,0\\r\\n3129,0\\r\\n3130,0\\r\\n3131,0\\r\\n3132,0\\r\\n3133,0\\r\\n3134,0\\r\\n3135,0\\r\\n3136,0\\r\\n3137,0\\r\\n3138,0\\r\\n3139,0\\r\\n3140,0\\r\\n3141,0\\r\\n3142,0\\r\\n3143,0\\r\\n3144,0\\r\\n3145,0\\r\\n3146,0\\r\\n3147,0\\r\\n3148,0\\r\\n3149,0\\r\\n3150,0\\r\\n3151,0\\r\\n3152,0\\r\\n3153,0\\r\\n3154,0\\r\\n3155,0\\r\\n3156,0\\r\\n3157,0\\r\\n3158,0\\r\\n3159,0\\r\\n3160,0\\r\\n3161,0\\r\\n3162,0\\r\\n3163,0\\r\\n3164,0\\r\\n3165,0\\r\\n3166,0\\r\\n3167,0\\r\\n3168,0\\r\\n3169,0\\r\\n3170,0\\r\\n3171,0\\r\\n3172,0\\r\\n3173,0\\r\\n3174,0\\r\\n3175,0\\r\\n3176,0\\r\\n3177,0\\r\\n3178,0\\r\\n3179,0\\r\\n3180,0\\r\\n3181,0\\r\\n3182,0\\r\\n3183,0\\r\\n3184,0\\r\\n3185,0\\r\\n3186,0\\r\\n3187,0\\r\\n3188,0\\r\\n3189,0\\r\\n3190,0\\r\\n3191,0\\r\\n3192,0\\r\\n3193,0\\r\\n3194,0\\r\\n3195,0\\r\\n3196,0\\r\\n3197,0\\r\\n3198,0\\r\\n3199,0\\r\\n3200,0\\r\\n3201,0\\r\\n3202,0\\r\\n3203,0\\r\\n3204,0\\r\\n3205,0\\r\\n3206,0\\r\\n3207,0\\r\\n3208,0\\r\\n3209,0\\r\\n3210,0\\r\\n3211,0\\r\\n3212,0\\r\\n3213,0\\r\\n3214,0\\r\\n3215,0\\r\\n3216,0\\r\\n3217,0\\r\\n3218,0\\r\\n3219,0\\r\\n3220,0\\r\\n3221,0\\r\\n3222,0\\r\\n3223,0\\r\\n3224,0\\r\\n3225,0\\r\\n3226,0\\r\\n3227,0\\r\\n3228,0\\r\\n3229,0\\r\\n3230,0\\r\\n3231,0\\r\\n3232,0\\r\\n3233,0\\r\\n3234,0\\r\\n3235,0\\r\\n3236,0\\r\\n3237,0\\r\\n3238,0\\r\\n3239,0\\r\\n3240,0\\r\\n3241,0\\r\\n3242,0\\r\\n3243,0\\r\\n3244,0\\r\\n3245,0\\r\\n3246,0\\r\\n3247,0\\r\\n3248,0\\r\\n3249,0\\r\\n3250,0\\r\\n3251,0\\r\\n3252,0\\r\\n3253,0\\r\\n3254,0\\r\\n3255,0\\r\\n3256,0\\r\\n3257,0\\r\\n3258,0\\r\\n3259,0\\r\\n3260,0\\r\\n3261,0\\r\\n3262,0\\r\\n3263,0\\r\\n3264,0\\r\\n3265,0\\r\\n3266,0\\r\\n3267,0\\r\\n3268,0\\r\\n3269,0\\r\\n3270,0\\r\\n3271,0\\r\\n3272,0\\r\\n3273,0\\r\\n3274,0\\r\\n3275,0\\r\\n3276,0\\r\\n3277,0\\r\\n3278,0\\r\\n3279,0\\r\\n3280,0\\r\\n3281,0\\r\\n3282,0\\r\\n3283,0\\r\\n3284,0\\r\\n3285,0\\r\\n3286,0\\r\\n3287,0\\r\\n3288,0\\r\\n3289,0\\r\\n3290,0\\r\\n3291,0\\r\\n3292,0\\r\\n3293,0\\r\\n3294,0\\r\\n3295,0\\r\\n3296,0\\r\\n3297,0\\r\\n3298,0\\r\\n3299,0\\r\\n3300,0\\r\\n3301,0\\r\\n3302,0\\r\\n3303,0\\r\\n3304,0\\r\\n3305,0\\r\\n3306,0\\r\\n3307,0\\r\\n3308,0\\r\\n3309,0\\r\\n3310,0\\r\\n3311,0\\r\\n3312,0\\r\\n3313,0\\r\\n3314,0\\r\\n3315,0\\r\\n3316,0\\r\\n3317,0\\r\\n3318,0\\r\\n3319,0\\r\\n3320,0\\r\\n3321,0\\r\\n3322,0\\r\\n3323,0\\r\\n3324,0\\r\\n3325,0\\r\\n3326,0\\r\\n3327,0\\r\\n3328,0\\r\\n3329,0\\r\\n3330,0\\r\\n3331,0\\r\\n3332,0\\r\\n3333,0\\r\\n3334,0\\r\\n3335,0\\r\\n3336,0\\r\\n3337,0\\r\\n3338,0\\r\\n3339,0\\r\\n3340,0\\r\\n3341,0\\r\\n3342,0\\r\\n3343,0\\r\\n3344,0\\r\\n3345,0\\r\\n3346,0\\r\\n3347,0\\r\\n3348,0\\r\\n3349,0\\r\\n3350,0\\r\\n3351,0\\r\\n3352,0\\r\\n3353,0\\r\\n3354,0\\r\\n3355,0\\r\\n3356,0\\r\\n3357,0\\r\\n3358,0\\r\\n3359,0\\r\\n3360,0\\r\\n3361,0\\r\\n3362,0\\r\\n3363,0\\r\\n3364,0\\r\\n3365,0\\r\\n3366,0\\r\\n3367,0\\r\\n3368,0\\r\\n3369,0\\r\\n3370,0\\r\\n3371,0\\r\\n3372,0\\r\\n3373,0\\r\\n3374,0\\r\\n3375,0\\r\\n3376,0\\r\\n3377,0\\r\\n3378,0\\r\\n3379,0\\r\\n3380,0\\r\\n3381,0\\r\\n3382,0\\r\\n3383,0\\r\\n3384,0\\r\\n3385,0\\r\\n3386,0\\r\\n3387,0\\r\\n3388,0\\r\\n3389,0\\r\\n3390,0\\r\\n3391,0\\r\\n3392,0\\r\\n3393,0\\r\\n3394,0\\r\\n3395,0\\r\\n3396,0\\r\\n3397,0\\r\\n3398,0\\r\\n3399,0\\r\\n3400,0\\r\\n3401,0\\r\\n3402,0\\r\\n3403,0\\r\\n3404,0\\r\\n3405,0\\r\\n3406,0\\r\\n3407,0\\r\\n3408,0\\r\\n3409,0\\r\\n3410,0\\r\\n3411,0\\r\\n3412,0\\r\\n3413,0\\r\\n3414,0\\r\\n3415,0\\r\\n3416,0\\r\\n3417,0\\r\\n3418,0\\r\\n3419,0\\r\\n3420,0\\r\\n3421,0\\r\\n3422,0\\r\\n3423,0\\r\\n3424,0\\r\\n3425,0\\r\\n3426,0\\r\\n3427,0\\r\\n3428,0\\r\\n3429,0\\r\\n3430,0\\r\\n3431,0\\r\\n3432,0\\r\\n3433,0\\r\\n3434,0\\r\\n3435,0\\r\\n3436,0\\r\\n3437,0\\r\\n3438,0\\r\\n3439,0\\r\\n3440,0\\r\\n3441,0\\r\\n3442,0\\r\\n3443,0\\r\\n3444,0\\r\\n3445,0\\r\\n3446,0\\r\\n3447,0\\r\\n3448,0\\r\\n3449,0\\r\\n3450,0\\r\\n3451,0\\r\\n3452,0\\r\\n3453,0\\r\\n3454,0\\r\\n3455,0\\r\\n3456,0\\r\\n3457,0\\r\\n3458,0\\r\\n3459,0\\r\\n3460,0\\r\\n3461,0\\r\\n3462,0\\r\\n3463,0\\r\\n3464,0\\r\\n3465,0\\r\\n3466,0\\r\\n3467,0\\r\\n3468,0\\r\\n3469,0\\r\\n3470,0\\r\\n3471,0\\r\\n3472,0\\r\\n3473,0\\r\\n3474,0\\r\\n3475,0\\r\\n3476,0\\r\\n3477,0\\r\\n3478,0\\r\\n3479,0\\r\\n3480,0\\r\\n3481,0\\r\\n3482,0\\r\\n3483,0\\r\\n3484,0\\r\\n3485,0\\r\\n3486,0\\r\\n3487,0\\r\\n3488,0\\r\\n3489,0\\r\\n3490,0\\r\\n3491,0\\r\\n3492,0\\r\\n3493,0\\r\\n3494,0\\r\\n3495,0\\r\\n3496,0\\r\\n3497,0\\r\\n3498,0\\r\\n3499,0\\r\\n3500,0\\r\\n3501,0\\r\\n3502,0\\r\\n3503,0\\r\\n3504,0\\r\\n3505,0\\r\\n3506,0\\r\\n3507,0\\r\\n3508,0\\r\\n3509,0\\r\\n3510,0\\r\\n3511,0\\r\\n3512,0\\r\\n3513,0\\r\\n3514,0\\r\\n3515,0\\r\\n3516,0\\r\\n3517,0\\r\\n3518,0\\r\\n3519,0\\r\\n3520,0\\r\\n3521,0\\r\\n3522,0\\r\\n3523,0\\r\\n3524,0\\r\\n3525,0\\r\\n3526,0\\r\\n3527,0\\r\\n3528,0\\r\\n3529,0\\r\\n3530,0\\r\\n3531,0\\r\\n3532,0\\r\\n3533,0\\r\\n3534,0\\r\\n3535,0\\r\\n3536,0\\r\\n3537,0\\r\\n3538,0\\r\\n3539,0\\r\\n3540,0\\r\\n3541,0\\r\\n3542,0\\r\\n3543,0\\r\\n3544,0\\r\\n3545,0\\r\\n3546,0\\r\\n3547,0\\r\\n3548,0\\r\\n3549,0\\r\\n3550,0\\r\\n3551,0\\r\\n3552,0\\r\\n3553,0\\r\\n3554,0\\r\\n3555,0\\r\\n3556,0\\r\\n3557,0\\r\\n3558,0\\r\\n3559,0\\r\\n3560,0\\r\\n3561,0\\r\\n3562,0\\r\\n3563,0\\r\\n3564,0\\r\\n3565,0\\r\\n3566,0\\r\\n3567,0\\r\\n3568,0\\r\\n3569,0\\r\\n3570,0\\r\\n3571,0\\r\\n3572,0\\r\\n3573,0\\r\\n3574,0\\r\\n3575,0\\r\\n3576,0\\r\\n3577,0\\r\\n3578,0\\r\\n3579,0\\r\\n3580,0\\r\\n3581,0\\r\\n3582,0\\r\\n3583,0\\r\\n3584,0\\r\\n3585,0\\r\\n3586,0\\r\\n3587,0\\r\\n3588,0\\r\\n3589,0\\r\\n3590,0\\r\\n3591,0\\r\\n3592,0\\r\\n3593,0\\r\\n3594,0\\r\\n3595,0\\r\\n3596,0\\r\\n3597,0\\r\\n3598,0\\r\\n3599,0\\r\\n3600,0\\r\\n3601,0\\r\\n3602,0\\r\\n3603,0\\r\\n3604,0\\r\\n3605,0\\r\\n3606,0\\r\\n3607,0\\r\\n3608,0\\r\\n3609,0\\r\\n3610,0\\r\\n3611,0\\r\\n3612,0\\r\\n3613,0\\r\\n3614,0\\r\\n3615,0\\r\\n3616,0\\r\\n3617,0\\r\\n3618,0\\r\\n3619,0\\r\\n3620,0\\r\\n3621,0\\r\\n3622,0\\r\\n3623,0\\r\\n3624,0\\r\\n3625,0\\r\\n3626,0\\r\\n3627,0\\r\\n3628,0\\r\\n3629,0\\r\\n3630,0\\r\\n3631,0\\r\\n3632,0\\r\\n3633,0\\r\\n3634,0\\r\\n3635,0\\r\\n3636,0\\r\\n3637,0\\r\\n3638,0\\r\\n3639,0\\r\\n3640,0\\r\\n3641,0\\r\\n3642,0\\r\\n3643,0\\r\\n3644,0\\r\\n3645,0\\r\\n3646,0\\r\\n3647,0\\r\\n3648,0\\r\\n3649,0\\r\\n3650,0\\r\\n3651,0\\r\\n3652,0\\r\\n3653,0\\r\\n3654,0\\r\\n3655,0\\r\\n3656,0\\r\\n3657,0\\r\\n3658,0\\r\\n3659,0\\r\\n3660,0\\r\\n3661,0\\r\\n3662,0\\r\\n3663,0\\r\\n3664,0\\r\\n3665,0\\r\\n3666,0\\r\\n3667,0\\r\\n3668,0\\r\\n3669,0\\r\\n3670,0\\r\\n3671,0\\r\\n3672,0\\r\\n3673,0\\r\\n3674,0\\r\\n3675,0\\r\\n3676,0\\r\\n3677,0\\r\\n3678,0\\r\\n3679,0\\r\\n3680,0\\r\\n3681,0\\r\\n3682,0\\r\\n3683,0\\r\\n3684,0\\r\\n3685,0\\r\\n3686,0\\r\\n3687,0\\r\\n3688,0\\r\\n3689,0\\r\\n3690,0\\r\\n3691,0\\r\\n3692,0\\r\\n3693,0\\r\\n3694,0\\r\\n3695,0\\r\\n3696,0\\r\\n3697,0\\r\\n3698,0\\r\\n3699,0\\r\\n3700,0\\r\\n3701,0\\r\\n3702,0\\r\\n3703,0\\r\\n3704,0\\r\\n3705,0\\r\\n3706,0\\r\\n3707,0\\r\\n3708,0\\r\\n3709,0\\r\\n3710,0\\r\\n3711,0\\r\\n3712,0\\r\\n3713,0\\r\\n3714,0\\r\\n3715,0\\r\\n3716,0\\r\\n3717,0\\r\\n3718,0\\r\\n3719,0\\r\\n3720,0\\r\\n3721,0\\r\\n3722,0\\r\\n3723,0\\r\\n3724,0\\r\\n3725,0\\r\\n3726,0\\r\\n3727,0\\r\\n3728,0\\r\\n3729,0\\r\\n3730,0\\r\\n3731,0\\r\\n3732,0\\r\\n3733,0\\r\\n3734,0\\r\\n3735,0\\r\\n3736,0\\r\\n3737,0\\r\\n3738,0\\r\\n3739,0\\r\\n3740,0\\r\\n3741,0\\r\\n3742,0\\r\\n3743,0\\r\\n3744,0\\r\\n3745,0\\r\\n3746,0\\r\\n3747,0\\r\\n3748,0\\r\\n3749,0\\r\\n3750,0\\r\\n3751,0\\r\\n3752,0\\r\\n3753,0\\r\\n3754,0\\r\\n3755,0\\r\\n3756,0\\r\\n3757,0\\r\\n3758,0\\r\\n3759,0\\r\\n3760,0\\r\\n3761,0\\r\\n3762,0\\r\\n3763,0\\r\\n3764,0\\r\\n3765,0\\r\\n3766,0\\r\\n3767,0\\r\\n3768,0\\r\\n3769,0\\r\\n3770,0\\r\\n3771,0\\r\\n3772,0\\r\\n3773,0\\r\\n3774,0\\r\\n3775,0\\r\\n3776,0\\r\\n3777,0\\r\\n3778,0\\r\\n3779,0\\r\\n3780,0\\r\\n3781,0\\r\\n3782,0\\r\\n3783,0\\r\\n3784,0\\r\\n3785,0\\r\\n3786,0\\r\\n3787,0\\r\\n3788,0\\r\\n3789,0\\r\\n3790,0\\r\\n3791,0\\r\\n3792,0\\r\\n3793,0\\r\\n3794,0\\r\\n3795,0\\r\\n3796,0\\r\\n3797,0\\r\\n3798,0\\r\\n3799,0\\r\\n3800,0\\r\\n3801,0\\r\\n3802,0\\r\\n3803,0\\r\\n3804,0\\r\\n3805,0\\r\\n3806,0\\r\\n3807,0\\r\\n3808,0\\r\\n3809,0\\r\\n3810,0\\r\\n3811,0\\r\\n3812,0\\r\\n3813,0\\r\\n3814,0\\r\\n3815,0\\r\\n3816,0\\r\\n3817,0\\r\\n3818,0\\r\\n3819,0\\r\\n3820,0\\r\\n3821,0\\r\\n3822,0\\r\\n3823,0\\r\\n3824,0\\r\\n3825,0\\r\\n3826,0\\r\\n3827,0\\r\\n3828,0\\r\\n3829,0\\r\\n3830,0\\r\\n3831,0\\r\\n3832,0\\r\\n3833,0\\r\\n3834,0\\r\\n3835,0\\r\\n3836,0\\r\\n3837,0\\r\\n3838,0\\r\\n3839,0\\r\\n3840,0\\r\\n3841,0\\r\\n3842,0\\r\\n3843,0\\r\\n3844,0\\r\\n3845,0\\r\\n3846,0\\r\\n3847,0\\r\\n3848,0\\r\\n3849,0\\r\\n3850,0\\r\\n3851,0\\r\\n3852,0\\r\\n3853,0\\r\\n3854,0\\r\\n3855,0\\r\\n3856,0\\r\\n3857,0\\r\\n3858,0\\r\\n3859,0\\r\\n3860,0\\r\\n3861,0\\r\\n3862,0\\r\\n3863,0\\r\\n3864,0\\r\\n3865,0\\r\\n3866,0\\r\\n3867,0\\r\\n3868,0\\r\\n3869,0\\r\\n3870,0\\r\\n3871,0\\r\\n3872,0\\r\\n3873,0\\r\\n3874,0\\r\\n3875,0\\r\\n3876,0\\r\\n3877,0\\r\\n3878,0\\r\\n3879,0\\r\\n3880,0\\r\\n3881,0\\r\\n3882,0\\r\\n3883,0\\r\\n3884,0\\r\\n3885,0\\r\\n3886,0\\r\\n3887,0\\r\\n3888,0\\r\\n3889,0\\r\\n3890,0\\r\\n3891,0\\r\\n3892,0\\r\\n3893,0\\r\\n3894,0\\r\\n3895,0\\r\\n3896,0\\r\\n3897,0\\r\\n3898,0\\r\\n3899,0\\r\\n3900,0\\r\\n3901,0\\r\\n3902,0\\r\\n3903,0\\r\\n3904,0\\r\\n3905,0\\r\\n3906,0\\r\\n3907,0\\r\\n3908,0\\r\\n3909,0\\r\\n3910,0\\r\\n3911,0\\r\\n3912,0\\r\\n3913,0\\r\\n3914,0\\r\\n3915,0\\r\\n3916,0\\r\\n3917,0\\r\\n3918,0\\r\\n3919,0\\r\\n3920,0\\r\\n3921,0\\r\\n3922,0\\r\\n3923,0\\r\\n3924,0\\r\\n3925,0\\r\\n3926,0\\r\\n3927,0\\r\\n3928,0\\r\\n3929,0\\r\\n3930,0\\r\\n3931,0\\r\\n3932,0\\r\\n3933,0\\r\\n3934,0\\r\\n3935,0\\r\\n3936,0\\r\\n3937,0\\r\\n3938,0\\r\\n3939,0\\r\\n3940,0\\r\\n3941,0\\r\\n3942,0\\r\\n3943,0\\r\\n3944,0\\r\\n3945,0\\r\\n3946,0\\r\\n3947,0\\r\\n3948,0\\r\\n3949,0\\r\\n3950,0\\r\\n3951,0\\r\\n3952,0\\r\\n3953,0\\r\\n3954,0\\r\\n3955,0\\r\\n3956,0\\r\\n3957,0\\r\\n3958,0\\r\\n3959,0\\r\\n3960,0\\r\\n3961,0\\r\\n3962,0\\r\\n3963,0\\r\\n3964,0\\r\\n3965,0\\r\\n3966,0\\r\\n3967,0\\r\\n3968,0\\r\\n3969,0\\r\\n3970,0\\r\\n3971,0\\r\\n3972,0\\r\\n3973,0\\r\\n3974,0\\r\\n3975,0\\r\\n3976,0\\r\\n3977,0\\r\\n3978,0\\r\\n3979,0\\r\\n3980,0\\r\\n3981,0\\r\\n3982,0\\r\\n3983,0\\r\\n3984,0\\r\\n3985,0\\r\\n3986,0\\r\\n3987,0\\r\\n3988,0\\r\\n3989,0\\r\\n3990,0\\r\\n3991,0\\r\\n3992,0\\r\\n3993,0\\r\\n3994,0\\r\\n3995,0\\r\\n3996,0\\r\\n3997,0\\r\\n3998,0\\r\\n3999,0\\r\\n4000,0\\r\\n4001,0\\r\\n4002,0\\r\\n4003,0\\r\\n4004,0\\r\\n4005,0\\r\\n4006,0\\r\\n4007,0\\r\\n4008,0\\r\\n4009,0\\r\\n4010,0\\r\\n4011,0\\r\\n4012,0\\r\\n4013,0\\r\\n4014,0\\r\\n4015,0\\r\\n4016,0\\r\\n4017,0\\r\\n4018,0\\r\\n4019,0\\r\\n4020,0\\r\\n4021,0\\r\\n4022,0\\r\\n4023,0\\r\\n4024,0\\r\\n4025,0\\r\\n4026,0\\r\\n4027,0\\r\\n4028,0\\r\\n4029,0\\r\\n4030,0\\r\\n4031,0\\r\\n4032,0\\r\\n4033,0\\r\\n4034,0\\r\\n4035,0\\r\\n4036,0\\r\\n4037,0\\r\\n4038,0\\r\\n4039,0\\r\\n4040,0\\r\\n4041,0\\r\\n4042,0\\r\\n4043,0\\r\\n4044,0\\r\\n4045,0\\r\\n4046,0\\r\\n4047,0\\r\\n4048,0\\r\\n4049,0\\r\\n4050,0\\r\\n4051,0\\r\\n4052,0\\r\\n4053,0\\r\\n4054,0\\r\\n4055,0\\r\\n4056,0\\r\\n4057,0\\r\\n4058,0\\r\\n4059,0\\r\\n4060,0\\r\\n4061,0\\r\\n4062,0\\r\\n4063,0\\r\\n4064,0\\r\\n4065,0\\r\\n4066,0\\r\\n4067,0\\r\\n4068,0\\r\\n4069,0\\r\\n4070,0\\r\\n4071,0\\r\\n4072,0\\r\\n4073,0\\r\\n4074,0\\r\\n4075,0\\r\\n4076,0\\r\\n4077,0\\r\\n4078,0\\r\\n4079,0\\r\\n4080,0\\r\\n4081,0\\r\\n4082,0\\r\\n4083,0\\r\\n4084,0\\r\\n4085,0\\r\\n4086,0\\r\\n4087,0\\r\\n4088,0\\r\\n4089,0\\r\\n4090,0\\r\\n4091,0\\r\\n4092,0\\r\\n4093,0\\r\\n4094,0\\r\\n4095,0\\r\\n4096,0\\r\\n4097,0\\r\\n4098,0\\r\\n4099,0\\r\\n4100,0\\r\\n4101,0\\r\\n4102,0\\r\\n4103,0\\r\\n4104,0\\r\\n4105,0\\r\\n4106,0\\r\\n4107,0\\r\\n4108,0\\r\\n4109,0\\r\\n4110,0\\r\\n4111,0\\r\\n4112,0\\r\\n4113,0\\r\\n4114,0\\r\\n4115,0\\r\\n4116,0\\r\\n4117,0\\r\\n4118,0\\r\\n4119,0\\r\\n4120,0\\r\\n4121,0\\r\\n4122,0\\r\\n4123,0\\r\\n4124,0\\r\\n4125,0\\r\\n4126,0\\r\\n4127,0\\r\\n4128,0\\r\\n4129,0\\r\\n4130,0\\r\\n4131,0\\r\\n4132,0\\r\\n4133,0\\r\\n4134,0\\r\\n4135,0\\r\\n4136,0\\r\\n4137,0\\r\\n4138,0\\r\\n4139,0\\r\\n4140,0\\r\\n4141,0\\r\\n4142,0\\r\\n4143,0\\r\\n4144,0\\r\\n4145,0\\r\\n4146,0\\r\\n4147,0\\r\\n4148,0\\r\\n4149,0\\r\\n4150,0\\r\\n4151,0\\r\\n4152,0\\r\\n4153,0\\r\\n4154,0\\r\\n4155,0\\r\\n4156,0\\r\\n4157,0\\r\\n4158,0\\r\\n4159,0\\r\\n4160,0\\r\\n4161,0\\r\\n4162,0\\r\\n4163,0\\r\\n4164,0\\r\\n4165,0\\r\\n4166,0\\r\\n4167,0\\r\\n4168,0\\r\\n4169,0\\r\\n4170,0\\r\\n4171,0\\r\\n4172,0\\r\\n4173,0\\r\\n4174,0\\r\\n4175,0\\r\\n4176,0\\r\\n4177,0\\r\\n4178,0\\r\\n4179,0\\r\\n4180,0\\r\\n4181,0\\r\\n4182,0\\r\\n4183,0\\r\\n4184,0\\r\\n4185,0\\r\\n4186,0\\r\\n4187,0\\r\\n4188,0\\r\\n4189,0\\r\\n4190,0\\r\\n4191,0\\r\\n4192,0\\r\\n4193,0\\r\\n4194,0\\r\\n4195,0\\r\\n4196,0\\r\\n4197,0\\r\\n4198,0\\r\\n4199,0\\r\\n4200,0\\r\\n4201,0\\r\\n4202,0\\r\\n4203,0\\r\\n4204,0\\r\\n4205,0\\r\\n4206,0\\r\\n4207,0\\r\\n4208,0\\r\\n4209,0\\r\\n4210,0\\r\\n4211,0\\r\\n4212,0\\r\\n4213,0\\r\\n4214,0\\r\\n4215,0\\r\\n4216,0\\r\\n4217,0\\r\\n4218,0\\r\\n4219,0\\r\\n4220,0\\r\\n4221,0\\r\\n4222,0\\r\\n4223,0\\r\\n4224,0\\r\\n4225,0\\r\\n4226,0\\r\\n4227,0\\r\\n4228,0\\r\\n4229,0\\r\\n4230,0\\r\\n4231,0\\r\\n4232,0\\r\\n4233,0\\r\\n4234,0\\r\\n4235,0\\r\\n4236,0\\r\\n4237,0\\r\\n4238,0\\r\\n4239,0\\r\\n4240,0\\r\\n4241,0\\r\\n4242,0\\r\\n4243,0\\r\\n4244,0\\r\\n4245,0\\r\\n4246,0\\r\\n4247,0\\r\\n4248,0\\r\\n4249,0\\r\\n4250,0\\r\\n4251,0\\r\\n4252,0\\r\\n4253,0\\r\\n4254,0\\r\\n4255,0\\r\\n4256,0\\r\\n4257,0\\r\\n4258,0\\r\\n4259,0\\r\\n4260,0\\r\\n4261,0\\r\\n4262,0\\r\\n4263,0\\r\\n4264,0\\r\\n4265,0\\r\\n4266,0\\r\\n4267,0\\r\\n4268,0\\r\\n4269,0\\r\\n4270,0\\r\\n4271,0\\r\\n4272,0\\r\\n4273,0\\r\\n4274,0\\r\\n4275,0\\r\\n4276,0\\r\\n4277,0\\r\\n4278,0\\r\\n4279,0\\r\\n4280,0\\r\\n4281,0\\r\\n4282,0\\r\\n4283,0\\r\\n4284,0\\r\\n4285,0\\r\\n4286,0\\r\\n4287,0\\r\\n4288,0\\r\\n4289,0\\r\\n4290,0\\r\\n4291,0\\r\\n4292,0\\r\\n4293,0\\r\\n4294,0\\r\\n4295,0\\r\\n4296,0\\r\\n4297,0\\r\\n4298,0\\r\\n4299,0\\r\\n4300,0\\r\\n4301,0\\r\\n4302,0\\r\\n4303,0\\r\\n4304,0\\r\\n4305,0\\r\\n4306,0\\r\\n4307,0\\r\\n4308,0\\r\\n4309,0\\r\\n4310,0\\r\\n4311,0\\r\\n4312,0\\r\\n4313,0\\r\\n4314,0\\r\\n4315,0\\r\\n4316,0\\r\\n4317,0\\r\\n4318,0\\r\\n4319,0\\r\\n4320,0\\r\\n4321,0\\r\\n4322,0\\r\\n4323,0\\r\\n4324,0\\r\\n4325,0\\r\\n4326,0\\r\\n4327,0\\r\\n4328,0\\r\\n4329,0\\r\\n4330,0\\r\\n4331,0\\r\\n4332,0\\r\\n4333,0\\r\\n4334,0\\r\\n4335,0\\r\\n4336,0\\r\\n4337,0\\r\\n4338,0\\r\\n4339,0\\r\\n4340,0\\r\\n4341,0\\r\\n4342,0\\r\\n4343,0\\r\\n4344,0\\r\\n4345,0\\r\\n4346,0\\r\\n4347,0\\r\\n4348,0\\r\\n4349,0\\r\\n4350,0\\r\\n4351,0\\r\\n4352,0\\r\\n4353,0\\r\\n4354,0\\r\\n4355,0\\r\\n4356,0\\r\\n4357,0\\r\\n4358,0\\r\\n4359,0\\r\\n4360,0\\r\\n4361,0\\r\\n4362,0\\r\\n4363,0\\r\\n4364,0\\r\\n4365,0\\r\\n4366,0\\r\\n4367,0\\r\\n4368,0\\r\\n4369,0\\r\\n4370,0\\r\\n4371,0\\r\\n4372,0\\r\\n4373,0\\r\\n4374,0\\r\\n4375,0\\r\\n4376,0\\r\\n4377,0\\r\\n4378,0\\r\\n4379,0\\r\\n4380,0\\r\\n4381,0\\r\\n4382,0\\r\\n4383,0\\r\\n4384,0\\r\\n4385,0\\r\\n4386,0\\r\\n4387,0\\r\\n4388,0\\r\\n4389,0\\r\\n4390,0\\r\\n4391,0\\r\\n4392,0\\r\\n4393,0\\r\\n4394,0\\r\\n4395,0\\r\\n4396,0\\r\\n4397,0\\r\\n4398,0\\r\\n4399,0\\r\\n4400,0\\r\\n4401,0\\r\\n4402,0\\r\\n4403,0\\r\\n4404,0\\r\\n4405,0\\r\\n4406,0\\r\\n4407,0\\r\\n4408,0\\r\\n4409,0\\r\\n4410,0\\r\\n4411,0\\r\\n4412,0\\r\\n4413,0\\r\\n4414,0\\r\\n4415,0\\r\\n4416,0\\r\\n4417,0\\r\\n4418,0\\r\\n4419,0\\r\\n4420,0\\r\\n4421,0\\r\\n4422,0\\r\\n4423,0\\r\\n4424,0\\r\\n4425,0\\r\\n4426,0\\r\\n4427,0\\r\\n4428,0\\r\\n4429,0\\r\\n4430,0\\r\\n4431,0\\r\\n4432,0\\r\\n4433,0\\r\\n4434,0\\r\\n4435,0\\r\\n4436,0\\r\\n4437,0\\r\\n4438,0\\r\\n4439,0\\r\\n4440,0\\r\\n4441,0\\r\\n4442,0\\r\\n4443,0\\r\\n4444,0\\r\\n4445,0\\r\\n4446,0\\r\\n4447,0\\r\\n4448,0\\r\\n4449,0\\r\\n4450,0\\r\\n4451,0\\r\\n4452,0\\r\\n4453,0\\r\\n4454,0\\r\\n4455,0\\r\\n4456,0\\r\\n4457,0\\r\\n4458,0\\r\\n4459,0\\r\\n4460,0\\r\\n4461,0\\r\\n4462,0\\r\\n4463,0\\r\\n4464,0\\r\\n4465,0\\r\\n4466,0\\r\\n4467,0\\r\\n4468,0\\r\\n4469,0\\r\\n4470,0\\r\\n4471,0\\r\\n4472,0\\r\\n4473,0\\r\\n4474,0\\r\\n4475,0\\r\\n4476,0\\r\\n4477,0\\r\\n4478,0\\r\\n4479,0\\r\\n4480,0\\r\\n4481,0\\r\\n4482,0\\r\\n4483,0\\r\\n4484,0\\r\\n4485,0\\r\\n4486,0\\r\\n4487,0\\r\\n4488,0\\r\\n4489,0\\r\\n4490,0\\r\\n4491,0\\r\\n4492,0\\r\\n4493,0\\r\\n4494,0\\r\\n4495,0\\r\\n4496,0\\r\\n4497,0\\r\\n4498,0\\r\\n4499,0\\r\\n4500,0\\r\\n4501,0\\r\\n4502,0\\r\\n4503,0\\r\\n4504,0\\r\\n4505,0\\r\\n4506,0\\r\\n4507,0\\r\\n4508,0\\r\\n4509,0\\r\\n4510,0\\r\\n4511,0\\r\\n4512,0\\r\\n4513,0\\r\\n4514,0\\r\\n4515,0\\r\\n4516,0\\r\\n4517,0\\r\\n4518,0\\r\\n4519,0\\r\\n4520,0\\r\\n4521,0\\r\\n4522,0\\r\\n4523,0\\r\\n4524,0\\r\\n4525,0\\r\\n4526,0\\r\\n4527,0\\r\\n4528,0\\r\\n4529,0\\r\\n4530,0\\r\\n4531,0\\r\\n4532,0\\r\\n4533,0\\r\\n4534,0\\r\\n4535,0\\r\\n4536,0\\r\\n4537,0\\r\\n4538,0\\r\\n4539,0\\r\\n4540,0\\r\\n4541,0\\r\\n4542,0\\r\\n4543,0\\r\\n4544,0\\r\\n4545,0\\r\\n4546,0\\r\\n4547,0\\r\\n4548,0\\r\\n4549,0\\r\\n4550,0\\r\\n4551,0\\r\\n4552,0\\r\\n4553,0\\r\\n4554,0\\r\\n4555,0\\r\\n4556,0\\r\\n4557,0\\r\\n4558,0\\r\\n4559,0\\r\\n4560,0\\r\\n4561,0\\r\\n4562,0\\r\\n4563,0\\r\\n4564,0\\r\\n4565,0\\r\\n4566,0\\r\\n4567,0\\r\\n4568,0\\r\\n4569,0\\r\\n4570,0\\r\\n4571,0\\r\\n4572,0\\r\\n4573,0\\r\\n4574,0\\r\\n4575,0\\r\\n4576,0\\r\\n4577,0\\r\\n4578,0\\r\\n4579,0\\r\\n4580,0\\r\\n4581,0\\r\\n4582,0\\r\\n4583,0\\r\\n4584,0\\r\\n4585,0\\r\\n4586,0\\r\\n4587,0\\r\\n4588,0\\r\\n4589,0\\r\\n4590,0\\r\\n4591,0\\r\\n4592,0\\r\\n4593,0\\r\\n4594,0\\r\\n4595,0\\r\\n4596,0\\r\\n4597,0\\r\\n4598,0\\r\\n4599,0\\r\\n4600,0\\r\\n4601,0\\r\\n4602,0\\r\\n4603,0\\r\\n4604,0\\r\\n4605,0\\r\\n4606,0\\r\\n4607,0\\r\\n4608,0\\r\\n4609,0\\r\\n4610,0\\r\\n4611,0\\r\\n4612,0\\r\\n4613,0\\r\\n4614,0\\r\\n4615,0\\r\\n4616,0\\r\\n4617,0\\r\\n4618,0\\r\\n4619,0\\r\\n4620,0\\r\\n4621,0\\r\\n4622,0\\r\\n4623,0\\r\\n4624,0\\r\\n4625,0\\r\\n4626,0\\r\\n4627,0\\r\\n4628,0\\r\\n4629,0\\r\\n4630,0\\r\\n4631,0\\r\\n4632,0\\r\\n4633,0\\r\\n4634,0\\r\\n4635,0\\r\\n4636,0\\r\\n4637,0\\r\\n4638,0\\r\\n4639,0\\r\\n4640,0\\r\\n4641,0\\r\\n4642,0\\r\\n4643,0\\r\\n4644,0\\r\\n4645,0\\r\\n4646,0\\r\\n4647,0\\r\\n4648,0\\r\\n4649,0\\r\\n4650,0\\r\\n4651,0\\r\\n4652,0\\r\\n4653,0\\r\\n4654,0\\r\\n4655,0\\r\\n4656,0\\r\\n4657,0\\r\\n4658,0\\r\\n4659,0\\r\\n4660,0\\r\\n4661,0\\r\\n4662,0\\r\\n4663,0\\r\\n4664,0\\r\\n4665,0\\r\\n4666,0\\r\\n4667,0\\r\\n4668,0\\r\\n4669,0\\r\\n4670,0\\r\\n4671,0\\r\\n4672,0\\r\\n4673,0\\r\\n4674,0\\r\\n4675,0\\r\\n4676,0\\r\\n4677,0\\r\\n4678,0\\r\\n4679,0\\r\\n4680,0\\r\\n4681,0\\r\\n4682,0\\r\\n4683,0\\r\\n4684,0\\r\\n4685,0\\r\\n4686,0\\r\\n4687,0\\r\\n4688,0\\r\\n4689,0\\r\\n4690,0\\r\\n4691,0\\r\\n4692,0\\r\\n4693,0\\r\\n4694,0\\r\\n4695,0\\r\\n4696,0\\r\\n4697,0\\r\\n4698,0\\r\\n4699,0\\r\\n4700,0\\r\\n4701,0\\r\\n4702,0\\r\\n4703,0\\r\\n4704,0\\r\\n4705,0\\r\\n4706,0\\r\\n4707,0\\r\\n4708,0\\r\\n4709,0\\r\\n4710,0\\r\\n4711,0\\r\\n4712,0\\r\\n4713,0\\r\\n4714,0\\r\\n4715,0\\r\\n4716,0\\r\\n4717,0\\r\\n4718,0\\r\\n4719,0\\r\\n4720,0\\r\\n4721,0\\r\\n4722,0\\r\\n4723,0\\r\\n4724,0\\r\\n4725,0\\r\\n4726,0\\r\\n4727,0\\r\\n4728,0\\r\\n4729,0\\r\\n4730,0\\r\\n4731,0\\r\\n4732,0\\r\\n4733,0\\r\\n4734,0\\r\\n4735,0\\r\\n4736,0\\r\\n4737,0\\r\\n4738,0\\r\\n4739,0\\r\\n4740,0\\r\\n4741,0\\r\\n4742,0\\r\\n4743,0\\r\\n4744,0\\r\\n4745,0\\r\\n4746,0\\r\\n4747,0\\r\\n4748,0\\r\\n4749,0\\r\\n4750,0\\r\\n4751,0\\r\\n4752,0\\r\\n4753,0\\r\\n4754,0\\r\\n4755,0\\r\\n4756,0\\r\\n4757,0\\r\\n4758,0\\r\\n4759,0\\r\\n4760,0\\r\\n4761,0\\r\\n4762,0\\r\\n4763,0\\r\\n4764,0\\r\\n4765,0\\r\\n4766,0\\r\\n4767,0\\r\\n4768,0\\r\\n4769,0\\r\\n4770,0\\r\\n4771,0\\r\\n4772,0\\r\\n4773,0\\r\\n4774,0\\r\\n4775,0\\r\\n4776,0\\r\\n4777,0\\r\\n4778,0\\r\\n4779,0\\r\\n4780,0\\r\\n4781,0\\r\\n4782,0\\r\\n4783,0\\r\\n4784,0\\r\\n4785,0\\r\\n4786,0\\r\\n4787,0\\r\\n4788,0\\r\\n4789,0\\r\\n4790,0\\r\\n4791,0\\r\\n4792,0\\r\\n4793,0\\r\\n4794,0\\r\\n4795,0\\r\\n4796,0\\r\\n4797,0\\r\\n4798,0\\r\\n4799,0\\r\\n4800,0\\r\\n4801,0\\r\\n4802,0\\r\\n4803,0\\r\\n4804,0\\r\\n4805,0\\r\\n4806,0\\r\\n4807,0\\r\\n4808,0\\r\\n4809,0\\r\\n4810,0\\r\\n4811,0\\r\\n4812,0\\r\\n4813,0\\r\\n4814,0\\r\\n4815,0\\r\\n4816,0\\r\\n4817,0\\r\\n4818,0\\r\\n4819,0\\r\\n4820,0\\r\\n4821,0\\r\\n4822,0\\r\\n4823,0\\r\\n4824,0\\r\\n4825,0\\r\\n4826,0\\r\\n4827,0\\r\\n4828,0\\r\\n4829,0\\r\\n4830,0\\r\\n4831,0\\r\\n4832,0\\r\\n4833,0\\r\\n4834,0\\r\\n4835,0\\r\\n4836,0\\r\\n4837,0\\r\\n4838,0\\r\\n4839,0\\r\\n4840,0\\r\\n4841,0\\r\\n4842,0\\r\\n4843,0\\r\\n4844,0\\r\\n4845,0\\r\\n4846,0\\r\\n4847,0\\r\\n4848,0\\r\\n4849,0\\r\\n4850,0\\r\\n4851,0\\r\\n4852,0\\r\\n4853,0\\r\\n4854,0\\r\\n4855,0\\r\\n4856,0\\r\\n4857,0\\r\\n4858,0\\r\\n4859,0\\r\\n4860,0\\r\\n4861,0\\r\\n4862,0\\r\\n4863,0\\r\\n4864,0\\r\\n4865,0\\r\\n4866,0\\r\\n4867,0\\r\\n4868,0\\r\\n4869,0\\r\\n4870,0\\r\\n4871,0\\r\\n4872,0\\r\\n4873,0\\r\\n4874,0\\r\\n4875,0\\r\\n4876,0\\r\\n4877,0\\r\\n4878,0\\r\\n4879,0\\r\\n4880,0\\r\\n4881,0\\r\\n4882,0\\r\\n4883,0\\r\\n4884,0\\r\\n4885,0\\r\\n4886,0\\r\\n4887,0\\r\\n4888,0\\r\\n4889,0\\r\\n4890,0\\r\\n4891,0\\r\\n4892,0\\r\\n4893,0\\r\\n4894,0\\r\\n4895,0\\r\\n4896,0\\r\\n4897,0\\r\\n4898,0\\r\\n4899,0\\r\\n4900,0\\r\\n4901,0\\r\\n4902,0\\r\\n4903,0\\r\\n4904,0\\r\\n4905,0\\r\\n4906,0\\r\\n4907,0\\r\\n4908,0\\r\\n4909,0\\r\\n4910,0\\r\\n4911,0\\r\\n4912,0\\r\\n4913,0\\r\\n4914,0\\r\\n4915,0\\r\\n4916,0\\r\\n4917,0\\r\\n4918,0\\r\\n4919,0\\r\\n4920,0\\r\\n4921,0\\r\\n4922,0\\r\\n4923,0\\r\\n4924,0\\r\\n4925,0\\r\\n4926,0\\r\\n4927,0\\r\\n4928,0\\r\\n4929,0\\r\\n4930,0\\r\\n4931,0\\r\\n4932,0\\r\\n4933,0\\r\\n4934,0\\r\\n4935,0\\r\\n4936,0\\r\\n4937,0\\r\\n4938,0\\r\\n4939,0\\r\\n4940,0\\r\\n4941,0\\r\\n4942,0\\r\\n4943,0\\r\\n4944,0\\r\\n4945,0\\r\\n4946,0\\r\\n4947,0\\r\\n4948,0\\r\\n4949,0\\r\\n4950,0\\r\\n4951,0\\r\\n4952,0\\r\\n4953,0\\r\\n4954,0\\r\\n4955,0\\r\\n4956,0\\r\\n4957,0\\r\\n4958,0\\r\\n4959,0\\r\\n4960,0\\r\\n4961,0\\r\\n4962,0\\r\\n4963,0\\r\\n4964,0\\r\\n4965,0\\r\\n4966,0\\r\\n4967,0\\r\\n4968,0\\r\\n4969,0\\r\\n4970,0\\r\\n4971,0\\r\\n4972,0\\r\\n4973,0\\r\\n4974,0\\r\\n4975,0\\r\\n4976,0\\r\\n4977,0\\r\\n4978,0\\r\\n4979,0\\r\\n4980,0\\r\\n4981,0\\r\\n4982,0\\r\\n4983,0\\r\\n4984,0\\r\\n4985,0\\r\\n4986,0\\r\\n4987,0\\r\\n4988,0\\r\\n4989,0\\r\\n4990,0\\r\\n4991,0\\r\\n4992,0\\r\\n4993,0\\r\\n4994,0\\r\\n4995,0\\r\\n4996,0\\r\\n4997,0\\r\\n4998,0\\r\\n4999,0\\r\\n5000,0\\r\\n5001,0\\r\\n5002,0\\r\\n5003,0\\r\\n5004,0\\r\\n5005,0\\r\\n5006,0\\r\\n5007,0\\r\\n5008,0\\r\\n5009,0\\r\\n5010,0\\r\\n5011,0\\r\\n5012,0\\r\\n5013,0\\r\\n5014,0\\r\\n5015,0\\r\\n5016,0\\r\\n5017,0\\r\\n5018,0\\r\\n5019,0\\r\\n5020,0\\r\\n5021,0\\r\\n5022,0\\r\\n5023,0\\r\\n5024,0\\r\\n5025,0\\r\\n5026,0\\r\\n5027,0\\r\\n5028,0\\r\\n5029,0\\r\\n5030,0\\r\\n5031,0\\r\\n5032,0\\r\\n5033,0\\r\\n5034,0\\r\\n5035,0\\r\\n5036,0\\r\\n5037,0\\r\\n5038,0\\r\\n5039,0\\r\\n5040,0\\r\\n5041,0\\r\\n5042,0\\r\\n5043,0\\r\\n5044,0\\r\\n5045,0\\r\\n5046,0\\r\\n5047,0\\r\\n5048,0\\r\\n5049,0\\r\\n5050,0\\r\\n5051,0\\r\\n5052,0\\r\\n5053,0\\r\\n5054,0\\r\\n5055,0\\r\\n5056,0\\r\\n5057,0\\r\\n5058,0\\r\\n5059,0\\r\\n5060,0\\r\\n5061,0\\r\\n5062,0\\r\\n5063,0\\r\\n5064,0\\r\\n5065,0\\r\\n5066,0\\r\\n5067,0\\r\\n5068,0\\r\\n5069,0\\r\\n5070,0\\r\\n5071,0\\r\\n5072,0\\r\\n5073,0\\r\\n5074,0\\r\\n5075,0\\r\\n5076,0\\r\\n5077,0\\r\\n5078,0\\r\\n5079,0\\r\\n5080,0\\r\\n5081,0\\r\\n5082,0\\r\\n5083,0\\r\\n5084,0\\r\\n5085,0\\r\\n5086,0\\r\\n5087,0\\r\\n5088,0\\r\\n5089,0\\r\\n5090,0\\r\\n5091,0\\r\\n5092,0\\r\\n5093,0\\r\\n5094,0\\r\\n5095,0\\r\\n5096,0\\r\\n5097,0\\r\\n5098,0\\r\\n5099,0\\r\\n5100,0\\r\\n5101,0\\r\\n5102,0\\r\\n5103,0\\r\\n5104,0\\r\\n5105,0\\r\\n5106,0\\r\\n5107,0\\r\\n5108,0\\r\\n5109,0\\r\\n5110,0\\r\\n5111,0\\r\\n5112,0\\r\\n5113,0\\r\\n5114,0\\r\\n5115,0\\r\\n5116,0\\r\\n5117,0\\r\\n5118,0\\r\\n5119,0\\r\\n5120,0\\r\\n5121,0\\r\\n5122,0\\r\\n5123,0\\r\\n5124,0\\r\\n5125,0\\r\\n5126,0\\r\\n5127,0\\r\\n5128,0\\r\\n5129,0\\r\\n5130,0\\r\\n5131,0\\r\\n5132,0\\r\\n5133,0\\r\\n5134,0\\r\\n5135,0\\r\\n5136,0\\r\\n5137,0\\r\\n5138,0\\r\\n5139,0\\r\\n5140,0\\r\\n5141,0\\r\\n5142,0\\r\\n5143,0\\r\\n5144,0\\r\\n5145,0\\r\\n5146,0\\r\\n5147,0\\r\\n5148,0\\r\\n5149,0\\r\\n5150,0\\r\\n5151,0\\r\\n5152,0\\r\\n5153,0\\r\\n5154,0\\r\\n5155,0\\r\\n5156,0\\r\\n5157,0\\r\\n5158,0\\r\\n5159,0\\r\\n5160,0\\r\\n5161,0\\r\\n5162,0\\r\\n5163,0\\r\\n5164,0\\r\\n5165,0\\r\\n5166,0\\r\\n5167,0\\r\\n5168,0\\r\\n5169,0\\r\\n5170,0\\r\\n5171,0\\r\\n5172,0\\r\\n5173,0\\r\\n5174,0\\r\\n5175,0\\r\\n5176,0\\r\\n5177,0\\r\\n5178,0\\r\\n5179,0\\r\\n5180,0\\r\\n5181,0\\r\\n5182,0\\r\\n5183,0\\r\\n5184,0\\r\\n5185,0\\r\\n5186,0\\r\\n5187,0\\r\\n5188,0\\r\\n5189,0\\r\\n5190,0\\r\\n5191,0\\r\\n5192,0\\r\\n5193,0\\r\\n5194,0\\r\\n5195,0\\r\\n5196,0\\r\\n5197,0\\r\\n5198,0\\r\\n5199,0\\r\\n5200,0\\r\\n5201,0\\r\\n5202,0\\r\\n5203,0\\r\\n5204,0\\r\\n5205,0\\r\\n5206,0\\r\\n5207,0\\r\\n5208,0\\r\\n5209,0\\r\\n5210,0\\r\\n5211,0\\r\\n5212,0\\r\\n5213,0\\r\\n5214,0\\r\\n5215,0\\r\\n5216,0\\r\\n5217,0\\r\\n5218,0\\r\\n5219,0\\r\\n5220,0\\r\\n5221,0\\r\\n5222,0\\r\\n5223,0\\r\\n5224,0\\r\\n5225,0\\r\\n5226,0\\r\\n5227,0\\r\\n5228,0\\r\\n5229,0\\r\\n5230,0\\r\\n5231,0\\r\\n5232,0\\r\\n5233,0\\r\\n5234,0\\r\\n5235,0\\r\\n5236,0\\r\\n5237,0\\r\\n5238,0\\r\\n5239,0\\r\\n5240,0\\r\\n5241,0\\r\\n5242,0\\r\\n5243,0\\r\\n5244,0\\r\\n5245,0\\r\\n5246,0\\r\\n5247,0\\r\\n5248,0\\r\\n5249,0\\r\\n5250,0\\r\\n5251,0\\r\\n5252,0\\r\\n5253,0\\r\\n5254,0\\r\\n5255,0\\r\\n5256,0\\r\\n5257,0\\r\\n5258,0\\r\\n5259,0\\r\\n5260,0\\r\\n5261,0\\r\\n5262,0\\r\\n5263,0\\r\\n5264,0\\r\\n5265,0\\r\\n5266,0\\r\\n5267,0\\r\\n5268,0\\r\\n5269,0\\r\\n5270,0\\r\\n5271,0\\r\\n5272,0\\r\\n5273,0\\r\\n5274,0\\r\\n5275,0\\r\\n5276,0\\r\\n5277,0\\r\\n5278,0\\r\\n5279,0\\r\\n5280,0\\r\\n5281,0\\r\\n5282,0\\r\\n5283,0\\r\\n5284,0\\r\\n5285,0\\r\\n5286,0\\r\\n5287,0\\r\\n5288,0\\r\\n5289,0\\r\\n5290,0\\r\\n5291,0\\r\\n5292,0\\r\\n5293,0\\r\\n5294,0\\r\\n5295,0\\r\\n5296,0\\r\\n5297,0\\r\\n5298,0\\r\\n5299,0\\r\\n5300,0\\r\\n5301,0\\r\\n5302,0\\r\\n5303,0\\r\\n5304,0\\r\\n5305,0\\r\\n5306,0\\r\\n5307,0\\r\\n5308,0\\r\\n5309,0\\r\\n5310,0\\r\\n5311,0\\r\\n5312,0\\r\\n5313,0\\r\\n5314,0\\r\\n5315,0\\r\\n5316,0\\r\\n5317,0\\r\\n5318,0\\r\\n5319,0\\r\\n5320,0\\r\\n5321,0\\r\\n5322,0\\r\\n5323,0\\r\\n5324,0\\r\\n5325,0\\r\\n5326,0\\r\\n5327,0\\r\\n5328,0\\r\\n5329,0\\r\\n5330,0\\r\\n5331,0\\r\\n5332,0\\r\\n5333,0\\r\\n5334,0\\r\\n5335,0\\r\\n5336,0\\r\\n5337,0\\r\\n5338,0\\r\\n5339,0\\r\\n5340,0\\r\\n5341,0\\r\\n5342,0\\r\\n5343,0\\r\\n5344,0\\r\\n5345,0\\r\\n5346,0\\r\\n5347,0\\r\\n5348,0\\r\\n5349,0\\r\\n5350,0\\r\\n5351,0\\r\\n5352,0\\r\\n5353,0\\r\\n5354,0\\r\\n5355,0\\r\\n5356,0\\r\\n5357,0\\r\\n5358,0\\r\\n5359,0\\r\\n5360,0\\r\\n5361,0\\r\\n5362,0\\r\\n5363,0\\r\\n5364,0\\r\\n5365,0\\r\\n5366,0\\r\\n5367,0\\r\\n5368,0\\r\\n5369,0\\r\\n5370,0\\r\\n5371,0\\r\\n5372,0\\r\\n5373,0\\r\\n5374,0\\r\\n5375,0\\r\\n5376,0\\r\\n5377,0\\r\\n5378,0\\r\\n5379,0\\r\\n5380,0\\r\\n5381,0\\r\\n5382,0\\r\\n5383,0\\r\\n5384,0\\r\\n5385,0\\r\\n5386,0\\r\\n5387,0\\r\\n5388,0\\r\\n5389,0\\r\\n5390,0\\r\\n5391,0\\r\\n5392,0\\r\\n5393,0\\r\\n5394,0\\r\\n5395,0\\r\\n5396,0\\r\\n5397,0\\r\\n5398,0\\r\\n5399,0\\r\\n5400,0\\r\\n5401,0\\r\\n5402,0\\r\\n5403,0\\r\\n5404,0\\r\\n5405,0\\r\\n5406,0\\r\\n5407,0\\r\\n5408,0\\r\\n5409,0\\r\\n5410,0\\r\\n5411,0\\r\\n5412,0\\r\\n5413,0\\r\\n5414,0\\r\\n5415,0\\r\\n5416,0\\r\\n5417,0\\r\\n5418,0\\r\\n5419,0\\r\\n5420,0\\r\\n5421,0\\r\\n5422,0\\r\\n5423,0\\r\\n5424,0\\r\\n5425,0\\r\\n5426,0\\r\\n5427,0\\r\\n5428,0\\r\\n5429,0\\r\\n5430,0\\r\\n5431,0\\r\\n5432,0\\r\\n5433,0\\r\\n5434,0\\r\\n5435,0\\r\\n5436,0\\r\\n5437,0\\r\\n5438,0\\r\\n5439,0\\r\\n5440,0\\r\\n5441,0\\r\\n5442,0\\r\\n5443,0\\r\\n5444,0\\r\\n5445,0\\r\\n5446,0\\r\\n5447,0\\r\\n5448,0\\r\\n5449,0\\r\\n5450,0\\r\\n5451,0\\r\\n5452,0\\r\\n5453,0\\r\\n5454,0\\r\\n5455,0\\r\\n5456,0\\r\\n5457,0\\r\\n5458,0\\r\\n5459,0\\r\\n5460,0\\r\\n5461,0\\r\\n5462,0\\r\\n5463,0\\r\\n5464,0\\r\\n5465,0\\r\\n5466,0\\r\\n5467,0\\r\\n5468,0\\r\\n5469,0\\r\\n5470,0\\r\\n5471,0\\r\\n5472,0\\r\\n5473,0\\r\\n5474,0\\r\\n5475,0\\r\\n5476,0\\r\\n5477,0\\r\\n5478,0\\r\\n5479,0\\r\\n5480,0\\r\\n5481,0\\r\\n5482,0\\r\\n5483,0\\r\\n5484,0\\r\\n5485,0\\r\\n5486,0\\r\\n5487,0\\r\\n5488,0\\r\\n5489,0\\r\\n5490,0\\r\\n5491,0\\r\\n5492,0\\r\\n5493,0\\r\\n5494,0\\r\\n5495,0\\r\\n5496,0\\r\\n5497,0\\r\\n5498,0\\r\\n5499,0\\r\\n5500,0\\r\\n5501,0\\r\\n5502,0\\r\\n5503,0\\r\\n5504,0\\r\\n5505,0\\r\\n5506,0\\r\\n5507,0\\r\\n5508,0\\r\\n5509,0\\r\\n5510,0\\r\\n5511,0\\r\\n5512,0\\r\\n5513,0\\r\\n5514,0\\r\\n5515,0\\r\\n5516,0\\r\\n5517,0\\r\\n5518,0\\r\\n5519,0\\r\\n5520,0\\r\\n5521,0\\r\\n5522,0\\r\\n5523,0\\r\\n5524,0\\r\\n5525,0\\r\\n5526,0\\r\\n5527,0\\r\\n5528,0\\r\\n5529,0\\r\\n5530,0\\r\\n5531,0\\r\\n5532,0\\r\\n5533,0\\r\\n5534,0\\r\\n5535,0\\r\\n5536,0\\r\\n5537,0\\r\\n5538,0\\r\\n5539,0\\r\\n5540,0\\r\\n5541,0\\r\\n5542,0\\r\\n5543,0\\r\\n5544,0\\r\\n5545,0\\r\\n5546,0\\r\\n5547,0\\r\\n5548,0\\r\\n5549,0\\r\\n5550,0\\r\\n5551,0\\r\\n5552,0\\r\\n5553,0\\r\\n5554,0\\r\\n5555,0\\r\\n5556,0\\r\\n5557,0\\r\\n5558,0\\r\\n5559,0\\r\\n5560,0\\r\\n5561,0\\r\\n5562,0\\r\\n5563,0\\r\\n5564,0\\r\\n5565,0\\r\\n5566,0\\r\\n5567,0\\r\\n5568,0\\r\\n5569,0\\r\\n5570,0\\r\\n5571,0\\r\\n5572,0\\r\\n5573,0\\r\\n5574,0\\r\\n5575,0\\r\\n5576,0\\r\\n5577,0\\r\\n5578,0\\r\\n5579,0\\r\\n5580,0\\r\\n5581,0\\r\\n5582,0\\r\\n5583,0\\r\\n5584,0\\r\\n5585,0\\r\\n5586,0\\r\\n5587,0\\r\\n5588,0\\r\\n5589,0\\r\\n5590,0\\r\\n5591,0\\r\\n5592,0\\r\\n5593,0\\r\\n5594,0\\r\\n5595,0\\r\\n5596,0\\r\\n5597,0\\r\\n5598,0\\r\\n5599,0\\r\\n5600,0\\r\\n5601,0\\r\\n5602,0\\r\\n5603,0\\r\\n5604,0\\r\\n5605,0\\r\\n5606,0\\r\\n5607,0\\r\\n5608,0\\r\\n5609,0\\r\\n5610,0\\r\\n5611,0\\r\\n5612,0\\r\\n5613,0\\r\\n5614,0\\r\\n5615,0\\r\\n5616,0\\r\\n5617,0\\r\\n5618,0\\r\\n5619,0\\r\\n5620,0\\r\\n5621,0\\r\\n5622,0\\r\\n5623,0\\r\\n5624,0\\r\\n5625,0\\r\\n5626,0\\r\\n5627,0\\r\\n5628,0\\r\\n5629,0\\r\\n5630,0\\r\\n5631,0\\r\\n5632,0\\r\\n5633,0\\r\\n5634,0\\r\\n5635,0\\r\\n5636,0\\r\\n5637,0\\r\\n5638,0\\r\\n5639,0\\r\\n5640,0\\r\\n5641,0\\r\\n5642,0\\r\\n5643,0\\r\\n5644,0\\r\\n5645,0\\r\\n5646,0\\r\\n5647,0\\r\\n5648,0\\r\\n5649,0\\r\\n5650,0\\r\\n5651,0\\r\\n5652,0\\r\\n5653,0\\r\\n5654,0\\r\\n5655,0\\r\\n5656,0\\r\\n5657,0\\r\\n5658,0\\r\\n5659,0\\r\\n5660,0\\r\\n5661,0\\r\\n5662,0\\r\\n5663,0\\r\\n5664,0\\r\\n5665,0\\r\\n5666,0\\r\\n5667,0\\r\\n5668,0\\r\\n5669,0\\r\\n5670,0\\r\\n5671,0\\r\\n5672,0\\r\\n5673,0\\r\\n5674,0\\r\\n5675,0\\r\\n5676,0\\r\\n5677,0\\r\\n5678,0\\r\\n5679,0\\r\\n5680,0\\r\\n5681,0\\r\\n5682,0\\r\\n5683,0\\r\\n5684,0\\r\\n5685,0\\r\\n5686,0\\r\\n5687,0\\r\\n5688,0\\r\\n5689,0\\r\\n5690,0\\r\\n5691,0\\r\\n5692,0\\r\\n5693,0\\r\\n5694,0\\r\\n5695,0\\r\\n5696,0\\r\\n5697,0\\r\\n5698,0\\r\\n5699,0\\r\\n5700,0\\r\\n5701,0\\r\\n5702,0\\r\\n5703,0\\r\\n5704,0\\r\\n5705,0\\r\\n5706,0\\r\\n5707,0\\r\\n5708,0\\r\\n5709,0\\r\\n5710,0\\r\\n5711,0\\r\\n5712,0\\r\\n5713,0\\r\\n5714,0\\r\\n5715,0\\r\\n5716,0\\r\\n5717,0\\r\\n5718,0\\r\\n5719,0\\r\\n5720,0\\r\\n5721,0\\r\\n5722,0\\r\\n5723,0\\r\\n5724,0\\r\\n5725,0\\r\\n5726,0\\r\\n5727,0\\r\\n5728,0\\r\\n5729,0\\r\\n5730,0\\r\\n5731,0\\r\\n5732,0\\r\\n5733,0\\r\\n5734,0\\r\\n5735,0\\r\\n5736,0\\r\\n5737,0\\r\\n5738,0\\r\\n5739,0\\r\\n5740,0\\r\\n5741,0\\r\\n5742,0\\r\\n5743,0\\r\\n5744,0\\r\\n5745,0\\r\\n5746,0\\r\\n5747,0\\r\\n5748,0\\r\\n5749,0\\r\\n5750,0\\r\\n5751,0\\r\\n5752,0\\r\\n5753,0\\r\\n5754,0\\r\\n5755,0\\r\\n5756,0\\r\\n5757,0\\r\\n5758,0\\r\\n5759,0\\r\\n5760,0\\r\\n5761,0\\r\\n5762,0\\r\\n5763,0\\r\\n5764,0\\r\\n5765,0\\r\\n5766,0\\r\\n5767,0\\r\\n5768,0\\r\\n5769,0\\r\\n5770,0\\r\\n5771,0\\r\\n5772,0\\r\\n5773,0\\r\\n5774,0\\r\\n5775,0\\r\\n5776,0\\r\\n5777,0\\r\\n5778,0\\r\\n5779,0\\r\\n5780,0\\r\\n5781,0\\r\\n5782,0\\r\\n5783,0\\r\\n5784,0\\r\\n5785,0\\r\\n5786,0\\r\\n5787,0\\r\\n5788,0\\r\\n5789,0\\r\\n5790,0\\r\\n5791,0\\r\\n5792,0\\r\\n5793,0\\r\\n5794,0\\r\\n5795,0\\r\\n5796,0\\r\\n5797,0\\r\\n5798,0\\r\\n5799,0\\r\\n5800,0\\r\\n5801,0\\r\\n5802,0\\r\\n5803,0\\r\\n5804,0\\r\\n5805,0\\r\\n5806,0\\r\\n5807,0\\r\\n5808,0\\r\\n5809,0\\r\\n5810,0\\r\\n5811,0\\r\\n5812,0\\r\\n5813,0\\r\\n5814,0\\r\\n5815,0\\r\\n5816,0\\r\\n5817,0\\r\\n5818,0\\r\\n5819,0\\r\\n5820,0\\r\\n5821,0\\r\\n5822,0\\r\\n5823,0\\r\\n5824,0\\r\\n5825,0\\r\\n5826,0\\r\\n5827,0\\r\\n5828,0\\r\\n5829,0\\r\\n5830,0\\r\\n5831,0\\r\\n5832,0\\r\\n5833,0\\r\\n5834,0\\r\\n5835,0\\r\\n5836,0\\r\\n5837,0\\r\\n5838,0\\r\\n5839,0\\r\\n5840,0\\r\\n5841,0\\r\\n5842,0\\r\\n5843,0\\r\\n5844,0\\r\\n5845,0\\r\\n5846,0\\r\\n5847,0\\r\\n5848,0\\r\\n5849,0\\r\\n5850,0\\r\\n5851,0\\r\\n5852,0\\r\\n5853,0\\r\\n5854,0\\r\\n5855,0\\r\\n5856,0\\r\\n5857,0\\r\\n5858,0\\r\\n5859,0\\r\\n5860,0\\r\\n5861,0\\r\\n5862,0\\r\\n5863,0\\r\\n5864,0\\r\\n5865,0\\r\\n5866,0\\r\\n5867,0\\r\\n5868,0\\r\\n5869,0\\r\\n5870,0\\r\\n5871,0\\r\\n5872,0\\r\\n5873,0\\r\\n5874,0\\r\\n5875,0\\r\\n5876,0\\r\\n5877,0\\r\\n5878,0\\r\\n5879,0\\r\\n5880,0\\r\\n5881,0\\r\\n5882,0\\r\\n5883,0\\r\\n5884,0\\r\\n5885,0\\r\\n5886,0\\r\\n5887,0\\r\\n5888,0\\r\\n5889,0\\r\\n5890,0\\r\\n5891,0\\r\\n5892,0\\r\\n5893,0\\r\\n5894,0\\r\\n5895,0\\r\\n5896,0\\r\\n5897,0\\r\\n5898,0\\r\\n5899,0\\r\\n5900,0\\r\\n5901,0\\r\\n5902,0\\r\\n5903,0\\r\\n5904,0\\r\\n5905,0\\r\\n5906,0\\r\\n5907,0\\r\\n5908,0\\r\\n5909,0\\r\\n5910,0\\r\\n5911,0\\r\\n5912,0\\r\\n5913,0\\r\\n5914,0\\r\\n5915,0\\r\\n5916,0\\r\\n5917,0\\r\\n5918,0\\r\\n5919,0\\r\\n5920,0\\r\\n5921,0\\r\\n5922,0\\r\\n5923,0\\r\\n5924,0\\r\\n5925,0\\r\\n5926,0\\r\\n5927,0\\r\\n5928,0\\r\\n5929,0\\r\\n5930,0\\r\\n5931,0\\r\\n5932,0\\r\\n5933,0\\r\\n5934,0\\r\\n5935,0\\r\\n5936,0\\r\\n5937,0\\r\\n5938,0\\r\\n5939,0\\r\\n5940,0\\r\\n5941,0\\r\\n5942,0\\r\\n5943,0\\r\\n5944,0\\r\\n5945,0\\r\\n5946,0\\r\\n5947,0\\r\\n5948,0\\r\\n5949,0\\r\\n5950,0\\r\\n5951,0\\r\\n5952,0\\r\\n5953,0\\r\\n5954,0\\r\\n5955,0\\r\\n5956,0\\r\\n5957,0\\r\\n5958,0\\r\\n5959,0\\r\\n5960,0\\r\\n5961,0\\r\\n5962,0\\r\\n5963,0\\r\\n5964,0\\r\\n5965,0\\r\\n5966,0\\r\\n5967,0\\r\\n5968,0\\r\\n5969,0\\r\\n5970,0\\r\\n5971,0\\r\\n5972,0\\r\\n5973,0\\r\\n5974,0\\r\\n5975,0\\r\\n5976,0\\r\\n5977,0\\r\\n5978,0\\r\\n5979,0\\r\\n5980,0\\r\\n5981,0\\r\\n5982,0\\r\\n5983,0\\r\\n5984,0\\r\\n5985,0\\r\\n5986,0\\r\\n5987,0\\r\\n5988,0\\r\\n5989,0\\r\\n5990,0\\r\\n5991,0\\r\\n5992,0\\r\\n5993,0\\r\\n5994,0\\r\\n5995,0\\r\\n5996,0\\r\\n5997,0\\r\\n5998,0\\r\\n5999,0\\r\\n6000,0\\r\\n6001,0\\r\\n6002,0\\r\\n6003,0\\r\\n6004,0\\r\\n6005,0\\r\\n6006,0\\r\\n6007,0\\r\\n6008,0\\r\\n6009,0\\r\\n6010,0\\r\\n6011,0\\r\\n6012,0\\r\\n6013,0\\r\\n6014,0\\r\\n6015,0\\r\\n6016,0\\r\\n6017,0\\r\\n6018,0\\r\\n6019,0\\r\\n6020,0\\r\\n6021,0\\r\\n6022,0\\r\\n6023,0\\r\\n6024,0\\r\\n6025,0\\r\\n6026,0\\r\\n6027,0\\r\\n6028,0\\r\\n6029,0\\r\\n6030,0\\r\\n6031,0\\r\\n6032,0\\r\\n6033,0\\r\\n6034,0\\r\\n6035,0\\r\\n6036,0\\r\\n6037,0\\r\\n6038,0\\r\\n6039,0\\r\\n6040,0\\r\\n6041,0\\r\\n6042,0\\r\\n6043,0\\r\\n6044,0\\r\\n6045,0\\r\\n6046,0\\r\\n6047,0\\r\\n6048,0\\r\\n6049,0\\r\\n6050,0\\r\\n6051,0\\r\\n6052,0\\r\\n6053,0\\r\\n6054,0\\r\\n6055,0\\r\\n6056,0\\r\\n6057,0\\r\\n6058,0\\r\\n6059,0\\r\\n6060,0\\r\\n6061,0\\r\\n6062,0\\r\\n6063,0\\r\\n6064,0\\r\\n6065,0\\r\\n6066,0\\r\\n6067,0\\r\\n6068,0\\r\\n6069,0\\r\\n6070,0\\r\\n6071,0\\r\\n6072,0\\r\\n6073,0\\r\\n6074,0\\r\\n6075,0\\r\\n6076,0\\r\\n6077,0\\r\\n6078,0\\r\\n6079,0\\r\\n6080,0\\r\\n6081,0\\r\\n6082,0\\r\\n6083,0\\r\\n6084,0\\r\\n6085,0\\r\\n6086,0\\r\\n6087,0\\r\\n6088,0\\r\\n6089,0\\r\\n6090,0\\r\\n6091,0\\r\\n6092,0\\r\\n6093,0\\r\\n6094,0\\r\\n6095,0\\r\\n6096,0\\r\\n6097,0\\r\\n6098,0\\r\\n6099,0\\r\\n6100,0\\r\\n6101,0\\r\\n6102,0\\r\\n6103,0\\r\\n6104,0\\r\\n6105,0\\r\\n6106,0\\r\\n6107,0\\r\\n6108,0\\r\\n6109,0\\r\\n6110,0\\r\\n6111,0\\r\\n6112,0\\r\\n6113,0\\r\\n6114,0\\r\\n6115,0\\r\\n6116,0\\r\\n6117,0\\r\\n6118,0\\r\\n6119,0\\r\\n6120,0\\r\\n6121,0\\r\\n6122,0\\r\\n6123,0\\r\\n6124,0\\r\\n6125,0\\r\\n6126,0\\r\\n6127,0\\r\\n6128,0\\r\\n6129,0\\r\\n6130,0\\r\\n6131,0\\r\\n6132,0\\r\\n6133,0\\r\\n6134,0\\r\\n6135,0\\r\\n6136,0\\r\\n6137,0\\r\\n6138,0\\r\\n6139,0\\r\\n6140,0\\r\\n6141,0\\r\\n6142,0\\r\\n6143,0\\r\\n6144,0\\r\\n6145,0\\r\\n6146,0\\r\\n6147,0\\r\\n6148,0\\r\\n6149,0\\r\\n6150,0\\r\\n6151,0\\r\\n6152,0\\r\\n6153,0\\r\\n6154,0\\r\\n6155,0\\r\\n6156,0\\r\\n6157,0\\r\\n6158,0\\r\\n6159,0\\r\\n6160,0\\r\\n6161,0\\r\\n6162,0\\r\\n6163,0\\r\\n6164,0\\r\\n6165,0\\r\\n6166,0\\r\\n6167,0\\r\\n6168,0\\r\\n6169,0\\r\\n6170,0\\r\\n6171,0\\r\\n6172,0\\r\\n6173,0\\r\\n6174,0\\r\\n6175,0\\r\\n6176,0\\r\\n6177,0\\r\\n6178,0\\r\\n6179,0\\r\\n6180,0\\r\\n6181,0\\r\\n6182,0\\r\\n6183,0\\r\\n6184,0\\r\\n6185,0\\r\\n6186,0\\r\\n6187,0\\r\\n6188,0\\r\\n6189,0\\r\\n6190,0\\r\\n6191,0\\r\\n6192,0\\r\\n6193,0\\r\\n6194,0\\r\\n6195,0\\r\\n6196,0\\r\\n6197,0\\r\\n6198,0\\r\\n6199,0\\r\\n6200,0\\r\\n6201,0\\r\\n6202,0\\r\\n6203,0\\r\\n6204,0\\r\\n6205,0\\r\\n6206,0\\r\\n6207,0\\r\\n6208,0\\r\\n6209,0\\r\\n6210,0\\r\\n6211,0\\r\\n6212,0\\r\\n6213,0\\r\\n6214,0\\r\\n6215,0\\r\\n6216,0\\r\\n6217,0\\r\\n6218,0\\r\\n6219,0\\r\\n6220,0\\r\\n6221,0\\r\\n6222,0\\r\\n6223,0\\r\\n6224,0\\r\\n6225,0\\r\\n6226,0\\r\\n6227,0\\r\\n6228,0\\r\\n6229,0\\r\\n6230,0\\r\\n6231,0\\r\\n6232,0\\r\\n6233,0\\r\\n6234,0\\r\\n6235,0\\r\\n6236,0\\r\\n6237,0\\r\\n6238,0\\r\\n6239,0\\r\\n6240,0\\r\\n6241,0\\r\\n6242,0\\r\\n6243,0\\r\\n6244,0\\r\\n6245,0\\r\\n6246,0\\r\\n6247,0\\r\\n6248,0\\r\\n6249,0\\r\\n6250,0\\r\\n6251,0\\r\\n6252,0\\r\\n6253,0\\r\\n6254,0\\r\\n6255,0\\r\\n6256,0\\r\\n6257,0\\r\\n6258,0\\r\\n6259,0\\r\\n6260,0\\r\\n6261,0\\r\\n6262,0\\r\\n6263,0\\r\\n6264,0\\r\\n6265,0\\r\\n6266,0\\r\\n6267,0\\r\\n6268,0\\r\\n6269,0\\r\\n6270,0\\r\\n6271,0\\r\\n6272,0\\r\\n6273,0\\r\\n6274,0\\r\\n6275,0\\r\\n6276,0\\r\\n6277,0\\r\\n6278,0\\r\\n6279,0\\r\\n6280,0\\r\\n6281,0\\r\\n6282,0\\r\\n6283,0\\r\\n6284,0\\r\\n6285,0\\r\\n6286,0\\r\\n6287,0\\r\\n6288,0\\r\\n6289,0\\r\\n6290,0\\r\\n6291,0\\r\\n6292,0\\r\\n6293,0\\r\\n6294,0\\r\\n6295,0\\r\\n6296,0\\r\\n6297,0\\r\\n6298,0\\r\\n6299,0\\r\\n6300,0\\r\\n6301,0\\r\\n6302,0\\r\\n6303,0\\r\\n6304,0\\r\\n6305,0\\r\\n6306,0\\r\\n6307,0\\r\\n6308,0\\r\\n6309,0\\r\\n6310,0\\r\\n6311,0\\r\\n6312,0\\r\\n6313,0\\r\\n6314,0\\r\\n6315,0\\r\\n6316,0\\r\\n6317,0\\r\\n6318,0\\r\\n6319,0\\r\\n6320,0\\r\\n6321,0\\r\\n6322,0\\r\\n6323,0\\r\\n6324,0\\r\\n6325,0\\r\\n6326,0\\r\\n6327,0\\r\\n6328,0\\r\\n6329,0\\r\\n6330,0\\r\\n6331,0\\r\\n6332,0\\r\\n6333,0\\r\\n6334,0\\r\\n6335,0\\r\\n6336,0\\r\\n6337,0\\r\\n6338,0\\r\\n6339,0\\r\\n6340,0\\r\\n6341,0\\r\\n6342,0\\r\\n6343,0\\r\\n6344,0\\r\\n6345,0\\r\\n6346,0\\r\\n6347,0\\r\\n6348,0\\r\\n6349,0\\r\\n6350,0\\r\\n6351,0\\r\\n6352,0\\r\\n6353,0\\r\\n6354,0\\r\\n6355,0\\r\\n6356,0\\r\\n6357,0\\r\\n6358,0\\r\\n6359,0\\r\\n6360,0\\r\\n6361,0\\r\\n6362,0\\r\\n6363,0\\r\\n6364,0\\r\\n6365,0\\r\\n6366,0\\r\\n6367,0\\r\\n6368,0\\r\\n6369,0\\r\\n6370,0\\r\\n6371,0\\r\\n6372,0\\r\\n6373,0\\r\\n6374,0\\r\\n6375,0\\r\\n6376,0\\r\\n6377,0\\r\\n6378,0\\r\\n6379,0\\r\\n6380,0\\r\\n6381,0\\r\\n6382,0\\r\\n6383,0\\r\\n6384,0\\r\\n6385,0\\r\\n6386,0\\r\\n6387,0\\r\\n6388,0\\r\\n6389,0\\r\\n6390,0\\r\\n6391,0\\r\\n6392,0\\r\\n6393,0\\r\\n6394,0\\r\\n6395,0\\r\\n6396,0\\r\\n6397,0\\r\\n6398,0\\r\\n6399,0\\r\\n6400,0\\r\\n6401,0\\r\\n6402,0\\r\\n6403,0\\r\\n6404,0\\r\\n6405,0\\r\\n6406,0\\r\\n6407,0\\r\\n6408,0\\r\\n6409,0\\r\\n6410,0\\r\\n6411,0\\r\\n6412,0\\r\\n6413,0\\r\\n6414,0\\r\\n6415,0\\r\\n6416,0\\r\\n6417,0\\r\\n6418,0\\r\\n6419,0\\r\\n6420,0\\r\\n6421,0\\r\\n6422,0\\r\\n6423,0\\r\\n6424,0\\r\\n6425,0\\r\\n6426,0\\r\\n6427,0\\r\\n6428,0\\r\\n6429,0\\r\\n6430,0\\r\\n6431,0\\r\\n6432,0\\r\\n6433,0\\r\\n6434,0\\r\\n6435,0\\r\\n6436,0\\r\\n6437,0\\r\\n6438,0\\r\\n6439,0\\r\\n6440,0\\r\\n6441,0\\r\\n6442,0\\r\\n6443,0\\r\\n6444,0\\r\\n6445,0\\r\\n6446,0\\r\\n6447,0\\r\\n6448,0\\r\\n6449,0\\r\\n6450,0\\r\\n6451,0\\r\\n6452,0\\r\\n6453,0\\r\\n6454,0\\r\\n6455,0\\r\\n6456,0\\r\\n6457,0\\r\\n6458,0\\r\\n6459,0\\r\\n6460,0\\r\\n6461,0\\r\\n6462,0\\r\\n6463,0\\r\\n6464,0\\r\\n6465,0\\r\\n6466,0\\r\\n6467,0\\r\\n6468,0\\r\\n6469,0\\r\\n6470,0\\r\\n6471,0\\r\\n6472,0\\r\\n6473,0\\r\\n6474,0\\r\\n6475,0\\r\\n6476,0\\r\\n6477,0\\r\\n6478,0\\r\\n6479,0\\r\\n6480,0\\r\\n6481,0\\r\\n6482,0\\r\\n6483,0\\r\\n6484,0\\r\\n6485,0\\r\\n6486,0\\r\\n6487,0\\r\\n6488,0\\r\\n6489,0\\r\\n6490,0\\r\\n6491,0\\r\\n6492,0\\r\\n6493,0\\r\\n6494,0\\r\\n6495,0\\r\\n6496,0\\r\\n6497,0\\r\\n6498,0\\r\\n6499,0\\r\\n6500,0\\r\\n6501,0\\r\\n6502,0\\r\\n6503,0\\r\\n6504,0\\r\\n6505,0\\r\\n6506,0\\r\\n6507,0\\r\\n6508,0\\r\\n6509,0\\r\\n6510,0\\r\\n6511,0\\r\\n6512,0\\r\\n6513,0\\r\\n6514,0\\r\\n6515,0\\r\\n6516,0\\r\\n6517,0\\r\\n6518,0\\r\\n6519,0\\r\\n6520,0\\r\\n6521,0\\r\\n6522,0\\r\\n6523,0\\r\\n6524,0\\r\\n6525,0\\r\\n6526,0\\r\\n6527,0\\r\\n6528,0\\r\\n6529,0\\r\\n6530,0\\r\\n6531,0\\r\\n6532,0\\r\\n6533,0\\r\\n6534,0\\r\\n6535,0\\r\\n6536,0\\r\\n6537,0\\r\\n6538,0\\r\\n6539,0\\r\\n6540,0\\r\\n6541,0\\r\\n6542,0\\r\\n6543,0\\r\\n6544,0\\r\\n6545,0\\r\\n6546,0\\r\\n6547,0\\r\\n6548,0\\r\\n6549,0\\r\\n6550,0\\r\\n6551,0\\r\\n6552,0\\r\\n6553,0\\r\\n6554,0\\r\\n6555,0\\r\\n6556,0\\r\\n6557,0\\r\\n6558,0\\r\\n6559,0\\r\\n6560,0\\r\\n6561,0\\r\\n6562,0\\r\\n6563,0\\r\\n6564,0\\r\\n6565,0\\r\\n6566,0\\r\\n6567,0\\r\\n6568,0\\r\\n6569,0\\r\\n6570,0\\r\\n6571,0\\r\\n6572,0\\r\\n6573,0\\r\\n6574,0\\r\\n6575,0\\r\\n6576,0\\r\\n6577,0\\r\\n6578,0\\r\\n6579,0\\r\\n6580,0\\r\\n6581,0\\r\\n6582,0\\r\\n6583,0\\r\\n6584,0\\r\\n6585,0\\r\\n6586,0\\r\\n6587,0\\r\\n6588,0\\r\\n6589,0\\r\\n6590,0\\r\\n6591,0\\r\\n6592,0\\r\\n6593,0\\r\\n6594,0\\r\\n6595,0\\r\\n6596,0\\r\\n6597,0\\r\\n6598,0\\r\\n6599,0\\r\\n6600,0\\r\\n6601,0\\r\\n6602,0\\r\\n6603,0\\r\\n6604,0\\r\\n6605,0\\r\\n6606,0\\r\\n6607,0\\r\\n6608,0\\r\\n6609,0\\r\\n6610,0\\r\\n6611,0\\r\\n6612,0\\r\\n6613,0\\r\\n6614,0\\r\\n6615,0\\r\\n6616,0\\r\\n6617,0\\r\\n6618,0\\r\\n6619,0\\r\\n6620,0\\r\\n6621,0\\r\\n6622,0\\r\\n6623,0\\r\\n6624,0\\r\\n6625,0\\r\\n6626,0\\r\\n6627,0\\r\\n6628,0\\r\\n6629,0\\r\\n6630,0\\r\\n6631,0\\r\\n6632,0\\r\\n6633,0\\r\\n6634,0\\r\\n6635,0\\r\\n6636,0\\r\\n6637,0\\r\\n6638,0\\r\\n6639,0\\r\\n6640,0\\r\\n6641,0\\r\\n6642,0\\r\\n6643,0\\r\\n6644,0\\r\\n6645,0\\r\\n6646,0\\r\\n6647,0\\r\\n6648,0\\r\\n6649,0\\r\\n6650,0\\r\\n6651,0\\r\\n6652,0\\r\\n6653,0\\r\\n6654,0\\r\\n6655,0\\r\\n6656,0\\r\\n6657,0\\r\\n6658,0\\r\\n6659,0\\r\\n6660,0\\r\\n6661,0\\r\\n6662,0\\r\\n6663,0\\r\\n6664,0\\r\\n6665,0\\r\\n6666,0\\r\\n6667,0\\r\\n6668,0\\r\\n6669,0\\r\\n6670,0\\r\\n6671,0\\r\\n6672,0\\r\\n6673,0\\r\\n6674,0\\r\\n6675,0\\r\\n6676,0\\r\\n6677,0\\r\\n6678,0\\r\\n6679,0\\r\\n6680,0\\r\\n6681,0\\r\\n6682,0\\r\\n6683,0\\r\\n6684,0\\r\\n6685,0\\r\\n6686,0\\r\\n6687,0\\r\\n6688,0\\r\\n6689,0\\r\\n6690,0\\r\\n6691,0\\r\\n6692,0\\r\\n6693,0\\r\\n6694,0\\r\\n6695,0\\r\\n6696,0\\r\\n6697,0\\r\\n6698,0\\r\\n6699,0\\r\\n6700,0\\r\\n6701,0\\r\\n6702,0\\r\\n6703,0\\r\\n6704,0\\r\\n6705,0\\r\\n6706,0\\r\\n6707,0\\r\\n6708,0\\r\\n6709,0\\r\\n6710,0\\r\\n6711,0\\r\\n6712,0\\r\\n6713,0\\r\\n6714,0\\r\\n6715,0\\r\\n6716,0\\r\\n6717,0\\r\\n6718,0\\r\\n6719,0\\r\\n6720,0\\r\\n6721,0\\r\\n6722,0\\r\\n6723,0\\r\\n6724,0\\r\\n6725,0\\r\\n6726,0\\r\\n6727,0\\r\\n6728,0\\r\\n6729,0\\r\\n6730,0\\r\\n6731,0\\r\\n6732,0\\r\\n6733,0\\r\\n6734,0\\r\\n6735,0\\r\\n6736,0\\r\\n6737,0\\r\\n6738,0\\r\\n6739,0\\r\\n6740,0\\r\\n6741,0\\r\\n6742,0\\r\\n6743,0\\r\\n6744,0\\r\\n6745,0\\r\\n6746,0\\r\\n6747,0\\r\\n6748,0\\r\\n6749,0\\r\\n6750,0\\r\\n6751,0\\r\\n6752,0\\r\\n6753,0\\r\\n6754,0\\r\\n6755,0\\r\\n6756,0\\r\\n6757,0\\r\\n6758,0\\r\\n6759,0\\r\\n6760,0\\r\\n6761,0\\r\\n6762,0\\r\\n6763,0\\r\\n6764,0\\r\\n6765,0\\r\\n6766,0\\r\\n6767,0\\r\\n6768,0\\r\\n6769,0\\r\\n6770,0\\r\\n6771,0\\r\\n6772,0\\r\\n6773,0\\r\\n6774,0\\r\\n6775,0\\r\\n6776,0\\r\\n6777,0\\r\\n6778,0\\r\\n6779,0\\r\\n6780,0\\r\\n6781,0\\r\\n6782,0\\r\\n6783,0\\r\\n6784,0\\r\\n6785,0\\r\\n6786,0\\r\\n6787,0\\r\\n6788,0\\r\\n6789,0\\r\\n6790,0\\r\\n6791,0\\r\\n6792,0\\r\\n6793,0\\r\\n6794,0\\r\\n6795,0\\r\\n6796,0\\r\\n6797,0\\r\\n6798,0\\r\\n6799,0\\r\\n6800,0\\r\\n6801,0\\r\\n6802,0\\r\\n6803,0\\r\\n6804,0\\r\\n6805,0\\r\\n6806,0\\r\\n6807,0\\r\\n6808,0\\r\\n6809,0\\r\\n6810,0\\r\\n6811,0\\r\\n6812,0\\r\\n6813,0\\r\\n6814,0\\r\\n6815,0\\r\\n6816,0\\r\\n6817,0\\r\\n6818,0\\r\\n6819,0\\r\\n6820,0\\r\\n6821,0\\r\\n6822,0\\r\\n6823,0\\r\\n6824,0\\r\\n6825,0\\r\\n6826,0\\r\\n6827,0\\r\\n6828,0\\r\\n6829,0\\r\\n6830,0\\r\\n6831,0\\r\\n6832,0\\r\\n6833,0\\r\\n6834,0\\r\\n6835,0\\r\\n6836,0\\r\\n6837,0\\r\\n6838,0\\r\\n6839,0\\r\\n6840,0\\r\\n6841,0\\r\\n6842,0\\r\\n6843,0\\r\\n6844,0\\r\\n6845,0\\r\\n6846,0\\r\\n6847,0\\r\\n6848,0\\r\\n6849,0\\r\\n6850,0\\r\\n6851,0\\r\\n6852,0\\r\\n6853,0\\r\\n6854,0\\r\\n6855,0\\r\\n6856,0\\r\\n6857,0\\r\\n6858,0\\r\\n6859,0\\r\\n6860,0\\r\\n6861,0\\r\\n6862,0\\r\\n6863,0\\r\\n6864,0\\r\\n6865,0\\r\\n6866,0\\r\\n6867,0\\r\\n6868,0\\r\\n6869,0\\r\\n6870,0\\r\\n6871,0\\r\\n6872,0\\r\\n6873,0\\r\\n6874,0\\r\\n6875,0\\r\\n6876,0\\r\\n6877,0\\r\\n6878,0\\r\\n6879,0\\r\\n6880,0\\r\\n6881,0\\r\\n6882,0\\r\\n6883,0\\r\\n6884,0\\r\\n6885,0\\r\\n6886,0\\r\\n6887,0\\r\\n6888,0\\r\\n6889,0\\r\\n6890,0\\r\\n6891,0\\r\\n6892,0\\r\\n6893,0\\r\\n6894,0\\r\\n6895,0\\r\\n6896,0\\r\\n6897,0\\r\\n6898,0\\r\\n6899,0\\r\\n6900,0\\r\\n6901,0\\r\\n6902,0\\r\\n6903,0\\r\\n6904,0\\r\\n6905,0\\r\\n6906,0\\r\\n6907,0\\r\\n6908,0\\r\\n6909,0\\r\\n6910,0\\r\\n6911,0\\r\\n6912,0\\r\\n6913,0\\r\\n6914,0\\r\\n6915,0\\r\\n6916,0\\r\\n6917,0\\r\\n6918,0\\r\\n6919,0\\r\\n6920,0\\r\\n6921,0\\r\\n6922,0\\r\\n6923,0\\r\\n6924,0\\r\\n6925,0\\r\\n6926,0\\r\\n6927,0\\r\\n6928,0\\r\\n6929,0\\r\\n6930,0\\r\\n6931,0\\r\\n6932,0\\r\\n6933,0\\r\\n6934,0\\r\\n6935,0\\r\\n6936,0\\r\\n6937,0\\r\\n6938,0\\r\\n6939,0\\r\\n6940,0\\r\\n6941,0\\r\\n6942,0\\r\\n6943,0\\r\\n6944,0\\r\\n6945,0\\r\\n6946,0\\r\\n6947,0\\r\\n6948,0\\r\\n6949,0\\r\\n6950,0\\r\\n6951,0\\r\\n6952,0\\r\\n6953,0\\r\\n6954,0\\r\\n6955,0\\r\\n6956,0\\r\\n6957,0\\r\\n6958,0\\r\\n6959,0\\r\\n6960,0\\r\\n6961,0\\r\\n6962,0\\r\\n6963,0\\r\\n6964,0\\r\\n6965,0\\r\\n6966,0\\r\\n6967,0\\r\\n6968,0\\r\\n6969,0\\r\\n6970,0\\r\\n6971,0\\r\\n6972,0\\r\\n6973,0\\r\\n6974,0\\r\\n6975,0\\r\\n6976,0\\r\\n6977,0\\r\\n6978,0\\r\\n6979,0\\r\\n6980,0\\r\\n6981,0\\r\\n6982,0\\r\\n6983,0\\r\\n6984,0\\r\\n6985,0\\r\\n6986,0\\r\\n6987,0\\r\\n6988,0\\r\\n6989,0\\r\\n6990,0\\r\\n6991,0\\r\\n6992,0\\r\\n6993,0\\r\\n6994,0\\r\\n6995,0\\r\\n6996,0\\r\\n6997,0\\r\\n6998,0\\r\\n6999,0\\r\\n7000,0\\r\\n7001,0\\r\\n7002,0\\r\\n7003,0\\r\\n7004,0\\r\\n7005,0\\r\\n7006,0\\r\\n7007,0\\r\\n7008,0\\r\\n7009,0\\r\\n7010,0\\r\\n7011,0\\r\\n7012,0\\r\\n7013,0\\r\\n7014,0\\r\\n7015,0\\r\\n7016,0\\r\\n7017,0\\r\\n7018,0\\r\\n7019,0\\r\\n7020,0\\r\\n7021,0\\r\\n7022,0\\r\\n7023,0\\r\\n7024,0\\r\\n7025,0\\r\\n7026,0\\r\\n7027,0\\r\\n7028,0\\r\\n7029,0\\r\\n7030,0\\r\\n7031,0\\r\\n7032,0\\r\\n7033,0\\r\\n7034,0\\r\\n7035,0\\r\\n7036,0\\r\\n7037,0\\r\\n7038,0\\r\\n7039,0\\r\\n7040,0\\r\\n7041,0\\r\\n7042,0\\r\\n7043,0\\r\\n7044,0\\r\\n7045,0\\r\\n7046,0\\r\\n7047,0\\r\\n7048,0\\r\\n7049,0\\r\\n7050,0\\r\\n7051,0\\r\\n7052,0\\r\\n7053,0\\r\\n7054,0\\r\\n7055,0\\r\\n7056,0\\r\\n7057,0\\r\\n7058,0\\r\\n7059,0\\r\\n7060,0\\r\\n7061,0\\r\\n7062,0\\r\\n7063,0\\r\\n7064,0\\r\\n7065,0\\r\\n7066,0\\r\\n7067,0\\r\\n7068,0\\r\\n7069,0\\r\\n7070,0\\r\\n7071,0\\r\\n7072,0\\r\\n7073,0\\r\\n7074,0\\r\\n7075,0\\r\\n7076,0\\r\\n7077,0\\r\\n7078,0\\r\\n7079,0\\r\\n7080,0\\r\\n7081,0\\r\\n7082,0\\r\\n7083,0\\r\\n7084,0\\r\\n7085,0\\r\\n7086,0\\r\\n7087,0\\r\\n7088,0\\r\\n7089,0\\r\\n7090,0\\r\\n7091,0\\r\\n7092,0\\r\\n7093,0\\r\\n7094,0\\r\\n7095,0\\r\\n7096,0\\r\\n7097,0\\r\\n7098,0\\r\\n7099,0\\r\\n7100,0\\r\\n7101,0\\r\\n7102,0\\r\\n7103,0\\r\\n7104,0\\r\\n7105,0\\r\\n7106,0\\r\\n7107,0\\r\\n7108,0\\r\\n7109,0\\r\\n7110,0\\r\\n7111,0\\r\\n7112,0\\r\\n7113,0\\r\\n7114,0\\r\\n7115,0\\r\\n7116,0\\r\\n7117,0\\r\\n7118,0\\r\\n7119,0\\r\\n7120,0\\r\\n7121,0\\r\\n7122,0\\r\\n7123,0\\r\\n7124,0\\r\\n7125,0\\r\\n7126,0\\r\\n7127,0\\r\\n7128,0\\r\\n7129,0\\r\\n7130,0\\r\\n7131,0\\r\\n7132,0\\r\\n7133,0\\r\\n7134,0\\r\\n7135,0\\r\\n7136,0\\r\\n7137,0\\r\\n7138,0\\r\\n7139,0\\r\\n7140,0\\r\\n7141,0\\r\\n7142,0\\r\\n7143,0\\r\\n7144,0\\r\\n7145,0\\r\\n7146,0\\r\\n7147,0\\r\\n7148,0\\r\\n7149,0\\r\\n7150,0\\r\\n7151,0\\r\\n7152,0\\r\\n7153,0\\r\\n7154,0\\r\\n7155,0\\r\\n7156,0\\r\\n7157,0\\r\\n7158,0\\r\\n7159,0\\r\\n7160,0\\r\\n7161,0\\r\\n7162,0\\r\\n7163,0\\r\\n7164,0\\r\\n7165,0\\r\\n7166,0\\r\\n7167,0\\r\\n7168,0\\r\\n7169,0\\r\\n7170,0\\r\\n7171,0\\r\\n7172,0\\r\\n7173,0\\r\\n7174,0\\r\\n7175,0\\r\\n7176,0\\r\\n7177,0\\r\\n7178,0\\r\\n7179,0\\r\\n7180,0\\r\\n7181,0\\r\\n7182,0\\r\\n7183,0\\r\\n7184,0\\r\\n7185,0\\r\\n7186,0\\r\\n7187,0\\r\\n7188,0\\r\\n7189,0\\r\\n7190,0\\r\\n7191,0\\r\\n7192,0\\r\\n7193,0\\r\\n7194,0\\r\\n7195,0\\r\\n7196,0\\r\\n7197,0\\r\\n7198,0\\r\\n7199,0\\r\\n7200,0\\r\\n7201,0\\r\\n7202,0\\r\\n7203,0\\r\\n7204,0\\r\\n7205,0\\r\\n7206,0\\r\\n7207,0\\r\\n7208,0\\r\\n7209,0\\r\\n7210,0\\r\\n7211,0\\r\\n7212,0\\r\\n7213,0\\r\\n7214,0\\r\\n7215,0\\r\\n7216,0\\r\\n7217,0\\r\\n7218,0\\r\\n7219,0\\r\\n7220,0\\r\\n7221,0\\r\\n7222,0\\r\\n7223,0\\r\\n7224,0\\r\\n7225,0\\r\\n7226,0\\r\\n7227,0\\r\\n7228,0\\r\\n7229,0\\r\\n7230,0\\r\\n7231,0\\r\\n7232,0\\r\\n7233,0\\r\\n7234,0\\r\\n7235,0\\r\\n7236,0\\r\\n7237,0\\r\\n7238,0\\r\\n7239,0\\r\\n7240,0\\r\\n7241,0\\r\\n7242,0\\r\\n7243,0\\r\\n7244,0\\r\\n7245,0\\r\\n7246,0\\r\\n7247,0\\r\\n7248,0\\r\\n7249,0\\r\\n7250,0\\r\\n7251,0\\r\\n7252,0\\r\\n7253,0\\r\\n7254,0\\r\\n7255,0\\r\\n7256,0\\r\\n7257,0\\r\\n7258,0\\r\\n7259,0\\r\\n7260,0\\r\\n7261,0\\r\\n7262,0\\r\\n7263,0\\r\\n7264,0\\r\\n7265,0\\r\\n7266,0\\r\\n7267,0\\r\\n7268,0\\r\\n7269,0\\r\\n7270,0\\r\\n7271,0\\r\\n7272,0\\r\\n7273,0\\r\\n7274,0\\r\\n7275,0\\r\\n7276,0\\r\\n7277,0\\r\\n7278,0\\r\\n7279,0\\r\\n7280,0\\r\\n7281,0\\r\\n7282,0\\r\\n7283,0\\r\\n7284,0\\r\\n7285,0\\r\\n7286,0\\r\\n7287,0\\r\\n7288,0\\r\\n7289,0\\r\\n7290,0\\r\\n7291,0\\r\\n7292,0\\r\\n7293,0\\r\\n7294,0\\r\\n7295,0\\r\\n7296,0\\r\\n7297,0\\r\\n7298,0\\r\\n7299,0\\r\\n7300,0\\r\\n7301,0\\r\\n7302,0\\r\\n7303,0\\r\\n7304,0\\r\\n7305,0\\r\\n7306,0\\r\\n7307,0\\r\\n7308,0\\r\\n7309,0\\r\\n7310,0\\r\\n7311,0\\r\\n7312,0\\r\\n7313,0\\r\\n7314,0\\r\\n7315,0\\r\\n7316,0\\r\\n7317,0\\r\\n7318,0\\r\\n7319,0\\r\\n7320,0\\r\\n7321,0\\r\\n7322,0\\r\\n7323,0\\r\\n7324,0\\r\\n7325,0\\r\\n7326,0\\r\\n7327,0\\r\\n7328,0\\r\\n7329,0\\r\\n7330,0\\r\\n7331,0\\r\\n7332,0\\r\\n7333,0\\r\\n7334,0\\r\\n7335,0\\r\\n7336,0\\r\\n7337,0\\r\\n7338,0\\r\\n7339,0\\r\\n7340,0\\r\\n7341,0\\r\\n7342,0\\r\\n7343,0\\r\\n7344,0\\r\\n7345,0\\r\\n7346,0\\r\\n7347,0\\r\\n7348,0\\r\\n7349,0\\r\\n7350,0\\r\\n7351,0\\r\\n7352,0\\r\\n7353,0\\r\\n7354,0\\r\\n7355,0\\r\\n7356,0\\r\\n7357,0\\r\\n7358,0\\r\\n7359,0\\r\\n7360,0\\r\\n7361,0\\r\\n7362,0\\r\\n7363,0\\r\\n7364,0\\r\\n7365,0\\r\\n7366,0\\r\\n7367,0\\r\\n7368,0\\r\\n7369,0\\r\\n7370,0\\r\\n7371,0\\r\\n7372,0\\r\\n7373,0\\r\\n7374,0\\r\\n7375,0\\r\\n7376,0\\r\\n7377,0\\r\\n7378,0\\r\\n7379,0\\r\\n7380,0\\r\\n7381,0\\r\\n7382,0\\r\\n7383,0\\r\\n7384,0\\r\\n7385,0\\r\\n7386,0\\r\\n7387,0\\r\\n7388,0\\r\\n7389,0\\r\\n7390,0\\r\\n7391,0\\r\\n7392,0\\r\\n7393,0\\r\\n7394,0\\r\\n7395,0\\r\\n7396,0\\r\\n7397,0\\r\\n7398,0\\r\\n7399,0\\r\\n7400,0\\r\\n7401,0\\r\\n7402,0\\r\\n7403,0\\r\\n7404,0\\r\\n7405,0\\r\\n7406,0\\r\\n7407,0\\r\\n7408,0\\r\\n7409,0\\r\\n7410,0\\r\\n7411,0\\r\\n7412,0\\r\\n7413,0\\r\\n7414,0\\r\\n7415,0\\r\\n7416,0\\r\\n7417,0\\r\\n7418,0\\r\\n7419,0\\r\\n7420,0\\r\\n7421,0\\r\\n7422,0\\r\\n7423,0\\r\\n7424,0\\r\\n7425,0\\r\\n7426,0\\r\\n7427,0\\r\\n7428,0\\r\\n7429,0\\r\\n7430,0\\r\\n7431,0\\r\\n7432,0\\r\\n7433,0\\r\\n7434,0\\r\\n7435,0\\r\\n7436,0\\r\\n7437,0\\r\\n7438,0\\r\\n7439,0\\r\\n7440,0\\r\\n7441,0\\r\\n7442,0\\r\\n7443,0\\r\\n7444,0\\r\\n7445,0\\r\\n7446,0\\r\\n7447,0\\r\\n7448,0\\r\\n7449,0\\r\\n7450,0\\r\\n7451,0\\r\\n7452,0\\r\\n7453,0\\r\\n7454,0\\r\\n7455,0\\r\\n7456,0\\r\\n7457,0\\r\\n7458,0\\r\\n7459,0\\r\\n7460,0\\r\\n7461,0\\r\\n7462,0\\r\\n7463,0\\r\\n7464,0\\r\\n7465,0\\r\\n7466,0\\r\\n7467,0\\r\\n7468,0\\r\\n7469,0\\r\\n7470,0\\r\\n7471,0\\r\\n7472,0\\r\\n7473,0\\r\\n7474,0\\r\\n7475,0\\r\\n7476,0\\r\\n7477,0\\r\\n7478,0\\r\\n7479,0\\r\\n7480,0\\r\\n7481,0\\r\\n7482,0\\r\\n7483,0\\r\\n7484,0\\r\\n7485,0\\r\\n7486,0\\r\\n7487,0\\r\\n7488,0\\r\\n7489,0\\r\\n7490,0\\r\\n7491,0\\r\\n7492,0\\r\\n7493,0\\r\\n7494,0\\r\\n7495,0\\r\\n7496,0\\r\\n7497,0\\r\\n7498,0\\r\\n7499,0\\r\\n7500,0\\r\\n7501,0\\r\\n7502,0\\r\\n7503,0\\r\\n7504,0\\r\\n7505,0\\r\\n7506,0\\r\\n7507,0\\r\\n7508,0\\r\\n7509,0\\r\\n7510,0\\r\\n7511,0\\r\\n7512,0\\r\\n7513,0\\r\\n7514,0\\r\\n7515,0\\r\\n7516,0\\r\\n7517,0\\r\\n7518,0\\r\\n7519,0\\r\\n7520,0\\r\\n7521,0\\r\\n7522,0\\r\\n7523,0\\r\\n7524,0\\r\\n7525,0\\r\\n7526,0\\r\\n7527,0\\r\\n7528,0\\r\\n7529,0\\r\\n7530,0\\r\\n7531,0\\r\\n7532,0\\r\\n7533,0\\r\\n7534,0\\r\\n7535,0\\r\\n7536,0\\r\\n7537,0\\r\\n7538,0\\r\\n7539,0\\r\\n7540,0\\r\\n7541,0\\r\\n7542,0\\r\\n7543,0\\r\\n7544,0\\r\\n7545,0\\r\\n7546,0\\r\\n7547,0\\r\\n7548,0\\r\\n7549,0\\r\\n7550,0\\r\\n7551,0\\r\\n7552,0\\r\\n7553,0\\r\\n7554,0\\r\\n7555,0\\r\\n7556,0\\r\\n7557,0\\r\\n7558,0\\r\\n7559,0\\r\\n7560,0\\r\\n7561,0\\r\\n7562,0\\r\\n7563,0\\r\\n7564,0\\r\\n7565,0\\r\\n7566,0\\r\\n7567,0\\r\\n7568,0\\r\\n7569,0\\r\\n7570,0\\r\\n7571,0\\r\\n7572,0\\r\\n7573,0\\r\\n7574,0\\r\\n7575,0\\r\\n7576,0\\r\\n7577,0\\r\\n7578,0\\r\\n7579,0\\r\\n7580,0\\r\\n7581,0\\r\\n7582,0\\r\\n7583,0\\r\\n7584,0\\r\\n7585,0\\r\\n7586,0\\r\\n7587,0\\r\\n7588,0\\r\\n7589,0\\r\\n7590,0\\r\\n7591,0\\r\\n7592,0\\r\\n7593,0\\r\\n7594,0\\r\\n7595,0\\r\\n7596,0\\r\\n7597,0\\r\\n7598,0\\r\\n7599,0\\r\\n7600,0\\r\\n7601,0\\r\\n7602,0\\r\\n7603,0\\r\\n7604,0\\r\\n7605,0\\r\\n7606,0\\r\\n7607,0\\r\\n7608,0\\r\\n7609,0\\r\\n7610,0\\r\\n7611,0\\r\\n7612,0\\r\\n7613,0\\r\\n7614,0\\r\\n7615,0\\r\\n7616,0\\r\\n7617,0\\r\\n7618,0\\r\\n7619,0\\r\\n7620,0\\r\\n7621,0\\r\\n7622,0\\r\\n7623,0\\r\\n7624,0\\r\\n7625,0\\r\\n7626,0\\r\\n7627,0\\r\\n7628,0\\r\\n7629,0\\r\\n7630,0\\r\\n7631,0\\r\\n7632,0\\r\\n7633,0\\r\\n7634,0\\r\\n7635,0\\r\\n7636,0\\r\\n7637,0\\r\\n7638,0\\r\\n7639,0\\r\\n7640,0\\r\\n7641,0\\r\\n7642,0\\r\\n7643,0\\r\\n7644,0\\r\\n7645,0\\r\\n7646,0\\r\\n7647,0\\r\\n7648,0\\r\\n7649,0\\r\\n7650,0\\r\\n7651,0\\r\\n7652,0\\r\\n7653,0\\r\\n7654,0\\r\\n7655,0\\r\\n7656,0\\r\\n7657,0\\r\\n7658,0\\r\\n7659,0\\r\\n7660,0\\r\\n7661,0\\r\\n7662,0\\r\\n7663,0\\r\\n7664,0\\r\\n7665,0\\r\\n7666,0\\r\\n7667,0\\r\\n7668,0\\r\\n7669,0\\r\\n7670,0\\r\\n7671,0\\r\\n7672,0\\r\\n7673,0\\r\\n7674,0\\r\\n7675,0\\r\\n7676,0\\r\\n7677,0\\r\\n7678,0\\r\\n7679,0\\r\\n7680,0\\r\\n7681,0\\r\\n7682,0\\r\\n7683,0\\r\\n7684,0\\r\\n7685,0\\r\\n7686,0\\r\\n7687,0\\r\\n7688,0\\r\\n7689,0\\r\\n7690,0\\r\\n7691,0\\r\\n7692,0\\r\\n7693,0\\r\\n7694,0\\r\\n7695,0\\r\\n7696,0\\r\\n7697,0\\r\\n7698,0\\r\\n7699,0\\r\\n7700,0\\r\\n7701,0\\r\\n7702,0\\r\\n7703,0\\r\\n7704,0\\r\\n7705,0\\r\\n7706,0\\r\\n7707,0\\r\\n7708,0\\r\\n7709,0\\r\\n7710,0\\r\\n7711,0\\r\\n7712,0\\r\\n7713,0\\r\\n7714,0\\r\\n7715,0\\r\\n7716,0\\r\\n7717,0\\r\\n7718,0\\r\\n7719,0\\r\\n7720,0\\r\\n7721,0\\r\\n7722,0\\r\\n7723,0\\r\\n7724,0\\r\\n7725,0\\r\\n7726,0\\r\\n7727,0\\r\\n7728,0\\r\\n7729,0\\r\\n7730,0\\r\\n7731,0\\r\\n7732,0\\r\\n7733,0\\r\\n7734,0\\r\\n7735,0\\r\\n7736,0\\r\\n7737,0\\r\\n7738,0\\r\\n7739,0\\r\\n7740,0\\r\\n7741,0\\r\\n7742,0\\r\\n7743,0\\r\\n7744,0\\r\\n7745,0\\r\\n7746,0\\r\\n7747,0\\r\\n7748,0\\r\\n7749,0\\r\\n7750,0\\r\\n7751,0\\r\\n7752,0\\r\\n7753,0\\r\\n7754,0\\r\\n7755,0\\r\\n7756,0\\r\\n7757,0\\r\\n7758,0\\r\\n7759,0\\r\\n7760,0\\r\\n7761,0\\r\\n7762,0\\r\\n7763,0\\r\\n7764,0\\r\\n7765,0\\r\\n7766,0\\r\\n7767,0\\r\\n7768,0\\r\\n7769,0\\r\\n7770,0\\r\\n7771,0\\r\\n7772,0\\r\\n7773,0\\r\\n7774,0\\r\\n7775,0\\r\\n7776,0\\r\\n7777,0\\r\\n7778,0\\r\\n7779,0\\r\\n7780,0\\r\\n7781,0\\r\\n7782,0\\r\\n7783,0\\r\\n7784,0\\r\\n7785,0\\r\\n7786,0\\r\\n7787,0\\r\\n7788,0\\r\\n7789,0\\r\\n7790,0\\r\\n7791,0\\r\\n7792,0\\r\\n7793,0\\r\\n7794,0\\r\\n7795,0\\r\\n7796,0\\r\\n7797,0\\r\\n7798,0\\r\\n7799,0\\r\\n7800,0\\r\\n7801,0\\r\\n7802,0\\r\\n7803,0\\r\\n7804,0\\r\\n7805,0\\r\\n7806,0\\r\\n7807,0\\r\\n7808,0\\r\\n7809,0\\r\\n7810,0\\r\\n7811,0\\r\\n7812,0\\r\\n7813,0\\r\\n7814,0\\r\\n7815,0\\r\\n7816,0\\r\\n7817,0\\r\\n7818,0\\r\\n7819,0\\r\\n7820,0\\r\\n7821,0\\r\\n7822,0\\r\\n7823,0\\r\\n7824,0\\r\\n7825,0\\r\\n7826,0\\r\\n7827,0\\r\\n7828,0\\r\\n7829,0\\r\\n7830,0\\r\\n7831,0\\r\\n7832,0\\r\\n7833,0\\r\\n7834,0\\r\\n7835,0\\r\\n7836,0\\r\\n7837,0\\r\\n7838,0\\r\\n7839,0\\r\\n7840,0\\r\\n7841,0\\r\\n7842,0\\r\\n7843,0\\r\\n7844,0\\r\\n7845,0\\r\\n7846,0\\r\\n7847,0\\r\\n7848,0\\r\\n7849,0\\r\\n7850,0\\r\\n7851,0\\r\\n7852,0\\r\\n7853,0\\r\\n7854,0\\r\\n7855,0\\r\\n7856,0\\r\\n7857,0\\r\\n7858,0\\r\\n7859,0\\r\\n7860,0\\r\\n7861,0\\r\\n7862,0\\r\\n7863,0\\r\\n7864,0\\r\\n7865,0\\r\\n7866,0\\r\\n7867,0\\r\\n7868,0\\r\\n7869,0\\r\\n7870,0\\r\\n7871,0\\r\\n7872,0\\r\\n7873,0\\r\\n7874,0\\r\\n7875,0\\r\\n7876,0\\r\\n7877,0\\r\\n7878,0\\r\\n7879,0\\r\\n7880,0\\r\\n7881,0\\r\\n7882,0\\r\\n7883,0\\r\\n7884,0\\r\\n7885,0\\r\\n7886,0\\r\\n7887,0\\r\\n7888,0\\r\\n7889,0\\r\\n7890,0\\r\\n7891,0\\r\\n7892,0\\r\\n7893,0\\r\\n7894,0\\r\\n7895,0\\r\\n7896,0\\r\\n7897,0\\r\\n7898,0\\r\\n7899,0\\r\\n7900,0\\r\\n7901,0\\r\\n7902,0\\r\\n7903,0\\r\\n7904,0\\r\\n7905,0\\r\\n7906,0\\r\\n7907,0\\r\\n7908,0\\r\\n7909,0\\r\\n7910,0\\r\\n7911,0\\r\\n7912,0\\r\\n7913,0\\r\\n7914,0\\r\\n7915,0\\r\\n7916,0\\r\\n7917,0\\r\\n7918,0\\r\\n7919,0\\r\\n7920,0\\r\\n7921,0\\r\\n7922,0\\r\\n7923,0\\r\\n7924,0\\r\\n7925,0\\r\\n7926,0\\r\\n7927,0\\r\\n7928,0\\r\\n7929,0\\r\\n7930,0\\r\\n7931,0\\r\\n7932,0\\r\\n7933,0\\r\\n7934,0\\r\\n7935,0\\r\\n7936,0\\r\\n7937,0\\r\\n7938,0\\r\\n7939,0\\r\\n7940,0\\r\\n7941,0\\r\\n7942,0\\r\\n7943,0\\r\\n7944,0\\r\\n7945,0\\r\\n7946,0\\r\\n7947,0\\r\\n7948,0\\r\\n7949,0\\r\\n7950,0\\r\\n7951,0\\r\\n7952,0\\r\\n7953,0\\r\\n7954,0\\r\\n7955,0\\r\\n7956,0\\r\\n7957,0\\r\\n7958,0\\r\\n7959,0\\r\\n7960,0\\r\\n7961,0\\r\\n7962,0\\r\\n7963,0\\r\\n7964,0\\r\\n7965,0\\r\\n7966,0\\r\\n7967,0\\r\\n7968,0\\r\\n7969,0\\r\\n7970,0\\r\\n7971,0\\r\\n7972,0\\r\\n7973,0\\r\\n7974,0\\r\\n7975,0\\r\\n7976,0\\r\\n7977,0\\r\\n7978,0\\r\\n7979,0\\r\\n7980,0\\r\\n7981,0\\r\\n7982,0\\r\\n7983,0\\r\\n7984,0\\r\\n7985,0\\r\\n7986,0\\r\\n7987,0\\r\\n7988,0\\r\\n7989,0\\r\\n7990,0\\r\\n7991,0\\r\\n7992,0\\r\\n7993,0\\r\\n7994,0\\r\\n7995,0\\r\\n7996,0\\r\\n7997,0\\r\\n7998,0\\r\\n7999,0\\r\\n8000,0\\r\\n8001,0\\r\\n8002,0\\r\\n8003,0\\r\\n8004,0\\r\\n8005,0\\r\\n8006,0\\r\\n8007,0\\r\\n8008,0\\r\\n8009,0\\r\\n8010,0\\r\\n8011,0\\r\\n8012,0\\r\\n8013,0\\r\\n8014,0\\r\\n8015,0\\r\\n8016,0\\r\\n8017,0\\r\\n8018,0\\r\\n8019,0\\r\\n8020,0\\r\\n8021,0\\r\\n8022,0\\r\\n8023,0\\r\\n8024,0\\r\\n8025,0\\r\\n8026,0\\r\\n8027,0\\r\\n8028,0\\r\\n8029,0\\r\\n8030,0\\r\\n8031,0\\r\\n8032,0\\r\\n8033,0\\r\\n8034,0\\r\\n8035,0\\r\\n8036,0\\r\\n8037,0\\r\\n8038,0\\r\\n8039,0\\r\\n8040,0\\r\\n8041,0\\r\\n8042,0\\r\\n8043,0\\r\\n8044,0\\r\\n8045,0\\r\\n8046,0\\r\\n8047,0\\r\\n8048,0\\r\\n8049,0\\r\\n8050,0\\r\\n8051,0\\r\\n8052,0\\r\\n8053,0\\r\\n8054,0\\r\\n8055,0\\r\\n8056,0\\r\\n8057,0\\r\\n8058,0\\r\\n8059,0\\r\\n8060,0\\r\\n8061,0\\r\\n8062,0\\r\\n8063,0\\r\\n8064,0\\r\\n8065,0\\r\\n8066,0\\r\\n8067,0\\r\\n8068,0\\r\\n8069,0\\r\\n8070,0\\r\\n8071,0\\r\\n8072,0\\r\\n8073,0\\r\\n8074,0\\r\\n8075,0\\r\\n8076,0\\r\\n8077,0\\r\\n8078,0\\r\\n8079,0\\r\\n8080,0\\r\\n8081,0\\r\\n8082,0\\r\\n8083,0\\r\\n8084,0\\r\\n8085,0\\r\\n8086,0\\r\\n8087,0\\r\\n8088,0\\r\\n8089,0\\r\\n8090,0\\r\\n8091,0\\r\\n8092,0\\r\\n8093,0\\r\\n8094,0\\r\\n8095,0\\r\\n8096,0\\r\\n8097,0\\r\\n8098,0\\r\\n8099,0\\r\\n8100,0\\r\\n8101,0\\r\\n8102,0\\r\\n8103,0\\r\\n8104,0\\r\\n8105,0\\r\\n8106,0\\r\\n8107,0\\r\\n8108,0\\r\\n8109,0\\r\\n8110,0\\r\\n8111,0\\r\\n8112,0\\r\\n8113,0\\r\\n8114,0\\r\\n8115,0\\r\\n8116,0\\r\\n8117,0\\r\\n8118,0\\r\\n8119,0\\r\\n8120,0\\r\\n8121,0\\r\\n8122,0\\r\\n8123,0\\r\\n8124,0\\r\\n8125,0\\r\\n8126,0\\r\\n8127,0\\r\\n8128,0\\r\\n8129,0\\r\\n8130,0\\r\\n8131,0\\r\\n8132,0\\r\\n8133,0\\r\\n8134,0\\r\\n8135,0\\r\\n8136,0\\r\\n8137,0\\r\\n8138,0\\r\\n8139,0\\r\\n8140,0\\r\\n8141,0\\r\\n8142,0\\r\\n8143,0\\r\\n8144,0\\r\\n8145,0\\r\\n8146,0\\r\\n8147,0\\r\\n8148,0\\r\\n8149,0\\r\\n8150,0\\r\\n8151,0\\r\\n8152,0\\r\\n8153,0\\r\\n8154,0\\r\\n8155,0\\r\\n8156,0\\r\\n8157,0\\r\\n8158,0\\r\\n8159,0\\r\\n8160,0\\r\\n8161,0\\r\\n8162,0\\r\\n8163,0\\r\\n8164,0\\r\\n8165,0\\r\\n8166,0\\r\\n8167,0\\r\\n8168,0\\r\\n8169,0\\r\\n8170,0\\r\\n8171,0\\r\\n8172,0\\r\\n8173,0\\r\\n8174,0\\r\\n8175,0\\r\\n8176,0\\r\\n8177,0\\r\\n8178,0\\r\\n8179,0\\r\\n8180,0\\r\\n8181,0\\r\\n8182,0\\r\\n8183,0\\r\\n8184,0\\r\\n8185,0\\r\\n8186,0\\r\\n8187,0\\r\\n8188,0\\r\\n8189,0\\r\\n8190,0\\r\\n8191,0\\r\\n8192,0\\r\\n8193,0\\r\\n8194,0\\r\\n8195,0\\r\\n8196,0\\r\\n8197,0\\r\\n8198,0\\r\\n8199,0\\r\\n8200,0\\r\\n8201,0\\r\\n8202,0\\r\\n8203,0\\r\\n8204,0\\r\\n8205,0\\r\\n8206,0\\r\\n8207,0\\r\\n8208,0\\r\\n8209,0\\r\\n8210,0\\r\\n8211,0\\r\\n8212,0\\r\\n8213,0\\r\\n8214,0\\r\\n8215,0\\r\\n8216,0\\r\\n8217,0\\r\\n8218,0\\r\\n8219,0\\r\\n8220,0\\r\\n8221,0\\r\\n8222,0\\r\\n8223,0\\r\\n8224,0\\r\\n8225,0\\r\\n8226,0\\r\\n8227,0\\r\\n8228,0\\r\\n8229,0\\r\\n8230,0\\r\\n8231,0\\r\\n8232,0\\r\\n8233,0\\r\\n8234,0\\r\\n8235,0\\r\\n8236,0\\r\\n8237,0\\r\\n8238,0\\r\\n8239,0\\r\\n8240,0\\r\\n8241,0\\r\\n8242,0\\r\\n8243,0\\r\\n8244,0\\r\\n8245,0\\r\\n8246,0\\r\\n8247,0\\r\\n8248,0\\r\\n8249,0\\r\\n8250,0\\r\\n8251,0\\r\\n8252,0\\r\\n8253,0\\r\\n8254,0\\r\\n8255,0\\r\\n8256,0\\r\\n8257,0\\r\\n8258,0\\r\\n8259,0\\r\\n8260,0\\r\\n8261,0\\r\\n8262,0\\r\\n8263,0\\r\\n8264,0\\r\\n8265,0\\r\\n8266,0\\r\\n8267,0\\r\\n8268,0\\r\\n8269,0\\r\\n8270,0\\r\\n8271,0\\r\\n8272,0\\r\\n8273,0\\r\\n8274,0\\r\\n8275,0\\r\\n8276,0\\r\\n8277,0\\r\\n8278,0\\r\\n8279,0\\r\\n8280,0\\r\\n8281,0\\r\\n8282,0\\r\\n8283,0\\r\\n8284,0\\r\\n8285,0\\r\\n8286,0\\r\\n8287,0\\r\\n8288,0\\r\\n8289,0\\r\\n8290,0\\r\\n8291,0\\r\\n8292,0\\r\\n8293,0\\r\\n8294,0\\r\\n8295,0\\r\\n8296,0\\r\\n8297,0\\r\\n8298,0\\r\\n8299,0\\r\\n8300,0\\r\\n8301,0\\r\\n8302,0\\r\\n8303,0\\r\\n8304,0\\r\\n8305,0\\r\\n8306,0\\r\\n8307,0\\r\\n8308,0\\r\\n8309,0\\r\\n8310,0\\r\\n8311,0\\r\\n8312,0\\r\\n8313,0\\r\\n8314,0\\r\\n8315,0\\r\\n8316,0\\r\\n8317,0\\r\\n8318,0\\r\\n8319,0\\r\\n8320,0\\r\\n8321,0\\r\\n8322,0\\r\\n8323,0\\r\\n8324,0\\r\\n8325,0\\r\\n8326,0\\r\\n8327,0\\r\\n8328,0\\r\\n8329,0\\r\\n8330,0\\r\\n8331,0\\r\\n8332,0\\r\\n8333,0\\r\\n8334,0\\r\\n8335,0\\r\\n8336,0\\r\\n8337,0\\r\\n8338,0\\r\\n8339,0\\r\\n8340,0\\r\\n8341,0\\r\\n8342,0\\r\\n8343,0\\r\\n8344,0\\r\\n8345,0\\r\\n8346,0\\r\\n8347,0\\r\\n8348,0\\r\\n8349,0\\r\\n8350,0\\r\\n8351,0\\r\\n8352,0\\r\\n8353,0\\r\\n8354,0\\r\\n8355,0\\r\\n8356,0\\r\\n8357,0\\r\\n8358,0\\r\\n8359,0\\r\\n8360,0\\r\\n8361,0\\r\\n8362,0\\r\\n8363,0\\r\\n8364,0\\r\\n8365,0\\r\\n8366,0\\r\\n8367,0\\r\\n8368,0\\r\\n8369,0\\r\\n8370,0\\r\\n8371,0\\r\\n8372,0\\r\\n8373,0\\r\\n8374,0\\r\\n8375,0\\r\\n8376,0\\r\\n8377,0\\r\\n8378,0\\r\\n8379,0\\r\\n8380,0\\r\\n8381,0\\r\\n8382,0\\r\\n8383,0\\r\\n8384,0\\r\\n8385,0\\r\\n8386,0\\r\\n8387,0\\r\\n8388,0\\r\\n8389,0\\r\\n8390,0\\r\\n8391,0\\r\\n8392,0\\r\\n8393,0\\r\\n8394,0\\r\\n8395,0\\r\\n8396,0\\r\\n8397,0\\r\\n8398,0\\r\\n8399,0\\r\\n8400,0\\r\\n8401,0\\r\\n8402,0\\r\\n8403,0\\r\\n8404,0\\r\\n8405,0\\r\\n8406,0\\r\\n8407,0\\r\\n8408,0\\r\\n8409,0\\r\\n8410,0\\r\\n8411,0\\r\\n8412,0\\r\\n8413,0\\r\\n8414,0\\r\\n8415,0\\r\\n8416,0\\r\\n8417,0\\r\\n8418,0\\r\\n8419,0\\r\\n8420,0\\r\\n8421,0\\r\\n8422,0\\r\\n8423,0\\r\\n8424,0\\r\\n8425,0\\r\\n8426,0\\r\\n8427,0\\r\\n8428,0\\r\\n8429,0\\r\\n8430,0\\r\\n8431,0\\r\\n8432,0\\r\\n8433,0\\r\\n8434,0\\r\\n8435,0\\r\\n8436,0\\r\\n8437,0\\r\\n8438,0\\r\\n8439,0\\r\\n8440,0\\r\\n8441,0\\r\\n8442,0\\r\\n8443,0\\r\\n8444,0\\r\\n8445,0\\r\\n8446,0\\r\\n8447,0\\r\\n8448,0\\r\\n8449,0\\r\\n8450,0\\r\\n8451,0\\r\\n8452,0\\r\\n8453,0\\r\\n8454,0\\r\\n8455,0\\r\\n8456,0\\r\\n8457,0\\r\\n8458,0\\r\\n8459,0\\r\\n8460,0\\r\\n8461,0\\r\\n8462,0\\r\\n8463,0\\r\\n8464,0\\r\\n8465,0\\r\\n8466,0\\r\\n8467,0\\r\\n8468,0\\r\\n8469,0\\r\\n8470,0\\r\\n8471,0\\r\\n8472,0\\r\\n8473,0\\r\\n8474,0\\r\\n8475,0\\r\\n8476,0\\r\\n8477,0\\r\\n8478,0\\r\\n8479,0\\r\\n8480,0\\r\\n8481,0\\r\\n8482,0\\r\\n8483,0\\r\\n8484,0\\r\\n8485,0\\r\\n8486,0\\r\\n8487,0\\r\\n8488,0\\r\\n8489,0\\r\\n8490,0\\r\\n8491,0\\r\\n8492,0\\r\\n8493,0\\r\\n8494,0\\r\\n8495,0\\r\\n8496,0\\r\\n8497,0\\r\\n8498,0\\r\\n8499,0\\r\\n8500,0\\r\\n8501,0\\r\\n8502,0\\r\\n8503,0\\r\\n8504,0\\r\\n8505,0\\r\\n8506,0\\r\\n8507,0\\r\\n8508,0\\r\\n8509,0\\r\\n8510,0\\r\\n8511,0\\r\\n8512,0\\r\\n8513,0\\r\\n8514,0\\r\\n8515,0\\r\\n8516,0\\r\\n8517,0\\r\\n8518,0\\r\\n8519,0\\r\\n8520,0\\r\\n8521,0\\r\\n8522,0\\r\\n8523,0\\r\\n8524,0\\r\\n8525,0\\r\\n8526,0\\r\\n8527,0\\r\\n8528,0\\r\\n8529,0\\r\\n8530,0\\r\\n8531,0\\r\\n8532,0\\r\\n8533,0\\r\\n8534,0\\r\\n8535,0\\r\\n8536,0\\r\\n8537,0\\r\\n8538,0\\r\\n8539,0\\r\\n8540,0\\r\\n8541,0\\r\\n8542,0\\r\\n8543,0\\r\\n8544,0\\r\\n8545,0\\r\\n8546,0\\r\\n8547,0\\r\\n8548,0\\r\\n8549,0\\r\\n8550,0\\r\\n8551,0\\r\\n8552,0\\r\\n8553,0\\r\\n8554,0\\r\\n8555,0\\r\\n8556,0\\r\\n8557,0\\r\\n8558,0\\r\\n8559,0\\r\\n8560,0\\r\\n8561,0\\r\\n8562,0\\r\\n8563,0\\r\\n8564,0\\r\\n8565,0\\r\\n8566,0\\r\\n8567,0\\r\\n8568,0\\r\\n8569,0\\r\\n8570,0\\r\\n8571,0\\r\\n8572,0\\r\\n8573,0\\r\\n8574,0\\r\\n8575,0\\r\\n8576,0\\r\\n8577,0\\r\\n8578,0\\r\\n8579,0\\r\\n8580,0\\r\\n8581,0\\r\\n8582,0\\r\\n8583,0\\r\\n8584,0\\r\\n8585,0\\r\\n8586,0\\r\\n8587,0\\r\\n8588,0\\r\\n8589,0\\r\\n8590,0\\r\\n8591,0\\r\\n8592,0\\r\\n8593,0\\r\\n8594,0\\r\\n8595,0\\r\\n8596,0\\r\\n8597,0\\r\\n8598,0\\r\\n8599,0\\r\\n8600,0\\r\\n8601,0\\r\\n8602,0\\r\\n8603,0\\r\\n8604,0\\r\\n8605,0\\r\\n8606,0\\r\\n8607,0\\r\\n8608,0\\r\\n8609,0\\r\\n8610,0\\r\\n8611,0\\r\\n8612,0\\r\\n8613,0\\r\\n8614,0\\r\\n8615,0\\r\\n8616,0\\r\\n8617,0\\r\\n8618,0\\r\\n8619,0\\r\\n8620,0\\r\\n8621,0\\r\\n8622,0\\r\\n8623,0\\r\\n8624,0\\r\\n8625,0\\r\\n8626,0\\r\\n8627,0\\r\\n8628,0\\r\\n8629,0\\r\\n8630,0\\r\\n8631,0\\r\\n8632,0\\r\\n8633,0\\r\\n8634,0\\r\\n8635,0\\r\\n8636,0\\r\\n8637,0\\r\\n8638,0\\r\\n8639,0\\r\\n8640,0\\r\\n8641,0\\r\\n8642,0\\r\\n8643,0\\r\\n8644,0\\r\\n8645,0\\r\\n8646,0\\r\\n8647,0\\r\\n8648,0\\r\\n8649,0\\r\\n8650,0\\r\\n8651,0\\r\\n8652,0\\r\\n8653,0\\r\\n8654,0\\r\\n8655,0\\r\\n8656,0\\r\\n8657,0\\r\\n8658,0\\r\\n8659,0\\r\\n8660,0\\r\\n8661,0\\r\\n8662,0\\r\\n8663,0\\r\\n8664,0\\r\\n8665,0\\r\\n8666,0\\r\\n8667,0\\r\\n8668,0\\r\\n8669,0\\r\\n8670,0\\r\\n8671,0\\r\\n8672,0\\r\\n8673,0\\r\\n8674,0\\r\\n8675,0\\r\\n8676,0\\r\\n8677,0\\r\\n8678,0\\r\\n8679,0\\r\\n8680,0\\r\\n8681,0\\r\\n8682,0\\r\\n8683,0\\r\\n8684,0\\r\\n8685,0\\r\\n8686,0\\r\\n8687,0\\r\\n8688,0\\r\\n8689,0\\r\\n8690,0\\r\\n8691,0\\r\\n8692,0\\r\\n8693,0\\r\\n8694,0\\r\\n8695,0\\r\\n8696,0\\r\\n8697,0\\r\\n8698,0\\r\\n8699,0\\r\\n8700,0\\r\\n8701,0\\r\\n8702,0\\r\\n8703,0\\r\\n8704,0\\r\\n8705,0\\r\\n8706,0\\r\\n8707,0\\r\\n8708,0\\r\\n8709,0\\r\\n8710,0\\r\\n8711,0\\r\\n8712,0\\r\\n8713,0\\r\\n8714,0\\r\\n8715,0\\r\\n8716,0\\r\\n8717,0\\r\\n8718,0\\r\\n8719,0\\r\\n8720,0\\r\\n8721,0\\r\\n8722,0\\r\\n8723,0\\r\\n8724,0\\r\\n8725,0\\r\\n8726,0\\r\\n8727,0\\r\\n8728,0\\r\\n8729,0\\r\\n8730,0\\r\\n8731,0\\r\\n8732,0\\r\\n8733,0\\r\\n8734,0\\r\\n8735,0\\r\\n8736,0\\r\\n8737,0\\r\\n8738,0\\r\\n8739,0\\r\\n8740,0\\r\\n8741,0\\r\\n8742,0\\r\\n8743,0\\r\\n8744,0\\r\\n8745,0\\r\\n8746,0\\r\\n8747,0\\r\\n8748,0\\r\\n8749,0\\r\\n8750,0\\r\\n8751,0\\r\\n8752,0\\r\\n8753,0\\r\\n8754,0\\r\\n8755,0\\r\\n8756,0\\r\\n8757,0\\r\\n8758,0\\r\\n8759,0\\r\\n8760,0\\r\\n8761,0\\r\\n8762,0\\r\\n8763,0\\r\\n8764,0\\r\\n8765,0\\r\\n8766,0\\r\\n8767,0\\r\\n8768,0\\r\\n8769,0\\r\\n8770,0\\r\\n8771,0\\r\\n8772,0\\r\\n8773,0\\r\\n8774,0\\r\\n8775,0\\r\\n8776,0\\r\\n8777,0\\r\\n8778,0\\r\\n8779,0\\r\\n8780,0\\r\\n8781,0\\r\\n8782,0\\r\\n8783,0\\r\\n8784,0\\r\\n8785,0\\r\\n8786,0\\r\\n8787,0\\r\\n8788,0\\r\\n8789,0\\r\\n8790,0\\r\\n8791,0\\r\\n8792,0\\r\\n8793,0\\r\\n8794,0\\r\\n8795,0\\r\\n8796,0\\r\\n8797,0\\r\\n8798,0\\r\\n8799,0\\r\\n8800,0\\r\\n8801,0\\r\\n8802,0\\r\\n8803,0\\r\\n8804,0\\r\\n8805,0\\r\\n8806,0\\r\\n8807,0\\r\\n8808,0\\r\\n8809,0\\r\\n8810,0\\r\\n8811,0\\r\\n8812,0\\r\\n8813,0\\r\\n8814,0\\r\\n8815,0\\r\\n8816,0\\r\\n8817,0\\r\\n8818,0\\r\\n8819,0\\r\\n8820,0\\r\\n8821,0\\r\\n8822,0\\r\\n8823,0\\r\\n8824,0\\r\\n8825,0\\r\\n8826,0\\r\\n8827,0\\r\\n8828,0\\r\\n8829,0\\r\\n8830,0\\r\\n8831,0\\r\\n8832,0\\r\\n8833,0\\r\\n8834,0\\r\\n8835,0\\r\\n8836,0\\r\\n8837,0\\r\\n8838,0\\r\\n8839,0\\r\\n8840,0\\r\\n8841,0\\r\\n8842,0\\r\\n8843,0\\r\\n8844,0\\r\\n8845,0\\r\\n8846,0\\r\\n8847,0\\r\\n8848,0\\r\\n8849,0\\r\\n8850,0\\r\\n8851,0\\r\\n8852,0\\r\\n8853,0\\r\\n8854,0\\r\\n8855,0\\r\\n8856,0\\r\\n8857,0\\r\\n8858,0\\r\\n8859,0\\r\\n8860,0\\r\\n8861,0\\r\\n8862,0\\r\\n8863,0\\r\\n8864,0\\r\\n8865,0\\r\\n8866,0\\r\\n8867,0\\r\\n8868,0\\r\\n8869,0\\r\\n8870,0\\r\\n8871,0\\r\\n8872,0\\r\\n8873,0\\r\\n8874,0\\r\\n8875,0\\r\\n8876,0\\r\\n8877,0\\r\\n8878,0\\r\\n8879,0\\r\\n8880,0\\r\\n8881,0\\r\\n8882,0\\r\\n8883,0\\r\\n8884,0\\r\\n8885,0\\r\\n8886,0\\r\\n8887,0\\r\\n8888,0\\r\\n8889,0\\r\\n8890,0\\r\\n8891,0\\r\\n8892,0\\r\\n8893,0\\r\\n8894,0\\r\\n8895,0\\r\\n8896,0\\r\\n8897,0\\r\\n8898,0\\r\\n8899,0\\r\\n8900,0\\r\\n8901,0\\r\\n8902,0\\r\\n8903,0\\r\\n8904,0\\r\\n8905,0\\r\\n8906,0\\r\\n8907,0\\r\\n8908,0\\r\\n8909,0\\r\\n8910,0\\r\\n8911,0\\r\\n8912,0\\r\\n8913,0\\r\\n8914,0\\r\\n8915,0\\r\\n8916,0\\r\\n8917,0\\r\\n8918,0\\r\\n8919,0\\r\\n8920,0\\r\\n8921,0\\r\\n8922,0\\r\\n8923,0\\r\\n8924,0\\r\\n8925,0\\r\\n8926,0\\r\\n8927,0\\r\\n8928,0\\r\\n8929,0\\r\\n8930,0\\r\\n8931,0\\r\\n8932,0\\r\\n8933,0\\r\\n8934,0\\r\\n8935,0\\r\\n8936,0\\r\\n8937,0\\r\\n8938,0\\r\\n8939,0\\r\\n8940,0\\r\\n8941,0\\r\\n8942,0\\r\\n8943,0\\r\\n8944,0\\r\\n8945,0\\r\\n8946,0\\r\\n8947,0\\r\\n8948,0\\r\\n8949,0\\r\\n8950,0\\r\\n8951,0\\r\\n8952,0\\r\\n8953,0\\r\\n8954,0\\r\\n8955,0\\r\\n8956,0\\r\\n8957,0\\r\\n8958,0\\r\\n8959,0\\r\\n8960,0\\r\\n8961,0\\r\\n8962,0\\r\\n8963,0\\r\\n8964,0\\r\\n8965,0\\r\\n8966,0\\r\\n8967,0\\r\\n8968,0\\r\\n8969,0\\r\\n8970,0\\r\\n8971,0\\r\\n8972,0\\r\\n8973,0\\r\\n8974,0\\r\\n8975,0\\r\\n8976,0\\r\\n8977,0\\r\\n8978,0\\r\\n8979,0\\r\\n8980,0\\r\\n8981,0\\r\\n8982,0\\r\\n8983,0\\r\\n8984,0\\r\\n8985,0\\r\\n8986,0\\r\\n8987,0\\r\\n8988,0\\r\\n8989,0\\r\\n8990,0\\r\\n8991,0\\r\\n8992,0\\r\\n8993,0\\r\\n8994,0\\r\\n8995,0\\r\\n8996,0\\r\\n8997,0\\r\\n8998,0\\r\\n8999,0\\r\\n9000,0\\r\\n9001,0\\r\\n9002,0\\r\\n9003,0\\r\\n9004,0\\r\\n9005,0\\r\\n9006,0\\r\\n9007,0\\r\\n9008,0\\r\\n9009,0\\r\\n9010,0\\r\\n9011,0\\r\\n9012,0\\r\\n9013,0\\r\\n9014,0\\r\\n9015,0\\r\\n9016,0\\r\\n9017,0\\r\\n9018,0\\r\\n9019,0\\r\\n9020,0\\r\\n9021,0\\r\\n9022,0\\r\\n9023,0\\r\\n9024,0\\r\\n9025,0\\r\\n9026,0\\r\\n9027,0\\r\\n9028,0\\r\\n9029,0\\r\\n9030,0\\r\\n9031,0\\r\\n9032,0\\r\\n9033,0\\r\\n9034,0\\r\\n9035,0\\r\\n9036,0\\r\\n9037,0\\r\\n9038,0\\r\\n9039,0\\r\\n9040,0\\r\\n9041,0\\r\\n9042,0\\r\\n9043,0\\r\\n9044,0\\r\\n9045,0\\r\\n9046,0\\r\\n9047,0\\r\\n9048,0\\r\\n9049,0\\r\\n9050,0\\r\\n9051,0\\r\\n9052,0\\r\\n9053,0\\r\\n9054,0\\r\\n9055,0\\r\\n9056,0\\r\\n9057,0\\r\\n9058,0\\r\\n9059,0\\r\\n9060,0\\r\\n9061,0\\r\\n9062,0\\r\\n9063,0\\r\\n9064,0\\r\\n9065,0\\r\\n9066,0\\r\\n9067,0\\r\\n9068,0\\r\\n9069,0\\r\\n9070,0\\r\\n9071,0\\r\\n9072,0\\r\\n9073,0\\r\\n9074,0\\r\\n9075,0\\r\\n9076,0\\r\\n9077,0\\r\\n9078,0\\r\\n9079,0\\r\\n9080,0\\r\\n9081,0\\r\\n9082,0\\r\\n9083,0\\r\\n9084,0\\r\\n9085,0\\r\\n9086,0\\r\\n9087,0\\r\\n9088,0\\r\\n9089,0\\r\\n9090,0\\r\\n9091,0\\r\\n9092,0\\r\\n9093,0\\r\\n9094,0\\r\\n9095,0\\r\\n9096,0\\r\\n9097,0\\r\\n9098,0\\r\\n9099,0\\r\\n9100,0\\r\\n9101,0\\r\\n9102,0\\r\\n9103,0\\r\\n9104,0\\r\\n9105,0\\r\\n9106,0\\r\\n9107,0\\r\\n9108,0\\r\\n9109,0\\r\\n9110,0\\r\\n9111,0\\r\\n9112,0\\r\\n9113,0\\r\\n9114,0\\r\\n9115,0\\r\\n9116,0\\r\\n9117,0\\r\\n9118,0\\r\\n9119,0\\r\\n9120,0\\r\\n9121,0\\r\\n9122,0\\r\\n9123,0\\r\\n9124,0\\r\\n9125,0\\r\\n9126,0\\r\\n9127,0\\r\\n9128,0\\r\\n9129,0\\r\\n9130,0\\r\\n9131,0\\r\\n9132,0\\r\\n9133,0\\r\\n9134,0\\r\\n9135,0\\r\\n9136,0\\r\\n9137,0\\r\\n9138,0\\r\\n9139,0\\r\\n9140,0\\r\\n9141,0\\r\\n9142,0\\r\\n9143,0\\r\\n9144,0\\r\\n9145,0\\r\\n9146,0\\r\\n9147,0\\r\\n9148,0\\r\\n9149,0\\r\\n9150,0\\r\\n9151,0\\r\\n9152,0\\r\\n9153,0\\r\\n9154,0\\r\\n9155,0\\r\\n9156,0\\r\\n9157,0\\r\\n9158,0\\r\\n9159,0\\r\\n9160,0\\r\\n9161,0\\r\\n9162,0\\r\\n9163,0\\r\\n9164,0\\r\\n9165,0\\r\\n9166,0\\r\\n9167,0\\r\\n9168,0\\r\\n9169,0\\r\\n9170,0\\r\\n9171,0\\r\\n9172,0\\r\\n9173,0\\r\\n9174,0\\r\\n9175,0\\r\\n9176,0\\r\\n9177,0\\r\\n9178,0\\r\\n9179,0\\r\\n9180,0\\r\\n9181,0\\r\\n9182,0\\r\\n9183,0\\r\\n9184,0\\r\\n9185,0\\r\\n9186,0\\r\\n9187,0\\r\\n9188,0\\r\\n9189,0\\r\\n9190,0\\r\\n9191,0\\r\\n9192,0\\r\\n9193,0\\r\\n9194,0\\r\\n9195,0\\r\\n9196,0\\r\\n9197,0\\r\\n9198,0\\r\\n9199,0\\r\\n9200,0\\r\\n9201,0\\r\\n9202,0\\r\\n9203,0\\r\\n9204,0\\r\\n9205,0\\r\\n9206,0\\r\\n9207,0\\r\\n9208,0\\r\\n9209,0\\r\\n9210,0\\r\\n9211,0\\r\\n9212,0\\r\\n9213,0\\r\\n9214,0\\r\\n9215,0\\r\\n9216,0\\r\\n9217,0\\r\\n9218,0\\r\\n9219,0\\r\\n9220,0\\r\\n9221,0\\r\\n9222,0\\r\\n9223,0\\r\\n9224,0\\r\\n9225,0\\r\\n9226,0\\r\\n9227,0\\r\\n9228,0\\r\\n9229,0\\r\\n9230,0\\r\\n9231,0\\r\\n9232,0\\r\\n9233,0\\r\\n9234,0\\r\\n9235,0\\r\\n9236,0\\r\\n9237,0\\r\\n9238,0\\r\\n9239,0\\r\\n9240,0\\r\\n9241,0\\r\\n9242,0\\r\\n9243,0\\r\\n9244,0\\r\\n9245,0\\r\\n9246,0\\r\\n9247,0\\r\\n9248,0\\r\\n9249,0\\r\\n9250,0\\r\\n9251,0\\r\\n9252,0\\r\\n9253,0\\r\\n9254,0\\r\\n9255,0\\r\\n9256,0\\r\\n9257,0\\r\\n9258,0\\r\\n9259,0\\r\\n9260,0\\r\\n9261,0\\r\\n9262,0\\r\\n9263,0\\r\\n9264,0\\r\\n9265,0\\r\\n9266,0\\r\\n9267,0\\r\\n9268,0\\r\\n9269,0\\r\\n9270,0\\r\\n9271,0\\r\\n9272,0\\r\\n9273,0\\r\\n9274,0\\r\\n9275,0\\r\\n9276,0\\r\\n9277,0\\r\\n9278,0\\r\\n9279,0\\r\\n9280,0\\r\\n9281,0\\r\\n9282,0\\r\\n9283,0\\r\\n9284,0\\r\\n9285,0\\r\\n9286,0\\r\\n9287,0\\r\\n9288,0\\r\\n9289,0\\r\\n9290,0\\r\\n9291,0\\r\\n9292,0\\r\\n9293,0\\r\\n9294,0\\r\\n9295,0\\r\\n9296,0\\r\\n9297,0\\r\\n9298,0\\r\\n9299,0\\r\\n9300,0\\r\\n9301,0\\r\\n9302,0\\r\\n9303,0\\r\\n9304,0\\r\\n9305,0\\r\\n9306,0\\r\\n9307,0\\r\\n9308,0\\r\\n9309,0\\r\\n9310,0\\r\\n9311,0\\r\\n9312,0\\r\\n9313,0\\r\\n9314,0\\r\\n9315,0\\r\\n9316,0\\r\\n9317,0\\r\\n9318,0\\r\\n9319,0\\r\\n9320,0\\r\\n9321,0\\r\\n9322,0\\r\\n9323,0\\r\\n9324,0\\r\\n9325,0\\r\\n9326,0\\r\\n9327,0\\r\\n9328,0\\r\\n9329,0\\r\\n9330,0\\r\\n9331,0\\r\\n9332,0\\r\\n9333,0\\r\\n9334,0\\r\\n9335,0\\r\\n9336,0\\r\\n9337,0\\r\\n9338,0\\r\\n9339,0\\r\\n9340,0\\r\\n9341,0\\r\\n9342,0\\r\\n9343,0\\r\\n9344,0\\r\\n9345,0\\r\\n9346,0\\r\\n9347,0\\r\\n9348,0\\r\\n9349,0\\r\\n9350,0\\r\\n9351,0\\r\\n9352,0\\r\\n9353,0\\r\\n9354,0\\r\\n9355,0\\r\\n9356,0\\r\\n9357,0\\r\\n9358,0\\r\\n9359,0\\r\\n9360,0\\r\\n9361,0\\r\\n9362,0\\r\\n9363,0\\r\\n9364,0\\r\\n9365,0\\r\\n9366,0\\r\\n9367,0\\r\\n9368,0\\r\\n9369,0\\r\\n9370,0\\r\\n9371,0\\r\\n9372,0\\r\\n9373,0\\r\\n9374,0\\r\\n9375,0\\r\\n9376,0\\r\\n9377,0\\r\\n9378,0\\r\\n9379,0\\r\\n9380,0\\r\\n9381,0\\r\\n9382,0\\r\\n9383,0\\r\\n9384,0\\r\\n9385,0\\r\\n9386,0\\r\\n9387,0\\r\\n9388,0\\r\\n9389,0\\r\\n9390,0\\r\\n9391,0\\r\\n9392,0\\r\\n9393,0\\r\\n9394,0\\r\\n9395,0\\r\\n9396,0\\r\\n9397,0\\r\\n9398,0\\r\\n9399,0\\r\\n9400,0\\r\\n9401,0\\r\\n9402,0\\r\\n9403,0\\r\\n9404,0\\r\\n9405,0\\r\\n9406,0\\r\\n9407,0\\r\\n9408,0\\r\\n9409,0\\r\\n9410,0\\r\\n9411,0\\r\\n9412,0\\r\\n9413,0\\r\\n9414,0\\r\\n9415,0\\r\\n9416,0\\r\\n9417,0\\r\\n9418,0\\r\\n9419,0\\r\\n9420,0\\r\\n9421,0\\r\\n9422,0\\r\\n9423,0\\r\\n9424,0\\r\\n9425,0\\r\\n9426,0\\r\\n9427,0\\r\\n9428,0\\r\\n9429,0\\r\\n9430,0\\r\\n9431,0\\r\\n9432,0\\r\\n9433,0\\r\\n9434,0\\r\\n9435,0\\r\\n9436,0\\r\\n9437,0\\r\\n9438,0\\r\\n9439,0\\r\\n9440,0\\r\\n9441,0\\r\\n9442,0\\r\\n9443,0\\r\\n9444,0\\r\\n9445,0\\r\\n9446,0\\r\\n9447,0\\r\\n9448,0\\r\\n9449,0\\r\\n9450,0\\r\\n9451,0\\r\\n9452,0\\r\\n9453,0\\r\\n9454,0\\r\\n9455,0\\r\\n9456,0\\r\\n9457,0\\r\\n9458,0\\r\\n9459,0\\r\\n9460,0\\r\\n9461,0\\r\\n9462,0\\r\\n9463,0\\r\\n9464,0\\r\\n9465,0\\r\\n9466,0\\r\\n9467,0\\r\\n9468,0\\r\\n9469,0\\r\\n9470,0\\r\\n9471,0\\r\\n9472,0\\r\\n9473,0\\r\\n9474,0\\r\\n9475,0\\r\\n9476,0\\r\\n9477,0\\r\\n9478,0\\r\\n9479,0\\r\\n9480,0\\r\\n9481,0\\r\\n9482,0\\r\\n9483,0\\r\\n9484,0\\r\\n9485,0\\r\\n9486,0\\r\\n9487,0\\r\\n9488,0\\r\\n9489,0\\r\\n9490,0\\r\\n9491,0\\r\\n9492,0\\r\\n9493,0\\r\\n9494,0\\r\\n9495,0\\r\\n9496,0\\r\\n9497,0\\r\\n9498,0\\r\\n9499,0\\r\\n9500,0\\r\\n9501,0\\r\\n9502,0\\r\\n9503,0\\r\\n9504,0\\r\\n9505,0\\r\\n9506,0\\r\\n9507,0\\r\\n9508,0\\r\\n9509,0\\r\\n9510,0\\r\\n9511,0\\r\\n9512,0\\r\\n9513,0\\r\\n9514,0\\r\\n9515,0\\r\\n9516,0\\r\\n9517,0\\r\\n9518,0\\r\\n9519,0\\r\\n9520,0\\r\\n9521,0\\r\\n9522,0\\r\\n9523,0\\r\\n9524,0\\r\\n9525,0\\r\\n9526,0\\r\\n9527,0\\r\\n9528,0\\r\\n9529,0\\r\\n9530,0\\r\\n9531,0\\r\\n9532,0\\r\\n9533,0\\r\\n9534,0\\r\\n9535,0\\r\\n9536,0\\r\\n9537,0\\r\\n9538,0\\r\\n9539,0\\r\\n9540,0\\r\\n9541,0\\r\\n9542,0\\r\\n9543,0\\r\\n9544,0\\r\\n9545,0\\r\\n9546,0\\r\\n9547,0\\r\\n9548,0\\r\\n9549,0\\r\\n9550,0\\r\\n9551,0\\r\\n9552,0\\r\\n9553,0\\r\\n9554,0\\r\\n9555,0\\r\\n9556,0\\r\\n9557,0\\r\\n9558,0\\r\\n9559,0\\r\\n9560,0\\r\\n9561,0\\r\\n9562,0\\r\\n9563,0\\r\\n9564,0\\r\\n9565,0\\r\\n9566,0\\r\\n9567,0\\r\\n9568,0\\r\\n9569,0\\r\\n9570,0\\r\\n9571,0\\r\\n9572,0\\r\\n9573,0\\r\\n9574,0\\r\\n9575,0\\r\\n9576,0\\r\\n9577,0\\r\\n9578,0\\r\\n9579,0\\r\\n9580,0\\r\\n9581,0\\r\\n9582,0\\r\\n9583,0\\r\\n9584,0\\r\\n9585,0\\r\\n9586,0\\r\\n9587,0\\r\\n9588,0\\r\\n9589,0\\r\\n9590,0\\r\\n9591,0\\r\\n9592,0\\r\\n9593,0\\r\\n9594,0\\r\\n9595,0\\r\\n9596,0\\r\\n9597,0\\r\\n9598,0\\r\\n9599,0\\r\\n9600,0\\r\\n9601,0\\r\\n9602,0\\r\\n9603,0\\r\\n9604,0\\r\\n9605,0\\r\\n9606,0\\r\\n9607,0\\r\\n9608,0\\r\\n9609,0\\r\\n9610,0\\r\\n9611,0\\r\\n9612,0\\r\\n9613,0\\r\\n9614,0\\r\\n9615,0\\r\\n9616,0\\r\\n9617,0\\r\\n9618,0\\r\\n9619,0\\r\\n9620,0\\r\\n9621,0\\r\\n9622,0\\r\\n9623,0\\r\\n9624,0\\r\\n9625,0\\r\\n9626,0\\r\\n9627,0\\r\\n9628,0\\r\\n9629,0\\r\\n9630,0\\r\\n9631,0\\r\\n9632,0\\r\\n9633,0\\r\\n9634,0\\r\\n9635,0\\r\\n9636,0\\r\\n9637,0\\r\\n9638,0\\r\\n9639,0\\r\\n9640,0\\r\\n9641,0\\r\\n9642,0\\r\\n9643,0\\r\\n9644,0\\r\\n9645,0\\r\\n9646,0\\r\\n9647,0\\r\\n9648,0\\r\\n9649,0\\r\\n9650,0\\r\\n9651,0\\r\\n9652,0\\r\\n9653,0\\r\\n9654,0\\r\\n9655,0\\r\\n9656,0\\r\\n9657,0\\r\\n9658,0\\r\\n9659,0\\r\\n9660,0\\r\\n9661,0\\r\\n9662,0\\r\\n9663,0\\r\\n9664,0\\r\\n9665,0\\r\\n9666,0\\r\\n9667,0\\r\\n9668,0\\r\\n9669,0\\r\\n9670,0\\r\\n9671,0\\r\\n9672,0\\r\\n9673,0\\r\\n9674,0\\r\\n9675,0\\r\\n9676,0\\r\\n9677,0\\r\\n9678,0\\r\\n9679,0\\r\\n9680,0\\r\\n9681,0\\r\\n9682,0\\r\\n9683,0\\r\\n9684,0\\r\\n9685,0\\r\\n9686,0\\r\\n9687,0\\r\\n9688,0\\r\\n9689,0\\r\\n9690,0\\r\\n9691,0\\r\\n9692,0\\r\\n9693,0\\r\\n9694,0\\r\\n9695,0\\r\\n9696,0\\r\\n9697,0\\r\\n9698,0\\r\\n9699,0\\r\\n9700,0\\r\\n9701,0\\r\\n9702,0\\r\\n9703,0\\r\\n9704,0\\r\\n9705,0\\r\\n9706,0\\r\\n9707,0\\r\\n9708,0\\r\\n9709,0\\r\\n9710,0\\r\\n9711,0\\r\\n9712,0\\r\\n9713,0\\r\\n9714,0\\r\\n9715,0\\r\\n9716,0\\r\\n9717,0\\r\\n9718,0\\r\\n9719,0\\r\\n9720,0\\r\\n9721,0\\r\\n9722,0\\r\\n9723,0\\r\\n9724,0\\r\\n9725,0\\r\\n9726,0\\r\\n9727,0\\r\\n9728,0\\r\\n9729,0\\r\\n9730,0\\r\\n9731,0\\r\\n9732,0\\r\\n9733,0\\r\\n9734,0\\r\\n9735,0\\r\\n9736,0\\r\\n9737,0\\r\\n9738,0\\r\\n9739,0\\r\\n9740,0\\r\\n9741,0\\r\\n9742,0\\r\\n9743,0\\r\\n9744,0\\r\\n9745,0\\r\\n9746,0\\r\\n9747,0\\r\\n9748,0\\r\\n9749,0\\r\\n9750,0\\r\\n9751,0\\r\\n9752,0\\r\\n9753,0\\r\\n9754,0\\r\\n9755,0\\r\\n9756,0\\r\\n9757,0\\r\\n9758,0\\r\\n9759,0\\r\\n9760,0\\r\\n9761,0\\r\\n9762,0\\r\\n9763,0\\r\\n9764,0\\r\\n9765,0\\r\\n9766,0\\r\\n9767,0\\r\\n9768,0\\r\\n9769,0\\r\\n9770,0\\r\\n9771,0\\r\\n9772,0\\r\\n9773,0\\r\\n9774,0\\r\\n9775,0\\r\\n9776,0\\r\\n9777,0\\r\\n9778,0\\r\\n9779,0\\r\\n9780,0\\r\\n9781,0\\r\\n9782,0\\r\\n9783,0\\r\\n9784,0\\r\\n9785,0\\r\\n9786,0\\r\\n9787,0\\r\\n9788,0\\r\\n9789,0\\r\\n9790,0\\r\\n9791,0\\r\\n9792,0\\r\\n9793,0\\r\\n9794,0\\r\\n9795,0\\r\\n9796,0\\r\\n9797,0\\r\\n9798,0\\r\\n9799,0\\r\\n9800,0\\r\\n9801,0\\r\\n9802,0\\r\\n9803,0\\r\\n9804,0\\r\\n9805,0\\r\\n9806,0\\r\\n9807,0\\r\\n9808,0\\r\\n9809,0\\r\\n9810,0\\r\\n9811,0\\r\\n9812,0\\r\\n9813,0\\r\\n9814,0\\r\\n9815,0\\r\\n9816,0\\r\\n9817,0\\r\\n9818,0\\r\\n9819,0\\r\\n9820,0\\r\\n9821,0\\r\\n9822,0\\r\\n9823,0\\r\\n9824,0\\r\\n9825,0\\r\\n9826,0\\r\\n9827,0\\r\\n9828,0\\r\\n9829,0\\r\\n9830,0\\r\\n9831,0\\r\\n9832,0\\r\\n9833,0\\r\\n9834,0\\r\\n9835,0\\r\\n9836,0\\r\\n9837,0\\r\\n9838,0\\r\\n9839,0\\r\\n9840,0\\r\\n9841,0\\r\\n9842,0\\r\\n9843,0\\r\\n9844,0\\r\\n9845,0\\r\\n9846,0\\r\\n9847,0\\r\\n9848,0\\r\\n9849,0\\r\\n9850,0\\r\\n9851,0\\r\\n9852,0\\r\\n9853,0\\r\\n9854,0\\r\\n9855,0\\r\\n9856,0\\r\\n9857,0\\r\\n9858,0\\r\\n9859,0\\r\\n9860,0\\r\\n9861,0\\r\\n9862,0\\r\\n9863,0\\r\\n9864,0\\r\\n9865,0\\r\\n9866,0\\r\\n9867,0\\r\\n9868,0\\r\\n9869,0\\r\\n9870,0\\r\\n9871,0\\r\\n9872,0\\r\\n9873,0\\r\\n9874,0\\r\\n9875,0\\r\\n9876,0\\r\\n9877,0\\r\\n9878,0\\r\\n9879,0\\r\\n9880,0\\r\\n9881,0\\r\\n9882,0\\r\\n9883,0\\r\\n9884,0\\r\\n9885,0\\r\\n9886,0\\r\\n9887,0\\r\\n9888,0\\r\\n9889,0\\r\\n9890,0\\r\\n9891,0\\r\\n9892,0\\r\\n9893,0\\r\\n9894,0\\r\\n9895,0\\r\\n9896,0\\r\\n9897,0\\r\\n9898,0\\r\\n9899,0\\r\\n9900,0\\r\\n9901,0\\r\\n9902,0\\r\\n9903,0\\r\\n9904,0\\r\\n9905,0\\r\\n9906,0\\r\\n9907,0\\r\\n9908,0\\r\\n9909,0\\r\\n9910,0\\r\\n9911,0\\r\\n9912,0\\r\\n9913,0\\r\\n9914,0\\r\\n9915,0\\r\\n9916,0\\r\\n9917,0\\r\\n9918,0\\r\\n9919,0\\r\\n9920,0\\r\\n9921,0\\r\\n9922,0\\r\\n9923,0\\r\\n9924,0\\r\\n9925,0\\r\\n9926,0\\r\\n9927,0\\r\\n9928,0\\r\\n9929,0\\r\\n9930,0\\r\\n9931,0\\r\\n9932,0\\r\\n9933,0\\r\\n9934,0\\r\\n9935,0\\r\\n9936,0\\r\\n9937,0\\r\\n9938,0\\r\\n9939,0\\r\\n9940,0\\r\\n9941,0\\r\\n9942,0\\r\\n9943,0\\r\\n9944,0\\r\\n9945,0\\r\\n9946,0\\r\\n9947,0\\r\\n9948,0\\r\\n9949,0\\r\\n9950,0\\r\\n9951,0\\r\\n9952,0\\r\\n9953,0\\r\\n9954,0\\r\\n9955,0\\r\\n9956,0\\r\\n9957,0\\r\\n9958,0\\r\\n9959,0\\r\\n9960,0\\r\\n9961,0\\r\\n9962,0\\r\\n9963,0\\r\\n9964,0\\r\\n9965,0\\r\\n9966,0\\r\\n9967,0\\r\\n9968,0\\r\\n9969,0\\r\\n9970,0\\r\\n9971,0\\r\\n9972,0\\r\\n9973,0\\r\\n9974,0\\r\\n9975,0\\r\\n9976,0\\r\\n9977,0\\r\\n9978,0\\r\\n9979,0\\r\\n9980,0\\r\\n9981,0\\r\\n9982,0\\r\\n9983,0\\r\\n9984,0\\r\\n9985,0\\r\\n9986,0\\r\\n9987,0\\r\\n9988,0\\r\\n9989,0\\r\\n9990,0\\r\\n9991,0\\r\\n9992,0\\r\\n9993,0\\r\\n9994,0\\r\\n9995,0\\r\\n9996,0\\r\\n9997,0\\r\\n9998,0\\r\\n9999,0\\r\\n10000,0\\r\\n10001,0\\r\\n10002,0\\r\\n10003,0\\r\\n10004,0\\r\\n10005,0\\r\\n10006,0\\r\\n10007,0\\r\\n10008,0\\r\\n10009,0\\r\\n10010,0\\r\\n10011,0\\r\\n10012,0\\r\\n10013,0\\r\\n10014,0\\r\\n10015,0\\r\\n10016,0\\r\\n10017,0\\r\\n10018,0\\r\\n10019,0\\r\\n10020,0\\r\\n10021,0\\r\\n10022,0\\r\\n10023,0\\r\\n10024,0\\r\\n10025,0\\r\\n10026,0\\r\\n10027,0\\r\\n10028,0\\r\\n10029,0\\r\\n10030,0\\r\\n10031,0\\r\\n10032,0\\r\\n10033,0\\r\\n10034,0\\r\\n10035,0\\r\\n10036,0\\r\\n10037,0\\r\\n10038,0\\r\\n10039,0\\r\\n10040,0\\r\\n10041,0\\r\\n10042,0\\r\\n10043,0\\r\\n10044,0\\r\\n10045,0\\r\\n10046,0\\r\\n10047,0\\r\\n10048,0\\r\\n10049,0\\r\\n10050,0\\r\\n10051,0\\r\\n10052,0\\r\\n10053,0\\r\\n10054,0\\r\\n10055,0\\r\\n10056,0\\r\\n10057,0\\r\\n10058,0\\r\\n10059,0\\r\\n10060,0\\r\\n10061,0\\r\\n10062,0\\r\\n10063,0\\r\\n10064,0\\r\\n10065,0\\r\\n10066,0\\r\\n10067,0\\r\\n10068,0\\r\\n10069,0\\r\\n10070,0\\r\\n10071,0\\r\\n10072,0\\r\\n10073,0\\r\\n10074,0\\r\\n10075,0\\r\\n10076,0\\r\\n10077,0\\r\\n10078,0\\r\\n10079,0\\r\\n10080,0\\r\\n10081,0\\r\\n10082,0\\r\\n10083,0\\r\\n10084,0\\r\\n10085,0\\r\\n10086,0\\r\\n10087,0\\r\\n10088,0\\r\\n10089,0\\r\\n10090,0\\r\\n10091,0\\r\\n10092,0\\r\\n10093,0\\r\\n10094,0\\r\\n10095,0\\r\\n10096,0\\r\\n10097,0\\r\\n10098,0\\r\\n10099,0\\r\\n10100,0\\r\\n10101,0\\r\\n10102,0\\r\\n10103,0\\r\\n10104,0\\r\\n10105,0\\r\\n10106,0\\r\\n10107,0\\r\\n10108,0\\r\\n10109,0\\r\\n10110,0\\r\\n10111,0\\r\\n10112,0\\r\\n10113,0\\r\\n10114,0\\r\\n10115,0\\r\\n10116,0\\r\\n10117,0\\r\\n10118,0\\r\\n10119,0\\r\\n10120,0\\r\\n10121,0\\r\\n10122,0\\r\\n10123,0\\r\\n10124,0\\r\\n10125,0\\r\\n10126,0\\r\\n10127,0\\r\\n10128,0\\r\\n10129,0\\r\\n10130,0\\r\\n10131,0\\r\\n10132,0\\r\\n10133,0\\r\\n10134,0\\r\\n10135,0\\r\\n10136,0\\r\\n10137,0\\r\\n10138,0\\r\\n10139,0\\r\\n10140,0\\r\\n10141,0\\r\\n10142,0\\r\\n10143,0\\r\\n10144,0\\r\\n10145,0\\r\\n10146,0\\r\\n10147,0\\r\\n10148,0\\r\\n10149,0\\r\\n10150,0\\r\\n10151,0\\r\\n10152,0\\r\\n10153,0\\r\\n10154,0\\r\\n10155,0\\r\\n10156,0\\r\\n10157,0\\r\\n10158,0\\r\\n10159,0\\r\\n10160,0\\r\\n10161,0\\r\\n10162,0\\r\\n10163,0\\r\\n10164,0\\r\\n10165,0\\r\\n10166,0\\r\\n10167,0\\r\\n10168,0\\r\\n10169,0\\r\\n10170,0\\r\\n10171,0\\r\\n10172,0\\r\\n10173,0\\r\\n10174,0\\r\\n10175,0\\r\\n10176,0\\r\\n10177,0\\r\\n10178,0\\r\\n10179,0\\r\\n10180,0\\r\\n10181,0\\r\\n10182,0\\r\\n10183,0\\r\\n10184,0\\r\\n10185,0\\r\\n10186,0\\r\\n10187,0\\r\\n10188,0\\r\\n10189,0\\r\\n10190,0\\r\\n10191,0\\r\\n10192,0\\r\\n10193,0\\r\\n10194,0\\r\\n10195,0\\r\\n10196,0\\r\\n10197,0\\r\\n10198,0\\r\\n10199,0\\r\\n10200,0\\r\\n10201,0\\r\\n10202,0\\r\\n10203,0\\r\\n10204,0\\r\\n10205,0\\r\\n10206,0\\r\\n10207,0\\r\\n10208,0\\r\\n10209,0\\r\\n10210,0\\r\\n10211,0\\r\\n10212,0\\r\\n10213,0\\r\\n10214,0\\r\\n10215,0\\r\\n10216,0\\r\\n10217,0\\r\\n10218,0\\r\\n10219,0\\r\\n10220,0\\r\\n10221,0\\r\\n10222,0\\r\\n10223,0\\r\\n10224,0\\r\\n10225,0\\r\\n10226,0\\r\\n10227,0\\r\\n10228,0\\r\\n10229,0\\r\\n10230,0\\r\\n10231,0\\r\\n10232,0\\r\\n10233,0\\r\\n10234,0\\r\\n10235,0\\r\\n10236,0\\r\\n10237,0\\r\\n10238,0\\r\\n10239,0\\r\\n10240,0\\r\\n10241,0\\r\\n10242,0\\r\\n10243,0\\r\\n10244,0\\r\\n10245,0\\r\\n10246,0\\r\\n10247,0\\r\\n10248,0\\r\\n10249,0\\r\\n10250,0\\r\\n10251,0\\r\\n10252,0\\r\\n10253,0\\r\\n10254,0\\r\\n10255,0\\r\\n10256,0\\r\\n10257,0\\r\\n10258,0\\r\\n10259,0\\r\\n10260,0\\r\\n10261,0\\r\\n10262,0\\r\\n10263,0\\r\\n10264,0\\r\\n10265,0\\r\\n10266,0\\r\\n10267,0\\r\\n10268,0\\r\\n10269,0\\r\\n10270,0\\r\\n10271,0\\r\\n10272,0\\r\\n10273,0\\r\\n10274,0\\r\\n10275,0\\r\\n10276,0\\r\\n10277,0\\r\\n10278,0\\r\\n10279,0\\r\\n10280,0\\r\\n10281,0\\r\\n10282,0\\r\\n10283,0\\r\\n10284,0\\r\\n10285,0\\r\\n10286,0\\r\\n10287,0\\r\\n10288,0\\r\\n10289,0\\r\\n10290,0\\r\\n10291,0\\r\\n10292,0\\r\\n10293,0\\r\\n10294,0\\r\\n10295,0\\r\\n10296,0\\r\\n10297,0\\r\\n10298,0\\r\\n10299,0\\r\\n10300,0\\r\\n10301,0\\r\\n10302,0\\r\\n10303,0\\r\\n10304,0\\r\\n10305,0\\r\\n10306,0\\r\\n10307,0\\r\\n10308,0\\r\\n10309,0\\r\\n10310,0\\r\\n10311,0\\r\\n10312,0\\r\\n10313,0\\r\\n10314,0\\r\\n10315,0\\r\\n10316,0\\r\\n10317,0\\r\\n10318,0\\r\\n10319,0\\r\\n10320,0\\r\\n10321,0\\r\\n10322,0\\r\\n10323,0\\r\\n10324,0\\r\\n10325,0\\r\\n10326,0\\r\\n10327,0\\r\\n10328,0\\r\\n10329,0\\r\\n10330,0\\r\\n10331,0\\r\\n10332,0\\r\\n10333,0\\r\\n10334,0\\r\\n10335,0\\r\\n10336,0\\r\\n10337,0\\r\\n10338,0\\r\\n10339,0\\r\\n10340,0\\r\\n10341,0\\r\\n10342,0\\r\\n10343,0\\r\\n10344,0\\r\\n10345,0\\r\\n10346,0\\r\\n10347,0\\r\\n10348,0\\r\\n10349,0\\r\\n10350,0\\r\\n10351,0\\r\\n10352,0\\r\\n10353,0\\r\\n10354,0\\r\\n10355,0\\r\\n10356,0\\r\\n10357,0\\r\\n10358,0\\r\\n10359,0\\r\\n10360,0\\r\\n10361,0\\r\\n10362,0\\r\\n10363,0\\r\\n10364,0\\r\\n10365,0\\r\\n10366,0\\r\\n10367,0\\r\\n10368,0\\r\\n10369,0\\r\\n10370,0\\r\\n10371,0\\r\\n10372,0\\r\\n10373,0\\r\\n10374,0\\r\\n10375,0\\r\\n10376,0\\r\\n10377,0\\r\\n10378,0\\r\\n10379,0\\r\\n10380,0\\r\\n10381,0\\r\\n10382,0\\r\\n10383,0\\r\\n10384,0\\r\\n10385,0\\r\\n10386,0\\r\\n10387,0\\r\\n10388,0\\r\\n10389,0\\r\\n10390,0\\r\\n10391,0\\r\\n10392,0\\r\\n10393,0\\r\\n10394,0\\r\\n10395,0\\r\\n10396,0\\r\\n10397,0\\r\\n10398,0\\r\\n10399,0\\r\\n10400,0\\r\\n10401,0\\r\\n10402,0\\r\\n10403,0\\r\\n10404,0\\r\\n10405,0\\r\\n10406,0\\r\\n10407,0\\r\\n10408,0\\r\\n10409,0\\r\\n10410,0\\r\\n10411,0\\r\\n10412,0\\r\\n10413,0\\r\\n10414,0\\r\\n10415,0\\r\\n10416,0\\r\\n10417,0\\r\\n10418,0\\r\\n10419,0\\r\\n10420,0\\r\\n10421,0\\r\\n10422,0\\r\\n10423,0\\r\\n10424,0\\r\\n10425,0\\r\\n10426,0\\r\\n10427,0\\r\\n10428,0\\r\\n10429,0\\r\\n10430,0\\r\\n10431,0\\r\\n10432,0\\r\\n10433,0\\r\\n10434,0\\r\\n10435,0\\r\\n10436,0\\r\\n10437,0\\r\\n10438,0\\r\\n10439,0\\r\\n10440,0\\r\\n10441,0\\r\\n10442,0\\r\\n10443,0\\r\\n10444,0\\r\\n10445,0\\r\\n10446,0\\r\\n10447,0\\r\\n10448,0\\r\\n10449,0\\r\\n10450,0\\r\\n10451,0\\r\\n10452,0\\r\\n10453,0\\r\\n10454,0\\r\\n10455,0\\r\\n10456,0\\r\\n10457,0\\r\\n10458,0\\r\\n10459,0\\r\\n10460,0\\r\\n10461,0\\r\\n10462,0\\r\\n10463,0\\r\\n10464,0\\r\\n10465,0\\r\\n10466,0\\r\\n10467,0\\r\\n10468,0\\r\\n10469,0\\r\\n10470,0\\r\\n10471,0\\r\\n10472,0\\r\\n10473,0\\r\\n10474,0\\r\\n10475,0\\r\\n10476,0\\r\\n10477,0\\r\\n10478,0\\r\\n10479,0\\r\\n10480,0\\r\\n10481,0\\r\\n10482,0\\r\\n10483,0\\r\\n10484,0\\r\\n10485,0\\r\\n10486,0\\r\\n10487,0\\r\\n10488,0\\r\\n10489,0\\r\\n10490,0\\r\\n10491,0\\r\\n10492,0\\r\\n10493,0\\r\\n10494,0\\r\\n10495,0\\r\\n10496,0\\r\\n10497,0\\r\\n10498,0\\r\\n10499,0\\r\\n10500,0\\r\\n10501,0\\r\\n10502,0\\r\\n10503,0\\r\\n10504,0\\r\\n10505,0\\r\\n10506,0\\r\\n10507,0\\r\\n10508,0\\r\\n10509,0\\r\\n10510,0\\r\\n10511,0\\r\\n10512,0\\r\\n10513,0\\r\\n10514,0\\r\\n10515,0\\r\\n10516,0\\r\\n10517,0\\r\\n10518,0\\r\\n10519,0\\r\\n10520,0\\r\\n10521,0\\r\\n10522,0\\r\\n10523,0\\r\\n10524,0\\r\\n10525,0\\r\\n10526,0\\r\\n10527,0\\r\\n10528,0\\r\\n10529,0\\r\\n10530,0\\r\\n10531,0\\r\\n10532,0\\r\\n10533,0\\r\\n10534,0\\r\\n10535,0\\r\\n10536,0\\r\\n10537,0\\r\\n10538,0\\r\\n10539,0\\r\\n10540,0\\r\\n10541,0\\r\\n10542,0\\r\\n10543,0\\r\\n10544,0\\r\\n10545,0\\r\\n10546,0\\r\\n10547,0\\r\\n10548,0\\r\\n10549,0\\r\\n10550,0\\r\\n10551,0\\r\\n10552,0\\r\\n10553,0\\r\\n10554,0\\r\\n10555,0\\r\\n10556,0\\r\\n10557,0\\r\\n10558,0\\r\\n10559,0\\r\\n10560,0\\r\\n10561,0\\r\\n10562,0\\r\\n10563,0\\r\\n10564,0\\r\\n10565,0\\r\\n10566,0\\r\\n10567,0\\r\\n10568,0\\r\\n10569,0\\r\\n10570,0\\r\\n10571,0\\r\\n10572,0\\r\\n10573,0\\r\\n10574,0\\r\\n10575,0\\r\\n10576,0\\r\\n10577,0\\r\\n10578,0\\r\\n10579,0\\r\\n10580,0\\r\\n10581,0\\r\\n10582,0\\r\\n10583,0\\r\\n10584,0\\r\\n10585,0\\r\\n10586,0\\r\\n10587,0\\r\\n10588,0\\r\\n10589,0\\r\\n10590,0\\r\\n10591,0\\r\\n10592,0\\r\\n10593,0\\r\\n10594,0\\r\\n10595,0\\r\\n10596,0\\r\\n10597,0\\r\\n10598,0\\r\\n10599,0\\r\\n10600,0\\r\\n10601,0\\r\\n10602,0\\r\\n10603,0\\r\\n10604,0\\r\\n10605,0\\r\\n10606,0\\r\\n10607,0\\r\\n10608,0\\r\\n10609,0\\r\\n10610,0\\r\\n10611,0\\r\\n10612,0\\r\\n10613,0\\r\\n10614,0\\r\\n10615,0\\r\\n10616,0\\r\\n10617,0\\r\\n10618,0\\r\\n10619,0\\r\\n10620,0\\r\\n10621,0\\r\\n10622,0\\r\\n10623,0\\r\\n10624,0\\r\\n10625,0\\r\\n10626,0\\r\\n10627,0\\r\\n10628,0\\r\\n10629,0\\r\\n10630,0\\r\\n10631,0\\r\\n10632,0\\r\\n10633,0\\r\\n10634,0\\r\\n10635,0\\r\\n10636,0\\r\\n10637,0\\r\\n10638,0\\r\\n10639,0\\r\\n10640,0\\r\\n10641,0\\r\\n10642,0\\r\\n10643,0\\r\\n10644,0\\r\\n10645,0\\r\\n10646,0\\r\\n10647,0\\r\\n10648,0\\r\\n10649,0\\r\\n10650,0\\r\\n10651,0\\r\\n10652,0\\r\\n10653,0\\r\\n10654,0\\r\\n10655,0\\r\\n10656,0\\r\\n10657,0\\r\\n10658,0\\r\\n10659,0\\r\\n10660,0\\r\\n10661,0\\r\\n10662,0\\r\\n10663,0\\r\\n10664,0\\r\\n10665,0\\r\\n10666,0\\r\\n10667,0\\r\\n10668,0\\r\\n10669,0\\r\\n10670,0\\r\\n10671,0\\r\\n10672,0\\r\\n10673,0\\r\\n10674,0\\r\\n10675,0\\r\\n10676,0\\r\\n10677,0\\r\\n10678,0\\r\\n10679,0\\r\\n10680,0\\r\\n10681,0\\r\\n10682,0\\r\\n10683,0\\r\\n10684,0\\r\\n10685,0\\r\\n10686,0\\r\\n10687,0\\r\\n10688,0\\r\\n10689,0\\r\\n10690,0\\r\\n10691,0\\r\\n10692,0\\r\\n10693,0\\r\\n10694,0\\r\\n10695,0\\r\\n10696,0\\r\\n10697,0\\r\\n10698,0\\r\\n10699,0\\r\\n10700,0\\r\\n10701,0\\r\\n10702,0\\r\\n10703,0\\r\\n10704,0\\r\\n10705,0\\r\\n10706,0\\r\\n10707,0\\r\\n10708,0\\r\\n10709,0\\r\\n10710,0\\r\\n10711,0\\r\\n10712,0\\r\\n10713,0\\r\\n10714,0\\r\\n10715,0\\r\\n10716,0\\r\\n10717,0\\r\\n10718,0\\r\\n10719,0\\r\\n10720,0\\r\\n10721,0\\r\\n10722,0\\r\\n10723,0\\r\\n10724,0\\r\\n10725,0\\r\\n10726,0\\r\\n10727,0\\r\\n10728,0\\r\\n10729,0\\r\\n10730,0\\r\\n10731,0\\r\\n10732,0\\r\\n10733,0\\r\\n10734,0\\r\\n10735,0\\r\\n10736,0\\r\\n10737,0\\r\\n10738,0\\r\\n10739,0\\r\\n10740,0\\r\\n10741,0\\r\\n10742,0\\r\\n10743,0\\r\\n10744,0\\r\\n10745,0\\r\\n10746,0\\r\\n10747,0\\r\\n10748,0\\r\\n10749,0\\r\\n10750,0\\r\\n10751,0\\r\\n10752,0\\r\\n10753,0\\r\\n10754,0\\r\\n10755,0\\r\\n10756,0\\r\\n10757,0\\r\\n10758,0\\r\\n10759,0\\r\\n10760,0\\r\\n10761,0\\r\\n10762,0\\r\\n10763,0\\r\\n10764,0\\r\\n10765,0\\r\\n10766,0\\r\\n10767,0\\r\\n10768,0\\r\\n10769,0\\r\\n10770,0\\r\\n10771,0\\r\\n10772,0\\r\\n10773,0\\r\\n10774,0\\r\\n10775,0\\r\\n10776,0\\r\\n10777,0\\r\\n10778,0\\r\\n10779,0\\r\\n10780,0\\r\\n10781,0\\r\\n10782,0\\r\\n10783,0\\r\\n10784,0\\r\\n10785,0\\r\\n10786,0\\r\\n10787,0\\r\\n10788,0\\r\\n10789,0\\r\\n10790,0\\r\\n10791,0\\r\\n10792,0\\r\\n10793,0\\r\\n10794,0\\r\\n10795,0\\r\\n10796,0\\r\\n10797,0\\r\\n10798,0\\r\\n10799,0\\r\\n10800,0\\r\\n10801,0\\r\\n10802,0\\r\\n10803,0\\r\\n10804,0\\r\\n10805,0\\r\\n10806,0\\r\\n10807,0\\r\\n10808,0\\r\\n10809,0\\r\\n10810,0\\r\\n10811,0\\r\\n10812,0\\r\\n10813,0\\r\\n10814,0\\r\\n10815,0\\r\\n10816,0\\r\\n10817,0\\r\\n10818,0\\r\\n10819,0\\r\\n10820,0\\r\\n10821,0\\r\\n10822,0\\r\\n10823,0\\r\\n10824,0\\r\\n10825,0\\r\\n10826,0\\r\\n10827,0\\r\\n10828,0\\r\\n10829,0\\r\\n10830,0\\r\\n10831,0\\r\\n10832,0\\r\\n10833,0\\r\\n10834,0\\r\\n10835,0\\r\\n10836,0\\r\\n10837,0\\r\\n10838,0\\r\\n10839,0\\r\\n10840,0\\r\\n10841,0\\r\\n10842,0\\r\\n10843,0\\r\\n10844,0\\r\\n10845,0\\r\\n10846,0\\r\\n10847,0\\r\\n10848,0\\r\\n10849,0\\r\\n10850,0\\r\\n10851,0\\r\\n10852,0\\r\\n10853,0\\r\\n10854,0\\r\\n10855,0\\r\\n10856,0\\r\\n10857,0\\r\\n10858,0\\r\\n10859,0\\r\\n10860,0\\r\\n10861,0\\r\\n10862,0\\r\\n10863,0\\r\\n10864,0\\r\\n10865,0\\r\\n10866,0\\r\\n10867,0\\r\\n10868,0\\r\\n10869,0\\r\\n10870,0\\r\\n10871,0\\r\\n10872,0\\r\\n10873,0\\r\\n10874,0\\r\\n10875,0\\r\\n10876,0\\r\\n10877,0\\r\\n10878,0\\r\\n10879,0\\r\\n10880,0\\r\\n10881,0\\r\\n10882,0\\r\\n10883,0\\r\\n10884,0\\r\\n10885,0\\r\\n10886,0\\r\\n10887,0\\r\\n10888,0\\r\\n10889,0\\r\\n10890,0\\r\\n10891,0\\r\\n10892,0\\r\\n10893,0\\r\\n10894,0\\r\\n10895,0\\r\\n10896,0\\r\\n10897,0\\r\\n10898,0\\r\\n10899,0\\r\\n10900,0\\r\\n10901,0\\r\\n10902,0\\r\\n10903,0\\r\\n10904,0\\r\\n10905,0\\r\\n10906,0\\r\\n10907,0\\r\\n10908,0\\r\\n10909,0\\r\\n10910,0\\r\\n10911,0\\r\\n10912,0\\r\\n10913,0\\r\\n10914,0\\r\\n10915,0\\r\\n10916,0\\r\\n10917,0\\r\\n10918,0\\r\\n10919,0\\r\\n10920,0\\r\\n10921,0\\r\\n10922,0\\r\\n10923,0\\r\\n10924,0\\r\\n10925,0\\r\\n10926,0\\r\\n10927,0\\r\\n10928,0\\r\\n10929,0\\r\\n10930,0\\r\\n10931,0\\r\\n10932,0\\r\\n10933,0\\r\\n10934,0\\r\\n10935,0\\r\\n10936,0\\r\\n10937,0\\r\\n10938,0\\r\\n10939,0\\r\\n10940,0\\r\\n10941,0\\r\\n10942,0\\r\\n10943,0\\r\\n10944,0\\r\\n10945,0\\r\\n10946,0\\r\\n10947,0\\r\\n10948,0\\r\\n10949,0\\r\\n10950,0\\r\\n10951,0\\r\\n10952,0\\r\\n10953,0\\r\\n10954,0\\r\\n10955,0\\r\\n10956,0\\r\\n10957,0\\r\\n10958,0\\r\\n10959,0\\r\\n10960,0\\r\\n10961,0\\r\\n10962,0\\r\\n10963,0\\r\\n10964,0\\r\\n10965,0\\r\\n10966,0\\r\\n10967,0\\r\\n10968,0\\r\\n10969,0\\r\\n10970,0\\r\\n10971,0\\r\\n10972,0\\r\\n10973,0\\r\\n10974,0\\r\\n10975,0\\r\\n10976,0\\r\\n10977,0\\r\\n10978,0\\r\\n10979,0\\r\\n10980,0\\r\\n10981,0\\r\\n10982,0\\r\\n10983,0\\r\\n10984,0\\r\\n10985,0\\r\\n10986,0\\r\\n10987,0\\r\\n10988,0\\r\\n10989,0\\r\\n10990,0\\r\\n10991,0\\r\\n10992,0\\r\\n10993,0\\r\\n10994,0\\r\\n10995,0\\r\\n10996,0\\r\\n10997,0\\r\\n10998,0\\r\\n10999,0\\r\\n11000,0\\r\\n11001,0\\r\\n11002,0\\r\\n11003,0\\r\\n11004,0\\r\\n11005,0\\r\\n11006,0\\r\\n11007,0\\r\\n11008,0\\r\\n11009,0\\r\\n11010,0\\r\\n11011,0\\r\\n11012,0\\r\\n11013,0\\r\\n11014,0\\r\\n11015,0\\r\\n11016,0\\r\\n11017,0\\r\\n11018,0\\r\\n11019,0\\r\\n11020,0\\r\\n11021,0\\r\\n11022,0\\r\\n11023,0\\r\\n11024,0\\r\\n11025,0\\r\\n11026,0\\r\\n11027,0\\r\\n11028,0\\r\\n11029,0\\r\\n11030,0\\r\\n11031,0\\r\\n11032,0\\r\\n11033,0\\r\\n11034,0\\r\\n11035,0\\r\\n11036,0\\r\\n11037,0\\r\\n11038,0\\r\\n11039,0\\r\\n11040,0\\r\\n11041,0\\r\\n11042,0\\r\\n11043,0\\r\\n11044,0\\r\\n11045,0\\r\\n11046,0\\r\\n11047,0\\r\\n11048,0\\r\\n11049,0\\r\\n11050,0\\r\\n11051,0\\r\\n11052,0\\r\\n11053,0\\r\\n11054,0\\r\\n11055,0\\r\\n11056,0\\r\\n11057,0\\r\\n11058,0\\r\\n11059,0\\r\\n11060,0\\r\\n11061,0\\r\\n11062,0\\r\\n11063,0\\r\\n11064,0\\r\\n11065,0\\r\\n11066,0\\r\\n11067,0\\r\\n11068,0\\r\\n11069,0\\r\\n11070,0\\r\\n11071,0\\r\\n11072,0\\r\\n11073,0\\r\\n11074,0\\r\\n11075,0\\r\\n11076,0\\r\\n11077,0\\r\\n11078,0\\r\\n11079,0\\r\\n11080,0\\r\\n11081,0\\r\\n11082,0\\r\\n11083,0\\r\\n11084,0\\r\\n11085,0\\r\\n11086,0\\r\\n11087,0\\r\\n11088,0\\r\\n11089,0\\r\\n11090,0\\r\\n11091,0\\r\\n11092,0\\r\\n11093,0\\r\\n11094,0\\r\\n11095,0\\r\\n11096,0\\r\\n11097,0\\r\\n11098,0\\r\\n11099,0\\r\\n11100,0\\r\\n11101,0\\r\\n11102,0\\r\\n11103,0\\r\\n11104,0\\r\\n11105,0\\r\\n11106,0\\r\\n11107,0\\r\\n11108,0\\r\\n11109,0\\r\\n11110,0\\r\\n11111,0\\r\\n11112,0\\r\\n11113,0\\r\\n11114,0\\r\\n11115,0\\r\\n11116,0\\r\\n11117,0\\r\\n11118,0\\r\\n11119,0\\r\\n11120,0\\r\\n11121,0\\r\\n11122,0\\r\\n11123,0\\r\\n11124,0\\r\\n11125,0\\r\\n11126,0\\r\\n11127,0\\r\\n11128,0\\r\\n11129,0\\r\\n11130,0\\r\\n11131,0\\r\\n11132,0\\r\\n11133,0\\r\\n11134,0\\r\\n11135,0\\r\\n11136,0\\r\\n11137,0\\r\\n11138,0\\r\\n11139,0\\r\\n11140,0\\r\\n11141,0\\r\\n11142,0\\r\\n11143,0\\r\\n11144,0\\r\\n11145,0\\r\\n11146,0\\r\\n11147,0\\r\\n11148,0\\r\\n11149,0\\r\\n11150,0\\r\\n11151,0\\r\\n11152,0\\r\\n11153,0\\r\\n11154,0\\r\\n11155,0\\r\\n11156,0\\r\\n11157,0\\r\\n11158,0\\r\\n11159,0\\r\\n11160,0\\r\\n11161,0\\r\\n11162,0\\r\\n11163,0\\r\\n11164,0\\r\\n11165,0\\r\\n11166,0\\r\\n11167,0\\r\\n11168,0\\r\\n11169,0\\r\\n11170,0\\r\\n11171,0\\r\\n11172,0\\r\\n11173,0\\r\\n11174,0\\r\\n11175,0\\r\\n11176,0\\r\\n11177,0\\r\\n11178,0\\r\\n11179,0\\r\\n11180,0\\r\\n11181,0\\r\\n11182,0\\r\\n11183,0\\r\\n11184,0\\r\\n11185,0\\r\\n11186,0\\r\\n11187,0\\r\\n11188,0\\r\\n11189,0\\r\\n11190,0\\r\\n11191,0\\r\\n11192,0\\r\\n11193,0\\r\\n11194,0\\r\\n11195,0\\r\\n11196,0\\r\\n11197,0\\r\\n11198,0\\r\\n11199,0\\r\\n11200,0\\r\\n11201,0\\r\\n11202,0\\r\\n11203,0\\r\\n11204,0\\r\\n11205,0\\r\\n11206,0\\r\\n11207,0\\r\\n11208,0\\r\\n11209,0\\r\\n11210,0\\r\\n11211,0\\r\\n11212,0\\r\\n11213,0\\r\\n11214,0\\r\\n11215,0\\r\\n11216,0\\r\\n11217,0\\r\\n11218,0\\r\\n11219,0\\r\\n11220,0\\r\\n11221,0\\r\\n11222,0\\r\\n11223,0\\r\\n11224,0\\r\\n11225,0\\r\\n11226,0\\r\\n11227,0\\r\\n11228,0\\r\\n11229,0\\r\\n11230,0\\r\\n11231,0\\r\\n11232,0\\r\\n11233,0\\r\\n11234,0\\r\\n11235,0\\r\\n11236,0\\r\\n11237,0\\r\\n11238,0\\r\\n11239,0\\r\\n11240,0\\r\\n11241,0\\r\\n11242,0\\r\\n11243,0\\r\\n11244,0\\r\\n11245,0\\r\\n11246,0\\r\\n11247,0\\r\\n11248,0\\r\\n11249,0\\r\\n11250,0\\r\\n11251,0\\r\\n11252,0\\r\\n11253,0\\r\\n11254,0\\r\\n11255,0\\r\\n11256,0\\r\\n11257,0\\r\\n11258,0\\r\\n11259,0\\r\\n11260,0\\r\\n11261,0\\r\\n11262,0\\r\\n11263,0\\r\\n11264,0\\r\\n11265,0\\r\\n11266,0\\r\\n11267,0\\r\\n11268,0\\r\\n11269,0\\r\\n11270,0\\r\\n11271,0\\r\\n11272,0\\r\\n11273,0\\r\\n11274,0\\r\\n11275,0\\r\\n11276,0\\r\\n11277,0\\r\\n11278,0\\r\\n11279,0\\r\\n11280,0\\r\\n11281,0\\r\\n11282,0\\r\\n11283,0\\r\\n11284,0\\r\\n11285,0\\r\\n11286,0\\r\\n11287,0\\r\\n11288,0\\r\\n11289,0\\r\\n11290,0\\r\\n11291,0\\r\\n11292,0\\r\\n11293,0\\r\\n11294,0\\r\\n11295,0\\r\\n11296,0\\r\\n11297,0\\r\\n11298,0\\r\\n11299,0\\r\\n11300,0\\r\\n11301,0\\r\\n11302,0\\r\\n11303,0\\r\\n11304,0\\r\\n11305,0\\r\\n11306,0\\r\\n11307,0\\r\\n11308,0\\r\\n11309,0\\r\\n11310,0\\r\\n11311,0\\r\\n11312,0\\r\\n11313,0\\r\\n11314,0\\r\\n11315,0\\r\\n11316,0\\r\\n11317,0\\r\\n11318,0\\r\\n11319,0\\r\\n11320,0\\r\\n11321,0\\r\\n11322,0\\r\\n11323,0\\r\\n11324,0\\r\\n11325,0\\r\\n11326,0\\r\\n11327,0\\r\\n11328,0\\r\\n11329,0\\r\\n11330,0\\r\\n11331,0\\r\\n11332,0\\r\\n11333,0\\r\\n11334,0\\r\\n11335,0\\r\\n11336,0\\r\\n11337,0\\r\\n11338,0\\r\\n11339,0\\r\\n11340,0\\r\\n11341,0\\r\\n11342,0\\r\\n11343,0\\r\\n11344,0\\r\\n11345,0\\r\\n11346,0\\r\\n11347,0\\r\\n11348,0\\r\\n11349,0\\r\\n11350,0\\r\\n11351,0\\r\\n11352,0\\r\\n11353,0\\r\\n11354,0\\r\\n11355,0\\r\\n11356,0\\r\\n11357,0\\r\\n11358,0\\r\\n11359,0\\r\\n11360,0\\r\\n11361,0\\r\\n11362,0\\r\\n11363,0\\r\\n11364,0\\r\\n11365,0\\r\\n11366,0\\r\\n11367,0\\r\\n11368,0\\r\\n11369,0\\r\\n11370,0\\r\\n11371,0\\r\\n11372,0\\r\\n11373,0\\r\\n11374,0\\r\\n11375,0\\r\\n11376,0\\r\\n11377,0\\r\\n11378,0\\r\\n11379,0\\r\\n11380,0\\r\\n11381,0\\r\\n11382,0\\r\\n11383,0\\r\\n11384,0\\r\\n11385,0\\r\\n11386,0\\r\\n11387,0\\r\\n11388,0\\r\\n11389,0\\r\\n11390,0\\r\\n11391,0\\r\\n11392,0\\r\\n11393,0\\r\\n11394,0\\r\\n11395,0\\r\\n11396,0\\r\\n11397,0\\r\\n11398,0\\r\\n11399,0\\r\\n11400,0\\r\\n11401,0\\r\\n11402,0\\r\\n11403,0\\r\\n11404,0\\r\\n11405,0\\r\\n11406,0\\r\\n11407,0\\r\\n11408,0\\r\\n11409,0\\r\\n11410,0\\r\\n11411,0\\r\\n11412,0\\r\\n11413,0\\r\\n11414,0\\r\\n11415,0\\r\\n11416,0\\r\\n11417,0\\r\\n11418,0\\r\\n11419,0\\r\\n11420,0\\r\\n11421,0\\r\\n11422,0\\r\\n11423,0\\r\\n11424,0\\r\\n11425,0\\r\\n11426,0\\r\\n11427,0\\r\\n11428,0\\r\\n11429,0\\r\\n11430,0\\r\\n11431,0\\r\\n11432,0\\r\\n11433,0\\r\\n11434,0\\r\\n11435,0\\r\\n11436,0\\r\\n11437,0\\r\\n11438,0\\r\\n11439,0\\r\\n11440,0\\r\\n11441,0\\r\\n11442,0\\r\\n11443,0\\r\\n11444,0\\r\\n11445,0\\r\\n11446,0\\r\\n11447,0\\r\\n11448,0\\r\\n11449,0\\r\\n11450,0\\r\\n11451,0\\r\\n11452,0\\r\\n11453,0\\r\\n11454,0\\r\\n11455,0\\r\\n11456,0\\r\\n11457,0\\r\\n11458,0\\r\\n11459,0\\r\\n11460,0\\r\\n11461,0\\r\\n11462,0\\r\\n11463,0\\r\\n11464,0\\r\\n11465,0\\r\\n11466,0\\r\\n11467,0\\r\\n11468,0\\r\\n11469,0\\r\\n11470,0\\r\\n11471,0\\r\\n11472,0\\r\\n11473,0\\r\\n11474,0\\r\\n11475,0\\r\\n11476,0\\r\\n11477,0\\r\\n11478,0\\r\\n11479,0\\r\\n11480,0\\r\\n11481,0\\r\\n11482,0\\r\\n11483,0\\r\\n11484,0\\r\\n11485,0\\r\\n11486,0\\r\\n11487,0\\r\\n11488,0\\r\\n11489,0\\r\\n11490,0\\r\\n11491,0\\r\\n11492,0\\r\\n11493,0\\r\\n11494,0\\r\\n11495,0\\r\\n11496,0\\r\\n11497,0\\r\\n11498,0\\r\\n11499,0\\r\\n11500,0\\r\\n11501,0\\r\\n11502,0\\r\\n11503,0\\r\\n11504,0\\r\\n11505,0\\r\\n11506,0\\r\\n11507,0\\r\\n11508,0\\r\\n11509,0\\r\\n11510,0\\r\\n11511,0\\r\\n11512,0\\r\\n11513,0\\r\\n11514,0\\r\\n11515,0\\r\\n11516,0\\r\\n11517,0\\r\\n11518,0\\r\\n11519,0\\r\\n11520,0\\r\\n11521,0\\r\\n11522,0\\r\\n11523,0\\r\\n11524,0\\r\\n11525,0\\r\\n11526,0\\r\\n11527,0\\r\\n11528,0\\r\\n11529,0\\r\\n11530,0\\r\\n11531,0\\r\\n11532,0\\r\\n11533,0\\r\\n11534,0\\r\\n11535,0\\r\\n11536,0\\r\\n11537,0\\r\\n11538,0\\r\\n11539,0\\r\\n11540,0\\r\\n11541,0\\r\\n11542,0\\r\\n11543,0\\r\\n11544,0\\r\\n11545,0\\r\\n11546,0\\r\\n11547,0\\r\\n11548,0\\r\\n11549,0\\r\\n11550,0\\r\\n11551,0\\r\\n11552,0\\r\\n11553,0\\r\\n11554,0\\r\\n11555,0\\r\\n11556,0\\r\\n11557,0\\r\\n11558,0\\r\\n11559,0\\r\\n11560,0\\r\\n11561,0\\r\\n11562,0\\r\\n11563,0\\r\\n11564,0\\r\\n11565,0\\r\\n11566,0\\r\\n11567,0\\r\\n11568,0\\r\\n11569,0\\r\\n11570,0\\r\\n11571,0\\r\\n11572,0\\r\\n11573,0\\r\\n11574,0\\r\\n11575,0\\r\\n11576,0\\r\\n11577,0\\r\\n11578,0\\r\\n11579,0\\r\\n11580,0\\r\\n11581,0\\r\\n11582,0\\r\\n11583,0\\r\\n11584,0\\r\\n11585,0\\r\\n11586,0\\r\\n11587,0\\r\\n11588,0\\r\\n11589,0\\r\\n11590,0\\r\\n11591,0\\r\\n11592,0\\r\\n11593,0\\r\\n11594,0\\r\\n11595,0\\r\\n11596,0\\r\\n11597,0\\r\\n11598,0\\r\\n11599,0\\r\\n11600,0\\r\\n11601,0\\r\\n11602,0\\r\\n11603,0\\r\\n11604,0\\r\\n11605,0\\r\\n11606,0\\r\\n11607,0\\r\\n11608,0\\r\\n11609,0\\r\\n11610,0\\r\\n11611,0\\r\\n11612,0\\r\\n11613,0\\r\\n11614,0\\r\\n11615,0\\r\\n11616,0\\r\\n11617,0\\r\\n11618,0\\r\\n11619,0\\r\\n11620,0\\r\\n11621,0\\r\\n11622,0\\r\\n11623,0\\r\\n11624,0\\r\\n11625,0\\r\\n11626,0\\r\\n11627,0\\r\\n11628,0\\r\\n11629,0\\r\\n11630,0\\r\\n11631,0\\r\\n11632,0\\r\\n11633,0\\r\\n11634,0\\r\\n11635,0\\r\\n11636,0\\r\\n11637,0\\r\\n11638,0\\r\\n11639,0\\r\\n11640,0\\r\\n11641,0\\r\\n11642,0\\r\\n11643,0\\r\\n11644,0\\r\\n11645,0\\r\\n11646,0\\r\\n11647,0\\r\\n11648,0\\r\\n11649,0\\r\\n11650,0\\r\\n11651,0\\r\\n11652,0\\r\\n11653,0\\r\\n11654,0\\r\\n11655,0\\r\\n11656,0\\r\\n11657,0\\r\\n11658,0\\r\\n11659,0\\r\\n11660,0\\r\\n11661,0\\r\\n11662,0\\r\\n11663,0\\r\\n11664,0\\r\\n11665,0\\r\\n11666,0\\r\\n11667,0\\r\\n11668,0\\r\\n11669,0\\r\\n11670,0\\r\\n11671,0\\r\\n11672,0\\r\\n11673,0\\r\\n11674,0\\r\\n11675,0\\r\\n11676,0\\r\\n11677,0\\r\\n11678,0\\r\\n11679,0\\r\\n11680,0\\r\\n11681,0\\r\\n11682,0\\r\\n11683,0\\r\\n11684,0\\r\\n11685,0\\r\\n11686,0\\r\\n11687,0\\r\\n11688,0\\r\\n11689,0\\r\\n11690,0\\r\\n11691,0\\r\\n11692,0\\r\\n11693,0\\r\\n11694,0\\r\\n11695,0\\r\\n11696,0\\r\\n11697,0\\r\\n11698,0\\r\\n11699,0\\r\\n11700,0\\r\\n11701,0\\r\\n11702,0\\r\\n11703,0\\r\\n11704,0\\r\\n11705,0\\r\\n11706,0\\r\\n11707,0\\r\\n11708,0\\r\\n11709,0\\r\\n11710,0\\r\\n11711,0\\r\\n11712,0\\r\\n11713,0\\r\\n11714,0\\r\\n11715,0\\r\\n11716,0\\r\\n11717,0\\r\\n11718,0\\r\\n11719,0\\r\\n11720,0\\r\\n11721,0\\r\\n11722,0\\r\\n11723,0\\r\\n11724,0\\r\\n11725,0\\r\\n11726,0\\r\\n11727,0\\r\\n11728,0\\r\\n11729,0\\r\\n11730,0\\r\\n11731,0\\r\\n11732,0\\r\\n11733,0\\r\\n11734,0\\r\\n11735,0\\r\\n11736,0\\r\\n11737,0\\r\\n11738,0\\r\\n11739,0\\r\\n11740,0\\r\\n11741,0\\r\\n11742,0\\r\\n11743,0\\r\\n11744,0\\r\\n11745,0\\r\\n11746,0\\r\\n11747,0\\r\\n11748,0\\r\\n11749,0\\r\\n11750,0\\r\\n11751,0\\r\\n11752,0\\r\\n11753,0\\r\\n11754,0\\r\\n11755,0\\r\\n11756,0\\r\\n11757,0\\r\\n11758,0\\r\\n11759,0\\r\\n11760,0\\r\\n11761,0\\r\\n11762,0\\r\\n11763,0\\r\\n11764,0\\r\\n11765,0\\r\\n11766,0\\r\\n11767,0\\r\\n11768,0\\r\\n11769,0\\r\\n11770,0\\r\\n11771,0\\r\\n11772,0\\r\\n11773,0\\r\\n11774,0\\r\\n11775,0\\r\\n11776,0\\r\\n11777,0\\r\\n11778,0\\r\\n11779,0\\r\\n11780,0\\r\\n11781,0\\r\\n11782,0\\r\\n11783,0\\r\\n11784,0\\r\\n11785,0\\r\\n11786,0\\r\\n11787,0\\r\\n11788,0\\r\\n11789,0\\r\\n11790,0\\r\\n11791,0\\r\\n11792,0\\r\\n11793,0\\r\\n11794,0\\r\\n11795,0\\r\\n11796,0\\r\\n11797,0\\r\\n11798,0\\r\\n11799,0\\r\\n11800,0\\r\\n11801,0\\r\\n11802,0\\r\\n11803,0\\r\\n11804,0\\r\\n11805,0\\r\\n11806,0\\r\\n11807,0\\r\\n11808,0\\r\\n11809,0\\r\\n11810,0\\r\\n11811,0\\r\\n11812,0\\r\\n11813,0\\r\\n11814,0\\r\\n11815,0\\r\\n11816,0\\r\\n11817,0\\r\\n11818,0\\r\\n11819,0\\r\\n11820,0\\r\\n11821,0\\r\\n11822,0\\r\\n11823,0\\r\\n11824,0\\r\\n11825,0\\r\\n11826,0\\r\\n11827,0\\r\\n11828,0\\r\\n11829,0\\r\\n11830,0\\r\\n11831,0\\r\\n11832,0\\r\\n11833,0\\r\\n11834,0\\r\\n11835,0\\r\\n11836,0\\r\\n11837,0\\r\\n11838,0\\r\\n11839,0\\r\\n11840,0\\r\\n11841,0\\r\\n11842,0\\r\\n11843,0\\r\\n11844,0\\r\\n11845,0\\r\\n11846,0\\r\\n11847,0\\r\\n11848,0\\r\\n11849,0\\r\\n11850,0\\r\\n11851,0\\r\\n11852,0\\r\\n11853,0\\r\\n11854,0\\r\\n11855,0\\r\\n11856,0\\r\\n11857,0\\r\\n11858,0\\r\\n11859,0\\r\\n11860,0\\r\\n11861,0\\r\\n11862,0\\r\\n11863,0\\r\\n11864,0\\r\\n11865,0\\r\\n11866,0\\r\\n11867,0\\r\\n11868,0\\r\\n11869,0\\r\\n11870,0\\r\\n11871,0\\r\\n11872,0\\r\\n11873,0\\r\\n11874,0\\r\\n11875,0\\r\\n11876,0\\r\\n11877,0\\r\\n11878,0\\r\\n11879,0\\r\\n11880,0\\r\\n11881,0\\r\\n11882,0\\r\\n11883,0\\r\\n11884,0\\r\\n11885,0\\r\\n11886,0\\r\\n11887,0\\r\\n11888,0\\r\\n11889,0\\r\\n11890,0\\r\\n11891,0\\r\\n11892,0\\r\\n11893,0\\r\\n11894,0\\r\\n11895,0\\r\\n11896,0\\r\\n11897,0\\r\\n11898,0\\r\\n11899,0\\r\\n11900,0\\r\\n11901,0\\r\\n11902,0\\r\\n11903,0\\r\\n11904,0\\r\\n11905,0\\r\\n11906,0\\r\\n11907,0\\r\\n11908,0\\r\\n11909,0\\r\\n11910,0\\r\\n11911,0\\r\\n11912,0\\r\\n11913,0\\r\\n11914,0\\r\\n11915,0\\r\\n11916,0\\r\\n11917,0\\r\\n11918,0\\r\\n11919,0\\r\\n11920,0\\r\\n11921,0\\r\\n11922,0\\r\\n11923,0\\r\\n11924,0\\r\\n11925,0\\r\\n11926,0\\r\\n11927,0\\r\\n11928,0\\r\\n11929,0\\r\\n11930,0\\r\\n11931,0\\r\\n11932,0\\r\\n11933,0\\r\\n11934,0\\r\\n11935,0\\r\\n11936,0\\r\\n11937,0\\r\\n11938,0\\r\\n11939,0\\r\\n11940,0\\r\\n11941,0\\r\\n11942,0\\r\\n11943,0\\r\\n11944,0\\r\\n11945,0\\r\\n11946,0\\r\\n11947,0\\r\\n11948,0\\r\\n11949,0\\r\\n11950,0\\r\\n11951,0\\r\\n11952,0\\r\\n11953,0\\r\\n11954,0\\r\\n11955,0\\r\\n11956,0\\r\\n11957,0\\r\\n11958,0\\r\\n11959,0\\r\\n11960,0\\r\\n11961,0\\r\\n11962,0\\r\\n11963,0\\r\\n11964,0\\r\\n11965,0\\r\\n11966,0\\r\\n11967,0\\r\\n11968,0\\r\\n11969,0\\r\\n11970,0\\r\\n11971,0\\r\\n11972,0\\r\\n11973,0\\r\\n11974,0\\r\\n11975,0\\r\\n11976,0\\r\\n11977,0\\r\\n11978,0\\r\\n11979,0\\r\\n11980,0\\r\\n11981,0\\r\\n11982,0\\r\\n11983,0\\r\\n11984,0\\r\\n11985,0\\r\\n11986,0\\r\\n11987,0\\r\\n11988,0\\r\\n11989,0\\r\\n11990,0\\r\\n11991,0\\r\\n11992,0\\r\\n11993,0\\r\\n11994,0\\r\\n11995,0\\r\\n11996,0\\r\\n11997,0\\r\\n11998,0\\r\\n11999,0\\r\\n12000,0\\r\\n12001,0\\r\\n12002,0\\r\\n12003,0\\r\\n12004,0\\r\\n12005,0\\r\\n12006,0\\r\\n12007,0\\r\\n12008,0\\r\\n12009,0\\r\\n12010,0\\r\\n12011,0\\r\\n12012,0\\r\\n12013,0\\r\\n12014,0\\r\\n12015,0\\r\\n12016,0\\r\\n12017,0\\r\\n12018,0\\r\\n12019,0\\r\\n12020,0\\r\\n12021,0\\r\\n12022,0\\r\\n12023,0\\r\\n12024,0\\r\\n12025,0\\r\\n12026,0\\r\\n12027,0\\r\\n12028,0\\r\\n12029,0\\r\\n12030,0\\r\\n12031,0\\r\\n12032,0\\r\\n12033,0\\r\\n12034,0\\r\\n12035,0\\r\\n12036,0\\r\\n12037,0\\r\\n12038,0\\r\\n12039,0\\r\\n12040,0\\r\\n12041,0\\r\\n12042,0\\r\\n12043,0\\r\\n12044,0\\r\\n12045,0\\r\\n12046,0\\r\\n12047,0\\r\\n12048,0\\r\\n12049,0\\r\\n12050,0\\r\\n12051,0\\r\\n12052,0\\r\\n12053,0\\r\\n12054,0\\r\\n12055,0\\r\\n12056,0\\r\\n12057,0\\r\\n12058,0\\r\\n12059,0\\r\\n12060,0\\r\\n12061,0\\r\\n12062,0\\r\\n12063,0\\r\\n12064,0\\r\\n12065,0\\r\\n12066,0\\r\\n12067,0\\r\\n12068,0\\r\\n12069,0\\r\\n12070,0\\r\\n12071,0\\r\\n12072,0\\r\\n12073,0\\r\\n12074,0\\r\\n12075,0\\r\\n12076,0\\r\\n12077,0\\r\\n12078,0\\r\\n12079,0\\r\\n12080,0\\r\\n12081,0\\r\\n12082,0\\r\\n12083,0\\r\\n12084,0\\r\\n12085,0\\r\\n12086,0\\r\\n12087,0\\r\\n12088,0\\r\\n12089,0\\r\\n12090,0\\r\\n12091,0\\r\\n12092,0\\r\\n12093,0\\r\\n12094,0\\r\\n12095,0\\r\\n12096,0\\r\\n12097,0\\r\\n12098,0\\r\\n12099,0\\r\\n12100,0\\r\\n12101,0\\r\\n12102,0\\r\\n12103,0\\r\\n12104,0\\r\\n12105,0\\r\\n12106,0\\r\\n12107,0\\r\\n12108,0\\r\\n12109,0\\r\\n12110,0\\r\\n12111,0\\r\\n12112,0\\r\\n12113,0\\r\\n12114,0\\r\\n12115,0\\r\\n12116,0\\r\\n12117,0\\r\\n12118,0\\r\\n12119,0\\r\\n12120,0\\r\\n12121,0\\r\\n12122,0\\r\\n12123,0\\r\\n12124,0\\r\\n12125,0\\r\\n12126,0\\r\\n12127,0\\r\\n12128,0\\r\\n12129,0\\r\\n12130,0\\r\\n12131,0\\r\\n12132,0\\r\\n12133,0\\r\\n12134,0\\r\\n12135,0\\r\\n12136,0\\r\\n12137,0\\r\\n12138,0\\r\\n12139,0\\r\\n12140,0\\r\\n12141,0\\r\\n12142,0\\r\\n12143,0\\r\\n12144,0\\r\\n12145,0\\r\\n12146,0\\r\\n12147,0\\r\\n12148,0\\r\\n12149,0\\r\\n12150,0\\r\\n12151,0\\r\\n12152,0\\r\\n12153,0\\r\\n12154,0\\r\\n12155,0\\r\\n12156,0\\r\\n12157,0\\r\\n12158,0\\r\\n12159,0\\r\\n12160,0\\r\\n12161,0\\r\\n12162,0\\r\\n12163,0\\r\\n12164,0\\r\\n12165,0\\r\\n12166,0\\r\\n12167,0\\r\\n12168,0\\r\\n12169,0\\r\\n12170,0\\r\\n12171,0\\r\\n12172,0\\r\\n12173,0\\r\\n12174,0\\r\\n12175,0\\r\\n12176,0\\r\\n12177,0\\r\\n12178,0\\r\\n12179,0\\r\\n12180,0\\r\\n12181,0\\r\\n12182,0\\r\\n12183,0\\r\\n12184,0\\r\\n12185,0\\r\\n12186,0\\r\\n12187,0\\r\\n12188,0\\r\\n12189,0\\r\\n12190,0\\r\\n12191,0\\r\\n12192,0\\r\\n12193,0\\r\\n12194,0\\r\\n12195,0\\r\\n12196,0\\r\\n12197,0\\r\\n12198,0\\r\\n12199,0\\r\\n12200,0\\r\\n12201,0\\r\\n12202,0\\r\\n12203,0\\r\\n12204,0\\r\\n12205,0\\r\\n12206,0\\r\\n12207,0\\r\\n12208,0\\r\\n12209,0\\r\\n12210,0\\r\\n12211,0\\r\\n12212,0\\r\\n12213,0\\r\\n12214,0\\r\\n12215,0\\r\\n12216,0\\r\\n12217,0\\r\\n12218,0\\r\\n12219,0\\r\\n12220,0\\r\\n12221,0\\r\\n12222,0\\r\\n12223,0\\r\\n12224,0\\r\\n12225,0\\r\\n12226,0\\r\\n12227,0\\r\\n12228,0\\r\\n12229,0\\r\\n12230,0\\r\\n12231,0\\r\\n12232,0\\r\\n12233,0\\r\\n12234,0\\r\\n12235,0\\r\\n12236,0\\r\\n12237,0\\r\\n12238,0\\r\\n12239,0\\r\\n12240,0\\r\\n12241,0\\r\\n12242,0\\r\\n12243,0\\r\\n12244,0\\r\\n12245,0\\r\\n12246,0\\r\\n12247,0\\r\\n12248,0\\r\\n12249,0\\r\\n12250,0\\r\\n12251,0\\r\\n12252,0\\r\\n12253,0\\r\\n12254,0\\r\\n12255,0\\r\\n12256,0\\r\\n12257,0\\r\\n12258,0\\r\\n12259,0\\r\\n12260,0\\r\\n12261,0\\r\\n12262,0\\r\\n12263,0\\r\\n12264,0\\r\\n12265,0\\r\\n12266,0\\r\\n12267,0\\r\\n12268,0\\r\\n12269,0\\r\\n12270,0\\r\\n12271,0\\r\\n12272,0\\r\\n12273,0\\r\\n12274,0\\r\\n12275,0\\r\\n12276,0\\r\\n12277,0\\r\\n12278,0\\r\\n12279,0\\r\\n12280,0\\r\\n12281,0\\r\\n12282,0\\r\\n12283,0\\r\\n12284,0\\r\\n12285,0\\r\\n12286,0\\r\\n12287,0\\r\\n12288,0\\r\\n12289,0\\r\\n12290,0\\r\\n12291,0\\r\\n12292,0\\r\\n12293,0\\r\\n12294,0\\r\\n12295,0\\r\\n12296,0\\r\\n12297,0\\r\\n12298,0\\r\\n12299,0\\r\\n12300,0\\r\\n12301,0\\r\\n12302,0\\r\\n12303,0\\r\\n12304,0\\r\\n12305,0\\r\\n12306,0\\r\\n12307,0\\r\\n12308,0\\r\\n12309,0\\r\\n12310,0\\r\\n12311,0\\r\\n12312,0\\r\\n12313,0\\r\\n12314,0\\r\\n12315,0\\r\\n12316,0\\r\\n12317,0\\r\\n12318,0\\r\\n12319,0\\r\\n12320,0\\r\\n12321,0\\r\\n12322,0\\r\\n12323,0\\r\\n12324,0\\r\\n12325,0\\r\\n12326,0\\r\\n12327,0\\r\\n12328,0\\r\\n12329,0\\r\\n12330,0\\r\\n12331,0\\r\\n12332,0\\r\\n12333,0\\r\\n12334,0\\r\\n12335,0\\r\\n12336,0\\r\\n12337,0\\r\\n12338,0\\r\\n12339,0\\r\\n12340,0\\r\\n12341,0\\r\\n12342,0\\r\\n12343,0\\r\\n12344,0\\r\\n12345,0\\r\\n12346,0\\r\\n12347,0\\r\\n12348,0\\r\\n12349,0\\r\\n12350,0\\r\\n12351,0\\r\\n12352,0\\r\\n12353,0\\r\\n12354,0\\r\\n12355,0\\r\\n12356,0\\r\\n12357,0\\r\\n12358,0\\r\\n12359,0\\r\\n12360,0\\r\\n12361,0\\r\\n12362,0\\r\\n12363,0\\r\\n12364,0\\r\\n12365,0\\r\\n12366,0\\r\\n12367,0\\r\\n12368,0\\r\\n12369,0\\r\\n12370,0\\r\\n12371,0\\r\\n12372,0\\r\\n12373,0\\r\\n12374,0\\r\\n12375,0\\r\\n12376,0\\r\\n12377,0\\r\\n12378,0\\r\\n12379,0\\r\\n12380,0\\r\\n12381,0\\r\\n12382,0\\r\\n12383,0\\r\\n12384,0\\r\\n12385,0\\r\\n12386,0\\r\\n12387,0\\r\\n12388,0\\r\\n12389,0\\r\\n12390,0\\r\\n12391,0\\r\\n12392,0\\r\\n12393,0\\r\\n12394,0\\r\\n12395,0\\r\\n12396,0\\r\\n12397,0\\r\\n12398,0\\r\\n12399,0\\r\\n12400,0\\r\\n12401,0\\r\\n12402,0\\r\\n12403,0\\r\\n12404,0\\r\\n12405,0\\r\\n12406,0\\r\\n12407,0\\r\\n12408,0\\r\\n12409,0\\r\\n12410,0\\r\\n12411,0\\r\\n12412,0\\r\\n12413,0\\r\\n12414,0\\r\\n12415,0\\r\\n12416,0\\r\\n12417,0\\r\\n12418,0\\r\\n12419,0\\r\\n12420,0\\r\\n12421,0\\r\\n12422,0\\r\\n12423,0\\r\\n12424,0\\r\\n12425,0\\r\\n12426,0\\r\\n12427,0\\r\\n12428,0\\r\\n12429,0\\r\\n12430,0\\r\\n12431,0\\r\\n12432,0\\r\\n12433,0\\r\\n12434,0\\r\\n12435,0\\r\\n12436,0\\r\\n12437,0\\r\\n12438,0\\r\\n12439,0\\r\\n12440,0\\r\\n12441,0\\r\\n12442,0\\r\\n12443,0\\r\\n12444,0\\r\\n12445,0\\r\\n12446,0\\r\\n12447,0\\r\\n12448,0\\r\\n12449,0\\r\\n12450,0\\r\\n12451,0\\r\\n12452,0\\r\\n12453,0\\r\\n12454,0\\r\\n12455,0\\r\\n12456,0\\r\\n12457,0\\r\\n12458,0\\r\\n12459,0\\r\\n12460,0\\r\\n12461,0\\r\\n12462,0\\r\\n12463,0\\r\\n12464,0\\r\\n12465,0\\r\\n12466,0\\r\\n12467,0\\r\\n12468,0\\r\\n12469,0\\r\\n12470,0\\r\\n12471,0\\r\\n12472,0\\r\\n12473,0\\r\\n12474,0\\r\\n12475,0\\r\\n12476,0\\r\\n12477,0\\r\\n12478,0\\r\\n12479,0\\r\\n12480,0\\r\\n12481,0\\r\\n12482,0\\r\\n12483,0\\r\\n12484,0\\r\\n12485,0\\r\\n12486,0\\r\\n12487,0\\r\\n12488,0\\r\\n12489,0\\r\\n12490,0\\r\\n12491,0\\r\\n12492,0\\r\\n12493,0\\r\\n12494,0\\r\\n12495,0\\r\\n12496,0\\r\\n12497,0\\r\\n12498,0\\r\\n12499,0\\r\\n12500,0\\r\\n12501,0\\r\\n12502,0\\r\\n12503,0\\r\\n12504,0\\r\\n12505,0\\r\\n12506,0\\r\\n12507,0\\r\\n12508,0\\r\\n12509,0\\r\\n12510,0\\r\\n12511,0\\r\\n12512,0\\r\\n12513,0\\r\\n12514,0\\r\\n12515,0\\r\\n12516,0\\r\\n12517,0\\r\\n12518,0\\r\\n12519,0\\r\\n12520,0\\r\\n12521,0\\r\\n12522,0\\r\\n12523,0\\r\\n12524,0\\r\\n12525,0\\r\\n12526,0\\r\\n12527,0\\r\\n12528,0\\r\\n12529,0\\r\\n12530,0\\r\\n12531,0\\r\\n12532,0\\r\\n12533,0\\r\\n12534,0\\r\\n12535,0\\r\\n12536,0\\r\\n12537,0\\r\\n12538,0\\r\\n12539,0\\r\\n12540,0\\r\\n12541,0\\r\\n12542,0\\r\\n12543,0\\r\\n12544,0\\r\\n12545,0\\r\\n12546,0\\r\\n12547,0\\r\\n12548,0\\r\\n12549,0\\r\\n12550,0\\r\\n12551,0\\r\\n12552,0\\r\\n12553,0\\r\\n12554,0\\r\\n12555,0\\r\\n12556,0\\r\\n12557,0\\r\\n12558,0\\r\\n12559,0\\r\\n12560,0\\r\\n12561,0\\r\\n12562,0\\r\\n12563,0\\r\\n12564,0\\r\\n12565,0\\r\\n12566,0\\r\\n12567,0\\r\\n12568,0\\r\\n12569,0\\r\\n12570,0\\r\\n12571,0\\r\\n12572,0\\r\\n12573,0\\r\\n12574,0\\r\\n12575,0\\r\\n12576,0\\r\\n12577,0\\r\\n12578,0\\r\\n12579,0\\r\\n12580,0\\r\\n12581,0\\r\\n12582,0\\r\\n12583,0\\r\\n12584,0\\r\\n12585,0\\r\\n12586,0\\r\\n12587,0\\r\\n12588,0\\r\\n12589,0\\r\\n12590,0\\r\\n12591,0\\r\\n12592,0\\r\\n12593,0\\r\\n12594,0\\r\\n12595,0\\r\\n12596,0\\r\\n12597,0\\r\\n12598,0\\r\\n12599,0\\r\\n12600,0\\r\\n12601,0\\r\\n12602,0\\r\\n12603,0\\r\\n12604,0\\r\\n12605,0\\r\\n12606,0\\r\\n12607,0\\r\\n12608,0\\r\\n12609,0\\r\\n12610,0\\r\\n12611,0\\r\\n12612,0\\r\\n12613,0\\r\\n12614,0\\r\\n12615,0\\r\\n12616,0\\r\\n12617,0\\r\\n12618,0\\r\\n12619,0\\r\\n12620,0\\r\\n12621,0\\r\\n12622,0\\r\\n12623,0\\r\\n12624,0\\r\\n12625,0\\r\\n12626,0\\r\\n12627,0\\r\\n12628,0\\r\\n12629,0\\r\\n12630,0\\r\\n12631,0\\r\\n12632,0\\r\\n12633,0\\r\\n12634,0\\r\\n12635,0\\r\\n12636,0\\r\\n12637,0\\r\\n12638,0\\r\\n12639,0\\r\\n12640,0\\r\\n12641,0\\r\\n12642,0\\r\\n12643,0\\r\\n12644,0\\r\\n12645,0\\r\\n12646,0\\r\\n12647,0\\r\\n12648,0\\r\\n12649,0\\r\\n12650,0\\r\\n12651,0\\r\\n12652,0\\r\\n12653,0\\r\\n12654,0\\r\\n12655,0\\r\\n12656,0\\r\\n12657,0\\r\\n12658,0\\r\\n12659,0\\r\\n12660,0\\r\\n12661,0\\r\\n12662,0\\r\\n12663,0\\r\\n12664,0\\r\\n12665,0\\r\\n12666,0\\r\\n12667,0\\r\\n12668,0\\r\\n12669,0\\r\\n12670,0\\r\\n12671,0\\r\\n12672,0\\r\\n12673,0\\r\\n12674,0\\r\\n12675,0\\r\\n12676,0\\r\\n12677,0\\r\\n12678,0\\r\\n12679,0\\r\\n12680,0\\r\\n12681,0\\r\\n12682,0\\r\\n12683,0\\r\\n12684,0\\r\\n12685,0\\r\\n12686,0\\r\\n12687,0\\r\\n12688,0\\r\\n12689,0\\r\\n12690,0\\r\\n12691,0\\r\\n12692,0\\r\\n12693,0\\r\\n12694,0\\r\\n12695,0\\r\\n12696,0\\r\\n12697,0\\r\\n12698,0\\r\\n12699,0\\r\\n12700,0\\r\\n12701,0\\r\\n12702,0\\r\\n12703,0\\r\\n12704,0\\r\\n12705,0\\r\\n12706,0\\r\\n12707,0\\r\\n12708,0\\r\\n12709,0\\r\\n12710,0\\r\\n12711,0\\r\\n12712,0\\r\\n12713,0\\r\\n12714,0\\r\\n12715,0\\r\\n12716,0\\r\\n12717,0\\r\\n12718,0\\r\\n12719,0\\r\\n12720,0\\r\\n12721,0\\r\\n12722,0\\r\\n12723,0\\r\\n12724,0\\r\\n12725,0\\r\\n12726,0\\r\\n12727,0\\r\\n12728,0\\r\\n12729,0\\r\\n12730,0\\r\\n12731,0\\r\\n12732,0\\r\\n12733,0\\r\\n12734,0\\r\\n12735,0\\r\\n12736,0\\r\\n12737,0\\r\\n12738,0\\r\\n12739,0\\r\\n12740,0\\r\\n12741,0\\r\\n12742,0\\r\\n12743,0\\r\\n12744,0\\r\\n12745,0\\r\\n12746,0\\r\\n12747,0\\r\\n12748,0\\r\\n12749,0\\r\\n12750,0\\r\\n12751,0\\r\\n12752,0\\r\\n12753,0\\r\\n12754,0\\r\\n12755,0\\r\\n12756,0\\r\\n12757,0\\r\\n12758,0\\r\\n12759,0\\r\\n12760,0\\r\\n12761,0\\r\\n12762,0\\r\\n12763,0\\r\\n12764,0\\r\\n12765,0\\r\\n12766,0\\r\\n12767,0\\r\\n12768,0\\r\\n12769,0\\r\\n12770,0\\r\\n12771,0\\r\\n12772,0\\r\\n12773,0\\r\\n12774,0\\r\\n12775,0\\r\\n12776,0\\r\\n12777,0\\r\\n12778,0\\r\\n12779,0\\r\\n12780,0\\r\\n12781,0\\r\\n12782,0\\r\\n12783,0\\r\\n12784,0\\r\\n12785,0\\r\\n12786,0\\r\\n12787,0\\r\\n12788,0\\r\\n12789,0\\r\\n12790,0\\r\\n12791,0\\r\\n12792,0\\r\\n12793,0\\r\\n12794,0\\r\\n12795,0\\r\\n12796,0\\r\\n12797,0\\r\\n12798,0\\r\\n12799,0\\r\\n12800,0\\r\\n12801,0\\r\\n12802,0\\r\\n12803,0\\r\\n12804,0\\r\\n12805,0\\r\\n12806,0\\r\\n12807,0\\r\\n12808,0\\r\\n12809,0\\r\\n12810,0\\r\\n12811,0\\r\\n12812,0\\r\\n12813,0\\r\\n12814,0\\r\\n12815,0\\r\\n12816,0\\r\\n12817,0\\r\\n12818,0\\r\\n12819,0\\r\\n12820,0\\r\\n12821,0\\r\\n12822,0\\r\\n12823,0\\r\\n12824,0\\r\\n12825,0\\r\\n12826,0\\r\\n12827,0\\r\\n12828,0\\r\\n12829,0\\r\\n12830,0\\r\\n12831,0\\r\\n12832,0\\r\\n12833,0\\r\\n12834,0\\r\\n12835,0\\r\\n12836,0\\r\\n12837,0\\r\\n12838,0\\r\\n12839,0\\r\\n12840,0\\r\\n12841,0\\r\\n12842,0\\r\\n12843,0\\r\\n12844,0\\r\\n12845,0\\r\\n12846,0\\r\\n12847,0\\r\\n12848,0\\r\\n12849,0\\r\\n12850,0\\r\\n12851,0\\r\\n12852,0\\r\\n12853,0\\r\\n12854,0\\r\\n12855,0\\r\\n12856,0\\r\\n12857,0\\r\\n12858,0\\r\\n12859,0\\r\\n12860,0\\r\\n12861,0\\r\\n12862,0\\r\\n12863,0\\r\\n12864,0\\r\\n12865,0\\r\\n12866,0\\r\\n12867,0\\r\\n12868,0\\r\\n12869,0\\r\\n12870,0\\r\\n12871,0\\r\\n12872,0\\r\\n12873,0\\r\\n12874,0\\r\\n12875,0\\r\\n12876,0\\r\\n12877,0\\r\\n12878,0\\r\\n12879,0\\r\\n12880,0\\r\\n12881,0\\r\\n12882,0\\r\\n12883,0\\r\\n12884,0\\r\\n12885,0\\r\\n12886,0\\r\\n12887,0\\r\\n12888,0\\r\\n12889,0\\r\\n12890,0\\r\\n12891,0\\r\\n12892,0\\r\\n12893,0\\r\\n12894,0\\r\\n12895,0\\r\\n12896,0\\r\\n12897,0\\r\\n12898,0\\r\\n12899,0\\r\\n12900,0\\r\\n12901,0\\r\\n12902,0\\r\\n12903,0\\r\\n12904,0\\r\\n12905,0\\r\\n12906,0\\r\\n12907,0\\r\\n12908,0\\r\\n12909,0\\r\\n12910,0\\r\\n12911,0\\r\\n12912,0\\r\\n12913,0\\r\\n12914,0\\r\\n12915,0\\r\\n12916,0\\r\\n12917,0\\r\\n12918,0\\r\\n12919,0\\r\\n12920,0\\r\\n12921,0\\r\\n12922,0\\r\\n12923,0\\r\\n12924,0\\r\\n12925,0\\r\\n12926,0\\r\\n12927,0\\r\\n12928,0\\r\\n12929,0\\r\\n12930,0\\r\\n12931,0\\r\\n12932,0\\r\\n12933,0\\r\\n12934,0\\r\\n12935,0\\r\\n12936,0\\r\\n12937,0\\r\\n12938,0\\r\\n12939,0\\r\\n12940,0\\r\\n12941,0\\r\\n12942,0\\r\\n12943,0\\r\\n12944,0\\r\\n12945,0\\r\\n12946,0\\r\\n12947,0\\r\\n12948,0\\r\\n12949,0\\r\\n12950,0\\r\\n12951,0\\r\\n12952,0\\r\\n12953,0\\r\\n12954,0\\r\\n12955,0\\r\\n12956,0\\r\\n12957,0\\r\\n12958,0\\r\\n12959,0\\r\\n12960,0\\r\\n12961,0\\r\\n12962,0\\r\\n12963,0\\r\\n12964,0\\r\\n12965,0\\r\\n12966,0\\r\\n12967,0\\r\\n12968,0\\r\\n12969,0\\r\\n12970,0\\r\\n12971,0\\r\\n12972,0\\r\\n12973,0\\r\\n12974,0\\r\\n12975,0\\r\\n12976,0\\r\\n12977,0\\r\\n12978,0\\r\\n12979,0\\r\\n12980,0\\r\\n12981,0\\r\\n12982,0\\r\\n12983,0\\r\\n12984,0\\r\\n12985,0\\r\\n12986,0\\r\\n12987,0\\r\\n12988,0\\r\\n12989,0\\r\\n12990,0\\r\\n12991,0\\r\\n12992,0\\r\\n12993,0\\r\\n12994,0\\r\\n12995,0\\r\\n12996,0\\r\\n12997,0\\r\\n12998,0\\r\\n12999,0\\r\\n13000,0\\r\\n13001,0\\r\\n13002,0\\r\\n13003,0\\r\\n13004,0\\r\\n13005,0\\r\\n13006,0\\r\\n13007,0\\r\\n13008,0\\r\\n13009,0\\r\\n13010,0\\r\\n13011,0\\r\\n13012,0\\r\\n13013,0\\r\\n13014,0\\r\\n13015,0\\r\\n13016,0\\r\\n13017,0\\r\\n13018,0\\r\\n13019,0\\r\\n13020,0\\r\\n13021,0\\r\\n13022,0\\r\\n13023,0\\r\\n13024,0\\r\\n13025,0\\r\\n13026,0\\r\\n13027,0\\r\\n13028,0\\r\\n13029,0\\r\\n13030,0\\r\\n13031,0\\r\\n13032,0\\r\\n13033,0\\r\\n13034,0\\r\\n13035,0\\r\\n13036,0\\r\\n13037,0\\r\\n13038,0\\r\\n13039,0\\r\\n13040,0\\r\\n13041,0\\r\\n13042,0\\r\\n13043,0\\r\\n13044,0\\r\\n13045,0\\r\\n13046,0\\r\\n13047,0\\r\\n13048,0\\r\\n13049,0\\r\\n13050,0\\r\\n13051,0\\r\\n13052,0\\r\\n13053,0\\r\\n13054,0\\r\\n13055,0\\r\\n13056,0\\r\\n13057,0\\r\\n13058,0\\r\\n13059,0\\r\\n13060,0\\r\\n13061,0\\r\\n13062,0\\r\\n13063,0\\r\\n13064,0\\r\\n13065,0\\r\\n13066,0\\r\\n13067,0\\r\\n13068,0\\r\\n13069,0\\r\\n13070,0\\r\\n13071,0\\r\\n13072,0\\r\\n13073,0\\r\\n13074,0\\r\\n13075,0\\r\\n13076,0\\r\\n13077,0\\r\\n13078,0\\r\\n13079,0\\r\\n13080,0\\r\\n13081,0\\r\\n13082,0\\r\\n13083,0\\r\\n13084,0\\r\\n13085,0\\r\\n13086,0\\r\\n13087,0\\r\\n13088,0\\r\\n13089,0\\r\\n13090,0\\r\\n13091,0\\r\\n13092,0\\r\\n13093,0\\r\\n13094,0\\r\\n13095,0\\r\\n13096,0\\r\\n13097,0\\r\\n13098,0\\r\\n13099,0\\r\\n13100,0\\r\\n13101,0\\r\\n13102,0\\r\\n13103,0\\r\\n13104,0\\r\\n13105,0\\r\\n13106,0\\r\\n13107,0\\r\\n13108,0\\r\\n13109,0\\r\\n13110,0\\r\\n13111,0\\r\\n13112,0\\r\\n13113,0\\r\\n13114,0\\r\\n13115,0\\r\\n13116,0\\r\\n13117,0\\r\\n13118,0\\r\\n13119,0\\r\\n13120,0\\r\\n13121,0\\r\\n13122,0\\r\\n13123,0\\r\\n13124,0\\r\\n13125,0\\r\\n13126,0\\r\\n13127,0\\r\\n13128,0\\r\\n13129,0\\r\\n13130,0\\r\\n13131,0\\r\\n13132,0\\r\\n13133,0\\r\\n13134,0\\r\\n13135,0\\r\\n13136,0\\r\\n13137,0\\r\\n13138,0\\r\\n13139,0\\r\\n13140,0\\r\\n13141,0\\r\\n13142,0\\r\\n13143,0\\r\\n13144,0\\r\\n13145,0\\r\\n13146,0\\r\\n13147,0\\r\\n13148,0\\r\\n13149,0\\r\\n13150,0\\r\\n13151,0\\r\\n13152,0\\r\\n13153,0\\r\\n13154,0\\r\\n13155,0\\r\\n13156,0\\r\\n13157,0\\r\\n13158,0\\r\\n13159,0\\r\\n13160,0\\r\\n13161,0\\r\\n13162,0\\r\\n13163,0\\r\\n13164,0\\r\\n13165,0\\r\\n13166,0\\r\\n13167,0\\r\\n13168,0\\r\\n13169,0\\r\\n13170,0\\r\\n13171,0\\r\\n13172,0\\r\\n13173,0\\r\\n13174,0\\r\\n13175,0\\r\\n13176,0\\r\\n13177,0\\r\\n13178,0\\r\\n13179,0\\r\\n13180,0\\r\\n13181,0\\r\\n13182,0\\r\\n13183,0\\r\\n13184,0\\r\\n13185,0\\r\\n13186,0\\r\\n13187,0\\r\\n13188,0\\r\\n13189,0\\r\\n13190,0\\r\\n13191,0\\r\\n13192,0\\r\\n13193,0\\r\\n13194,0\\r\\n13195,0\\r\\n13196,0\\r\\n13197,0\\r\\n13198,0\\r\\n13199,0\\r\\n13200,0\\r\\n13201,0\\r\\n13202,0\\r\\n13203,0\\r\\n13204,0\\r\\n13205,0\\r\\n13206,0\\r\\n13207,0\\r\\n13208,0\\r\\n13209,0\\r\\n13210,0\\r\\n13211,0\\r\\n13212,0\\r\\n13213,0\\r\\n13214,0\\r\\n13215,0\\r\\n13216,0\\r\\n13217,0\\r\\n13218,0\\r\\n13219,0\\r\\n13220,0\\r\\n13221,0\\r\\n13222,0\\r\\n13223,0\\r\\n13224,0\\r\\n13225,0\\r\\n13226,0\\r\\n13227,0\\r\\n13228,0\\r\\n13229,0\\r\\n13230,0\\r\\n13231,0\\r\\n13232,0\\r\\n13233,0\\r\\n13234,0\\r\\n13235,0\\r\\n13236,0\\r\\n13237,0\\r\\n13238,0\\r\\n13239,0\\r\\n13240,0\\r\\n13241,0\\r\\n13242,0\\r\\n13243,0\\r\\n13244,0\\r\\n13245,0\\r\\n13246,0\\r\\n13247,0\\r\\n13248,0\\r\\n13249,0\\r\\n13250,0\\r\\n13251,0\\r\\n13252,0\\r\\n13253,0\\r\\n13254,0\\r\\n13255,0\\r\\n13256,0\\r\\n13257,0\\r\\n13258,0\\r\\n13259,0\\r\\n13260,0\\r\\n13261,0\\r\\n13262,0\\r\\n13263,0\\r\\n13264,0\\r\\n13265,0\\r\\n13266,0\\r\\n13267,0\\r\\n13268,0\\r\\n13269,0\\r\\n13270,0\\r\\n13271,0\\r\\n13272,0\\r\\n13273,0\\r\\n13274,0\\r\\n13275,0\\r\\n13276,0\\r\\n13277,0\\r\\n13278,0\\r\\n13279,0\\r\\n13280,0\\r\\n13281,0\\r\\n13282,0\\r\\n13283,0\\r\\n13284,0\\r\\n13285,0\\r\\n13286,0\\r\\n13287,0\\r\\n13288,0\\r\\n13289,0\\r\\n13290,0\\r\\n13291,0\\r\\n13292,0\\r\\n13293,0\\r\\n13294,0\\r\\n13295,0\\r\\n13296,0\\r\\n13297,0\\r\\n13298,0\\r\\n13299,0\\r\\n13300,0\\r\\n13301,0\\r\\n13302,0\\r\\n13303,0\\r\\n13304,0\\r\\n13305,0\\r\\n13306,0\\r\\n13307,0\\r\\n13308,0\\r\\n13309,0\\r\\n13310,0\\r\\n13311,0\\r\\n13312,0\\r\\n13313,0\\r\\n13314,0\\r\\n13315,0\\r\\n13316,0\\r\\n13317,0\\r\\n13318,0\\r\\n13319,0\\r\\n13320,0\\r\\n13321,0\\r\\n13322,0\\r\\n13323,0\\r\\n13324,0\\r\\n13325,0\\r\\n13326,0\\r\\n13327,0\\r\\n13328,0\\r\\n13329,0\\r\\n13330,0\\r\\n13331,0\\r\\n13332,0\\r\\n13333,0\\r\\n13334,0\\r\\n13335,0\\r\\n13336,0\\r\\n13337,0\\r\\n13338,0\\r\\n13339,0\\r\\n13340,0\\r\\n13341,0\\r\\n13342,0\\r\\n13343,0\\r\\n13344,0\\r\\n13345,0\\r\\n13346,0\\r\\n13347,0\\r\\n13348,0\\r\\n13349,0\\r\\n13350,0\\r\\n13351,0\\r\\n13352,0\\r\\n13353,0\\r\\n13354,0\\r\\n13355,0\\r\\n13356,0\\r\\n13357,0\\r\\n13358,0\\r\\n13359,0\\r\\n13360,0\\r\\n13361,0\\r\\n13362,0\\r\\n13363,0\\r\\n13364,0\\r\\n13365,0\\r\\n13366,0\\r\\n13367,0\\r\\n13368,0\\r\\n13369,0\\r\\n13370,0\\r\\n13371,0\\r\\n13372,0\\r\\n13373,0\\r\\n13374,0\\r\\n13375,0\\r\\n13376,0\\r\\n13377,0\\r\\n13378,0\\r\\n13379,0\\r\\n13380,0\\r\\n13381,0\\r\\n13382,0\\r\\n13383,0\\r\\n13384,0\\r\\n13385,0\\r\\n13386,0\\r\\n13387,0\\r\\n13388,0\\r\\n13389,0\\r\\n13390,0\\r\\n13391,0\\r\\n13392,0\\r\\n13393,0\\r\\n13394,0\\r\\n13395,0\\r\\n13396,0\\r\\n13397,0\\r\\n13398,0\\r\\n13399,0\\r\\n13400,0\\r\\n13401,0\\r\\n13402,0\\r\\n13403,0\\r\\n13404,0\\r\\n13405,0\\r\\n13406,0\\r\\n13407,0\\r\\n13408,0\\r\\n13409,0\\r\\n13410,0\\r\\n13411,0\\r\\n13412,0\\r\\n13413,0\\r\\n13414,0\\r\\n13415,0\\r\\n13416,0\\r\\n13417,0\\r\\n13418,0\\r\\n13419,0\\r\\n13420,0\\r\\n13421,0\\r\\n13422,0\\r\\n13423,0\\r\\n13424,0\\r\\n13425,0\\r\\n13426,0\\r\\n13427,0\\r\\n13428,0\\r\\n13429,0\\r\\n13430,0\\r\\n13431,0\\r\\n13432,0\\r\\n13433,0\\r\\n13434,0\\r\\n13435,0\\r\\n13436,0\\r\\n13437,0\\r\\n13438,0\\r\\n13439,0\\r\\n13440,0\\r\\n13441,0\\r\\n13442,0\\r\\n13443,0\\r\\n13444,0\\r\\n13445,0\\r\\n13446,0\\r\\n13447,0\\r\\n13448,0\\r\\n13449,0\\r\\n13450,0\\r\\n13451,0\\r\\n13452,0\\r\\n13453,0\\r\\n13454,0\\r\\n13455,0\\r\\n13456,0\\r\\n13457,0\\r\\n13458,0\\r\\n13459,0\\r\\n13460,0\\r\\n13461,0\\r\\n13462,0\\r\\n13463,0\\r\\n13464,0\\r\\n13465,0\\r\\n13466,0\\r\\n13467,0\\r\\n13468,0\\r\\n13469,0\\r\\n13470,0\\r\\n13471,0\\r\\n13472,0\\r\\n13473,0\\r\\n13474,0\\r\\n13475,0\\r\\n13476,0\\r\\n13477,0\\r\\n13478,0\\r\\n13479,0\\r\\n13480,0\\r\\n13481,0\\r\\n13482,0\\r\\n13483,0\\r\\n13484,0\\r\\n13485,0\\r\\n13486,0\\r\\n13487,0\\r\\n13488,0\\r\\n13489,0\\r\\n13490,0\\r\\n13491,0\\r\\n13492,0\\r\\n13493,0\\r\\n13494,0\\r\\n13495,0\\r\\n13496,0\\r\\n13497,0\\r\\n13498,0\\r\\n13499,0\\r\\n13500,0\\r\\n13501,0\\r\\n13502,0\\r\\n13503,0\\r\\n13504,0\\r\\n13505,0\\r\\n13506,0\\r\\n13507,0\\r\\n13508,0\\r\\n13509,0\\r\\n13510,0\\r\\n13511,0\\r\\n13512,0\\r\\n13513,0\\r\\n13514,0\\r\\n13515,0\\r\\n13516,0\\r\\n13517,0\\r\\n13518,0\\r\\n13519,0\\r\\n13520,0\\r\\n13521,0\\r\\n13522,0\\r\\n13523,0\\r\\n13524,0\\r\\n13525,0\\r\\n13526,0\\r\\n13527,0\\r\\n13528,0\\r\\n13529,0\\r\\n13530,0\\r\\n13531,0\\r\\n13532,0\\r\\n13533,0\\r\\n13534,0\\r\\n13535,0\\r\\n13536,0\\r\\n13537,0\\r\\n13538,0\\r\\n13539,0\\r\\n13540,0\\r\\n13541,0\\r\\n13542,0\\r\\n13543,0\\r\\n13544,0\\r\\n13545,0\\r\\n13546,0\\r\\n13547,0\\r\\n13548,0\\r\\n13549,0\\r\\n13550,0\\r\\n13551,0\\r\\n13552,0\\r\\n13553,0\\r\\n13554,0\\r\\n13555,0\\r\\n13556,0\\r\\n13557,0\\r\\n13558,0\\r\\n13559,0\\r\\n13560,0\\r\\n13561,0\\r\\n13562,0\\r\\n13563,0\\r\\n13564,0\\r\\n13565,0\\r\\n13566,0\\r\\n13567,0\\r\\n13568,0\\r\\n13569,0\\r\\n13570,0\\r\\n13571,0\\r\\n13572,0\\r\\n13573,0\\r\\n13574,0\\r\\n13575,0\\r\\n13576,0\\r\\n13577,0\\r\\n13578,0\\r\\n13579,0\\r\\n13580,0\\r\\n13581,0\\r\\n13582,0\\r\\n13583,0\\r\\n13584,0\\r\\n13585,0\\r\\n13586,0\\r\\n13587,0\\r\\n13588,0\\r\\n13589,0\\r\\n13590,0\\r\\n13591,0\\r\\n13592,0\\r\\n13593,0\\r\\n13594,0\\r\\n13595,0\\r\\n13596,0\\r\\n13597,0\\r\\n13598,0\\r\\n13599,0\\r\\n13600,0\\r\\n13601,0\\r\\n13602,0\\r\\n13603,0\\r\\n13604,0\\r\\n13605,0\\r\\n13606,0\\r\\n13607,0\\r\\n13608,0\\r\\n13609,0\\r\\n13610,0\\r\\n13611,0\\r\\n13612,0\\r\\n13613,0\\r\\n13614,0\\r\\n13615,0\\r\\n13616,0\\r\\n13617,0\\r\\n13618,0\\r\\n13619,0\\r\\n13620,0\\r\\n13621,0\\r\\n13622,0\\r\\n13623,0\\r\\n13624,0\\r\\n13625,0\\r\\n13626,0\\r\\n13627,0\\r\\n13628,0\\r\\n13629,0\\r\\n13630,0\\r\\n13631,0\\r\\n13632,0\\r\\n13633,0\\r\\n13634,0\\r\\n13635,0\\r\\n13636,0\\r\\n13637,0\\r\\n13638,0\\r\\n13639,0\\r\\n13640,0\\r\\n13641,0\\r\\n13642,0\\r\\n13643,0\\r\\n13644,0\\r\\n13645,0\\r\\n13646,0\\r\\n13647,0\\r\\n13648,0\\r\\n13649,0\\r\\n13650,0\\r\\n13651,0\\r\\n13652,0\\r\\n13653,0\\r\\n13654,0\\r\\n13655,0\\r\\n13656,0\\r\\n13657,0\\r\\n13658,0\\r\\n13659,0\\r\\n13660,0\\r\\n13661,0\\r\\n13662,0\\r\\n13663,0\\r\\n13664,0\\r\\n13665,0\\r\\n13666,0\\r\\n13667,0\\r\\n13668,0\\r\\n13669,0\\r\\n13670,0\\r\\n13671,0\\r\\n13672,0\\r\\n13673,0\\r\\n13674,0\\r\\n13675,0\\r\\n13676,0\\r\\n13677,0\\r\\n13678,0\\r\\n13679,0\\r\\n13680,0\\r\\n13681,0\\r\\n13682,0\\r\\n13683,0\\r\\n13684,0\\r\\n13685,0\\r\\n13686,0\\r\\n13687,0\\r\\n13688,0\\r\\n13689,0\\r\\n13690,0\\r\\n13691,0\\r\\n13692,0\\r\\n13693,0\\r\\n13694,0\\r\\n13695,0\\r\\n13696,0\\r\\n13697,0\\r\\n13698,0\\r\\n13699,0\\r\\n13700,0\\r\\n13701,0\\r\\n13702,0\\r\\n13703,0\\r\\n13704,0\\r\\n13705,0\\r\\n13706,0\\r\\n13707,0\\r\\n13708,0\\r\\n13709,0\\r\\n13710,0\\r\\n13711,0\\r\\n13712,0\\r\\n13713,0\\r\\n13714,0\\r\\n13715,0\\r\\n13716,0\\r\\n13717,0\\r\\n13718,0\\r\\n13719,0\\r\\n13720,0\\r\\n13721,0\\r\\n13722,0\\r\\n13723,0\\r\\n13724,0\\r\\n13725,0\\r\\n13726,0\\r\\n13727,0\\r\\n13728,0\\r\\n13729,0\\r\\n13730,0\\r\\n13731,0\\r\\n13732,0\\r\\n13733,0\\r\\n13734,0\\r\\n13735,0\\r\\n13736,0\\r\\n13737,0\\r\\n13738,0\\r\\n13739,0\\r\\n13740,0\\r\\n13741,0\\r\\n13742,0\\r\\n13743,0\\r\\n13744,0\\r\\n13745,0\\r\\n13746,0\\r\\n13747,0\\r\\n13748,0\\r\\n13749,0\\r\\n13750,0\\r\\n13751,0\\r\\n13752,0\\r\\n13753,0\\r\\n13754,0\\r\\n13755,0\\r\\n13756,0\\r\\n13757,0\\r\\n13758,0\\r\\n13759,0\\r\\n13760,0\\r\\n13761,0\\r\\n13762,0\\r\\n13763,0\\r\\n13764,0\\r\\n13765,0\\r\\n13766,0\\r\\n13767,0\\r\\n13768,0\\r\\n13769,0\\r\\n13770,0\\r\\n13771,0\\r\\n13772,0\\r\\n13773,0\\r\\n13774,0\\r\\n13775,0\\r\\n13776,0\\r\\n13777,0\\r\\n13778,0\\r\\n13779,0\\r\\n13780,0\\r\\n13781,0\\r\\n13782,0\\r\\n13783,0\\r\\n13784,0\\r\\n13785,0\\r\\n13786,0\\r\\n13787,0\\r\\n13788,0\\r\\n13789,0\\r\\n13790,0\\r\\n13791,0\\r\\n13792,0\\r\\n13793,0\\r\\n13794,0\\r\\n13795,0\\r\\n13796,0\\r\\n13797,0\\r\\n13798,0\\r\\n13799,0\\r\\n13800,0\\r\\n13801,0\\r\\n13802,0\\r\\n13803,0\\r\\n13804,0\\r\\n13805,0\\r\\n13806,0\\r\\n13807,0\\r\\n13808,0\\r\\n13809,0\\r\\n13810,0\\r\\n13811,0\\r\\n13812,0\\r\\n13813,0\\r\\n13814,0\\r\\n13815,0\\r\\n13816,0\\r\\n13817,0\\r\\n13818,0\\r\\n13819,0\\r\\n13820,0\\r\\n13821,0\\r\\n13822,0\\r\\n13823,0\\r\\n13824,0\\r\\n13825,0\\r\\n13826,0\\r\\n13827,0\\r\\n13828,0\\r\\n13829,0\\r\\n13830,0\\r\\n13831,0\\r\\n13832,0\\r\\n13833,0\\r\\n13834,0\\r\\n13835,0\\r\\n13836,0\\r\\n13837,0\\r\\n13838,0\\r\\n13839,0\\r\\n13840,0\\r\\n13841,0\\r\\n13842,0\\r\\n13843,0\\r\\n13844,0\\r\\n13845,0\\r\\n13846,0\\r\\n13847,0\\r\\n13848,0\\r\\n13849,0\\r\\n13850,0\\r\\n13851,0\\r\\n13852,0\\r\\n13853,0\\r\\n13854,0\\r\\n13855,0\\r\\n13856,0\\r\\n13857,0\\r\\n13858,0\\r\\n13859,0\\r\\n13860,0\\r\\n13861,0\\r\\n13862,0\\r\\n13863,0\\r\\n13864,0\\r\\n13865,0\\r\\n13866,0\\r\\n13867,0\\r\\n13868,0\\r\\n13869,0\\r\\n13870,0\\r\\n13871,0\\r\\n13872,0\\r\\n13873,0\\r\\n13874,0\\r\\n13875,0\\r\\n13876,0\\r\\n13877,0\\r\\n13878,0\\r\\n13879,0\\r\\n13880,0\\r\\n13881,0\\r\\n13882,0\\r\\n13883,0\\r\\n13884,0\\r\\n13885,0\\r\\n13886,0\\r\\n13887,0\\r\\n13888,0\\r\\n13889,0\\r\\n13890,0\\r\\n13891,0\\r\\n13892,0\\r\\n13893,0\\r\\n13894,0\\r\\n13895,0\\r\\n13896,0\\r\\n13897,0\\r\\n13898,0\\r\\n13899,0\\r\\n13900,0\\r\\n13901,0\\r\\n13902,0\\r\\n13903,0\\r\\n13904,0\\r\\n13905,0\\r\\n13906,0\\r\\n13907,0\\r\\n13908,0\\r\\n13909,0\\r\\n13910,0\\r\\n13911,0\\r\\n13912,0\\r\\n13913,0\\r\\n13914,0\\r\\n13915,0\\r\\n13916,0\\r\\n13917,0\\r\\n13918,0\\r\\n13919,0\\r\\n13920,0\\r\\n13921,0\\r\\n13922,0\\r\\n13923,0\\r\\n13924,0\\r\\n13925,0\\r\\n13926,0\\r\\n13927,0\\r\\n13928,0\\r\\n13929,0\\r\\n13930,0\\r\\n13931,0\\r\\n13932,0\\r\\n13933,0\\r\\n13934,0\\r\\n13935,0\\r\\n13936,0\\r\\n13937,0\\r\\n13938,0\\r\\n13939,0\\r\\n13940,0\\r\\n13941,0\\r\\n13942,0\\r\\n13943,0\\r\\n13944,0\\r\\n13945,0\\r\\n13946,0\\r\\n13947,0\\r\\n13948,0\\r\\n13949,0\\r\\n13950,0\\r\\n13951,0\\r\\n13952,0\\r\\n13953,0\\r\\n13954,0\\r\\n13955,0\\r\\n13956,0\\r\\n13957,0\\r\\n13958,0\\r\\n13959,0\\r\\n13960,0\\r\\n13961,0\\r\\n13962,0\\r\\n13963,0\\r\\n13964,0\\r\\n13965,0\\r\\n13966,0\\r\\n13967,0\\r\\n13968,0\\r\\n13969,0\\r\\n13970,0\\r\\n13971,0\\r\\n13972,0\\r\\n13973,0\\r\\n13974,0\\r\\n13975,0\\r\\n13976,0\\r\\n13977,0\\r\\n13978,0\\r\\n13979,0\\r\\n13980,0\\r\\n13981,0\\r\\n13982,0\\r\\n13983,0\\r\\n13984,0\\r\\n13985,0\\r\\n13986,0\\r\\n13987,0\\r\\n13988,0\\r\\n13989,0\\r\\n13990,0\\r\\n13991,0\\r\\n13992,0\\r\\n13993,0\\r\\n13994,0\\r\\n13995,0\\r\\n13996,0\\r\\n13997,0\\r\\n13998,0\\r\\n13999,0\\r\\n14000,0\\r\\n14001,0\\r\\n14002,0\\r\\n14003,0\\r\\n14004,0\\r\\n14005,0\\r\\n14006,0\\r\\n14007,0\\r\\n14008,0\\r\\n14009,0\\r\\n14010,0\\r\\n14011,0\\r\\n14012,0\\r\\n14013,0\\r\\n14014,0\\r\\n14015,0\\r\\n14016,0\\r\\n14017,0\\r\\n14018,0\\r\\n14019,0\\r\\n14020,0\\r\\n14021,0\\r\\n14022,0\\r\\n14023,0\\r\\n14024,0\\r\\n14025,0\\r\\n14026,0\\r\\n14027,0\\r\\n14028,0\\r\\n14029,0\\r\\n14030,0\\r\\n14031,0\\r\\n14032,0\\r\\n14033,0\\r\\n14034,0\\r\\n14035,0\\r\\n14036,0\\r\\n14037,0\\r\\n14038,0\\r\\n14039,0\\r\\n14040,0\\r\\n14041,0\\r\\n14042,0\\r\\n14043,0\\r\\n14044,0\\r\\n14045,0\\r\\n14046,0\\r\\n14047,0\\r\\n14048,0\\r\\n14049,0\\r\\n14050,0\\r\\n14051,0\\r\\n14052,0\\r\\n14053,0\\r\\n14054,0\\r\\n14055,0\\r\\n14056,0\\r\\n14057,0\\r\\n14058,0\\r\\n14059,0\\r\\n14060,0\\r\\n14061,0\\r\\n14062,0\\r\\n14063,0\\r\\n14064,0\\r\\n14065,0\\r\\n14066,0\\r\\n14067,0\\r\\n14068,0\\r\\n14069,0\\r\\n14070,0\\r\\n14071,0\\r\\n14072,0\\r\\n14073,0\\r\\n14074,0\\r\\n14075,0\\r\\n14076,0\\r\\n14077,0\\r\\n14078,0\\r\\n14079,0\\r\\n14080,0\\r\\n14081,0\\r\\n14082,0\\r\\n14083,0\\r\\n14084,0\\r\\n14085,0\\r\\n14086,0\\r\\n14087,0\\r\\n14088,0\\r\\n14089,0\\r\\n14090,0\\r\\n14091,0\\r\\n14092,0\\r\\n14093,0\\r\\n14094,0\\r\\n14095,0\\r\\n14096,0\\r\\n14097,0\\r\\n14098,0\\r\\n14099,0\\r\\n14100,0\\r\\n14101,0\\r\\n14102,0\\r\\n14103,0\\r\\n14104,0\\r\\n14105,0\\r\\n14106,0\\r\\n14107,0\\r\\n14108,0\\r\\n14109,0\\r\\n14110,0\\r\\n14111,0\\r\\n14112,0\\r\\n14113,0\\r\\n14114,0\\r\\n14115,0\\r\\n14116,0\\r\\n14117,0\\r\\n14118,0\\r\\n14119,0\\r\\n14120,0\\r\\n14121,0\\r\\n14122,0\\r\\n14123,0\\r\\n14124,0\\r\\n14125,0\\r\\n14126,0\\r\\n14127,0\\r\\n14128,0\\r\\n14129,0\\r\\n14130,0\\r\\n14131,0\\r\\n14132,0\\r\\n14133,0\\r\\n14134,0\\r\\n14135,0\\r\\n14136,0\\r\\n14137,0\\r\\n14138,0\\r\\n14139,0\\r\\n14140,0\\r\\n14141,0\\r\\n14142,0\\r\\n14143,0\\r\\n14144,0\\r\\n14145,0\\r\\n14146,0\\r\\n14147,0\\r\\n14148,0\\r\\n14149,0\\r\\n14150,0\\r\\n14151,0\\r\\n14152,0\\r\\n14153,0\\r\\n14154,0\\r\\n14155,0\\r\\n14156,0\\r\\n14157,0\\r\\n14158,0\\r\\n14159,0\\r\\n14160,0\\r\\n14161,0\\r\\n14162,0\\r\\n14163,0\\r\\n14164,0\\r\\n14165,0\\r\\n14166,0\\r\\n14167,0\\r\\n14168,0\\r\\n14169,0\\r\\n14170,0\\r\\n14171,0\\r\\n14172,0\\r\\n14173,0\\r\\n14174,0\\r\\n14175,0\\r\\n14176,0\\r\\n14177,0\\r\\n14178,0\\r\\n14179,0\\r\\n14180,0\\r\\n14181,0\\r\\n14182,0\\r\\n14183,0\\r\\n14184,0\\r\\n14185,0\\r\\n14186,0\\r\\n14187,0\\r\\n14188,0\\r\\n14189,0\\r\\n14190,0\\r\\n14191,0\\r\\n14192,0\\r\\n14193,0\\r\\n14194,0\\r\\n14195,0\\r\\n14196,0\\r\\n14197,0\\r\\n14198,0\\r\\n14199,0\\r\\n14200,0\\r\\n14201,0\\r\\n14202,0\\r\\n14203,0\\r\\n14204,0\\r\\n14205,0\\r\\n14206,0\\r\\n14207,0\\r\\n14208,0\\r\\n14209,0\\r\\n14210,0\\r\\n14211,0\\r\\n14212,0\\r\\n14213,0\\r\\n14214,0\\r\\n14215,0\\r\\n14216,0\\r\\n14217,0\\r\\n14218,0\\r\\n14219,0\\r\\n14220,0\\r\\n14221,0\\r\\n14222,0\\r\\n14223,0\\r\\n14224,0\\r\\n14225,0\\r\\n14226,0\\r\\n14227,0\\r\\n14228,0\\r\\n14229,0\\r\\n14230,0\\r\\n14231,0\\r\\n14232,0\\r\\n14233,0\\r\\n14234,0\\r\\n14235,0\\r\\n14236,0\\r\\n14237,0\\r\\n14238,0\\r\\n14239,0\\r\\n14240,0\\r\\n14241,0\\r\\n14242,0\\r\\n14243,0\\r\\n14244,0\\r\\n14245,0\\r\\n14246,0\\r\\n14247,0\\r\\n14248,0\\r\\n14249,0\\r\\n14250,0\\r\\n14251,0\\r\\n14252,0\\r\\n14253,0\\r\\n14254,0\\r\\n14255,0\\r\\n14256,0\\r\\n14257,0\\r\\n14258,0\\r\\n14259,0\\r\\n14260,0\\r\\n14261,0\\r\\n14262,0\\r\\n14263,0\\r\\n14264,0\\r\\n14265,0\\r\\n14266,0\\r\\n14267,0\\r\\n14268,0\\r\\n14269,0\\r\\n14270,0\\r\\n14271,0\\r\\n14272,0\\r\\n14273,0\\r\\n14274,0\\r\\n14275,0\\r\\n14276,0\\r\\n14277,0\\r\\n14278,0\\r\\n14279,0\\r\\n14280,0\\r\\n14281,0\\r\\n14282,0\\r\\n14283,0\\r\\n14284,0\\r\\n14285,0\\r\\n14286,0\\r\\n14287,0\\r\\n14288,0\\r\\n14289,0\\r\\n14290,0\\r\\n14291,0\\r\\n14292,0\\r\\n14293,0\\r\\n14294,0\\r\\n14295,0\\r\\n14296,0\\r\\n14297,0\\r\\n14298,0\\r\\n14299,0\\r\\n14300,0\\r\\n14301,0\\r\\n14302,0\\r\\n14303,0\\r\\n14304,0\\r\\n14305,0\\r\\n14306,0\\r\\n14307,0\\r\\n14308,0\\r\\n14309,0\\r\\n14310,0\\r\\n14311,0\\r\\n14312,0\\r\\n14313,0\\r\\n14314,0\\r\\n14315,0\\r\\n14316,0\\r\\n14317,0\\r\\n14318,0\\r\\n14319,0\\r\\n14320,0\\r\\n14321,0\\r\\n14322,0\\r\\n14323,0\\r\\n14324,0\\r\\n14325,0\\r\\n14326,0\\r\\n14327,0\\r\\n14328,0\\r\\n14329,0\\r\\n14330,0\\r\\n14331,0\\r\\n14332,0\\r\\n14333,0\\r\\n14334,0\\r\\n14335,0\\r\\n14336,0\\r\\n14337,0\\r\\n14338,0\\r\\n14339,0\\r\\n14340,0\\r\\n14341,0\\r\\n14342,0\\r\\n14343,0\\r\\n14344,0\\r\\n14345,0\\r\\n14346,0\\r\\n14347,0\\r\\n14348,0\\r\\n14349,0\\r\\n14350,0\\r\\n14351,0\\r\\n14352,0\\r\\n14353,0\\r\\n14354,0\\r\\n14355,0\\r\\n14356,0\\r\\n14357,0\\r\\n14358,0\\r\\n14359,0\\r\\n14360,0\\r\\n14361,0\\r\\n14362,0\\r\\n14363,0\\r\\n14364,0\\r\\n14365,0\\r\\n14366,0\\r\\n14367,0\\r\\n14368,0\\r\\n14369,0\\r\\n14370,0\\r\\n14371,0\\r\\n14372,0\\r\\n14373,0\\r\\n14374,0\\r\\n14375,0\\r\\n14376,0\\r\\n14377,0\\r\\n14378,0\\r\\n14379,0\\r\\n14380,0\\r\\n14381,0\\r\\n14382,0\\r\\n14383,0\\r\\n14384,0\\r\\n14385,0\\r\\n14386,0\\r\\n14387,0\\r\\n14388,0\\r\\n14389,0\\r\\n14390,0\\r\\n14391,0\\r\\n14392,0\\r\\n14393,0\\r\\n14394,0\\r\\n14395,0\\r\\n14396,0\\r\\n14397,0\\r\\n14398,0\\r\\n14399,0\\r\\n14400,0\\r\\n14401,0\\r\\n14402,0\\r\\n14403,0\\r\\n14404,0\\r\\n14405,0\\r\\n14406,0\\r\\n14407,0\\r\\n14408,0\\r\\n14409,0\\r\\n14410,0\\r\\n14411,0\\r\\n14412,0\\r\\n14413,0\\r\\n14414,0\\r\\n14415,0\\r\\n14416,0\\r\\n14417,0\\r\\n14418,0\\r\\n14419,0\\r\\n14420,0\\r\\n14421,0\\r\\n14422,0\\r\\n14423,0\\r\\n14424,0\\r\\n14425,0\\r\\n14426,0\\r\\n14427,0\\r\\n14428,0\\r\\n14429,0\\r\\n14430,0\\r\\n14431,0\\r\\n14432,0\\r\\n14433,0\\r\\n14434,0\\r\\n14435,0\\r\\n14436,0\\r\\n14437,0\\r\\n14438,0\\r\\n14439,0\\r\\n14440,0\\r\\n14441,0\\r\\n14442,0\\r\\n14443,0\\r\\n14444,0\\r\\n14445,0\\r\\n14446,0\\r\\n14447,0\\r\\n14448,0\\r\\n14449,0\\r\\n14450,0\\r\\n14451,0\\r\\n14452,0\\r\\n14453,0\\r\\n14454,0\\r\\n14455,0\\r\\n14456,0\\r\\n14457,0\\r\\n14458,0\\r\\n14459,0\\r\\n14460,0\\r\\n14461,0\\r\\n14462,0\\r\\n14463,0\\r\\n14464,0\\r\\n14465,0\\r\\n14466,0\\r\\n14467,0\\r\\n14468,0\\r\\n14469,0\\r\\n14470,0\\r\\n14471,0\\r\\n14472,0\\r\\n14473,0\\r\\n14474,0\\r\\n14475,0\\r\\n14476,0\\r\\n14477,0\\r\\n14478,0\\r\\n14479,0\\r\\n14480,0\\r\\n14481,0\\r\\n14482,0\\r\\n14483,0\\r\\n14484,0\\r\\n14485,0\\r\\n14486,0\\r\\n14487,0\\r\\n14488,0\\r\\n14489,0\\r\\n14490,0\\r\\n14491,0\\r\\n14492,0\\r\\n14493,0\\r\\n14494,0\\r\\n14495,0\\r\\n14496,0\\r\\n14497,0\\r\\n14498,0\\r\\n14499,0\\r\\n14500,0\\r\\n14501,0\\r\\n14502,0\\r\\n14503,0\\r\\n14504,0\\r\\n14505,0\\r\\n14506,0\\r\\n14507,0\\r\\n14508,0\\r\\n14509,0\\r\\n14510,0\\r\\n14511,0\\r\\n14512,0\\r\\n14513,0\\r\\n14514,0\\r\\n14515,0\\r\\n14516,0\\r\\n14517,0\\r\\n14518,0\\r\\n14519,0\\r\\n14520,0\\r\\n14521,0\\r\\n14522,0\\r\\n14523,0\\r\\n14524,0\\r\\n14525,0\\r\\n14526,0\\r\\n14527,0\\r\\n14528,0\\r\\n14529,0\\r\\n14530,0\\r\\n14531,0\\r\\n14532,0\\r\\n14533,0\\r\\n14534,0\\r\\n14535,0\\r\\n14536,0\\r\\n14537,0\\r\\n14538,0\\r\\n14539,0\\r\\n14540,0\\r\\n14541,0\\r\\n14542,0\\r\\n14543,0\\r\\n14544,0\\r\\n14545,0\\r\\n14546,0\\r\\n14547,0\\r\\n14548,0\\r\\n14549,0\\r\\n14550,0\\r\\n14551,0\\r\\n14552,0\\r\\n14553,0\\r\\n14554,0\\r\\n14555,0\\r\\n14556,0\\r\\n14557,0\\r\\n14558,0\\r\\n14559,0\\r\\n14560,0\\r\\n14561,0\\r\\n14562,0\\r\\n14563,0\\r\\n14564,0\\r\\n14565,0\\r\\n14566,0\\r\\n14567,0\\r\\n14568,0\\r\\n14569,0\\r\\n14570,0\\r\\n14571,0\\r\\n14572,0\\r\\n14573,0\\r\\n14574,0\\r\\n14575,0\\r\\n14576,0\\r\\n14577,0\\r\\n14578,0\\r\\n14579,0\\r\\n14580,0\\r\\n14581,0\\r\\n14582,0\\r\\n14583,0\\r\\n14584,0\\r\\n14585,0\\r\\n14586,0\\r\\n14587,0\\r\\n14588,0\\r\\n14589,0\\r\\n14590,0\\r\\n14591,0\\r\\n14592,0\\r\\n14593,0\\r\\n14594,0\\r\\n14595,0\\r\\n14596,0\\r\\n14597,0\\r\\n14598,0\\r\\n14599,0\\r\\n14600,0\\r\\n14601,0\\r\\n14602,0\\r\\n14603,0\\r\\n14604,0\\r\\n14605,0\\r\\n14606,0\\r\\n14607,0\\r\\n14608,0\\r\\n14609,0\\r\\n14610,0\\r\\n14611,0\\r\\n14612,0\\r\\n14613,0\\r\\n14614,0\\r\\n14615,0\\r\\n14616,0\\r\\n14617,0\\r\\n14618,0\\r\\n14619,0\\r\\n14620,0\\r\\n14621,0\\r\\n14622,0\\r\\n14623,0\\r\\n14624,0\\r\\n14625,0\\r\\n14626,0\\r\\n14627,0\\r\\n14628,0\\r\\n14629,0\\r\\n14630,0\\r\\n14631,0\\r\\n14632,0\\r\\n14633,0\\r\\n14634,0\\r\\n14635,0\\r\\n14636,0\\r\\n14637,0\\r\\n14638,0\\r\\n14639,0\\r\\n14640,0\\r\\n14641,0\\r\\n14642,0\\r\\n14643,0\\r\\n14644,0\\r\\n14645,0\\r\\n14646,0\\r\\n14647,0\\r\\n14648,0\\r\\n14649,0\\r\\n14650,0\\r\\n14651,0\\r\\n14652,0\\r\\n14653,0\\r\\n14654,0\\r\\n14655,0\\r\\n14656,0\\r\\n14657,0\\r\\n14658,0\\r\\n14659,0\\r\\n14660,0\\r\\n14661,0\\r\\n14662,0\\r\\n14663,0\\r\\n14664,0\\r\\n14665,0\\r\\n14666,0\\r\\n14667,0\\r\\n14668,0\\r\\n14669,0\\r\\n14670,0\\r\\n14671,0\\r\\n14672,0\\r\\n14673,0\\r\\n14674,0\\r\\n14675,0\\r\\n14676,0\\r\\n14677,0\\r\\n14678,0\\r\\n14679,0\\r\\n14680,0\\r\\n14681,0\\r\\n14682,0\\r\\n14683,0\\r\\n14684,0\\r\\n14685,0\\r\\n14686,0\\r\\n14687,0\\r\\n14688,0\\r\\n14689,0\\r\\n14690,0\\r\\n14691,0\\r\\n14692,0\\r\\n14693,0\\r\\n14694,0\\r\\n14695,0\\r\\n14696,0\\r\\n14697,0\\r\\n14698,0\\r\\n14699,0\\r\\n14700,0\\r\\n14701,0\\r\\n14702,0\\r\\n14703,0\\r\\n14704,0\\r\\n14705,0\\r\\n14706,0\\r\\n14707,0\\r\\n14708,0\\r\\n14709,0\\r\\n14710,0\\r\\n14711,0\\r\\n14712,0\\r\\n14713,0\\r\\n14714,0\\r\\n14715,0\\r\\n14716,0\\r\\n14717,0\\r\\n14718,0\\r\\n14719,0\\r\\n14720,0\\r\\n14721,0\\r\\n14722,0\\r\\n14723,0\\r\\n14724,0\\r\\n14725,0\\r\\n14726,0\\r\\n14727,0\\r\\n14728,0\\r\\n14729,0\\r\\n14730,0\\r\\n14731,0\\r\\n14732,0\\r\\n14733,0\\r\\n14734,0\\r\\n14735,0\\r\\n14736,0\\r\\n14737,0\\r\\n14738,0\\r\\n14739,0\\r\\n14740,0\\r\\n14741,0\\r\\n14742,0\\r\\n14743,0\\r\\n14744,0\\r\\n14745,0\\r\\n14746,0\\r\\n14747,0\\r\\n14748,0\\r\\n14749,0\\r\\n14750,0\\r\\n14751,0\\r\\n14752,0\\r\\n14753,0\\r\\n14754,0\\r\\n14755,0\\r\\n14756,0\\r\\n14757,0\\r\\n14758,0\\r\\n14759,0\\r\\n14760,0\\r\\n14761,0\\r\\n14762,0\\r\\n14763,0\\r\\n14764,0\\r\\n14765,0\\r\\n14766,0\\r\\n14767,0\\r\\n14768,0\\r\\n14769,0\\r\\n14770,0\\r\\n14771,0\\r\\n14772,0\\r\\n14773,0\\r\\n14774,0\\r\\n14775,0\\r\\n14776,0\\r\\n14777,0\\r\\n14778,0\\r\\n14779,0\\r\\n14780,0\\r\\n14781,0\\r\\n14782,0\\r\\n14783,0\\r\\n14784,0\\r\\n14785,0\\r\\n14786,0\\r\\n14787,0\\r\\n14788,0\\r\\n14789,0\\r\\n14790,0\\r\\n14791,0\\r\\n14792,0\\r\\n14793,0\\r\\n14794,0\\r\\n14795,0\\r\\n14796,0\\r\\n14797,0\\r\\n14798,0\\r\\n14799,0\\r\\n14800,0\\r\\n14801,0\\r\\n14802,0\\r\\n14803,0\\r\\n14804,0\\r\\n14805,0\\r\\n14806,0\\r\\n14807,0\\r\\n14808,0\\r\\n14809,0\\r\\n14810,0\\r\\n14811,0\\r\\n14812,0\\r\\n14813,0\\r\\n14814,0\\r\\n14815,0\\r\\n14816,0\\r\\n14817,0\\r\\n14818,0\\r\\n14819,0\\r\\n14820,0\\r\\n14821,0\\r\\n14822,0\\r\\n14823,0\\r\\n14824,0\\r\\n14825,0\\r\\n14826,0\\r\\n14827,0\\r\\n14828,0\\r\\n14829,0\\r\\n14830,0\\r\\n14831,0\\r\\n14832,0\\r\\n14833,0\\r\\n14834,0\\r\\n14835,0\\r\\n14836,0\\r\\n14837,0\\r\\n14838,0\\r\\n14839,0\\r\\n14840,0\\r\\n14841,0\\r\\n14842,0\\r\\n14843,0\\r\\n14844,0\\r\\n14845,0\\r\\n14846,0\\r\\n14847,0\\r\\n14848,0\\r\\n14849,0\\r\\n14850,0\\r\\n14851,0\\r\\n14852,0\\r\\n14853,0\\r\\n14854,0\\r\\n14855,0\\r\\n14856,0\\r\\n14857,0\\r\\n14858,0\\r\\n14859,0\\r\\n14860,0\\r\\n14861,0\\r\\n14862,0\\r\\n14863,0\\r\\n14864,0\\r\\n14865,0\\r\\n14866,0\\r\\n14867,0\\r\\n14868,0\\r\\n14869,0\\r\\n14870,0\\r\\n14871,0\\r\\n14872,0\\r\\n14873,0\\r\\n14874,0\\r\\n14875,0\\r\\n14876,0\\r\\n14877,0\\r\\n14878,0\\r\\n14879,0\\r\\n14880,0\\r\\n14881,0\\r\\n14882,0\\r\\n14883,0\\r\\n14884,0\\r\\n14885,0\\r\\n14886,0\\r\\n14887,0\\r\\n14888,0\\r\\n14889,0\\r\\n14890,0\\r\\n14891,0\\r\\n14892,0\\r\\n14893,0\\r\\n14894,0\\r\\n14895,0\\r\\n14896,0\\r\\n14897,0\\r\\n14898,0\\r\\n14899,0\\r\\n14900,0\\r\\n14901,0\\r\\n14902,0\\r\\n14903,0\\r\\n14904,0\\r\\n14905,0\\r\\n14906,0\\r\\n14907,0\\r\\n14908,0\\r\\n14909,0\\r\\n14910,0\\r\\n14911,0\\r\\n14912,0\\r\\n14913,0\\r\\n14914,0\\r\\n14915,0\\r\\n14916,0\\r\\n14917,0\\r\\n14918,0\\r\\n14919,0\\r\\n14920,0\\r\\n14921,0\\r\\n14922,0\\r\\n14923,0\\r\\n14924,0\\r\\n14925,0\\r\\n14926,0\\r\\n14927,0\\r\\n14928,0\\r\\n14929,0\\r\\n14930,0\\r\\n14931,0\\r\\n14932,0\\r\\n14933,0\\r\\n14934,0\\r\\n14935,0\\r\\n14936,0\\r\\n14937,0\\r\\n14938,0\\r\\n14939,0\\r\\n14940,0\\r\\n14941,0\\r\\n14942,0\\r\\n14943,0\\r\\n14944,0\\r\\n14945,0\\r\\n14946,0\\r\\n14947,0\\r\\n14948,0\\r\\n14949,0\\r\\n14950,0\\r\\n14951,0\\r\\n14952,0\\r\\n14953,0\\r\\n14954,0\\r\\n14955,0\\r\\n14956,0\\r\\n14957,0\\r\\n14958,0\\r\\n14959,0\\r\\n14960,0\\r\\n14961,0\\r\\n14962,0\\r\\n14963,0\\r\\n14964,0\\r\\n14965,0\\r\\n14966,0\\r\\n14967,0\\r\\n14968,0\\r\\n14969,0\\r\\n14970,0\\r\\n14971,0\\r\\n14972,0\\r\\n14973,0\\r\\n14974,0\\r\\n14975,0\\r\\n14976,0\\r\\n14977,0\\r\\n14978,0\\r\\n14979,0\\r\\n14980,0\\r\\n14981,0\\r\\n14982,0\\r\\n14983,0\\r\\n14984,0\\r\\n14985,0\\r\\n14986,0\\r\\n14987,0\\r\\n14988,0\\r\\n14989,0\\r\\n14990,0\\r\\n14991,0\\r\\n14992,0\\r\\n14993,0\\r\\n14994,0\\r\\n14995,0\\r\\n14996,0\\r\\n14997,0\\r\\n14998,0\\r\\n14999,0\\r\\n15000,0\\r\\n15001,0\\r\\n15002,0\\r\\n15003,0\\r\\n15004,0\\r\\n15005,0\\r\\n15006,0\\r\\n15007,0\\r\\n15008,0\\r\\n15009,0\\r\\n15010,0\\r\\n15011,0\\r\\n15012,0\\r\\n15013,0\\r\\n15014,0\\r\\n15015,0\\r\\n15016,0\\r\\n15017,0\\r\\n15018,0\\r\\n15019,0\\r\\n15020,0\\r\\n15021,0\\r\\n15022,0\\r\\n15023,0\\r\\n15024,0\\r\\n15025,0\\r\\n15026,0\\r\\n15027,0\\r\\n15028,0\\r\\n15029,0\\r\\n15030,0\\r\\n15031,0\\r\\n15032,0\\r\\n15033,0\\r\\n15034,0\\r\\n15035,0\\r\\n15036,0\\r\\n15037,0\\r\\n15038,0\\r\\n15039,0\\r\\n15040,0\\r\\n15041,0\\r\\n15042,0\\r\\n15043,0\\r\\n15044,0\\r\\n15045,0\\r\\n15046,0\\r\\n15047,0\\r\\n15048,0\\r\\n15049,0\\r\\n15050,0\\r\\n15051,0\\r\\n15052,0\\r\\n15053,0\\r\\n15054,0\\r\\n15055,0\\r\\n15056,0\\r\\n15057,0\\r\\n15058,0\\r\\n15059,0\\r\\n15060,0\\r\\n15061,0\\r\\n15062,0\\r\\n15063,0\\r\\n15064,0\\r\\n15065,0\\r\\n15066,0\\r\\n15067,0\\r\\n15068,0\\r\\n15069,0\\r\\n15070,0\\r\\n15071,0\\r\\n15072,0\\r\\n15073,0\\r\\n15074,0\\r\\n15075,0\\r\\n15076,0\\r\\n15077,0\\r\\n15078,0\\r\\n15079,0\\r\\n15080,0\\r\\n15081,0\\r\\n15082,0\\r\\n15083,0\\r\\n15084,0\\r\\n15085,0\\r\\n15086,0\\r\\n15087,0\\r\\n15088,0\\r\\n15089,0\\r\\n15090,0\\r\\n15091,0\\r\\n15092,0\\r\\n15093,0\\r\\n15094,0\\r\\n15095,0\\r\\n15096,0\\r\\n15097,0\\r\\n15098,0\\r\\n15099,0\\r\\n15100,0\\r\\n15101,0\\r\\n15102,0\\r\\n15103,0\\r\\n15104,0\\r\\n15105,0\\r\\n15106,0\\r\\n15107,0\\r\\n15108,0\\r\\n15109,0\\r\\n15110,0\\r\\n15111,0\\r\\n15112,0\\r\\n15113,0\\r\\n15114,0\\r\\n15115,0\\r\\n15116,0\\r\\n15117,0\\r\\n15118,0\\r\\n15119,0\\r\\n15120,0\\r\\n15121,0\\r\\n15122,0\\r\\n15123,0\\r\\n15124,0\\r\\n15125,0\\r\\n15126,0\\r\\n15127,0\\r\\n15128,0\\r\\n15129,0\\r\\n15130,0\\r\\n15131,0\\r\\n15132,0\\r\\n15133,0\\r\\n15134,0\\r\\n15135,0\\r\\n15136,0\\r\\n15137,0\\r\\n15138,0\\r\\n15139,0\\r\\n15140,0\\r\\n15141,0\\r\\n15142,0\\r\\n15143,0\\r\\n15144,0\\r\\n15145,0\\r\\n15146,0\\r\\n15147,0\\r\\n15148,0\\r\\n15149,0\\r\\n15150,0\\r\\n15151,0\\r\\n15152,0\\r\\n15153,0\\r\\n15154,0\\r\\n15155,0\\r\\n15156,0\\r\\n15157,0\\r\\n15158,0\\r\\n15159,0\\r\\n15160,0\\r\\n15161,0\\r\\n15162,0\\r\\n15163,0\\r\\n15164,0\\r\\n15165,0\\r\\n15166,0\\r\\n15167,0\\r\\n15168,0\\r\\n15169,0\\r\\n15170,0\\r\\n15171,0\\r\\n15172,0\\r\\n15173,0\\r\\n15174,0\\r\\n15175,0\\r\\n15176,0\\r\\n15177,0\\r\\n15178,0\\r\\n15179,0\\r\\n15180,0\\r\\n15181,0\\r\\n15182,0\\r\\n15183,0\\r\\n15184,0\\r\\n15185,0\\r\\n15186,0\\r\\n15187,0\\r\\n15188,0\\r\\n15189,0\\r\\n15190,0\\r\\n15191,0\\r\\n15192,0\\r\\n15193,0\\r\\n15194,0\\r\\n15195,0\\r\\n15196,0\\r\\n15197,0\\r\\n15198,0\\r\\n15199,0\\r\\n15200,0\\r\\n15201,0\\r\\n15202,0\\r\\n15203,0\\r\\n15204,0\\r\\n15205,0\\r\\n15206,0\\r\\n15207,0\\r\\n15208,0\\r\\n15209,0\\r\\n15210,0\\r\\n15211,0\\r\\n15212,0\\r\\n15213,0\\r\\n15214,0\\r\\n15215,0\\r\\n15216,0\\r\\n15217,0\\r\\n15218,0\\r\\n15219,0\\r\\n15220,0\\r\\n15221,0\\r\\n15222,0\\r\\n15223,0\\r\\n15224,0\\r\\n15225,0\\r\\n15226,0\\r\\n15227,0\\r\\n15228,0\\r\\n15229,0\\r\\n15230,0\\r\\n15231,0\\r\\n15232,0\\r\\n15233,0\\r\\n15234,0\\r\\n15235,0\\r\\n15236,0\\r\\n15237,0\\r\\n15238,0\\r\\n15239,0\\r\\n15240,0\\r\\n15241,0\\r\\n15242,0\\r\\n15243,0\\r\\n15244,0\\r\\n15245,0\\r\\n15246,0\\r\\n15247,0\\r\\n15248,0\\r\\n15249,0\\r\\n15250,0\\r\\n15251,0\\r\\n15252,0\\r\\n15253,0\\r\\n15254,0\\r\\n15255,0\\r\\n15256,0\\r\\n15257,0\\r\\n15258,0\\r\\n15259,0\\r\\n15260,0\\r\\n15261,0\\r\\n15262,0\\r\\n15263,0\\r\\n15264,0\\r\\n15265,0\\r\\n15266,0\\r\\n15267,0\\r\\n15268,0\\r\\n15269,0\\r\\n15270,0\\r\\n15271,0\\r\\n15272,0\\r\\n15273,0\\r\\n15274,0\\r\\n15275,0\\r\\n15276,0\\r\\n15277,0\\r\\n15278,0\\r\\n15279,0\\r\\n15280,0\\r\\n15281,0\\r\\n15282,0\\r\\n15283,0\\r\\n15284,0\\r\\n15285,0\\r\\n15286,0\\r\\n15287,0\\r\\n15288,0\\r\\n15289,0\\r\\n15290,0\\r\\n15291,0\\r\\n15292,0\\r\\n15293,0\\r\\n15294,0\\r\\n15295,0\\r\\n15296,0\\r\\n15297,0\\r\\n15298,0\\r\\n15299,0\\r\\n15300,0\\r\\n15301,0\\r\\n15302,0\\r\\n15303,0\\r\\n15304,0\\r\\n15305,0\\r\\n15306,0\\r\\n15307,0\\r\\n15308,0\\r\\n15309,0\\r\\n15310,0\\r\\n15311,0\\r\\n15312,0\\r\\n15313,0\\r\\n15314,0\\r\\n15315,0\\r\\n15316,0\\r\\n15317,0\\r\\n15318,0\\r\\n15319,0\\r\\n15320,0\\r\\n15321,0\\r\\n15322,0\\r\\n15323,0\\r\\n15324,0\\r\\n15325,0\\r\\n15326,0\\r\\n15327,0\\r\\n15328,0\\r\\n15329,0\\r\\n15330,0\\r\\n15331,0\\r\\n15332,0\\r\\n15333,0\\r\\n15334,0\\r\\n15335,0\\r\\n15336,0\\r\\n15337,0\\r\\n15338,0\\r\\n15339,0\\r\\n15340,0\\r\\n15341,0\\r\\n15342,0\\r\\n15343,0\\r\\n15344,0\\r\\n15345,0\\r\\n15346,0\\r\\n15347,0\\r\\n15348,0\\r\\n15349,0\\r\\n15350,0\\r\\n15351,0\\r\\n15352,0\\r\\n15353,0\\r\\n15354,0\\r\\n15355,0\\r\\n15356,0\\r\\n15357,0\\r\\n15358,0\\r\\n15359,0\\r\\n15360,0\\r\\n15361,0\\r\\n15362,0\\r\\n15363,0\\r\\n15364,0\\r\\n15365,0\\r\\n15366,0\\r\\n15367,0\\r\\n15368,0\\r\\n15369,0\\r\\n15370,0\\r\\n15371,0\\r\\n15372,0\\r\\n15373,0\\r\\n15374,0\\r\\n15375,0\\r\\n15376,0\\r\\n15377,0\\r\\n15378,0\\r\\n15379,0\\r\\n15380,0\\r\\n15381,0\\r\\n15382,0\\r\\n15383,0\\r\\n15384,0\\r\\n15385,0\\r\\n15386,0\\r\\n15387,0\\r\\n15388,0\\r\\n15389,0\\r\\n15390,0\\r\\n15391,0\\r\\n15392,0\\r\\n15393,0\\r\\n15394,0\\r\\n15395,0\\r\\n15396,0\\r\\n15397,0\\r\\n15398,0\\r\\n15399,0\\r\\n15400,0\\r\\n15401,0\\r\\n15402,0\\r\\n15403,0\\r\\n15404,0\\r\\n15405,0\\r\\n15406,0\\r\\n15407,0\\r\\n15408,0\\r\\n15409,0\\r\\n15410,0\\r\\n15411,0\\r\\n15412,0\\r\\n15413,0\\r\\n15414,0\\r\\n15415,0\\r\\n15416,0\\r\\n15417,0\\r\\n15418,0\\r\\n15419,0\\r\\n15420,0\\r\\n15421,0\\r\\n15422,0\\r\\n15423,0\\r\\n15424,0\\r\\n15425,0\\r\\n15426,0\\r\\n15427,0\\r\\n15428,0\\r\\n15429,0\\r\\n15430,0\\r\\n15431,0\\r\\n15432,0\\r\\n15433,0\\r\\n15434,0\\r\\n15435,0\\r\\n15436,0\\r\\n15437,0\\r\\n15438,0\\r\\n15439,0\\r\\n15440,0\\r\\n15441,0\\r\\n15442,0\\r\\n15443,0\\r\\n15444,0\\r\\n15445,0\\r\\n15446,0\\r\\n15447,0\\r\\n15448,0\\r\\n15449,0\\r\\n15450,0\\r\\n15451,0\\r\\n15452,0\\r\\n15453,0\\r\\n15454,0\\r\\n15455,0\\r\\n15456,0\\r\\n15457,0\\r\\n15458,0\\r\\n15459,0\\r\\n15460,0\\r\\n15461,0\\r\\n15462,0\\r\\n15463,0\\r\\n15464,0\\r\\n15465,0\\r\\n15466,0\\r\\n15467,0\\r\\n15468,0\\r\\n15469,0\\r\\n15470,0\\r\\n15471,0\\r\\n15472,0\\r\\n15473,0\\r\\n15474,0\\r\\n15475,0\\r\\n15476,0\\r\\n15477,0\\r\\n15478,0\\r\\n15479,0\\r\\n15480,0\\r\\n15481,0\\r\\n15482,0\\r\\n15483,0\\r\\n15484,0\\r\\n15485,0\\r\\n15486,0\\r\\n15487,0\\r\\n15488,0\\r\\n15489,0\\r\\n15490,0\\r\\n15491,0\\r\\n15492,0\\r\\n15493,0\\r\\n15494,0\\r\\n15495,0\\r\\n15496,0\\r\\n15497,0\\r\\n15498,0\\r\\n15499,0\\r\\n15500,0\\r\\n15501,0\\r\\n15502,0\\r\\n15503,0\\r\\n15504,0\\r\\n15505,0\\r\\n15506,0\\r\\n15507,0\\r\\n15508,0\\r\\n15509,0\\r\\n15510,0\\r\\n15511,0\\r\\n15512,0\\r\\n15513,0\\r\\n15514,0\\r\\n15515,0\\r\\n15516,0\\r\\n15517,0\\r\\n15518,0\\r\\n15519,0\\r\\n15520,0\\r\\n15521,0\\r\\n15522,0\\r\\n15523,0\\r\\n15524,0\\r\\n15525,0\\r\\n15526,0\\r\\n15527,0\\r\\n15528,0\\r\\n15529,0\\r\\n15530,0\\r\\n15531,0\\r\\n15532,0\\r\\n15533,0\\r\\n15534,0\\r\\n15535,0\\r\\n15536,0\\r\\n15537,0\\r\\n15538,0\\r\\n15539,0\\r\\n15540,0\\r\\n15541,0\\r\\n15542,0\\r\\n15543,0\\r\\n15544,0\\r\\n15545,0\\r\\n15546,0\\r\\n15547,0\\r\\n15548,0\\r\\n15549,0\\r\\n15550,0\\r\\n15551,0\\r\\n15552,0\\r\\n15553,0\\r\\n15554,0\\r\\n15555,0\\r\\n15556,0\\r\\n15557,0\\r\\n15558,0\\r\\n15559,0\\r\\n15560,0\\r\\n15561,0\\r\\n15562,0\\r\\n15563,0\\r\\n15564,0\\r\\n15565,0\\r\\n15566,0\\r\\n15567,0\\r\\n15568,0\\r\\n15569,0\\r\\n15570,0\\r\\n15571,0\\r\\n15572,0\\r\\n15573,0\\r\\n15574,0\\r\\n15575,0\\r\\n15576,0\\r\\n15577,0\\r\\n15578,0\\r\\n15579,0\\r\\n15580,0\\r\\n15581,0\\r\\n15582,0\\r\\n15583,0\\r\\n15584,0\\r\\n15585,0\\r\\n15586,0\\r\\n15587,0\\r\\n15588,0\\r\\n15589,0\\r\\n15590,0\\r\\n15591,0\\r\\n15592,0\\r\\n15593,0\\r\\n15594,0\\r\\n15595,0\\r\\n15596,0\\r\\n15597,0\\r\\n15598,0\\r\\n15599,0\\r\\n15600,0\\r\\n15601,0\\r\\n15602,0\\r\\n15603,0\\r\\n15604,0\\r\\n15605,0\\r\\n15606,0\\r\\n15607,0\\r\\n15608,0\\r\\n15609,0\\r\\n15610,0\\r\\n15611,0\\r\\n15612,0\\r\\n15613,0\\r\\n15614,0\\r\\n15615,0\\r\\n15616,0\\r\\n15617,0\\r\\n15618,0\\r\\n15619,0\\r\\n15620,0\\r\\n15621,0\\r\\n15622,0\\r\\n15623,0\\r\\n15624,0\\r\\n15625,0\\r\\n15626,0\\r\\n15627,0\\r\\n15628,0\\r\\n15629,0\\r\\n15630,0\\r\\n15631,0\\r\\n15632,0\\r\\n15633,0\\r\\n15634,0\\r\\n15635,0\\r\\n15636,0\\r\\n15637,0\\r\\n15638,0\\r\\n15639,0\\r\\n15640,0\\r\\n15641,0\\r\\n15642,0\\r\\n15643,0\\r\\n15644,0\\r\\n15645,0\\r\\n15646,0\\r\\n15647,0\\r\\n15648,0\\r\\n15649,0\\r\\n15650,0\\r\\n15651,0\\r\\n15652,0\\r\\n15653,0\\r\\n15654,0\\r\\n15655,0\\r\\n15656,0\\r\\n15657,0\\r\\n15658,0\\r\\n15659,0\\r\\n15660,0\\r\\n15661,0\\r\\n15662,0\\r\\n15663,0\\r\\n15664,0\\r\\n15665,0\\r\\n15666,0\\r\\n15667,0\\r\\n15668,0\\r\\n15669,0\\r\\n15670,0\\r\\n15671,0\\r\\n15672,0\\r\\n15673,0\\r\\n15674,0\\r\\n15675,0\\r\\n15676,0\\r\\n15677,0\\r\\n15678,0\\r\\n15679,0\\r\\n15680,0\\r\\n15681,0\\r\\n15682,0\\r\\n15683,0\\r\\n15684,0\\r\\n15685,0\\r\\n15686,0\\r\\n15687,0\\r\\n15688,0\\r\\n15689,0\\r\\n15690,0\\r\\n15691,0\\r\\n15692,0\\r\\n15693,0\\r\\n15694,0\\r\\n15695,0\\r\\n15696,0\\r\\n15697,0\\r\\n15698,0\\r\\n15699,0\\r\\n15700,0\\r\\n15701,0\\r\\n15702,0\\r\\n15703,0\\r\\n15704,0\\r\\n15705,0\\r\\n15706,0\\r\\n15707,0\\r\\n15708,0\\r\\n15709,0\\r\\n15710,0\\r\\n15711,0\\r\\n15712,0\\r\\n15713,0\\r\\n15714,0\\r\\n15715,0\\r\\n15716,0\\r\\n15717,0\\r\\n15718,0\\r\\n15719,0\\r\\n15720,0\\r\\n15721,0\\r\\n15722,0\\r\\n15723,0\\r\\n15724,0\\r\\n15725,0\\r\\n15726,0\\r\\n15727,0\\r\\n15728,0\\r\\n15729,0\\r\\n15730,0\\r\\n15731,0\\r\\n15732,0\\r\\n15733,0\\r\\n15734,0\\r\\n15735,0\\r\\n15736,0\\r\\n15737,0\\r\\n15738,0\\r\\n15739,0\\r\\n15740,0\\r\\n15741,0\\r\\n15742,0\\r\\n15743,0\\r\\n15744,0\\r\\n15745,0\\r\\n15746,0\\r\\n15747,0\\r\\n15748,0\\r\\n15749,0\\r\\n15750,0\\r\\n15751,0\\r\\n15752,0\\r\\n15753,0\\r\\n15754,0\\r\\n15755,0\\r\\n15756,0\\r\\n15757,0\\r\\n15758,0\\r\\n15759,0\\r\\n15760,0\\r\\n15761,0\\r\\n15762,0\\r\\n15763,0\\r\\n15764,0\\r\\n15765,0\\r\\n15766,0\\r\\n15767,0\\r\\n15768,0\\r\\n15769,0\\r\\n15770,0\\r\\n15771,0\\r\\n15772,0\\r\\n15773,0\\r\\n15774,0\\r\\n15775,0\\r\\n15776,0\\r\\n15777,0\\r\\n15778,0\\r\\n15779,0\\r\\n15780,0\\r\\n15781,0\\r\\n15782,0\\r\\n15783,0\\r\\n15784,0\\r\\n15785,0\\r\\n15786,0\\r\\n15787,0\\r\\n15788,0\\r\\n15789,0\\r\\n15790,0\\r\\n15791,0\\r\\n15792,0\\r\\n15793,0\\r\\n15794,0\\r\\n15795,0\\r\\n15796,0\\r\\n15797,0\\r\\n15798,0\\r\\n15799,0\\r\\n15800,0\\r\\n15801,0\\r\\n15802,0\\r\\n15803,0\\r\\n15804,0\\r\\n15805,0\\r\\n15806,0\\r\\n15807,0\\r\\n15808,0\\r\\n15809,0\\r\\n15810,0\\r\\n15811,0\\r\\n15812,0\\r\\n15813,0\\r\\n15814,0\\r\\n15815,0\\r\\n15816,0\\r\\n15817,0\\r\\n15818,0\\r\\n15819,0\\r\\n15820,0\\r\\n15821,0\\r\\n15822,0\\r\\n15823,0\\r\\n15824,0\\r\\n15825,0\\r\\n15826,0\\r\\n15827,0\\r\\n15828,0\\r\\n15829,0\\r\\n15830,0\\r\\n15831,0\\r\\n15832,0\\r\\n15833,0\\r\\n15834,0\\r\\n15835,0\\r\\n15836,0\\r\\n15837,0\\r\\n15838,0\\r\\n15839,0\\r\\n15840,0\\r\\n15841,0\\r\\n15842,0\\r\\n15843,0\\r\\n15844,0\\r\\n15845,0\\r\\n15846,0\\r\\n15847,0\\r\\n15848,0\\r\\n15849,0\\r\\n15850,0\\r\\n15851,0\\r\\n15852,0\\r\\n15853,0\\r\\n15854,0\\r\\n15855,0\\r\\n15856,0\\r\\n15857,0\\r\\n15858,0\\r\\n15859,0\\r\\n15860,0\\r\\n15861,0\\r\\n15862,0\\r\\n15863,0\\r\\n15864,0\\r\\n15865,0\\r\\n15866,0\\r\\n15867,0\\r\\n15868,0\\r\\n15869,0\\r\\n15870,0\\r\\n15871,0\\r\\n15872,0\\r\\n15873,0\\r\\n15874,0\\r\\n15875,0\\r\\n15876,0\\r\\n15877,0\\r\\n15878,0\\r\\n15879,0\\r\\n15880,0\\r\\n15881,0\\r\\n15882,0\\r\\n15883,0\\r\\n15884,0\\r\\n15885,0\\r\\n15886,0\\r\\n15887,0\\r\\n15888,0\\r\\n15889,0\\r\\n15890,0\\r\\n15891,0\\r\\n15892,0\\r\\n15893,0\\r\\n15894,0\\r\\n15895,0\\r\\n15896,0\\r\\n15897,0\\r\\n15898,0\\r\\n15899,0\\r\\n15900,0\\r\\n15901,0\\r\\n15902,0\\r\\n15903,0\\r\\n15904,0\\r\\n15905,0\\r\\n15906,0\\r\\n15907,0\\r\\n15908,0\\r\\n15909,0\\r\\n15910,0\\r\\n15911,0\\r\\n15912,0\\r\\n15913,0\\r\\n15914,0\\r\\n15915,0\\r\\n15916,0\\r\\n15917,0\\r\\n15918,0\\r\\n15919,0\\r\\n15920,0\\r\\n15921,0\\r\\n15922,0\\r\\n15923,0\\r\\n15924,0\\r\\n15925,0\\r\\n15926,0\\r\\n15927,0\\r\\n15928,0\\r\\n15929,0\\r\\n15930,0\\r\\n15931,0\\r\\n15932,0\\r\\n15933,0\\r\\n15934,0\\r\\n15935,0\\r\\n15936,0\\r\\n15937,0\\r\\n15938,0\\r\\n15939,0\\r\\n15940,0\\r\\n15941,0\\r\\n15942,0\\r\\n15943,0\\r\\n15944,0\\r\\n15945,0\\r\\n15946,0\\r\\n15947,0\\r\\n15948,0\\r\\n15949,0\\r\\n15950,0\\r\\n15951,0\\r\\n15952,0\\r\\n15953,0\\r\\n15954,0\\r\\n15955,0\\r\\n15956,0\\r\\n15957,0\\r\\n15958,0\\r\\n15959,0\\r\\n15960,0\\r\\n15961,0\\r\\n15962,0\\r\\n15963,0\\r\\n15964,0\\r\\n15965,0\\r\\n15966,0\\r\\n15967,0\\r\\n15968,0\\r\\n15969,0\\r\\n15970,0\\r\\n15971,0\\r\\n15972,0\\r\\n15973,0\\r\\n15974,0\\r\\n15975,0\\r\\n15976,0\\r\\n15977,0\\r\\n15978,0\\r\\n15979,0\\r\\n15980,0\\r\\n15981,0\\r\\n15982,0\\r\\n15983,0\\r\\n15984,0\\r\\n15985,0\\r\\n15986,0\\r\\n15987,0\\r\\n15988,0\\r\\n15989,0\\r\\n15990,0\\r\\n15991,0\\r\\n15992,0\\r\\n15993,0\\r\\n15994,0\\r\\n15995,0\\r\\n15996,0\\r\\n15997,0\\r\\n15998,0\\r\\n15999,0\\r\\n16000,0\\r\\n16001,0\\r\\n16002,0\\r\\n16003,0\\r\\n16004,0\\r\\n16005,0\\r\\n16006,0\\r\\n16007,0\\r\\n16008,0\\r\\n16009,0\\r\\n16010,0\\r\\n16011,0\\r\\n16012,0\\r\\n16013,0\\r\\n16014,0\\r\\n16015,0\\r\\n16016,0\\r\\n16017,0\\r\\n16018,0\\r\\n16019,0\\r\\n16020,0\\r\\n16021,0\\r\\n16022,0\\r\\n16023,0\\r\\n16024,0\\r\\n16025,0\\r\\n16026,0\\r\\n16027,0\\r\\n16028,0\\r\\n16029,0\\r\\n16030,0\\r\\n16031,0\\r\\n16032,0\\r\\n16033,0\\r\\n16034,0\\r\\n16035,0\\r\\n16036,0\\r\\n16037,0\\r\\n16038,0\\r\\n16039,0\\r\\n16040,0\\r\\n16041,0\\r\\n16042,0\\r\\n16043,0\\r\\n16044,0\\r\\n16045,0\\r\\n16046,0\\r\\n16047,0\\r\\n16048,0\\r\\n16049,0\\r\\n16050,0\\r\\n16051,0\\r\\n16052,0\\r\\n16053,0\\r\\n16054,0\\r\\n16055,0\\r\\n16056,0\\r\\n16057,0\\r\\n16058,0\\r\\n16059,0\\r\\n16060,0\\r\\n16061,0\\r\\n16062,0\\r\\n16063,0\\r\\n16064,0\\r\\n16065,0\\r\\n16066,0\\r\\n16067,0\\r\\n16068,0\\r\\n16069,0\\r\\n16070,0\\r\\n16071,0\\r\\n16072,0\\r\\n16073,0\\r\\n16074,0\\r\\n16075,0\\r\\n16076,0\\r\\n16077,0\\r\\n16078,0\\r\\n16079,0\\r\\n16080,0\\r\\n16081,0\\r\\n16082,0\\r\\n16083,0\\r\\n16084,0\\r\\n16085,0\\r\\n16086,0\\r\\n16087,0\\r\\n16088,0\\r\\n16089,0\\r\\n16090,0\\r\\n16091,0\\r\\n16092,0\\r\\n16093,0\\r\\n16094,0\\r\\n16095,0\\r\\n16096,0\\r\\n16097,0\\r\\n16098,0\\r\\n16099,0\\r\\n16100,0\\r\\n16101,0\\r\\n16102,0\\r\\n16103,0\\r\\n16104,0\\r\\n16105,0\\r\\n16106,0\\r\\n16107,0\\r\\n16108,0\\r\\n16109,0\\r\\n16110,0\\r\\n16111,0\\r\\n16112,0\\r\\n16113,0\\r\\n16114,0\\r\\n16115,0\\r\\n16116,0\\r\\n16117,0\\r\\n16118,0\\r\\n16119,0\\r\\n16120,0\\r\\n16121,0\\r\\n16122,0\\r\\n16123,0\\r\\n16124,0\\r\\n16125,0\\r\\n16126,0\\r\\n16127,0\\r\\n16128,0\\r\\n16129,0\\r\\n16130,0\\r\\n16131,0\\r\\n16132,0\\r\\n16133,0\\r\\n16134,0\\r\\n16135,0\\r\\n16136,0\\r\\n16137,0\\r\\n16138,0\\r\\n16139,0\\r\\n16140,0\\r\\n16141,0\\r\\n16142,0\\r\\n16143,0\\r\\n16144,0\\r\\n16145,0\\r\\n16146,0\\r\\n16147,0\\r\\n16148,0\\r\\n16149,0\\r\\n16150,0\\r\\n16151,0\\r\\n16152,0\\r\\n16153,0\\r\\n16154,0\\r\\n16155,0\\r\\n16156,0\\r\\n16157,0\\r\\n16158,0\\r\\n16159,0\\r\\n16160,0\\r\\n16161,0\\r\\n16162,0\\r\\n16163,0\\r\\n16164,0\\r\\n16165,0\\r\\n16166,0\\r\\n16167,0\\r\\n16168,0\\r\\n16169,0\\r\\n16170,0\\r\\n16171,0\\r\\n16172,0\\r\\n16173,0\\r\\n16174,0\\r\\n16175,0\\r\\n16176,0\\r\\n16177,0\\r\\n16178,0\\r\\n16179,0\\r\\n16180,0\\r\\n16181,0\\r\\n16182,0\\r\\n16183,0\\r\\n16184,0\\r\\n16185,0\\r\\n16186,0\\r\\n16187,0\\r\\n16188,0\\r\\n16189,0\\r\\n16190,0\\r\\n16191,0\\r\\n16192,0\\r\\n16193,0\\r\\n16194,0\\r\\n16195,0\\r\\n16196,0\\r\\n16197,0\\r\\n16198,0\\r\\n16199,0\\r\\n16200,0\\r\\n16201,0\\r\\n16202,0\\r\\n16203,0\\r\\n16204,0\\r\\n16205,0\\r\\n16206,0\\r\\n16207,0\\r\\n16208,0\\r\\n16209,0\\r\\n16210,0\\r\\n16211,0\\r\\n16212,0\\r\\n16213,0\\r\\n16214,0\\r\\n16215,0\\r\\n16216,0\\r\\n16217,0\\r\\n16218,0\\r\\n16219,0\\r\\n16220,0\\r\\n16221,0\\r\\n16222,0\\r\\n16223,0\\r\\n16224,0\\r\\n16225,0\\r\\n16226,0\\r\\n16227,0\\r\\n16228,0\\r\\n16229,0\\r\\n16230,0\\r\\n16231,0\\r\\n16232,0\\r\\n16233,0\\r\\n16234,0\\r\\n16235,0\\r\\n16236,0\\r\\n16237,0\\r\\n16238,0\\r\\n16239,0\\r\\n16240,0\\r\\n16241,0\\r\\n16242,0\\r\\n16243,0\\r\\n16244,0\\r\\n16245,0\\r\\n16246,0\\r\\n16247,0\\r\\n16248,0\\r\\n16249,0\\r\\n16250,0\\r\\n16251,0\\r\\n16252,0\\r\\n16253,0\\r\\n16254,0\\r\\n16255,0\\r\\n16256,0\\r\\n16257,0\\r\\n16258,0\\r\\n16259,0\\r\\n16260,0\\r\\n16261,0\\r\\n16262,0\\r\\n16263,0\\r\\n16264,0\\r\\n16265,0\\r\\n16266,0\\r\\n16267,0\\r\\n16268,0\\r\\n16269,0\\r\\n16270,0\\r\\n16271,0\\r\\n16272,0\\r\\n16273,0\\r\\n16274,0\\r\\n16275,0\\r\\n16276,0\\r\\n16277,0\\r\\n16278,0\\r\\n16279,0\\r\\n16280,0\\r\\n16281,0\\r\\n16282,0\\r\\n16283,0\\r\\n16284,0\\r\\n16285,0\\r\\n16286,0\\r\\n16287,0\\r\\n16288,0\\r\\n16289,0\\r\\n16290,0\\r\\n16291,0\\r\\n16292,0\\r\\n16293,0\\r\\n16294,0\\r\\n16295,0\\r\\n16296,0\\r\\n16297,0\\r\\n16298,0\\r\\n16299,0\\r\\n16300,0\\r\\n16301,0\\r\\n16302,0\\r\\n16303,0\\r\\n16304,0\\r\\n16305,0\\r\\n16306,0\\r\\n16307,0\\r\\n16308,0\\r\\n16309,0\\r\\n16310,0\\r\\n16311,0\\r\\n16312,0\\r\\n16313,0\\r\\n16314,0\\r\\n16315,0\\r\\n16316,0\\r\\n16317,0\\r\\n16318,0\\r\\n16319,0\\r\\n16320,0\\r\\n16321,0\\r\\n16322,0\\r\\n16323,0\\r\\n16324,0\\r\\n16325,0\\r\\n16326,0\\r\\n16327,0\\r\\n16328,0\\r\\n16329,0\\r\\n16330,0\\r\\n16331,0\\r\\n16332,0\\r\\n16333,0\\r\\n16334,0\\r\\n16335,0\\r\\n16336,0\\r\\n16337,0\\r\\n16338,0\\r\\n16339,0\\r\\n16340,0\\r\\n16341,0\\r\\n16342,0\\r\\n16343,0\\r\\n16344,0\\r\\n16345,0\\r\\n16346,0\\r\\n16347,0\\r\\n16348,0\\r\\n16349,0\\r\\n16350,0\\r\\n16351,0\\r\\n16352,0\\r\\n16353,0\\r\\n16354,0\\r\\n16355,0\\r\\n16356,0\\r\\n16357,0\\r\\n16358,0\\r\\n16359,0\\r\\n16360,0\\r\\n16361,0\\r\\n16362,0\\r\\n16363,0\\r\\n16364,0\\r\\n16365,0\\r\\n16366,0\\r\\n16367,0\\r\\n16368,0\\r\\n16369,0\\r\\n16370,0\\r\\n16371,0\\r\\n16372,0\\r\\n16373,0\\r\\n16374,0\\r\\n16375,0\\r\\n16376,0\\r\\n16377,0\\r\\n16378,0\\r\\n16379,0\\r\\n16380,0\\r\\n16381,0\\r\\n16382,0\\r\\n16383,0\\r\\n16384,0\\r\\n16385,0\\r\\n16386,0\\r\\n16387,0\\r\\n16388,0\\r\\n16389,0\\r\\n16390,0\\r\\n16391,0\\r\\n16392,0\\r\\n16393,0\\r\\n16394,0\\r\\n16395,0\\r\\n16396,0\\r\\n16397,0\\r\\n16398,0\\r\\n16399,0\\r\\n16400,0\\r\\n16401,0\\r\\n16402,0\\r\\n16403,0\\r\\n16404,0\\r\\n16405,0\\r\\n16406,0\\r\\n16407,0\\r\\n16408,0\\r\\n16409,0\\r\\n16410,0\\r\\n16411,0\\r\\n16412,0\\r\\n16413,0\\r\\n16414,0\\r\\n16415,0\\r\\n16416,0\\r\\n16417,0\\r\\n16418,0\\r\\n16419,0\\r\\n16420,0\\r\\n16421,0\\r\\n16422,0\\r\\n16423,0\\r\\n16424,0\\r\\n16425,0\\r\\n16426,0\\r\\n16427,0\\r\\n16428,0\\r\\n16429,0\\r\\n16430,0\\r\\n16431,0\\r\\n16432,0\\r\\n16433,0\\r\\n16434,0\\r\\n16435,0\\r\\n16436,0\\r\\n16437,0\\r\\n16438,0\\r\\n16439,0\\r\\n16440,0\\r\\n16441,0\\r\\n16442,0\\r\\n16443,0\\r\\n16444,0\\r\\n16445,0\\r\\n16446,0\\r\\n16447,0\\r\\n16448,0\\r\\n16449,0\\r\\n16450,0\\r\\n16451,0\\r\\n16452,0\\r\\n16453,0\\r\\n16454,0\\r\\n16455,0\\r\\n16456,0\\r\\n16457,0\\r\\n16458,0\\r\\n16459,0\\r\\n16460,0\\r\\n16461,0\\r\\n16462,0\\r\\n16463,0\\r\\n16464,0\\r\\n16465,0\\r\\n16466,0\\r\\n16467,0\\r\\n16468,0\\r\\n16469,0\\r\\n16470,0\\r\\n16471,0\\r\\n16472,0\\r\\n16473,0\\r\\n16474,0\\r\\n16475,0\\r\\n16476,0\\r\\n16477,0\\r\\n16478,0\\r\\n16479,0\\r\\n16480,0\\r\\n16481,0\\r\\n16482,0\\r\\n16483,0\\r\\n16484,0\\r\\n16485,0\\r\\n16486,0\\r\\n16487,0\\r\\n16488,0\\r\\n16489,0\\r\\n16490,0\\r\\n16491,0\\r\\n16492,0\\r\\n16493,0\\r\\n16494,0\\r\\n16495,0\\r\\n16496,0\\r\\n16497,0\\r\\n16498,0\\r\\n16499,0\\r\\n16500,0\\r\\n16501,0\\r\\n16502,0\\r\\n16503,0\\r\\n16504,0\\r\\n16505,0\\r\\n16506,0\\r\\n16507,0\\r\\n16508,0\\r\\n16509,0\\r\\n16510,0\\r\\n16511,0\\r\\n16512,0\\r\\n16513,0\\r\\n16514,0\\r\\n16515,0\\r\\n16516,0\\r\\n16517,0\\r\\n16518,0\\r\\n16519,0\\r\\n16520,0\\r\\n16521,0\\r\\n16522,0\\r\\n16523,0\\r\\n16524,0\\r\\n16525,0\\r\\n16526,0\\r\\n16527,0\\r\\n16528,0\\r\\n16529,0\\r\\n16530,0\\r\\n16531,0\\r\\n16532,0\\r\\n16533,0\\r\\n16534,0\\r\\n16535,0\\r\\n16536,0\\r\\n16537,0\\r\\n16538,0\\r\\n16539,0\\r\\n16540,0\\r\\n16541,0\\r\\n16542,0\\r\\n16543,0\\r\\n16544,0\\r\\n16545,0\\r\\n16546,0\\r\\n16547,0\\r\\n16548,0\\r\\n16549,0\\r\\n16550,0\\r\\n16551,0\\r\\n16552,0\\r\\n16553,0\\r\\n16554,0\\r\\n16555,0\\r\\n16556,0\\r\\n16557,0\\r\\n16558,0\\r\\n16559,0\\r\\n16560,0\\r\\n16561,0\\r\\n16562,0\\r\\n16563,0\\r\\n16564,0\\r\\n16565,0\\r\\n16566,0\\r\\n16567,0\\r\\n16568,0\\r\\n16569,0\\r\\n16570,0\\r\\n16571,0\\r\\n16572,0\\r\\n16573,0\\r\\n16574,0\\r\\n16575,0\\r\\n16576,0\\r\\n16577,0\\r\\n16578,0\\r\\n16579,0\\r\\n16580,0\\r\\n16581,0\\r\\n16582,0\\r\\n16583,0\\r\\n16584,0\\r\\n16585,0\\r\\n16586,0\\r\\n16587,0\\r\\n16588,0\\r\\n16589,0\\r\\n16590,0\\r\\n16591,0\\r\\n16592,0\\r\\n16593,0\\r\\n16594,0\\r\\n16595,0\\r\\n16596,0\\r\\n16597,0\\r\\n16598,0\\r\\n16599,0\\r\\n16600,0\\r\\n16601,0\\r\\n16602,0\\r\\n16603,0\\r\\n16604,0\\r\\n16605,0\\r\\n16606,0\\r\\n16607,0\\r\\n16608,0\\r\\n16609,0\\r\\n16610,0\\r\\n16611,0\\r\\n16612,0\\r\\n16613,0\\r\\n16614,0\\r\\n16615,0\\r\\n16616,0\\r\\n16617,0\\r\\n16618,0\\r\\n16619,0\\r\\n16620,0\\r\\n16621,0\\r\\n16622,0\\r\\n16623,0\\r\\n16624,0\\r\\n16625,0\\r\\n16626,0\\r\\n16627,0\\r\\n16628,0\\r\\n16629,0\\r\\n16630,0\\r\\n16631,0\\r\\n16632,0\\r\\n16633,0\\r\\n16634,0\\r\\n16635,0\\r\\n16636,0\\r\\n16637,0\\r\\n16638,0\\r\\n16639,0\\r\\n16640,0\\r\\n16641,0\\r\\n16642,0\\r\\n16643,0\\r\\n16644,0\\r\\n16645,0\\r\\n16646,0\\r\\n16647,0\\r\\n16648,0\\r\\n16649,0\\r\\n16650,0\\r\\n16651,0\\r\\n16652,0\\r\\n16653,0\\r\\n16654,0\\r\\n16655,0\\r\\n16656,0\\r\\n16657,0\\r\\n16658,0\\r\\n16659,0\\r\\n16660,0\\r\\n16661,0\\r\\n16662,0\\r\\n16663,0\\r\\n16664,0\\r\\n16665,0\\r\\n16666,0\\r\\n16667,0\\r\\n16668,0\\r\\n16669,0\\r\\n16670,0\\r\\n16671,0\\r\\n16672,0\\r\\n16673,0\\r\\n16674,0\\r\\n16675,0\\r\\n16676,0\\r\\n16677,0\\r\\n16678,0\\r\\n16679,0\\r\\n16680,0\\r\\n16681,0\\r\\n16682,0\\r\\n16683,0\\r\\n16684,0\\r\\n16685,0\\r\\n16686,0\\r\\n16687,0\\r\\n16688,0\\r\\n16689,0\\r\\n16690,0\\r\\n16691,0\\r\\n16692,0\\r\\n16693,0\\r\\n16694,0\\r\\n16695,0\\r\\n16696,0\\r\\n16697,0\\r\\n16698,0\\r\\n16699,0\\r\\n16700,0\\r\\n16701,0\\r\\n16702,0\\r\\n16703,0\\r\\n16704,0\\r\\n16705,0\\r\\n16706,0\\r\\n16707,0\\r\\n16708,0\\r\\n16709,0\\r\\n16710,0\\r\\n16711,0\\r\\n16712,0\\r\\n16713,0\\r\\n16714,0\\r\\n16715,0\\r\\n16716,0\\r\\n16717,0\\r\\n16718,0\\r\\n16719,0\\r\\n16720,0\\r\\n16721,0\\r\\n16722,0\\r\\n16723,0\\r\\n16724,0\\r\\n16725,0\\r\\n16726,0\\r\\n16727,0\\r\\n16728,0\\r\\n16729,0\\r\\n16730,0\\r\\n16731,0\\r\\n16732,0\\r\\n16733,0\\r\\n16734,0\\r\\n16735,0\\r\\n16736,0\\r\\n16737,0\\r\\n16738,0\\r\\n16739,0\\r\\n16740,0\\r\\n16741,0\\r\\n16742,0\\r\\n16743,0\\r\\n16744,0\\r\\n16745,0\\r\\n16746,0\\r\\n16747,0\\r\\n16748,0\\r\\n16749,0\\r\\n16750,0\\r\\n16751,0\\r\\n16752,0\\r\\n16753,0\\r\\n16754,0\\r\\n16755,0\\r\\n16756,0\\r\\n16757,0\\r\\n16758,0\\r\\n16759,0\\r\\n16760,0\\r\\n16761,0\\r\\n16762,0\\r\\n16763,0\\r\\n16764,0\\r\\n16765,0\\r\\n16766,0\\r\\n16767,0\\r\\n16768,0\\r\\n16769,0\\r\\n16770,0\\r\\n16771,0\\r\\n16772,0\\r\\n16773,0\\r\\n16774,0\\r\\n16775,0\\r\\n16776,0\\r\\n16777,0\\r\\n16778,0\\r\\n16779,0\\r\\n16780,0\\r\\n16781,0\\r\\n16782,0\\r\\n16783,0\\r\\n16784,0\\r\\n16785,0\\r\\n16786,0\\r\\n16787,0\\r\\n16788,0\\r\\n16789,0\\r\\n16790,0\\r\\n16791,0\\r\\n16792,0\\r\\n16793,0\\r\\n16794,0\\r\\n16795,0\\r\\n16796,0\\r\\n16797,0\\r\\n16798,0\\r\\n16799,0\\r\\n16800,0\\r\\n16801,0\\r\\n16802,0\\r\\n16803,0\\r\\n16804,0\\r\\n16805,0\\r\\n16806,0\\r\\n16807,0\\r\\n16808,0\\r\\n16809,0\\r\\n16810,0\\r\\n16811,0\\r\\n16812,0\\r\\n16813,0\\r\\n16814,0\\r\\n16815,0\\r\\n16816,0\\r\\n16817,0\\r\\n16818,0\\r\\n16819,0\\r\\n16820,0\\r\\n16821,0\\r\\n16822,0\\r\\n16823,0\\r\\n16824,0\\r\\n16825,0\\r\\n16826,0\\r\\n16827,0\\r\\n16828,0\\r\\n16829,0\\r\\n16830,0\\r\\n16831,0\\r\\n16832,0\\r\\n16833,0\\r\\n16834,0\\r\\n16835,0\\r\\n16836,0\\r\\n16837,0\\r\\n16838,0\\r\\n16839,0\\r\\n16840,0\\r\\n16841,0\\r\\n16842,0\\r\\n16843,0\\r\\n16844,0\\r\\n16845,0\\r\\n16846,0\\r\\n16847,0\\r\\n16848,0\\r\\n16849,0\\r\\n16850,0\\r\\n16851,0\\r\\n16852,0\\r\\n16853,0\\r\\n16854,0\\r\\n16855,0\\r\\n16856,0\\r\\n16857,0\\r\\n16858,0\\r\\n16859,0\\r\\n16860,0\\r\\n16861,0\\r\\n16862,0\\r\\n16863,0\\r\\n16864,0\\r\\n16865,0\\r\\n16866,0\\r\\n16867,0\\r\\n16868,0\\r\\n16869,0\\r\\n16870,0\\r\\n16871,0\\r\\n16872,0\\r\\n16873,0\\r\\n16874,0\\r\\n16875,0\\r\\n16876,0\\r\\n16877,0\\r\\n16878,0\\r\\n16879,0\\r\\n16880,0\\r\\n16881,0\\r\\n16882,0\\r\\n16883,0\\r\\n16884,0\\r\\n16885,0\\r\\n16886,0\\r\\n16887,0\\r\\n16888,0\\r\\n16889,0\\r\\n16890,0\\r\\n16891,0\\r\\n16892,0\\r\\n16893,0\\r\\n16894,0\\r\\n16895,0\\r\\n16896,0\\r\\n16897,0\\r\\n16898,0\\r\\n16899,0\\r\\n16900,0\\r\\n16901,0\\r\\n16902,0\\r\\n16903,0\\r\\n16904,0\\r\\n16905,0\\r\\n16906,0\\r\\n16907,0\\r\\n16908,0\\r\\n16909,0\\r\\n16910,0\\r\\n16911,0\\r\\n16912,0\\r\\n16913,0\\r\\n16914,0\\r\\n16915,0\\r\\n16916,0\\r\\n16917,0\\r\\n16918,0\\r\\n16919,0\\r\\n16920,0\\r\\n16921,0\\r\\n16922,0\\r\\n16923,0\\r\\n16924,0\\r\\n16925,0\\r\\n16926,0\\r\\n16927,0\\r\\n16928,0\\r\\n16929,0\\r\\n16930,0\\r\\n16931,0\\r\\n16932,0\\r\\n16933,0\\r\\n16934,0\\r\\n16935,0\\r\\n16936,0\\r\\n16937,0\\r\\n16938,0\\r\\n16939,0\\r\\n16940,0\\r\\n16941,0\\r\\n16942,0\\r\\n16943,0\\r\\n16944,0\\r\\n16945,0\\r\\n16946,0\\r\\n16947,0\\r\\n16948,0\\r\\n16949,0\\r\\n16950,0\\r\\n16951,0\\r\\n16952,0\\r\\n16953,0\\r\\n16954,0\\r\\n16955,0\\r\\n16956,0\\r\\n16957,0\\r\\n16958,0\\r\\n16959,0\\r\\n16960,0\\r\\n16961,0\\r\\n16962,0\\r\\n16963,0\\r\\n16964,0\\r\\n16965,0\\r\\n16966,0\\r\\n16967,0\\r\\n16968,0\\r\\n16969,0\\r\\n16970,0\\r\\n16971,0\\r\\n16972,0\\r\\n16973,0\\r\\n16974,0\\r\\n16975,0\\r\\n16976,0\\r\\n16977,0\\r\\n16978,0\\r\\n16979,0\\r\\n16980,0\\r\\n16981,0\\r\\n16982,0\\r\\n16983,0\\r\\n16984,0\\r\\n16985,0\\r\\n16986,0\\r\\n16987,0\\r\\n16988,0\\r\\n16989,0\\r\\n16990,0\\r\\n16991,0\\r\\n16992,0\\r\\n16993,0\\r\\n16994,0\\r\\n16995,0\\r\\n16996,0\\r\\n16997,0\\r\\n16998,0\\r\\n16999,0\\r\\n17000,0\\r\\n17001,0\\r\\n17002,0\\r\\n17003,0\\r\\n17004,0\\r\\n17005,0\\r\\n17006,0\\r\\n17007,0\\r\\n17008,0\\r\\n17009,0\\r\\n17010,0\\r\\n17011,0\\r\\n17012,0\\r\\n17013,0\\r\\n17014,0\\r\\n17015,0\\r\\n17016,0\\r\\n17017,0\\r\\n17018,0\\r\\n17019,0\\r\\n17020,0\\r\\n17021,0\\r\\n17022,0\\r\\n17023,0\\r\\n17024,0\\r\\n17025,0\\r\\n17026,0\\r\\n17027,0\\r\\n17028,0\\r\\n17029,0\\r\\n17030,0\\r\\n17031,0\\r\\n17032,0\\r\\n17033,0\\r\\n17034,0\\r\\n17035,0\\r\\n17036,0\\r\\n17037,0\\r\\n17038,0\\r\\n17039,0\\r\\n17040,0\\r\\n17041,0\\r\\n17042,0\\r\\n17043,0\\r\\n17044,0\\r\\n17045,0\\r\\n17046,0\\r\\n17047,0\\r\\n17048,0\\r\\n17049,0\\r\\n17050,0\\r\\n17051,0\\r\\n17052,0\\r\\n17053,0\\r\\n17054,0\\r\\n17055,0\\r\\n17056,0\\r\\n17057,0\\r\\n17058,0\\r\\n17059,0\\r\\n17060,0\\r\\n17061,0\\r\\n17062,0\\r\\n17063,0\\r\\n17064,0\\r\\n17065,0\\r\\n17066,0\\r\\n17067,0\\r\\n17068,0\\r\\n17069,0\\r\\n17070,0\\r\\n17071,0\\r\\n17072,0\\r\\n17073,0\\r\\n17074,0\\r\\n17075,0\\r\\n17076,0\\r\\n17077,0\\r\\n17078,0\\r\\n17079,0\\r\\n17080,0\\r\\n17081,0\\r\\n17082,0\\r\\n17083,0\\r\\n17084,0\\r\\n17085,0\\r\\n17086,0\\r\\n17087,0\\r\\n17088,0\\r\\n17089,0\\r\\n17090,0\\r\\n17091,0\\r\\n17092,0\\r\\n17093,0\\r\\n17094,0\\r\\n17095,0\\r\\n17096,0\\r\\n17097,0\\r\\n17098,0\\r\\n17099,0\\r\\n17100,0\\r\\n17101,0\\r\\n17102,0\\r\\n17103,0\\r\\n17104,0\\r\\n17105,0\\r\\n17106,0\\r\\n17107,0\\r\\n17108,0\\r\\n17109,0\\r\\n17110,0\\r\\n17111,0\\r\\n17112,0\\r\\n17113,0\\r\\n17114,0\\r\\n17115,0\\r\\n17116,0\\r\\n17117,0\\r\\n17118,0\\r\\n17119,0\\r\\n17120,0\\r\\n17121,0\\r\\n17122,0\\r\\n17123,0\\r\\n17124,0\\r\\n17125,0\\r\\n17126,0\\r\\n17127,0\\r\\n17128,0\\r\\n17129,0\\r\\n17130,0\\r\\n17131,0\\r\\n17132,0\\r\\n17133,0\\r\\n17134,0\\r\\n17135,0\\r\\n17136,0\\r\\n17137,0\\r\\n17138,0\\r\\n17139,0\\r\\n17140,0\\r\\n17141,0\\r\\n17142,0\\r\\n17143,0\\r\\n17144,0\\r\\n17145,0\\r\\n17146,0\\r\\n17147,0\\r\\n17148,0\\r\\n17149,0\\r\\n17150,0\\r\\n17151,0\\r\\n17152,0\\r\\n17153,0\\r\\n17154,0\\r\\n17155,0\\r\\n17156,0\\r\\n17157,0\\r\\n17158,0\\r\\n17159,0\\r\\n17160,0\\r\\n17161,0\\r\\n17162,0\\r\\n17163,0\\r\\n17164,0\\r\\n17165,0\\r\\n17166,0\\r\\n17167,0\\r\\n17168,0\\r\\n17169,0\\r\\n17170,0\\r\\n17171,0\\r\\n17172,0\\r\\n17173,0\\r\\n17174,0\\r\\n17175,0\\r\\n17176,0\\r\\n17177,0\\r\\n17178,0\\r\\n17179,0\\r\\n17180,0\\r\\n17181,0\\r\\n17182,0\\r\\n17183,0\\r\\n17184,0\\r\\n17185,0\\r\\n17186,0\\r\\n17187,0\\r\\n17188,0\\r\\n17189,0\\r\\n17190,0\\r\\n17191,0\\r\\n17192,0\\r\\n17193,0\\r\\n17194,0\\r\\n17195,0\\r\\n17196,0\\r\\n17197,0\\r\\n17198,0\\r\\n17199,0\\r\\n17200,0\\r\\n17201,0\\r\\n17202,0\\r\\n17203,0\\r\\n17204,0\\r\\n17205,0\\r\\n17206,0\\r\\n17207,0\\r\\n17208,0\\r\\n17209,0\\r\\n17210,0\\r\\n17211,0\\r\\n17212,0\\r\\n17213,0\\r\\n17214,0\\r\\n17215,0\\r\\n17216,0\\r\\n17217,0\\r\\n17218,0\\r\\n17219,0\\r\\n17220,0\\r\\n17221,0\\r\\n17222,0\\r\\n17223,0\\r\\n17224,0\\r\\n17225,0\\r\\n17226,0\\r\\n17227,0\\r\\n17228,0\\r\\n17229,0\\r\\n17230,0\\r\\n17231,0\\r\\n17232,0\\r\\n17233,0\\r\\n17234,0\\r\\n17235,0\\r\\n17236,0\\r\\n17237,0\\r\\n17238,0\\r\\n17239,0\\r\\n17240,0\\r\\n17241,0\\r\\n17242,0\\r\\n17243,0\\r\\n17244,0\\r\\n17245,0\\r\\n17246,0\\r\\n17247,0\\r\\n17248,0\\r\\n17249,0\\r\\n17250,0\\r\\n17251,0\\r\\n17252,0\\r\\n17253,0\\r\\n17254,0\\r\\n17255,0\\r\\n17256,0\\r\\n17257,0\\r\\n17258,0\\r\\n17259,0\\r\\n17260,0\\r\\n17261,0\\r\\n17262,0\\r\\n17263,0\\r\\n17264,0\\r\\n17265,0\\r\\n17266,0\\r\\n17267,0\\r\\n17268,0\\r\\n17269,0\\r\\n17270,0\\r\\n17271,0\\r\\n17272,0\\r\\n17273,0\\r\\n17274,0\\r\\n17275,0\\r\\n17276,0\\r\\n17277,0\\r\\n17278,0\\r\\n17279,0\\r\\n17280,0\\r\\n17281,0\\r\\n17282,0\\r\\n17283,0\\r\\n17284,0\\r\\n17285,0\\r\\n17286,0\\r\\n17287,0\\r\\n17288,0\\r\\n17289,0\\r\\n17290,0\\r\\n17291,0\\r\\n17292,0\\r\\n17293,0\\r\\n17294,0\\r\\n17295,0\\r\\n17296,0\\r\\n17297,0\\r\\n17298,0\\r\\n17299,0\\r\\n17300,0\\r\\n17301,0\\r\\n17302,0\\r\\n17303,0\\r\\n17304,0\\r\\n17305,0\\r\\n17306,0\\r\\n17307,0\\r\\n17308,0\\r\\n17309,0\\r\\n17310,0\\r\\n17311,0\\r\\n17312,0\\r\\n17313,0\\r\\n17314,0\\r\\n17315,0\\r\\n17316,0\\r\\n17317,0\\r\\n17318,0\\r\\n17319,0\\r\\n17320,0\\r\\n17321,0\\r\\n17322,0\\r\\n17323,0\\r\\n17324,0\\r\\n17325,0\\r\\n17326,0\\r\\n17327,0\\r\\n17328,0\\r\\n17329,0\\r\\n17330,0\\r\\n17331,0\\r\\n17332,0\\r\\n17333,0\\r\\n17334,0\\r\\n17335,0\\r\\n17336,0\\r\\n17337,0\\r\\n17338,0\\r\\n17339,0\\r\\n17340,0\\r\\n17341,0\\r\\n17342,0\\r\\n17343,0\\r\\n17344,0\\r\\n17345,0\\r\\n17346,0\\r\\n17347,0\\r\\n17348,0\\r\\n17349,0\\r\\n17350,0\\r\\n17351,0\\r\\n17352,0\\r\\n17353,0\\r\\n17354,0\\r\\n17355,0\\r\\n17356,0\\r\\n17357,0\\r\\n17358,0\\r\\n17359,0\\r\\n17360,0\\r\\n17361,0\\r\\n17362,0\\r\\n17363,0\\r\\n17364,0\\r\\n17365,0\\r\\n17366,0\\r\\n17367,0\\r\\n17368,0\\r\\n17369,0\\r\\n17370,0\\r\\n17371,0\\r\\n17372,0\\r\\n17373,0\\r\\n17374,0\\r\\n17375,0\\r\\n17376,0\\r\\n17377,0\\r\\n17378,0\\r\\n17379,0\\r\\n17380,0\\r\\n17381,0\\r\\n17382,0\\r\\n17383,0\\r\\n17384,0\\r\\n17385,0\\r\\n17386,0\\r\\n17387,0\\r\\n17388,0\\r\\n17389,0\\r\\n17390,0\\r\\n17391,0\\r\\n17392,0\\r\\n17393,0\\r\\n17394,0\\r\\n17395,0\\r\\n17396,0\\r\\n17397,0\\r\\n17398,0\\r\\n17399,0\\r\\n17400,0\\r\\n17401,0\\r\\n17402,0\\r\\n17403,0\\r\\n17404,0\\r\\n17405,0\\r\\n17406,0\\r\\n17407,0\\r\\n17408,0\\r\\n17409,0\\r\\n17410,0\\r\\n17411,0\\r\\n17412,0\\r\\n17413,0\\r\\n17414,0\\r\\n17415,0\\r\\n17416,0\\r\\n17417,0\\r\\n17418,0\\r\\n17419,0\\r\\n17420,0\\r\\n17421,0\\r\\n17422,0\\r\\n17423,0\\r\\n17424,0\\r\\n17425,0\\r\\n17426,0\\r\\n17427,0\\r\\n17428,0\\r\\n17429,0\\r\\n17430,0\\r\\n17431,0\\r\\n17432,0\\r\\n17433,0\\r\\n17434,0\\r\\n17435,0\\r\\n17436,0\\r\\n17437,0\\r\\n17438,0\\r\\n17439,0\\r\\n17440,0\\r\\n17441,0\\r\\n17442,0\\r\\n17443,0\\r\\n17444,0\\r\\n17445,0\\r\\n17446,0\\r\\n17447,0\\r\\n17448,0\\r\\n17449,0\\r\\n17450,0\\r\\n17451,0\\r\\n17452,0\\r\\n17453,0\\r\\n17454,0\\r\\n17455,0\\r\\n17456,0\\r\\n17457,0\\r\\n17458,0\\r\\n17459,0\\r\\n17460,0\\r\\n17461,0\\r\\n17462,0\\r\\n17463,0\\r\\n17464,0\\r\\n17465,0\\r\\n17466,0\\r\\n17467,0\\r\\n17468,0\\r\\n17469,0\\r\\n17470,0\\r\\n17471,0\\r\\n17472,0\\r\\n17473,0\\r\\n17474,0\\r\\n17475,0\\r\\n17476,0\\r\\n17477,0\\r\\n17478,0\\r\\n17479,0\\r\\n17480,0\\r\\n17481,0\\r\\n17482,0\\r\\n17483,0\\r\\n17484,0\\r\\n17485,0\\r\\n17486,0\\r\\n17487,0\\r\\n17488,0\\r\\n17489,0\\r\\n17490,0\\r\\n17491,0\\r\\n17492,0\\r\\n17493,0\\r\\n17494,0\\r\\n17495,0\\r\\n17496,0\\r\\n17497,0\\r\\n17498,0\\r\\n17499,0\\r\\n17500,0\\r\\n17501,0\\r\\n17502,0\\r\\n17503,0\\r\\n17504,0\\r\\n17505,0\\r\\n17506,0\\r\\n17507,0\\r\\n17508,0\\r\\n17509,0\\r\\n17510,0\\r\\n17511,0\\r\\n17512,0\\r\\n17513,0\\r\\n17514,0\\r\\n17515,0\\r\\n17516,0\\r\\n17517,0\\r\\n17518,0\\r\\n17519,0\\r\\n17520,0\\r\\n17521,0\\r\\n17522,0\\r\\n17523,0\\r\\n17524,0\\r\\n17525,0\\r\\n17526,0\\r\\n17527,0\\r\\n17528,0\\r\\n17529,0\\r\\n17530,0\\r\\n17531,0\\r\\n17532,0\\r\\n17533,0\\r\\n17534,0\\r\\n17535,0\\r\\n17536,0\\r\\n17537,0\\r\\n17538,0\\r\\n17539,0\\r\\n17540,0\\r\\n17541,0\\r\\n17542,0\\r\\n17543,0\\r\\n17544,0\\r\\n17545,0\\r\\n17546,0\\r\\n17547,0\\r\\n17548,0\\r\\n17549,0\\r\\n17550,0\\r\\n17551,0\\r\\n17552,0\\r\\n17553,0\\r\\n17554,0\\r\\n17555,0\\r\\n17556,0\\r\\n17557,0\\r\\n17558,0\\r\\n17559,0\\r\\n17560,0\\r\\n17561,0\\r\\n17562,0\\r\\n17563,0\\r\\n17564,0\\r\\n17565,0\\r\\n17566,0\\r\\n17567,0\\r\\n17568,0\\r\\n17569,0\\r\\n17570,0\\r\\n17571,0\\r\\n17572,0\\r\\n17573,0\\r\\n17574,0\\r\\n17575,0\\r\\n17576,0\\r\\n17577,0\\r\\n17578,0\\r\\n17579,0\\r\\n17580,0\\r\\n17581,0\\r\\n17582,0\\r\\n17583,0\\r\\n17584,0\\r\\n17585,0\\r\\n17586,0\\r\\n17587,0\\r\\n17588,0\\r\\n17589,0\\r\\n17590,0\\r\\n17591,0\\r\\n17592,0\\r\\n17593,0\\r\\n17594,0\\r\\n17595,0\\r\\n17596,0\\r\\n17597,0\\r\\n17598,0\\r\\n17599,0\\r\\n17600,0\\r\\n17601,0\\r\\n17602,0\\r\\n17603,0\\r\\n17604,0\\r\\n17605,0\\r\\n17606,0\\r\\n17607,0\\r\\n17608,0\\r\\n17609,0\\r\\n17610,0\\r\\n17611,0\\r\\n17612,0\\r\\n17613,0\\r\\n17614,0\\r\\n17615,0\\r\\n17616,0\\r\\n17617,0\\r\\n17618,0\\r\\n17619,0\\r\\n17620,0\\r\\n17621,0\\r\\n17622,0\\r\\n17623,0\\r\\n17624,0\\r\\n17625,0\\r\\n17626,0\\r\\n17627,0\\r\\n17628,0\\r\\n17629,0\\r\\n17630,0\\r\\n17631,0\\r\\n17632,0\\r\\n17633,0\\r\\n17634,0\\r\\n17635,0\\r\\n17636,0\\r\\n17637,0\\r\\n17638,0\\r\\n17639,0\\r\\n17640,0\\r\\n17641,0\\r\\n17642,0\\r\\n17643,0\\r\\n17644,0\\r\\n17645,0\\r\\n17646,0\\r\\n17647,0\\r\\n17648,0\\r\\n17649,0\\r\\n17650,0\\r\\n17651,0\\r\\n17652,0\\r\\n17653,0\\r\\n17654,0\\r\\n17655,0\\r\\n17656,0\\r\\n17657,0\\r\\n17658,0\\r\\n17659,0\\r\\n17660,0\\r\\n17661,0\\r\\n17662,0\\r\\n17663,0\\r\\n17664,0\\r\\n17665,0\\r\\n17666,0\\r\\n17667,0\\r\\n17668,0\\r\\n17669,0\\r\\n17670,0\\r\\n17671,0\\r\\n17672,0\\r\\n17673,0\\r\\n17674,0\\r\\n17675,0\\r\\n17676,0\\r\\n17677,0\\r\\n17678,0\\r\\n17679,0\\r\\n17680,0\\r\\n17681,0\\r\\n17682,0\\r\\n17683,0\\r\\n17684,0\\r\\n17685,0\\r\\n17686,0\\r\\n17687,0\\r\\n17688,0\\r\\n17689,0\\r\\n17690,0\\r\\n17691,0\\r\\n17692,0\\r\\n17693,0\\r\\n17694,0\\r\\n17695,0\\r\\n17696,0\\r\\n17697,0\\r\\n17698,0\\r\\n17699,0\\r\\n17700,0\\r\\n17701,0\\r\\n17702,0\\r\\n17703,0\\r\\n17704,0\\r\\n17705,0\\r\\n17706,0\\r\\n17707,0\\r\\n17708,0\\r\\n17709,0\\r\\n17710,0\\r\\n17711,0\\r\\n17712,0\\r\\n17713,0\\r\\n17714,0\\r\\n17715,0\\r\\n17716,0\\r\\n17717,0\\r\\n17718,0\\r\\n17719,0\\r\\n17720,0\\r\\n17721,0\\r\\n17722,0\\r\\n17723,0\\r\\n17724,0\\r\\n17725,0\\r\\n17726,0\\r\\n17727,0\\r\\n17728,0\\r\\n17729,0\\r\\n17730,0\\r\\n17731,0\\r\\n17732,0\\r\\n17733,0\\r\\n17734,0\\r\\n17735,0\\r\\n17736,0\\r\\n17737,0\\r\\n17738,0\\r\\n17739,0\\r\\n17740,0\\r\\n17741,0\\r\\n17742,0\\r\\n17743,0\\r\\n17744,0\\r\\n17745,0\\r\\n17746,0\\r\\n17747,0\\r\\n17748,0\\r\\n17749,0\\r\\n17750,0\\r\\n17751,0\\r\\n17752,0\\r\\n17753,0\\r\\n17754,0\\r\\n17755,0\\r\\n17756,0\\r\\n17757,0\\r\\n17758,0\\r\\n17759,0\\r\\n17760,0\\r\\n17761,0\\r\\n17762,0\\r\\n17763,0\\r\\n17764,0\\r\\n17765,0\\r\\n17766,0\\r\\n17767,0\\r\\n17768,0\\r\\n17769,0\\r\\n17770,0\\r\\n17771,0\\r\\n17772,0\\r\\n17773,0\\r\\n17774,0\\r\\n17775,0\\r\\n17776,0\\r\\n17777,0\\r\\n17778,0\\r\\n17779,0\\r\\n17780,0\\r\\n17781,0\\r\\n17782,0\\r\\n17783,0\\r\\n17784,0\\r\\n17785,0\\r\\n17786,0\\r\\n17787,0\\r\\n17788,0\\r\\n17789,0\\r\\n17790,0\\r\\n17791,0\\r\\n17792,0\\r\\n17793,0\\r\\n17794,0\\r\\n17795,0\\r\\n17796,0\\r\\n17797,0\\r\\n17798,0\\r\\n17799,0\\r\\n17800,0\\r\\n17801,0\\r\\n17802,0\\r\\n17803,0\\r\\n17804,0\\r\\n17805,0\\r\\n17806,0\\r\\n17807,0\\r\\n17808,0\\r\\n17809,0\\r\\n17810,0\\r\\n17811,0\\r\\n17812,0\\r\\n17813,0\\r\\n17814,0\\r\\n17815,0\\r\\n17816,0\\r\\n17817,0\\r\\n17818,0\\r\\n17819,0\\r\\n17820,0\\r\\n17821,0\\r\\n17822,0\\r\\n17823,0\\r\\n17824,0\\r\\n17825,0\\r\\n17826,0\\r\\n17827,0\\r\\n17828,0\\r\\n17829,0\\r\\n17830,0\\r\\n17831,0\\r\\n17832,0\\r\\n17833,0\\r\\n17834,0\\r\\n17835,0\\r\\n17836,0\\r\\n17837,0\\r\\n17838,0\\r\\n17839,0\\r\\n17840,0\\r\\n17841,0\\r\\n17842,0\\r\\n17843,0\\r\\n17844,0\\r\\n17845,0\\r\\n17846,0\\r\\n17847,0\\r\\n17848,0\\r\\n17849,0\\r\\n17850,0\\r\\n17851,0\\r\\n17852,0\\r\\n17853,0\\r\\n17854,0\\r\\n17855,0\\r\\n17856,0\\r\\n17857,0\\r\\n17858,0\\r\\n17859,0\\r\\n17860,0\\r\\n17861,0\\r\\n17862,0\\r\\n17863,0\\r\\n17864,0\\r\\n17865,0\\r\\n17866,0\\r\\n17867,0\\r\\n17868,0\\r\\n17869,0\\r\\n17870,0\\r\\n17871,0\\r\\n17872,0\\r\\n17873,0\\r\\n17874,0\\r\\n17875,0\\r\\n17876,0\\r\\n17877,0\\r\\n17878,0\\r\\n17879,0\\r\\n17880,0\\r\\n17881,0\\r\\n17882,0\\r\\n17883,0\\r\\n17884,0\\r\\n17885,0\\r\\n17886,0\\r\\n17887,0\\r\\n17888,0\\r\\n17889,0\\r\\n17890,0\\r\\n17891,0\\r\\n17892,0\\r\\n17893,0\\r\\n17894,0\\r\\n17895,0\\r\\n17896,0\\r\\n17897,0\\r\\n17898,0\\r\\n17899,0\\r\\n17900,0\\r\\n17901,0\\r\\n17902,0\\r\\n17903,0\\r\\n17904,0\\r\\n17905,0\\r\\n17906,0\\r\\n17907,0\\r\\n17908,0\\r\\n17909,0\\r\\n17910,0\\r\\n17911,0\\r\\n17912,0\\r\\n17913,0\\r\\n17914,0\\r\\n17915,0\\r\\n17916,0\\r\\n17917,0\\r\\n17918,0\\r\\n17919,0\\r\\n17920,0\\r\\n17921,0\\r\\n17922,0\\r\\n17923,0\\r\\n17924,0\\r\\n17925,0\\r\\n17926,0\\r\\n17927,0\\r\\n17928,0\\r\\n17929,0\\r\\n17930,0\\r\\n17931,0\\r\\n17932,0\\r\\n17933,0\\r\\n17934,0\\r\\n17935,0\\r\\n17936,0\\r\\n17937,0\\r\\n17938,0\\r\\n17939,0\\r\\n17940,0\\r\\n17941,0\\r\\n17942,0\\r\\n17943,0\\r\\n17944,0\\r\\n17945,0\\r\\n17946,0\\r\\n17947,0\\r\\n17948,0\\r\\n17949,0\\r\\n17950,0\\r\\n17951,0\\r\\n17952,0\\r\\n17953,0\\r\\n17954,0\\r\\n17955,0\\r\\n17956,0\\r\\n17957,0\\r\\n17958,0\\r\\n17959,0\\r\\n17960,0\\r\\n17961,0\\r\\n17962,0\\r\\n17963,0\\r\\n17964,0\\r\\n17965,0\\r\\n17966,0\\r\\n17967,0\\r\\n17968,0\\r\\n17969,0\\r\\n17970,0\\r\\n17971,0\\r\\n17972,0\\r\\n17973,0\\r\\n17974,0\\r\\n17975,0\\r\\n17976,0\\r\\n17977,0\\r\\n17978,0\\r\\n17979,0\\r\\n17980,0\\r\\n17981,0\\r\\n17982,0\\r\\n17983,0\\r\\n17984,0\\r\\n17985,0\\r\\n17986,0\\r\\n17987,0\\r\\n17988,0\\r\\n17989,0\\r\\n17990,0\\r\\n17991,0\\r\\n17992,0\\r\\n17993,0\\r\\n17994,0\\r\\n17995,0\\r\\n17996,0\\r\\n17997,0\\r\\n17998,0\\r\\n17999,0\\r\\n18000,0\\r\\n18001,0\\r\\n18002,0\\r\\n18003,0\\r\\n18004,0\\r\\n18005,0\\r\\n18006,0\\r\\n18007,0\\r\\n18008,0\\r\\n18009,0\\r\\n18010,0\\r\\n18011,0\\r\\n18012,0\\r\\n18013,0\\r\\n18014,0\\r\\n18015,0\\r\\n18016,0\\r\\n18017,0\\r\\n18018,0\\r\\n18019,0\\r\\n18020,0\\r\\n18021,0\\r\\n18022,0\\r\\n18023,0\\r\\n18024,0\\r\\n18025,0\\r\\n18026,0\\r\\n18027,0\\r\\n18028,0\\r\\n18029,0\\r\\n18030,0\\r\\n18031,0\\r\\n18032,0\\r\\n18033,0\\r\\n18034,0\\r\\n18035,0\\r\\n18036,0\\r\\n18037,0\\r\\n18038,0\\r\\n18039,0\\r\\n18040,0\\r\\n18041,0\\r\\n18042,0\\r\\n18043,0\\r\\n18044,0\\r\\n18045,0\\r\\n18046,0\\r\\n18047,0\\r\\n18048,0\\r\\n18049,0\\r\\n18050,0\\r\\n18051,0\\r\\n18052,0\\r\\n18053,0\\r\\n18054,0\\r\\n18055,0\\r\\n18056,0\\r\\n18057,0\\r\\n18058,0\\r\\n18059,0\\r\\n18060,0\\r\\n18061,0\\r\\n18062,0\\r\\n18063,0\\r\\n18064,0\\r\\n18065,0\\r\\n18066,0\\r\\n18067,0\\r\\n18068,0\\r\\n18069,0\\r\\n18070,0\\r\\n18071,0\\r\\n18072,0\\r\\n18073,0\\r\\n18074,0\\r\\n18075,0\\r\\n18076,0\\r\\n18077,0\\r\\n18078,0\\r\\n18079,0\\r\\n18080,0\\r\\n18081,0\\r\\n18082,0\\r\\n18083,0\\r\\n18084,0\\r\\n18085,0\\r\\n18086,0\\r\\n18087,0\\r\\n18088,0\\r\\n18089,0\\r\\n18090,0\\r\\n18091,0\\r\\n18092,0\\r\\n18093,0\\r\\n18094,0\\r\\n18095,0\\r\\n18096,0\\r\\n18097,0\\r\\n18098,0\\r\\n18099,0\\r\\n18100,0\\r\\n18101,0\\r\\n18102,0\\r\\n18103,0\\r\\n18104,0\\r\\n18105,0\\r\\n18106,0\\r\\n18107,0\\r\\n18108,0\\r\\n18109,0\\r\\n18110,0\\r\\n18111,0\\r\\n18112,0\\r\\n18113,0\\r\\n18114,0\\r\\n18115,0\\r\\n18116,0\\r\\n18117,0\\r\\n18118,0\\r\\n18119,0\\r\\n18120,0\\r\\n18121,0\\r\\n18122,0\\r\\n18123,0\\r\\n18124,0\\r\\n18125,0\\r\\n18126,0\\r\\n18127,0\\r\\n18128,0\\r\\n18129,0\\r\\n18130,0\\r\\n18131,0\\r\\n18132,0\\r\\n18133,0\\r\\n18134,0\\r\\n18135,0\\r\\n18136,0\\r\\n18137,0\\r\\n18138,0\\r\\n18139,0\\r\\n18140,0\\r\\n18141,0\\r\\n18142,0\\r\\n18143,0\\r\\n18144,0\\r\\n18145,0\\r\\n18146,0\\r\\n18147,0\\r\\n18148,0\\r\\n18149,0\\r\\n18150,0\\r\\n18151,0\\r\\n18152,0\\r\\n18153,0\\r\\n18154,0\\r\\n18155,0\\r\\n18156,0\\r\\n18157,0\\r\\n18158,0\\r\\n18159,0\\r\\n18160,0\\r\\n18161,0\\r\\n18162,0\\r\\n18163,0\\r\\n18164,0\\r\\n18165,0\\r\\n18166,0\\r\\n18167,0\\r\\n18168,0\\r\\n18169,0\\r\\n18170,0\\r\\n18171,0\\r\\n18172,0\\r\\n18173,0\\r\\n18174,0\\r\\n18175,0\\r\\n18176,0\\r\\n18177,0\\r\\n18178,0\\r\\n18179,0\\r\\n18180,0\\r\\n18181,0\\r\\n18182,0\\r\\n18183,0\\r\\n18184,0\\r\\n18185,0\\r\\n18186,0\\r\\n18187,0\\r\\n18188,0\\r\\n18189,0\\r\\n18190,0\\r\\n18191,0\\r\\n18192,0\\r\\n18193,0\\r\\n18194,0\\r\\n18195,0\\r\\n18196,0\\r\\n18197,0\\r\\n18198,0\\r\\n18199,0\\r\\n18200,0\\r\\n18201,0\\r\\n18202,0\\r\\n18203,0\\r\\n18204,0\\r\\n18205,0\\r\\n18206,0\\r\\n18207,0\\r\\n18208,0\\r\\n18209,0\\r\\n18210,0\\r\\n18211,0\\r\\n18212,0\\r\\n18213,0\\r\\n18214,0\\r\\n18215,0\\r\\n18216,0\\r\\n18217,0\\r\\n18218,0\\r\\n18219,0\\r\\n18220,0\\r\\n18221,0\\r\\n18222,0\\r\\n18223,0\\r\\n18224,0\\r\\n18225,0\\r\\n18226,0\\r\\n18227,0\\r\\n18228,0\\r\\n18229,0\\r\\n18230,0\\r\\n18231,0\\r\\n18232,0\\r\\n18233,0\\r\\n18234,0\\r\\n18235,0\\r\\n18236,0\\r\\n18237,0\\r\\n18238,0\\r\\n18239,0\\r\\n18240,0\\r\\n18241,0\\r\\n18242,0\\r\\n18243,0\\r\\n18244,0\\r\\n18245,0\\r\\n18246,0\\r\\n18247,0\\r\\n18248,0\\r\\n18249,0\\r\\n18250,0\\r\\n18251,0\\r\\n18252,0\\r\\n18253,0\\r\\n18254,0\\r\\n18255,0\\r\\n18256,0\\r\\n18257,0\\r\\n18258,0\\r\\n18259,0\\r\\n18260,0\\r\\n18261,0\\r\\n18262,0\\r\\n18263,0\\r\\n18264,0\\r\\n18265,0\\r\\n18266,0\\r\\n18267,0\\r\\n18268,0\\r\\n18269,0\\r\\n18270,0\\r\\n18271,0\\r\\n18272,0\\r\\n18273,0\\r\\n18274,0\\r\\n18275,0\\r\\n18276,0\\r\\n18277,0\\r\\n18278,0\\r\\n18279,0\\r\\n18280,0\\r\\n18281,0\\r\\n18282,0\\r\\n18283,0\\r\\n18284,0\\r\\n18285,0\\r\\n18286,0\\r\\n18287,0\\r\\n18288,0\\r\\n18289,0\\r\\n18290,0\\r\\n18291,0\\r\\n18292,0\\r\\n18293,0\\r\\n18294,0\\r\\n18295,0\\r\\n18296,0\\r\\n18297,0\\r\\n18298,0\\r\\n18299,0\\r\\n18300,0\\r\\n18301,0\\r\\n18302,0\\r\\n18303,0\\r\\n18304,0\\r\\n18305,0\\r\\n18306,0\\r\\n18307,0\\r\\n18308,0\\r\\n18309,0\\r\\n18310,0\\r\\n18311,0\\r\\n18312,0\\r\\n18313,0\\r\\n18314,0\\r\\n18315,0\\r\\n18316,0\\r\\n18317,0\\r\\n18318,0\\r\\n18319,0\\r\\n18320,0\\r\\n18321,0\\r\\n18322,0\\r\\n18323,0\\r\\n18324,0\\r\\n18325,0\\r\\n18326,0\\r\\n18327,0\\r\\n18328,0\\r\\n18329,0\\r\\n18330,0\\r\\n18331,0\\r\\n18332,0\\r\\n18333,0\\r\\n18334,0\\r\\n18335,0\\r\\n18336,0\\r\\n18337,0\\r\\n18338,0\\r\\n18339,0\\r\\n18340,0\\r\\n18341,0\\r\\n18342,0\\r\\n18343,0\\r\\n18344,0\\r\\n18345,0\\r\\n18346,0\\r\\n18347,0\\r\\n18348,0\\r\\n18349,0\\r\\n18350,0\\r\\n18351,0\\r\\n18352,0\\r\\n18353,0\\r\\n18354,0\\r\\n18355,0\\r\\n18356,0\\r\\n18357,0\\r\\n18358,0\\r\\n18359,0\\r\\n18360,0\\r\\n18361,0\\r\\n18362,0\\r\\n18363,0\\r\\n18364,0\\r\\n18365,0\\r\\n18366,0\\r\\n18367,0\\r\\n18368,0\\r\\n18369,0\\r\\n18370,0\\r\\n18371,0\\r\\n18372,0\\r\\n18373,0\\r\\n18374,0\\r\\n18375,0\\r\\n18376,0\\r\\n18377,0\\r\\n18378,0\\r\\n18379,0\\r\\n18380,0\\r\\n18381,0\\r\\n18382,0\\r\\n18383,0\\r\\n18384,0\\r\\n18385,0\\r\\n18386,0\\r\\n18387,0\\r\\n18388,0\\r\\n18389,0\\r\\n18390,0\\r\\n18391,0\\r\\n18392,0\\r\\n18393,0\\r\\n18394,0\\r\\n18395,0\\r\\n18396,0\\r\\n18397,0\\r\\n18398,0\\r\\n18399,0\\r\\n18400,0\\r\\n18401,0\\r\\n18402,0\\r\\n18403,0\\r\\n18404,0\\r\\n18405,0\\r\\n18406,0\\r\\n18407,0\\r\\n18408,0\\r\\n18409,0\\r\\n18410,0\\r\\n18411,0\\r\\n18412,0\\r\\n18413,0\\r\\n18414,0\\r\\n18415,0\\r\\n18416,0\\r\\n18417,0\\r\\n18418,0\\r\\n18419,0\\r\\n18420,0\\r\\n18421,0\\r\\n18422,0\\r\\n18423,0\\r\\n18424,0\\r\\n18425,0\\r\\n18426,0\\r\\n18427,0\\r\\n18428,0\\r\\n18429,0\\r\\n18430,0\\r\\n18431,0\\r\\n18432,0\\r\\n18433,0\\r\\n18434,0\\r\\n18435,0\\r\\n18436,0\\r\\n18437,0\\r\\n18438,0\\r\\n18439,0\\r\\n18440,0\\r\\n18441,0\\r\\n18442,0\\r\\n18443,0\\r\\n18444,0\\r\\n18445,0\\r\\n18446,0\\r\\n18447,0\\r\\n18448,0\\r\\n18449,0\\r\\n18450,0\\r\\n18451,0\\r\\n18452,0\\r\\n18453,0\\r\\n18454,0\\r\\n18455,0\\r\\n18456,0\\r\\n18457,0\\r\\n18458,0\\r\\n18459,0\\r\\n18460,0\\r\\n18461,0\\r\\n18462,0\\r\\n18463,0\\r\\n18464,0\\r\\n18465,0\\r\\n18466,0\\r\\n18467,0\\r\\n18468,0\\r\\n18469,0\\r\\n18470,0\\r\\n18471,0\\r\\n18472,0\\r\\n18473,0\\r\\n18474,0\\r\\n18475,0\\r\\n18476,0\\r\\n18477,0\\r\\n18478,0\\r\\n18479,0\\r\\n18480,0\\r\\n18481,0\\r\\n18482,0\\r\\n18483,0\\r\\n18484,0\\r\\n18485,0\\r\\n18486,0\\r\\n18487,0\\r\\n18488,0\\r\\n18489,0\\r\\n18490,0\\r\\n18491,0\\r\\n18492,0\\r\\n18493,0\\r\\n18494,0\\r\\n18495,0\\r\\n18496,0\\r\\n18497,0\\r\\n18498,0\\r\\n18499,0\\r\\n18500,0\\r\\n18501,0\\r\\n18502,0\\r\\n18503,0\\r\\n18504,0\\r\\n18505,0\\r\\n18506,0\\r\\n18507,0\\r\\n18508,0\\r\\n18509,0\\r\\n18510,0\\r\\n18511,0\\r\\n18512,0\\r\\n18513,0\\r\\n18514,0\\r\\n18515,0\\r\\n18516,0\\r\\n18517,0\\r\\n18518,0\\r\\n18519,0\\r\\n18520,0\\r\\n18521,0\\r\\n18522,0\\r\\n18523,0\\r\\n18524,0\\r\\n18525,0\\r\\n18526,0\\r\\n18527,0\\r\\n18528,0\\r\\n18529,0\\r\\n18530,0\\r\\n18531,0\\r\\n18532,0\\r\\n18533,0\\r\\n18534,0\\r\\n18535,0\\r\\n18536,0\\r\\n18537,0\\r\\n18538,0\\r\\n18539,0\\r\\n18540,0\\r\\n18541,0\\r\\n18542,0\\r\\n18543,0\\r\\n18544,0\\r\\n18545,0\\r\\n18546,0\\r\\n18547,0\\r\\n18548,0\\r\\n18549,0\\r\\n18550,0\\r\\n18551,0\\r\\n18552,0\\r\\n18553,0\\r\\n18554,0\\r\\n18555,0\\r\\n18556,0\\r\\n18557,0\\r\\n18558,0\\r\\n18559,0\\r\\n18560,0\\r\\n18561,0\\r\\n18562,0\\r\\n18563,0\\r\\n18564,0\\r\\n18565,0\\r\\n18566,0\\r\\n18567,0\\r\\n18568,0\\r\\n18569,0\\r\\n18570,0\\r\\n18571,0\\r\\n18572,0\\r\\n18573,0\\r\\n18574,0\\r\\n18575,0\\r\\n18576,0\\r\\n18577,0\\r\\n18578,0\\r\\n18579,0\\r\\n18580,0\\r\\n18581,0\\r\\n18582,0\\r\\n18583,0\\r\\n18584,0\\r\\n18585,0\\r\\n18586,0\\r\\n18587,0\\r\\n18588,0\\r\\n18589,0\\r\\n18590,0\\r\\n18591,0\\r\\n18592,0\\r\\n18593,0\\r\\n18594,0\\r\\n18595,0\\r\\n18596,0\\r\\n18597,0\\r\\n18598,0\\r\\n18599,0\\r\\n18600,0\\r\\n18601,0\\r\\n18602,0\\r\\n18603,0\\r\\n18604,0\\r\\n18605,0\\r\\n18606,0\\r\\n18607,0\\r\\n18608,0\\r\\n18609,0\\r\\n18610,0\\r\\n18611,0\\r\\n18612,0\\r\\n18613,0\\r\\n18614,0\\r\\n18615,0\\r\\n18616,0\\r\\n18617,0\\r\\n18618,0\\r\\n18619,0\\r\\n18620,0\\r\\n18621,0\\r\\n18622,0\\r\\n18623,0\\r\\n18624,0\\r\\n18625,0\\r\\n18626,0\\r\\n18627,0\\r\\n18628,0\\r\\n18629,0\\r\\n18630,0\\r\\n18631,0\\r\\n18632,0\\r\\n18633,0\\r\\n18634,0\\r\\n18635,0\\r\\n18636,0\\r\\n18637,0\\r\\n18638,0\\r\\n18639,0\\r\\n18640,0\\r\\n18641,0\\r\\n18642,0\\r\\n18643,0\\r\\n18644,0\\r\\n18645,0\\r\\n18646,0\\r\\n18647,0\\r\\n18648,0\\r\\n18649,0\\r\\n18650,0\\r\\n18651,0\\r\\n18652,0\\r\\n18653,0\\r\\n18654,0\\r\\n18655,0\\r\\n18656,0\\r\\n18657,0\\r\\n18658,0\\r\\n18659,0\\r\\n18660,0\\r\\n18661,0\\r\\n18662,0\\r\\n18663,0\\r\\n18664,0\\r\\n18665,0\\r\\n18666,0\\r\\n18667,0\\r\\n18668,0\\r\\n18669,0\\r\\n18670,0\\r\\n18671,0\\r\\n18672,0\\r\\n18673,0\\r\\n18674,0\\r\\n18675,0\\r\\n18676,0\\r\\n18677,0\\r\\n18678,0\\r\\n18679,0\\r\\n18680,0\\r\\n18681,0\\r\\n18682,0\\r\\n18683,0\\r\\n18684,0\\r\\n18685,0\\r\\n18686,0\\r\\n18687,0\\r\\n18688,0\\r\\n18689,0\\r\\n18690,0\\r\\n18691,0\\r\\n18692,0\\r\\n18693,0\\r\\n18694,0\\r\\n18695,0\\r\\n18696,0\\r\\n18697,0\\r\\n18698,0\\r\\n18699,0\\r\\n18700,0\\r\\n18701,0\\r\\n18702,0\\r\\n18703,0\\r\\n18704,0\\r\\n18705,0\\r\\n18706,0\\r\\n18707,0\\r\\n18708,0\\r\\n18709,0\\r\\n18710,0\\r\\n18711,0\\r\\n18712,0\\r\\n18713,0\\r\\n18714,0\\r\\n18715,0\\r\\n18716,0\\r\\n18717,0\\r\\n18718,0\\r\\n18719,0\\r\\n18720,0\\r\\n18721,0\\r\\n18722,0\\r\\n18723,0\\r\\n18724,0\\r\\n18725,0\\r\\n18726,0\\r\\n18727,0\\r\\n18728,0\\r\\n18729,0\\r\\n18730,0\\r\\n18731,0\\r\\n18732,0\\r\\n18733,0\\r\\n18734,0\\r\\n18735,0\\r\\n18736,0\\r\\n18737,0\\r\\n18738,0\\r\\n18739,0\\r\\n18740,0\\r\\n18741,0\\r\\n18742,0\\r\\n18743,0\\r\\n18744,0\\r\\n18745,0\\r\\n18746,0\\r\\n18747,0\\r\\n18748,0\\r\\n18749,0\\r\\n18750,0\\r\\n18751,0\\r\\n18752,0\\r\\n18753,0\\r\\n18754,0\\r\\n18755,0\\r\\n18756,0\\r\\n18757,0\\r\\n18758,0\\r\\n18759,0\\r\\n18760,0\\r\\n18761,0\\r\\n18762,0\\r\\n18763,0\\r\\n18764,0\\r\\n18765,0\\r\\n18766,0\\r\\n18767,0\\r\\n18768,0\\r\\n18769,0\\r\\n18770,0\\r\\n18771,0\\r\\n18772,0\\r\\n18773,0\\r\\n18774,0\\r\\n18775,0\\r\\n18776,0\\r\\n18777,0\\r\\n18778,0\\r\\n18779,0\\r\\n18780,0\\r\\n18781,0\\r\\n18782,0\\r\\n18783,0\\r\\n18784,0\\r\\n18785,0\\r\\n18786,0\\r\\n18787,0\\r\\n18788,0\\r\\n18789,0\\r\\n18790,0\\r\\n18791,0\\r\\n18792,0\\r\\n18793,0\\r\\n18794,0\\r\\n18795,0\\r\\n18796,0\\r\\n18797,0\\r\\n18798,0\\r\\n18799,0\\r\\n18800,0\\r\\n18801,0\\r\\n18802,0\\r\\n18803,0\\r\\n18804,0\\r\\n18805,0\\r\\n18806,0\\r\\n18807,0\\r\\n18808,0\\r\\n18809,0\\r\\n18810,0\\r\\n18811,0\\r\\n18812,0\\r\\n18813,0\\r\\n18814,0\\r\\n18815,0\\r\\n18816,0\\r\\n18817,0\\r\\n18818,0\\r\\n18819,0\\r\\n18820,0\\r\\n18821,0\\r\\n18822,0\\r\\n18823,0\\r\\n18824,0\\r\\n18825,0\\r\\n18826,0\\r\\n18827,0\\r\\n18828,0\\r\\n18829,0\\r\\n18830,0\\r\\n18831,0\\r\\n18832,0\\r\\n18833,0\\r\\n18834,0\\r\\n18835,0\\r\\n18836,0\\r\\n18837,0\\r\\n18838,0\\r\\n18839,0\\r\\n18840,0\\r\\n18841,0\\r\\n18842,0\\r\\n18843,0\\r\\n18844,0\\r\\n18845,0\\r\\n18846,0\\r\\n18847,0\\r\\n18848,0\\r\\n18849,0\\r\\n18850,0\\r\\n18851,0\\r\\n18852,0\\r\\n18853,0\\r\\n18854,0\\r\\n18855,0\\r\\n18856,0\\r\\n18857,0\\r\\n18858,0\\r\\n18859,0\\r\\n18860,0\\r\\n18861,0\\r\\n18862,0\\r\\n18863,0\\r\\n18864,0\\r\\n18865,0\\r\\n18866,0\\r\\n18867,0\\r\\n18868,0\\r\\n18869,0\\r\\n18870,0\\r\\n18871,0\\r\\n18872,0\\r\\n18873,0\\r\\n18874,0\\r\\n18875,0\\r\\n18876,0\\r\\n18877,0\\r\\n18878,0\\r\\n18879,0\\r\\n18880,0\\r\\n18881,0\\r\\n18882,0\\r\\n18883,0\\r\\n18884,0\\r\\n18885,0\\r\\n18886,0\\r\\n18887,0\\r\\n18888,0\\r\\n18889,0\\r\\n18890,0\\r\\n18891,0\\r\\n18892,0\\r\\n18893,0\\r\\n18894,0\\r\\n18895,0\\r\\n18896,0\\r\\n18897,0\\r\\n18898,0\\r\\n18899,0\\r\\n18900,0\\r\\n18901,0\\r\\n18902,0\\r\\n18903,0\\r\\n18904,0\\r\\n18905,0\\r\\n18906,0\\r\\n18907,0\\r\\n18908,0\\r\\n18909,0\\r\\n18910,0\\r\\n18911,0\\r\\n18912,0\\r\\n18913,0\\r\\n18914,0\\r\\n18915,0\\r\\n18916,0\\r\\n18917,0\\r\\n18918,0\\r\\n18919,0\\r\\n18920,0\\r\\n18921,0\\r\\n18922,0\\r\\n18923,0\\r\\n18924,0\\r\\n18925,0\\r\\n18926,0\\r\\n18927,0\\r\\n18928,0\\r\\n18929,0\\r\\n18930,0\\r\\n18931,0\\r\\n18932,0\\r\\n18933,0\\r\\n18934,0\\r\\n18935,0\\r\\n18936,0\\r\\n18937,0\\r\\n18938,0\\r\\n18939,0\\r\\n18940,0\\r\\n18941,0\\r\\n18942,0\\r\\n18943,0\\r\\n18944,0\\r\\n18945,0\\r\\n18946,0\\r\\n18947,0\\r\\n18948,0\\r\\n18949,0\\r\\n18950,0\\r\\n18951,0\\r\\n18952,0\\r\\n18953,0\\r\\n18954,0\\r\\n18955,0\\r\\n18956,0\\r\\n18957,0\\r\\n18958,0\\r\\n18959,0\\r\\n18960,0\\r\\n18961,0\\r\\n18962,0\\r\\n18963,0\\r\\n18964,0\\r\\n18965,0\\r\\n18966,0\\r\\n18967,0\\r\\n18968,0\\r\\n18969,0\\r\\n18970,0\\r\\n18971,0\\r\\n18972,0\\r\\n18973,0\\r\\n18974,0\\r\\n18975,0\\r\\n18976,0\\r\\n18977,0\\r\\n18978,0\\r\\n18979,0\\r\\n18980,0\\r\\n18981,0\\r\\n18982,0\\r\\n18983,0\\r\\n18984,0\\r\\n18985,0\\r\\n18986,0\\r\\n18987,0\\r\\n18988,0\\r\\n18989,0\\r\\n18990,0\\r\\n18991,0\\r\\n18992,0\\r\\n18993,0\\r\\n18994,0\\r\\n18995,0\\r\\n18996,0\\r\\n18997,0\\r\\n18998,0\\r\\n18999,0\\r\\n19000,0\\r\\n19001,0\\r\\n19002,0\\r\\n19003,0\\r\\n19004,0\\r\\n19005,0\\r\\n19006,0\\r\\n19007,0\\r\\n19008,0\\r\\n19009,0\\r\\n19010,0\\r\\n19011,0\\r\\n19012,0\\r\\n19013,0\\r\\n19014,0\\r\\n19015,0\\r\\n19016,0\\r\\n19017,0\\r\\n19018,0\\r\\n19019,0\\r\\n19020,0\\r\\n19021,0\\r\\n19022,0\\r\\n19023,0\\r\\n19024,0\\r\\n19025,0\\r\\n19026,0\\r\\n19027,0\\r\\n19028,0\\r\\n19029,0\\r\\n19030,0\\r\\n19031,0\\r\\n19032,0\\r\\n19033,0\\r\\n19034,0\\r\\n19035,0\\r\\n19036,0\\r\\n19037,0\\r\\n19038,0\\r\\n19039,0\\r\\n19040,0\\r\\n19041,0\\r\\n19042,0\\r\\n19043,0\\r\\n19044,0\\r\\n19045,0\\r\\n19046,0\\r\\n19047,0\\r\\n19048,0\\r\\n19049,0\\r\\n19050,0\\r\\n19051,0\\r\\n19052,0\\r\\n19053,0\\r\\n19054,0\\r\\n19055,0\\r\\n19056,0\\r\\n19057,0\\r\\n19058,0\\r\\n19059,0\\r\\n19060,0\\r\\n19061,0\\r\\n19062,0\\r\\n19063,0\\r\\n19064,0\\r\\n19065,0\\r\\n19066,0\\r\\n19067,0\\r\\n19068,0\\r\\n19069,0\\r\\n19070,0\\r\\n19071,0\\r\\n19072,0\\r\\n19073,0\\r\\n19074,0\\r\\n19075,0\\r\\n19076,0\\r\\n19077,0\\r\\n19078,0\\r\\n19079,0\\r\\n19080,0\\r\\n19081,0\\r\\n19082,0\\r\\n19083,0\\r\\n19084,0\\r\\n19085,0\\r\\n19086,0\\r\\n19087,0\\r\\n19088,0\\r\\n19089,0\\r\\n19090,0\\r\\n19091,0\\r\\n19092,0\\r\\n19093,0\\r\\n19094,0\\r\\n19095,0\\r\\n19096,0\\r\\n19097,0\\r\\n19098,0\\r\\n19099,0\\r\\n19100,0\\r\\n19101,0\\r\\n19102,0\\r\\n19103,0\\r\\n19104,0\\r\\n19105,0\\r\\n19106,0\\r\\n19107,0\\r\\n19108,0\\r\\n19109,0\\r\\n19110,0\\r\\n19111,0\\r\\n19112,0\\r\\n19113,0\\r\\n19114,0\\r\\n19115,0\\r\\n19116,0\\r\\n19117,0\\r\\n19118,0\\r\\n19119,0\\r\\n19120,0\\r\\n19121,0\\r\\n19122,0\\r\\n19123,0\\r\\n19124,0\\r\\n19125,0\\r\\n19126,0\\r\\n19127,0\\r\\n19128,0\\r\\n19129,0\\r\\n19130,0\\r\\n19131,0\\r\\n19132,0\\r\\n19133,0\\r\\n19134,0\\r\\n19135,0\\r\\n19136,0\\r\\n19137,0\\r\\n19138,0\\r\\n19139,0\\r\\n19140,0\\r\\n19141,0\\r\\n19142,0\\r\\n19143,0\\r\\n19144,0\\r\\n19145,0\\r\\n19146,0\\r\\n19147,0\\r\\n19148,0\\r\\n19149,0\\r\\n19150,0\\r\\n19151,0\\r\\n19152,0\\r\\n19153,0\\r\\n19154,0\\r\\n19155,0\\r\\n19156,0\\r\\n19157,0\\r\\n19158,0\\r\\n19159,0\\r\\n19160,0\\r\\n19161,0\\r\\n19162,0\\r\\n19163,0\\r\\n19164,0\\r\\n19165,0\\r\\n19166,0\\r\\n19167,0\\r\\n19168,0\\r\\n19169,0\\r\\n19170,0\\r\\n19171,0\\r\\n19172,0\\r\\n19173,0\\r\\n19174,0\\r\\n19175,0\\r\\n19176,0\\r\\n19177,0\\r\\n19178,0\\r\\n19179,0\\r\\n19180,0\\r\\n19181,0\\r\\n19182,0\\r\\n19183,0\\r\\n19184,0\\r\\n19185,0\\r\\n19186,0\\r\\n19187,0\\r\\n19188,0\\r\\n19189,0\\r\\n19190,0\\r\\n19191,0\\r\\n19192,0\\r\\n19193,0\\r\\n19194,0\\r\\n19195,0\\r\\n19196,0\\r\\n19197,0\\r\\n19198,0\\r\\n19199,0\\r\\n19200,0\\r\\n19201,0\\r\\n19202,0\\r\\n19203,0\\r\\n19204,0\\r\\n19205,0\\r\\n19206,0\\r\\n19207,0\\r\\n19208,0\\r\\n19209,0\\r\\n19210,0\\r\\n19211,0\\r\\n19212,0\\r\\n19213,0\\r\\n19214,0\\r\\n19215,0\\r\\n19216,0\\r\\n19217,0\\r\\n19218,0\\r\\n19219,0\\r\\n19220,0\\r\\n19221,0\\r\\n19222,0\\r\\n19223,0\\r\\n19224,0\\r\\n19225,0\\r\\n19226,0\\r\\n19227,0\\r\\n19228,0\\r\\n19229,0\\r\\n19230,0\\r\\n19231,0\\r\\n19232,0\\r\\n19233,0\\r\\n19234,0\\r\\n19235,0\\r\\n19236,0\\r\\n19237,0\\r\\n19238,0\\r\\n19239,0\\r\\n19240,0\\r\\n19241,0\\r\\n19242,0\\r\\n19243,0\\r\\n19244,0\\r\\n19245,0\\r\\n19246,0\\r\\n19247,0\\r\\n19248,0\\r\\n19249,0\\r\\n19250,0\\r\\n19251,0\\r\\n19252,0\\r\\n19253,0\\r\\n19254,0\\r\\n19255,0\\r\\n19256,0\\r\\n19257,0\\r\\n19258,0\\r\\n19259,0\\r\\n19260,0\\r\\n19261,0\\r\\n19262,0\\r\\n19263,0\\r\\n19264,0\\r\\n19265,0\\r\\n19266,0\\r\\n19267,0\\r\\n19268,0\\r\\n19269,0\\r\\n19270,0\\r\\n19271,0\\r\\n19272,0\\r\\n19273,0\\r\\n19274,0\\r\\n19275,0\\r\\n19276,0\\r\\n19277,0\\r\\n19278,0\\r\\n19279,0\\r\\n19280,0\\r\\n19281,0\\r\\n19282,0\\r\\n19283,0\\r\\n19284,0\\r\\n19285,0\\r\\n19286,0\\r\\n19287,0\\r\\n19288,0\\r\\n19289,0\\r\\n19290,0\\r\\n19291,0\\r\\n19292,0\\r\\n19293,0\\r\\n19294,0\\r\\n19295,0\\r\\n19296,0\\r\\n19297,0\\r\\n19298,0\\r\\n19299,0\\r\\n19300,0\\r\\n19301,0\\r\\n19302,0\\r\\n19303,0\\r\\n19304,0\\r\\n19305,0\\r\\n19306,0\\r\\n19307,0\\r\\n19308,0\\r\\n19309,0\\r\\n19310,0\\r\\n19311,0\\r\\n19312,0\\r\\n19313,0\\r\\n19314,0\\r\\n19315,0\\r\\n19316,0\\r\\n19317,0\\r\\n19318,0\\r\\n19319,0\\r\\n19320,0\\r\\n19321,0\\r\\n19322,0\\r\\n19323,0\\r\\n19324,0\\r\\n19325,0\\r\\n19326,0\\r\\n19327,0\\r\\n19328,0\\r\\n19329,0\\r\\n19330,0\\r\\n19331,0\\r\\n19332,0\\r\\n19333,0\\r\\n19334,0\\r\\n19335,0\\r\\n19336,0\\r\\n19337,0\\r\\n19338,0\\r\\n19339,0\\r\\n19340,0\\r\\n19341,0\\r\\n19342,0\\r\\n19343,0\\r\\n19344,0\\r\\n19345,0\\r\\n19346,0\\r\\n19347,0\\r\\n19348,0\\r\\n19349,0\\r\\n19350,0\\r\\n19351,0\\r\\n19352,0\\r\\n19353,0\\r\\n19354,0\\r\\n19355,0\\r\\n19356,0\\r\\n19357,0\\r\\n19358,0\\r\\n19359,0\\r\\n19360,0\\r\\n19361,0\\r\\n19362,0\\r\\n19363,0\\r\\n19364,0\\r\\n19365,0\\r\\n19366,0\\r\\n19367,0\\r\\n19368,0\\r\\n19369,0\\r\\n19370,0\\r\\n19371,0\\r\\n19372,0\\r\\n19373,0\\r\\n19374,0\\r\\n19375,0\\r\\n19376,0\\r\\n19377,0\\r\\n19378,0\\r\\n19379,0\\r\\n19380,0\\r\\n19381,0\\r\\n19382,0\\r\\n19383,0\\r\\n19384,0\\r\\n19385,0\\r\\n19386,0\\r\\n19387,0\\r\\n19388,0\\r\\n19389,0\\r\\n19390,0\\r\\n19391,0\\r\\n19392,0\\r\\n19393,0\\r\\n19394,0\\r\\n19395,0\\r\\n19396,0\\r\\n19397,0\\r\\n19398,0\\r\\n19399,0\\r\\n19400,0\\r\\n19401,0\\r\\n19402,0\\r\\n19403,0\\r\\n19404,0\\r\\n19405,0\\r\\n19406,0\\r\\n19407,0\\r\\n19408,0\\r\\n19409,0\\r\\n19410,0\\r\\n19411,0\\r\\n19412,0\\r\\n19413,0\\r\\n19414,0\\r\\n19415,0\\r\\n19416,0\\r\\n19417,0\\r\\n19418,0\\r\\n19419,0\\r\\n19420,0\\r\\n19421,0\\r\\n19422,0\\r\\n19423,0\\r\\n19424,0\\r\\n19425,0\\r\\n19426,0\\r\\n19427,0\\r\\n19428,0\\r\\n19429,0\\r\\n19430,0\\r\\n19431,0\\r\\n19432,0\\r\\n19433,0\\r\\n19434,0\\r\\n19435,0\\r\\n19436,0\\r\\n19437,0\\r\\n19438,0\\r\\n19439,0\\r\\n19440,0\\r\\n19441,0\\r\\n19442,0\\r\\n19443,0\\r\\n19444,0\\r\\n19445,0\\r\\n19446,0\\r\\n19447,0\\r\\n19448,0\\r\\n19449,0\\r\\n19450,0\\r\\n19451,0\\r\\n19452,0\\r\\n19453,0\\r\\n19454,0\\r\\n19455,0\\r\\n19456,0\\r\\n19457,0\\r\\n19458,0\\r\\n19459,0\\r\\n19460,0\\r\\n19461,0\\r\\n19462,0\\r\\n19463,0\\r\\n19464,0\\r\\n19465,0\\r\\n19466,0\\r\\n19467,0\\r\\n19468,0\\r\\n19469,0\\r\\n19470,0\\r\\n19471,0\\r\\n19472,0\\r\\n19473,0\\r\\n19474,0\\r\\n19475,0\\r\\n19476,0\\r\\n19477,0\\r\\n19478,0\\r\\n19479,0\\r\\n19480,0\\r\\n19481,0\\r\\n19482,0\\r\\n19483,0\\r\\n19484,0\\r\\n19485,0\\r\\n19486,0\\r\\n19487,0\\r\\n19488,0\\r\\n19489,0\\r\\n19490,0\\r\\n19491,0\\r\\n19492,0\\r\\n19493,0\\r\\n19494,0\\r\\n19495,0\\r\\n19496,0\\r\\n19497,0\\r\\n19498,0\\r\\n19499,0\\r\\n19500,0\\r\\n19501,0\\r\\n19502,0\\r\\n19503,0\\r\\n19504,0\\r\\n19505,0\\r\\n19506,0\\r\\n19507,0\\r\\n19508,0\\r\\n19509,0\\r\\n19510,0\\r\\n19511,0\\r\\n19512,0\\r\\n19513,0\\r\\n19514,0\\r\\n19515,0\\r\\n19516,0\\r\\n19517,0\\r\\n19518,0\\r\\n19519,0\\r\\n19520,0\\r\\n19521,0\\r\\n19522,0\\r\\n19523,0\\r\\n19524,0\\r\\n19525,0\\r\\n19526,0\\r\\n19527,0\\r\\n19528,0\\r\\n19529,0\\r\\n19530,0\\r\\n19531,0\\r\\n19532,0\\r\\n19533,0\\r\\n19534,0\\r\\n19535,0\\r\\n19536,0\\r\\n19537,0\\r\\n19538,0\\r\\n19539,0\\r\\n19540,0\\r\\n19541,0\\r\\n19542,0\\r\\n19543,0\\r\\n19544,0\\r\\n19545,0\\r\\n19546,0\\r\\n19547,0\\r\\n19548,0\\r\\n19549,0\\r\\n19550,0\\r\\n19551,0\\r\\n19552,0\\r\\n19553,0\\r\\n19554,0\\r\\n19555,0\\r\\n19556,0\\r\\n19557,0\\r\\n19558,0\\r\\n19559,0\\r\\n19560,0\\r\\n19561,0\\r\\n19562,0\\r\\n19563,0\\r\\n19564,0\\r\\n19565,0\\r\\n19566,0\\r\\n19567,0\\r\\n19568,0\\r\\n19569,0\\r\\n19570,0\\r\\n19571,0\\r\\n19572,0\\r\\n19573,0\\r\\n19574,0\\r\\n19575,0\\r\\n19576,0\\r\\n19577,0\\r\\n19578,0\\r\\n19579,0\\r\\n19580,0\\r\\n19581,0\\r\\n19582,0\\r\\n19583,0\\r\\n19584,0\\r\\n19585,0\\r\\n19586,0\\r\\n19587,0\\r\\n19588,0\\r\\n19589,0\\r\\n19590,0\\r\\n19591,0\\r\\n19592,0\\r\\n19593,0\\r\\n19594,0\\r\\n19595,0\\r\\n19596,0\\r\\n19597,0\\r\\n19598,0\\r\\n19599,0\\r\\n19600,0\\r\\n19601,0\\r\\n19602,0\\r\\n19603,0\\r\\n19604,0\\r\\n19605,0\\r\\n19606,0\\r\\n19607,0\\r\\n19608,0\\r\\n19609,0\\r\\n19610,0\\r\\n19611,0\\r\\n19612,0\\r\\n19613,0\\r\\n19614,0\\r\\n19615,0\\r\\n19616,0\\r\\n19617,0\\r\\n19618,0\\r\\n19619,0\\r\\n19620,0\\r\\n19621,0\\r\\n19622,0\\r\\n19623,0\\r\\n19624,0\\r\\n19625,0\\r\\n19626,0\\r\\n19627,0\\r\\n19628,0\\r\\n19629,0\\r\\n19630,0\\r\\n19631,0\\r\\n19632,0\\r\\n19633,0\\r\\n19634,0\\r\\n19635,0\\r\\n19636,0\\r\\n19637,0\\r\\n19638,0\\r\\n19639,0\\r\\n19640,0\\r\\n19641,0\\r\\n19642,0\\r\\n19643,0\\r\\n19644,0\\r\\n19645,0\\r\\n19646,0\\r\\n19647,0\\r\\n19648,0\\r\\n19649,0\\r\\n19650,0\\r\\n19651,0\\r\\n19652,0\\r\\n19653,0\\r\\n19654,0\\r\\n19655,0\\r\\n19656,0\\r\\n19657,0\\r\\n19658,0\\r\\n19659,0\\r\\n19660,0\\r\\n19661,0\\r\\n19662,0\\r\\n19663,0\\r\\n19664,0\\r\\n19665,0\\r\\n19666,0\\r\\n19667,0\\r\\n19668,0\\r\\n19669,0\\r\\n19670,0\\r\\n19671,0\\r\\n19672,0\\r\\n19673,0\\r\\n19674,0\\r\\n19675,0\\r\\n19676,0\\r\\n19677,0\\r\\n19678,0\\r\\n19679,0\\r\\n19680,0\\r\\n19681,0\\r\\n19682,0\\r\\n19683,0\\r\\n19684,0\\r\\n19685,0\\r\\n19686,0\\r\\n19687,0\\r\\n19688,0\\r\\n19689,0\\r\\n19690,0\\r\\n19691,0\\r\\n19692,0\\r\\n19693,0\\r\\n19694,0\\r\\n19695,0\\r\\n19696,0\\r\\n19697,0\\r\\n19698,0\\r\\n19699,0\\r\\n19700,0\\r\\n19701,0\\r\\n19702,0\\r\\n19703,0\\r\\n19704,0\\r\\n19705,0\\r\\n19706,0\\r\\n19707,0\\r\\n19708,0\\r\\n19709,0\\r\\n19710,0\\r\\n19711,0\\r\\n19712,0\\r\\n19713,0\\r\\n19714,0\\r\\n19715,0\\r\\n19716,0\\r\\n19717,0\\r\\n19718,0\\r\\n19719,0\\r\\n19720,0\\r\\n19721,0\\r\\n19722,0\\r\\n19723,0\\r\\n19724,0\\r\\n19725,0\\r\\n19726,0\\r\\n19727,0\\r\\n19728,0\\r\\n19729,0\\r\\n19730,0\\r\\n19731,0\\r\\n19732,0\\r\\n19733,0\\r\\n19734,0\\r\\n19735,0\\r\\n19736,0\\r\\n19737,0\\r\\n19738,0\\r\\n19739,0\\r\\n19740,0\\r\\n19741,0\\r\\n19742,0\\r\\n19743,0\\r\\n19744,0\\r\\n19745,0\\r\\n19746,0\\r\\n19747,0\\r\\n19748,0\\r\\n19749,0\\r\\n19750,0\\r\\n19751,0\\r\\n19752,0\\r\\n19753,0\\r\\n19754,0\\r\\n19755,0\\r\\n19756,0\\r\\n19757,0\\r\\n19758,0\\r\\n19759,0\\r\\n19760,0\\r\\n19761,0\\r\\n19762,0\\r\\n19763,0\\r\\n19764,0\\r\\n19765,0\\r\\n19766,0\\r\\n19767,0\\r\\n19768,0\\r\\n19769,0\\r\\n19770,0\\r\\n19771,0\\r\\n19772,0\\r\\n19773,0\\r\\n19774,0\\r\\n19775,0\\r\\n19776,0\\r\\n19777,0\\r\\n19778,0\\r\\n19779,0\\r\\n19780,0\\r\\n19781,0\\r\\n19782,0\\r\\n19783,0\\r\\n19784,0\\r\\n19785,0\\r\\n19786,0\\r\\n19787,0\\r\\n19788,0\\r\\n19789,0\\r\\n19790,0\\r\\n19791,0\\r\\n19792,0\\r\\n19793,0\\r\\n19794,0\\r\\n19795,0\\r\\n19796,0\\r\\n19797,0\\r\\n19798,0\\r\\n19799,0\\r\\n19800,0\\r\\n19801,0\\r\\n19802,0\\r\\n19803,0\\r\\n19804,0\\r\\n19805,0\\r\\n19806,0\\r\\n19807,0\\r\\n19808,0\\r\\n19809,0\\r\\n19810,0\\r\\n19811,0\\r\\n19812,0\\r\\n19813,0\\r\\n19814,0\\r\\n19815,0\\r\\n19816,0\\r\\n19817,0\\r\\n19818,0\\r\\n19819,0\\r\\n19820,0\\r\\n19821,0\\r\\n19822,0\\r\\n19823,0\\r\\n19824,0\\r\\n19825,0\\r\\n19826,0\\r\\n19827,0\\r\\n19828,0\\r\\n19829,0\\r\\n19830,0\\r\\n19831,0\\r\\n19832,0\\r\\n19833,0\\r\\n19834,0\\r\\n19835,0\\r\\n19836,0\\r\\n19837,0\\r\\n19838,0\\r\\n19839,0\\r\\n19840,0\\r\\n19841,0\\r\\n19842,0\\r\\n19843,0\\r\\n19844,0\\r\\n19845,0\\r\\n19846,0\\r\\n19847,0\\r\\n19848,0\\r\\n19849,0\\r\\n19850,0\\r\\n19851,0\\r\\n19852,0\\r\\n19853,0\\r\\n19854,0\\r\\n19855,0\\r\\n19856,0\\r\\n19857,0\\r\\n19858,0\\r\\n19859,0\\r\\n19860,0\\r\\n19861,0\\r\\n19862,0\\r\\n19863,0\\r\\n19864,0\\r\\n19865,0\\r\\n19866,0\\r\\n19867,0\\r\\n19868,0\\r\\n19869,0\\r\\n19870,0\\r\\n19871,0\\r\\n19872,0\\r\\n19873,0\\r\\n19874,0\\r\\n19875,0\\r\\n19876,0\\r\\n19877,0\\r\\n19878,0\\r\\n19879,0\\r\\n19880,0\\r\\n19881,0\\r\\n19882,0\\r\\n19883,0\\r\\n19884,0\\r\\n19885,0\\r\\n19886,0\\r\\n19887,0\\r\\n19888,0\\r\\n19889,0\\r\\n19890,0\\r\\n19891,0\\r\\n19892,0\\r\\n19893,0\\r\\n19894,0\\r\\n19895,0\\r\\n19896,0\\r\\n19897,0\\r\\n19898,0\\r\\n19899,0\\r\\n19900,0\\r\\n19901,0\\r\\n19902,0\\r\\n19903,0\\r\\n19904,0\\r\\n19905,0\\r\\n19906,0\\r\\n19907,0\\r\\n19908,0\\r\\n19909,0\\r\\n19910,0\\r\\n19911,0\\r\\n19912,0\\r\\n19913,0\\r\\n19914,0\\r\\n19915,0\\r\\n19916,0\\r\\n19917,0\\r\\n19918,0\\r\\n19919,0\\r\\n19920,0\\r\\n19921,0\\r\\n19922,0\\r\\n19923,0\\r\\n19924,0\\r\\n19925,0\\r\\n19926,0\\r\\n19927,0\\r\\n19928,0\\r\\n19929,0\\r\\n19930,0\\r\\n19931,0\\r\\n19932,0\\r\\n19933,0\\r\\n19934,0\\r\\n19935,0\\r\\n19936,0\\r\\n19937,0\\r\\n19938,0\\r\\n19939,0\\r\\n19940,0\\r\\n19941,0\\r\\n19942,0\\r\\n19943,0\\r\\n19944,0\\r\\n19945,0\\r\\n19946,0\\r\\n19947,0\\r\\n19948,0\\r\\n19949,0\\r\\n19950,0\\r\\n19951,0\\r\\n19952,0\\r\\n19953,0\\r\\n19954,0\\r\\n19955,0\\r\\n19956,0\\r\\n19957,0\\r\\n19958,0\\r\\n19959,0\\r\\n19960,0\\r\\n19961,0\\r\\n19962,0\\r\\n19963,0\\r\\n19964,0\\r\\n19965,0\\r\\n19966,0\\r\\n19967,0\\r\\n19968,0\\r\\n19969,0\\r\\n19970,0\\r\\n19971,0\\r\\n19972,0\\r\\n19973,0\\r\\n19974,0\\r\\n19975,0\\r\\n19976,0\\r\\n19977,0\\r\\n19978,0\\r\\n19979,0\\r\\n19980,0\\r\\n19981,0\\r\\n19982,0\\r\\n19983,0\\r\\n19984,0\\r\\n19985,0\\r\\n19986,0\\r\\n19987,0\\r\\n19988,0\\r\\n19989,0\\r\\n19990,0\\r\\n19991,0\\r\\n19992,0\\r\\n19993,0\\r\\n19994,0\\r\\n19995,0\\r\\n19996,0\\r\\n19997,0\\r\\n19998,0\\r\\n19999,0\\r\\n20000,0\\r\\n20001,0\\r\\n20002,0\\r\\n20003,0\\r\\n20004,0\\r\\n20005,0\\r\\n20006,0\\r\\n20007,0\\r\\n20008,0\\r\\n20009,0\\r\\n20010,0\\r\\n20011,0\\r\\n20012,0\\r\\n20013,0\\r\\n20014,0\\r\\n20015,0\\r\\n20016,0\\r\\n20017,0\\r\\n20018,0\\r\\n20019,0\\r\\n20020,0\\r\\n20021,0\\r\\n20022,0\\r\\n20023,0\\r\\n20024,0\\r\\n20025,0\\r\\n20026,0\\r\\n20027,0\\r\\n20028,0\\r\\n20029,0\\r\\n20030,0\\r\\n20031,0\\r\\n20032,0\\r\\n20033,0\\r\\n20034,0\\r\\n20035,0\\r\\n20036,0\\r\\n20037,0\\r\\n20038,0\\r\\n20039,0\\r\\n20040,0\\r\\n20041,0\\r\\n20042,0\\r\\n20043,0\\r\\n20044,0\\r\\n20045,0\\r\\n20046,0\\r\\n20047,0\\r\\n20048,0\\r\\n20049,0\\r\\n20050,0\\r\\n20051,0\\r\\n20052,0\\r\\n20053,0\\r\\n20054,0\\r\\n20055,0\\r\\n20056,0\\r\\n20057,0\\r\\n20058,0\\r\\n20059,0\\r\\n20060,0\\r\\n20061,0\\r\\n20062,0\\r\\n20063,0\\r\\n20064,0\\r\\n20065,0\\r\\n20066,0\\r\\n20067,0\\r\\n20068,0\\r\\n20069,0\\r\\n20070,0\\r\\n20071,0\\r\\n20072,0\\r\\n20073,0\\r\\n20074,0\\r\\n20075,0\\r\\n20076,0\\r\\n20077,0\\r\\n20078,0\\r\\n20079,0\\r\\n20080,0\\r\\n20081,0\\r\\n20082,0\\r\\n20083,0\\r\\n20084,0\\r\\n20085,0\\r\\n20086,0\\r\\n20087,0\\r\\n20088,0\\r\\n20089,0\\r\\n20090,0\\r\\n20091,0\\r\\n20092,0\\r\\n20093,0\\r\\n20094,0\\r\\n20095,0\\r\\n20096,0\\r\\n20097,0\\r\\n20098,0\\r\\n20099,0\\r\\n20100,0\\r\\n20101,0\\r\\n20102,0\\r\\n20103,0\\r\\n20104,0\\r\\n20105,0\\r\\n20106,0\\r\\n20107,0\\r\\n20108,0\\r\\n20109,0\\r\\n20110,0\\r\\n20111,0\\r\\n20112,0\\r\\n20113,0\\r\\n20114,0\\r\\n20115,0\\r\\n20116,0\\r\\n20117,0\\r\\n20118,0\\r\\n20119,0\\r\\n20120,0\\r\\n20121,0\\r\\n20122,0\\r\\n20123,0\\r\\n20124,0\\r\\n20125,0\\r\\n20126,0\\r\\n20127,0\\r\\n20128,0\\r\\n20129,0\\r\\n20130,0\\r\\n20131,0\\r\\n20132,0\\r\\n20133,0\\r\\n20134,0\\r\\n20135,0\\r\\n20136,0\\r\\n20137,0\\r\\n20138,0\\r\\n20139,0\\r\\n20140,0\\r\\n20141,0\\r\\n20142,0\\r\\n20143,0\\r\\n20144,0\\r\\n20145,0\\r\\n20146,0\\r\\n20147,0\\r\\n20148,0\\r\\n20149,0\\r\\n20150,0\\r\\n20151,0\\r\\n20152,0\\r\\n20153,0\\r\\n20154,0\\r\\n20155,0\\r\\n20156,0\\r\\n20157,0\\r\\n20158,0\\r\\n20159,0\\r\\n20160,0\\r\\n20161,0\\r\\n20162,0\\r\\n20163,0\\r\\n20164,0\\r\\n20165,0\\r\\n20166,0\\r\\n20167,0\\r\\n20168,0\\r\\n20169,0\\r\\n20170,0\\r\\n20171,0\\r\\n20172,0\\r\\n20173,0\\r\\n20174,0\\r\\n20175,0\\r\\n20176,0\\r\\n20177,0\\r\\n20178,0\\r\\n20179,0\\r\\n20180,0\\r\\n20181,0\\r\\n20182,0\\r\\n20183,0\\r\\n20184,0\\r\\n20185,0\\r\\n20186,0\\r\\n20187,0\\r\\n20188,0\\r\\n20189,0\\r\\n20190,0\\r\\n20191,0\\r\\n20192,0\\r\\n20193,0\\r\\n20194,0\\r\\n20195,0\\r\\n20196,0\\r\\n20197,0\\r\\n20198,0\\r\\n20199,0\\r\\n20200,0\\r\\n20201,0\\r\\n20202,0\\r\\n20203,0\\r\\n20204,0\\r\\n20205,0\\r\\n20206,0\\r\\n20207,0\\r\\n20208,0\\r\\n20209,0\\r\\n20210,0\\r\\n20211,0\\r\\n20212,0\\r\\n20213,0\\r\\n20214,0\\r\\n20215,0\\r\\n20216,0\\r\\n20217,0\\r\\n20218,0\\r\\n20219,0\\r\\n20220,0\\r\\n20221,0\\r\\n20222,0\\r\\n20223,0\\r\\n20224,0\\r\\n20225,0\\r\\n20226,0\\r\\n20227,0\\r\\n20228,0\\r\\n20229,0\\r\\n20230,0\\r\\n20231,0\\r\\n20232,0\\r\\n20233,0\\r\\n20234,0\\r\\n20235,0\\r\\n20236,0\\r\\n20237,0\\r\\n20238,0\\r\\n20239,0\\r\\n20240,0\\r\\n20241,0\\r\\n20242,0\\r\\n20243,0\\r\\n20244,0\\r\\n20245,0\\r\\n20246,0\\r\\n20247,0\\r\\n20248,0\\r\\n20249,0\\r\\n20250,0\\r\\n20251,0\\r\\n20252,0\\r\\n20253,0\\r\\n20254,0\\r\\n20255,0\\r\\n20256,0\\r\\n20257,0\\r\\n20258,0\\r\\n20259,0\\r\\n20260,0\\r\\n20261,0\\r\\n20262,0\\r\\n20263,0\\r\\n20264,0\\r\\n20265,0\\r\\n20266,0\\r\\n20267,0\\r\\n20268,0\\r\\n20269,0\\r\\n20270,0\\r\\n20271,0\\r\\n20272,0\\r\\n20273,0\\r\\n20274,0\\r\\n20275,0\\r\\n20276,0\\r\\n20277,0\\r\\n20278,0\\r\\n20279,0\\r\\n20280,0\\r\\n20281,0\\r\\n20282,0\\r\\n20283,0\\r\\n20284,0\\r\\n20285,0\\r\\n20286,0\\r\\n20287,0\\r\\n20288,0\\r\\n20289,0\\r\\n20290,0\\r\\n20291,0\\r\\n20292,0\\r\\n20293,0\\r\\n20294,0\\r\\n20295,0\\r\\n20296,0\\r\\n20297,0\\r\\n20298,0\\r\\n20299,0\\r\\n20300,0\\r\\n20301,0\\r\\n20302,0\\r\\n20303,0\\r\\n20304,0\\r\\n20305,0\\r\\n20306,0\\r\\n20307,0\\r\\n20308,0\\r\\n20309,0\\r\\n20310,0\\r\\n20311,0\\r\\n20312,0\\r\\n20313,0\\r\\n20314,0\\r\\n20315,0\\r\\n20316,0\\r\\n20317,0\\r\\n20318,0\\r\\n20319,0\\r\\n20320,0\\r\\n20321,0\\r\\n20322,0\\r\\n20323,0\\r\\n20324,0\\r\\n20325,0\\r\\n20326,0\\r\\n20327,0\\r\\n20328,0\\r\\n20329,0\\r\\n20330,0\\r\\n20331,0\\r\\n20332,0\\r\\n20333,0\\r\\n20334,0\\r\\n20335,0\\r\\n20336,0\\r\\n20337,0\\r\\n20338,0\\r\\n20339,0\\r\\n20340,0\\r\\n20341,0\\r\\n20342,0\\r\\n20343,0\\r\\n20344,0\\r\\n20345,0\\r\\n20346,0\\r\\n20347,0\\r\\n20348,0\\r\\n20349,0\\r\\n20350,0\\r\\n20351,0\\r\\n20352,0\\r\\n20353,0\\r\\n20354,0\\r\\n20355,0\\r\\n20356,0\\r\\n20357,0\\r\\n20358,0\\r\\n20359,0\\r\\n20360,0\\r\\n20361,0\\r\\n20362,0\\r\\n20363,0\\r\\n20364,0\\r\\n20365,0\\r\\n20366,0\\r\\n20367,0\\r\\n20368,0\\r\\n20369,0\\r\\n20370,0\\r\\n20371,0\\r\\n20372,0\\r\\n20373,0\\r\\n20374,0\\r\\n20375,0\\r\\n20376,0\\r\\n20377,0\\r\\n20378,0\\r\\n20379,0\\r\\n20380,0\\r\\n20381,0\\r\\n20382,0\\r\\n20383,0\\r\\n20384,0\\r\\n20385,0\\r\\n20386,0\\r\\n20387,0\\r\\n20388,0\\r\\n20389,0\\r\\n20390,0\\r\\n20391,0\\r\\n20392,0\\r\\n20393,0\\r\\n20394,0\\r\\n20395,0\\r\\n20396,0\\r\\n20397,0\\r\\n20398,0\\r\\n20399,0\\r\\n20400,0\\r\\n20401,0\\r\\n20402,0\\r\\n20403,0\\r\\n20404,0\\r\\n20405,0\\r\\n20406,0\\r\\n20407,0\\r\\n20408,0\\r\\n20409,0\\r\\n20410,0\\r\\n20411,0\\r\\n20412,0\\r\\n20413,0\\r\\n20414,0\\r\\n20415,0\\r\\n20416,0\\r\\n20417,0\\r\\n20418,0\\r\\n20419,0\\r\\n20420,0\\r\\n20421,0\\r\\n20422,0\\r\\n20423,0\\r\\n20424,0\\r\\n20425,0\\r\\n20426,0\\r\\n20427,0\\r\\n20428,0\\r\\n20429,0\\r\\n20430,0\\r\\n20431,0\\r\\n20432,0\\r\\n20433,0\\r\\n20434,0\\r\\n20435,0\\r\\n20436,0\\r\\n20437,0\\r\\n20438,0\\r\\n20439,0\\r\\n20440,0\\r\\n20441,0\\r\\n20442,0\\r\\n20443,0\\r\\n20444,0\\r\\n20445,0\\r\\n20446,0\\r\\n20447,0\\r\\n20448,0\\r\\n20449,0\\r\\n20450,0\\r\\n20451,0\\r\\n20452,0\\r\\n20453,0\\r\\n20454,0\\r\\n20455,0\\r\\n20456,0\\r\\n20457,0\\r\\n20458,0\\r\\n20459,0\\r\\n20460,0\\r\\n20461,0\\r\\n20462,0\\r\\n20463,0\\r\\n20464,0\\r\\n20465,0\\r\\n20466,0\\r\\n20467,0\\r\\n20468,0\\r\\n20469,0\\r\\n20470,0\\r\\n20471,0\\r\\n20472,0\\r\\n20473,0\\r\\n20474,0\\r\\n20475,0\\r\\n20476,0\\r\\n20477,0\\r\\n20478,0\\r\\n20479,0\\r\\n20480,0\\r\\n20481,0\\r\\n20482,0\\r\\n20483,0\\r\\n20484,0\\r\\n20485,0\\r\\n20486,0\\r\\n20487,0\\r\\n20488,0\\r\\n20489,0\\r\\n20490,0\\r\\n20491,0\\r\\n20492,0\\r\\n20493,0\\r\\n20494,0\\r\\n20495,0\\r\\n20496,0\\r\\n20497,0\\r\\n20498,0\\r\\n20499,0\\r\\n20500,0\\r\\n20501,0\\r\\n20502,0\\r\\n20503,0\\r\\n20504,0\\r\\n20505,0\\r\\n20506,0\\r\\n20507,0\\r\\n20508,0\\r\\n20509,0\\r\\n20510,0\\r\\n20511,0\\r\\n20512,0\\r\\n20513,0\\r\\n20514,0\\r\\n20515,0\\r\\n20516,0\\r\\n20517,0\\r\\n20518,0\\r\\n20519,0\\r\\n20520,0\\r\\n20521,0\\r\\n20522,0\\r\\n20523,0\\r\\n20524,0\\r\\n20525,0\\r\\n20526,0\\r\\n20527,0\\r\\n20528,0\\r\\n20529,0\\r\\n20530,0\\r\\n20531,0\\r\\n20532,0\\r\\n20533,0\\r\\n20534,0\\r\\n20535,0\\r\\n20536,0\\r\\n20537,0\\r\\n20538,0\\r\\n20539,0\\r\\n20540,0\\r\\n20541,0\\r\\n20542,0\\r\\n20543,0\\r\\n20544,0\\r\\n20545,0\\r\\n20546,0\\r\\n20547,0\\r\\n20548,0\\r\\n20549,0\\r\\n20550,0\\r\\n20551,0\\r\\n20552,0\\r\\n20553,0\\r\\n20554,0\\r\\n20555,0\\r\\n20556,0\\r\\n20557,0\\r\\n20558,0\\r\\n20559,0\\r\\n20560,0\\r\\n20561,0\\r\\n20562,0\\r\\n20563,0\\r\\n20564,0\\r\\n20565,0\\r\\n20566,0\\r\\n20567,0\\r\\n20568,0\\r\\n20569,0\\r\\n20570,0\\r\\n20571,0\\r\\n20572,0\\r\\n20573,0\\r\\n20574,0\\r\\n20575,0\\r\\n20576,0\\r\\n20577,0\\r\\n20578,0\\r\\n20579,0\\r\\n20580,0\\r\\n20581,0\\r\\n20582,0\\r\\n20583,0\\r\\n20584,0\\r\\n20585,0\\r\\n20586,0\\r\\n20587,0\\r\\n20588,0\\r\\n20589,0\\r\\n20590,0\\r\\n20591,0\\r\\n20592,0\\r\\n20593,0\\r\\n20594,0\\r\\n20595,0\\r\\n20596,0\\r\\n20597,0\\r\\n20598,0\\r\\n20599,0\\r\\n20600,0\\r\\n20601,0\\r\\n20602,0\\r\\n20603,0\\r\\n20604,0\\r\\n20605,0\\r\\n20606,0\\r\\n20607,0\\r\\n20608,0\\r\\n20609,0\\r\\n20610,0\\r\\n20611,0\\r\\n20612,0\\r\\n20613,0\\r\\n20614,0\\r\\n20615,0\\r\\n20616,0\\r\\n20617,0\\r\\n20618,0\\r\\n20619,0\\r\\n20620,0\\r\\n20621,0\\r\\n20622,0\\r\\n20623,0\\r\\n20624,0\\r\\n20625,0\\r\\n20626,0\\r\\n20627,0\\r\\n20628,0\\r\\n20629,0\\r\\n20630,0\\r\\n20631,0\\r\\n20632,0\\r\\n20633,0\\r\\n20634,0\\r\\n20635,0\\r\\n20636,0\\r\\n20637,0\\r\\n20638,0\\r\\n20639,0\\r\\n20640,0\\r\\n20641,0\\r\\n20642,0\\r\\n20643,0\\r\\n20644,0\\r\\n20645,0\\r\\n20646,0\\r\\n20647,0\\r\\n20648,0\\r\\n20649,0\\r\\n20650,0\\r\\n20651,0\\r\\n20652,0\\r\\n20653,0\\r\\n20654,0\\r\\n20655,0\\r\\n20656,0\\r\\n20657,0\\r\\n20658,0\\r\\n20659,0\\r\\n20660,0\\r\\n20661,0\\r\\n20662,0\\r\\n20663,0\\r\\n20664,0\\r\\n20665,0\\r\\n20666,0\\r\\n20667,0\\r\\n20668,0\\r\\n20669,0\\r\\n20670,0\\r\\n20671,0\\r\\n20672,0\\r\\n20673,0\\r\\n20674,0\\r\\n20675,0\\r\\n20676,0\\r\\n20677,0\\r\\n20678,0\\r\\n20679,0\\r\\n20680,0\\r\\n20681,0\\r\\n20682,0\\r\\n20683,0\\r\\n20684,0\\r\\n20685,0\\r\\n20686,0\\r\\n20687,0\\r\\n20688,0\\r\\n20689,0\\r\\n20690,0\\r\\n20691,0\\r\\n20692,0\\r\\n20693,0\\r\\n20694,0\\r\\n20695,0\\r\\n20696,0\\r\\n20697,0\\r\\n20698,0\\r\\n20699,0\\r\\n20700,0\\r\\n20701,0\\r\\n20702,0\\r\\n20703,0\\r\\n20704,0\\r\\n20705,0\\r\\n20706,0\\r\\n20707,0\\r\\n20708,0\\r\\n20709,0\\r\\n20710,0\\r\\n20711,0\\r\\n20712,0\\r\\n20713,0\\r\\n20714,0\\r\\n20715,0\\r\\n20716,0\\r\\n20717,0\\r\\n20718,0\\r\\n20719,0\\r\\n20720,0\\r\\n20721,0\\r\\n20722,0\\r\\n20723,0\\r\\n20724,0\\r\\n20725,0\\r\\n20726,0\\r\\n20727,0\\r\\n20728,0\\r\\n20729,0\\r\\n20730,0\\r\\n20731,0\\r\\n20732,0\\r\\n20733,0\\r\\n20734,0\\r\\n20735,0\\r\\n20736,0\\r\\n20737,0\\r\\n20738,0\\r\\n20739,0\\r\\n20740,0\\r\\n20741,0\\r\\n20742,0\\r\\n20743,0\\r\\n20744,0\\r\\n20745,0\\r\\n20746,0\\r\\n20747,0\\r\\n20748,0\\r\\n20749,0\\r\\n20750,0\\r\\n20751,0\\r\\n20752,0\\r\\n20753,0\\r\\n20754,0\\r\\n20755,0\\r\\n20756,0\\r\\n20757,0\\r\\n20758,0\\r\\n20759,0\\r\\n20760,0\\r\\n20761,0\\r\\n20762,0\\r\\n20763,0\\r\\n20764,0\\r\\n20765,0\\r\\n20766,0\\r\\n20767,0\\r\\n20768,0\\r\\n20769,0\\r\\n20770,0\\r\\n20771,0\\r\\n20772,0\\r\\n20773,0\\r\\n20774,0\\r\\n20775,0\\r\\n20776,0\\r\\n20777,0\\r\\n20778,0\\r\\n20779,0\\r\\n20780,0\\r\\n20781,0\\r\\n20782,0\\r\\n20783,0\\r\\n20784,0\\r\\n20785,0\\r\\n20786,0\\r\\n20787,0\\r\\n20788,0\\r\\n20789,0\\r\\n20790,0\\r\\n20791,0\\r\\n20792,0\\r\\n20793,0\\r\\n20794,0\\r\\n20795,0\\r\\n20796,0\\r\\n20797,0\\r\\n20798,0\\r\\n20799,0\\r\\n20800,0\\r\\n20801,0\\r\\n20802,0\\r\\n20803,0\\r\\n20804,0\\r\\n20805,0\\r\\n20806,0\\r\\n20807,0\\r\\n20808,0\\r\\n20809,0\\r\\n20810,0\\r\\n20811,0\\r\\n20812,0\\r\\n20813,0\\r\\n20814,0\\r\\n20815,0\\r\\n20816,0\\r\\n20817,0\\r\\n20818,0\\r\\n20819,0\\r\\n20820,0\\r\\n20821,0\\r\\n20822,0\\r\\n20823,0\\r\\n20824,0\\r\\n20825,0\\r\\n20826,0\\r\\n20827,0\\r\\n20828,0\\r\\n20829,0\\r\\n20830,0\\r\\n20831,0\\r\\n20832,0\\r\\n20833,0\\r\\n20834,0\\r\\n20835,0\\r\\n20836,0\\r\\n20837,0\\r\\n20838,0\\r\\n20839,0\\r\\n20840,0\\r\\n20841,0\\r\\n20842,0\\r\\n20843,0\\r\\n20844,0\\r\\n20845,0\\r\\n20846,0\\r\\n20847,0\\r\\n20848,0\\r\\n20849,0\\r\\n20850,0\\r\\n20851,0\\r\\n20852,0\\r\\n20853,0\\r\\n20854,0\\r\\n20855,0\\r\\n20856,0\\r\\n20857,0\\r\\n20858,0\\r\\n20859,0\\r\\n20860,0\\r\\n20861,0\\r\\n20862,0\\r\\n20863,0\\r\\n20864,0\\r\\n20865,0\\r\\n20866,0\\r\\n20867,0\\r\\n20868,0\\r\\n20869,0\\r\\n20870,0\\r\\n20871,0\\r\\n20872,0\\r\\n20873,0\\r\\n20874,0\\r\\n20875,0\\r\\n20876,0\\r\\n20877,0\\r\\n20878,0\\r\\n20879,0\\r\\n20880,0\\r\\n20881,0\\r\\n20882,0\\r\\n20883,0\\r\\n20884,0\\r\\n20885,0\\r\\n20886,0\\r\\n20887,0\\r\\n20888,0\\r\\n20889,0\\r\\n20890,0\\r\\n20891,0\\r\\n20892,0\\r\\n20893,0\\r\\n20894,0\\r\\n20895,0\\r\\n20896,0\\r\\n20897,0\\r\\n20898,0\\r\\n20899,0\\r\\n20900,0\\r\\n20901,0\\r\\n20902,0\\r\\n20903,0\\r\\n20904,0\\r\\n20905,0\\r\\n20906,0\\r\\n20907,0\\r\\n20908,0\\r\\n20909,0\\r\\n20910,0\\r\\n20911,0\\r\\n20912,0\\r\\n20913,0\\r\\n20914,0\\r\\n20915,0\\r\\n20916,0\\r\\n20917,0\\r\\n20918,0\\r\\n20919,0\\r\\n20920,0\\r\\n20921,0\\r\\n20922,0\\r\\n20923,0\\r\\n20924,0\\r\\n20925,0\\r\\n20926,0\\r\\n20927,0\\r\\n20928,0\\r\\n20929,0\\r\\n20930,0\\r\\n20931,0\\r\\n20932,0\\r\\n20933,0\\r\\n20934,0\\r\\n20935,0\\r\\n20936,0\\r\\n20937,0\\r\\n20938,0\\r\\n20939,0\\r\\n20940,0\\r\\n20941,0\\r\\n20942,0\\r\\n20943,0\\r\\n20944,0\\r\\n20945,0\\r\\n20946,0\\r\\n20947,0\\r\\n20948,0\\r\\n20949,0\\r\\n20950,0\\r\\n20951,0\\r\\n20952,0\\r\\n20953,0\\r\\n20954,0\\r\\n20955,0\\r\\n20956,0\\r\\n20957,0\\r\\n20958,0\\r\\n20959,0\\r\\n20960,0\\r\\n20961,0\\r\\n20962,0\\r\\n20963,0\\r\\n20964,0\\r\\n20965,0\\r\\n20966,0\\r\\n20967,0\\r\\n20968,0\\r\\n20969,0\\r\\n20970,0\\r\\n20971,0\\r\\n20972,0\\r\\n20973,0\\r\\n20974,0\\r\\n20975,0\\r\\n20976,0\\r\\n20977,0\\r\\n20978,0\\r\\n20979,0\\r\\n20980,0\\r\\n20981,0\\r\\n20982,0\\r\\n20983,0\\r\\n20984,0\\r\\n20985,0\\r\\n20986,0\\r\\n20987,0\\r\\n20988,0\\r\\n20989,0\\r\\n20990,0\\r\\n20991,0\\r\\n20992,0\\r\\n20993,0\\r\\n20994,0\\r\\n20995,0\\r\\n20996,0\\r\\n20997,0\\r\\n20998,0\\r\\n20999,0\\r\\n21000,0\\r\\n21001,0\\r\\n21002,0\\r\\n21003,0\\r\\n21004,0\\r\\n21005,0\\r\\n21006,0\\r\\n21007,0\\r\\n21008,0\\r\\n21009,0\\r\\n21010,0\\r\\n21011,0\\r\\n21012,0\\r\\n21013,0\\r\\n21014,0\\r\\n21015,0\\r\\n21016,0\\r\\n21017,0\\r\\n21018,0\\r\\n21019,0\\r\\n21020,0\\r\\n21021,0\\r\\n21022,0\\r\\n21023,0\\r\\n21024,0\\r\\n21025,0\\r\\n21026,0\\r\\n21027,0\\r\\n21028,0\\r\\n21029,0\\r\\n21030,0\\r\\n21031,0\\r\\n21032,0\\r\\n21033,0\\r\\n21034,0\\r\\n21035,0\\r\\n21036,0\\r\\n21037,0\\r\\n21038,0\\r\\n21039,0\\r\\n21040,0\\r\\n21041,0\\r\\n21042,0\\r\\n21043,0\\r\\n21044,0\\r\\n21045,0\\r\\n21046,0\\r\\n21047,0\\r\\n21048,0\\r\\n21049,0\\r\\n21050,0\\r\\n21051,0\\r\\n21052,0\\r\\n21053,0\\r\\n21054,0\\r\\n21055,0\\r\\n21056,0\\r\\n21057,0\\r\\n21058,0\\r\\n21059,0\\r\\n21060,0\\r\\n21061,0\\r\\n21062,0\\r\\n21063,0\\r\\n21064,0\\r\\n21065,0\\r\\n21066,0\\r\\n21067,0\\r\\n21068,0\\r\\n21069,0\\r\\n21070,0\\r\\n21071,0\\r\\n21072,0\\r\\n21073,0\\r\\n21074,0\\r\\n21075,0\\r\\n21076,0\\r\\n21077,0\\r\\n21078,0\\r\\n21079,0\\r\\n21080,0\\r\\n21081,0\\r\\n21082,0\\r\\n21083,0\\r\\n21084,0\\r\\n21085,0\\r\\n21086,0\\r\\n21087,0\\r\\n21088,0\\r\\n21089,0\\r\\n21090,0\\r\\n21091,0\\r\\n21092,0\\r\\n21093,0\\r\\n21094,0\\r\\n21095,0\\r\\n21096,0\\r\\n21097,0\\r\\n21098,0\\r\\n21099,0\\r\\n21100,0\\r\\n21101,0\\r\\n21102,0\\r\\n21103,0\\r\\n21104,0\\r\\n21105,0\\r\\n21106,0\\r\\n21107,0\\r\\n21108,0\\r\\n21109,0\\r\\n21110,0\\r\\n21111,0\\r\\n21112,0\\r\\n21113,0\\r\\n21114,0\\r\\n21115,0\\r\\n21116,0\\r\\n21117,0\\r\\n21118,0\\r\\n21119,0\\r\\n21120,0\\r\\n21121,0\\r\\n21122,0\\r\\n21123,0\\r\\n21124,0\\r\\n21125,0\\r\\n21126,0\\r\\n21127,0\\r\\n21128,0\\r\\n21129,0\\r\\n21130,0\\r\\n21131,0\\r\\n21132,0\\r\\n21133,0\\r\\n21134,0\\r\\n21135,0\\r\\n21136,0\\r\\n21137,0\\r\\n21138,0\\r\\n21139,0\\r\\n21140,0\\r\\n21141,0\\r\\n21142,0\\r\\n21143,0\\r\\n21144,0\\r\\n21145,0\\r\\n21146,0\\r\\n21147,0\\r\\n21148,0\\r\\n21149,0\\r\\n21150,0\\r\\n21151,0\\r\\n21152,0\\r\\n21153,0\\r\\n21154,0\\r\\n21155,0\\r\\n21156,0\\r\\n21157,0\\r\\n21158,0\\r\\n21159,0\\r\\n21160,0\\r\\n21161,0\\r\\n21162,0\\r\\n21163,0\\r\\n21164,0\\r\\n21165,0\\r\\n21166,0\\r\\n21167,0\\r\\n21168,0\\r\\n21169,0\\r\\n21170,0\\r\\n21171,0\\r\\n21172,0\\r\\n21173,0\\r\\n21174,0\\r\\n21175,0\\r\\n21176,0\\r\\n21177,0\\r\\n21178,0\\r\\n21179,0\\r\\n21180,0\\r\\n21181,0\\r\\n21182,0\\r\\n21183,0\\r\\n21184,0\\r\\n21185,0\\r\\n21186,0\\r\\n21187,0\\r\\n21188,0\\r\\n21189,0\\r\\n21190,0\\r\\n21191,0\\r\\n21192,0\\r\\n21193,0\\r\\n21194,0\\r\\n21195,0\\r\\n21196,0\\r\\n21197,0\\r\\n21198,0\\r\\n21199,0\\r\\n21200,0\\r\\n21201,0\\r\\n21202,0\\r\\n21203,0\\r\\n21204,0\\r\\n21205,0\\r\\n21206,0\\r\\n21207,0\\r\\n21208,0\\r\\n21209,0\\r\\n21210,0\\r\\n21211,0\\r\\n21212,0\\r\\n21213,0\\r\\n21214,0\\r\\n21215,0\\r\\n21216,0\\r\\n21217,0\\r\\n21218,0\\r\\n21219,0\\r\\n21220,0\\r\\n21221,0\\r\\n21222,0\\r\\n21223,0\\r\\n21224,0\\r\\n21225,0\\r\\n21226,0\\r\\n21227,0\\r\\n21228,0\\r\\n21229,0\\r\\n21230,0\\r\\n21231,0\\r\\n21232,0\\r\\n21233,0\\r\\n21234,0\\r\\n21235,0\\r\\n21236,0\\r\\n21237,0\\r\\n21238,0\\r\\n21239,0\\r\\n21240,0\\r\\n21241,0\\r\\n21242,0\\r\\n21243,0\\r\\n21244,0\\r\\n21245,0\\r\\n21246,0\\r\\n21247,0\\r\\n21248,0\\r\\n21249,0\\r\\n21250,0\\r\\n21251,0\\r\\n21252,0\\r\\n21253,0\\r\\n21254,0\\r\\n21255,0\\r\\n21256,0\\r\\n21257,0\\r\\n21258,0\\r\\n21259,0\\r\\n21260,0\\r\\n21261,0\\r\\n21262,0\\r\\n21263,0\\r\\n21264,0\\r\\n21265,0\\r\\n21266,0\\r\\n21267,0\\r\\n21268,0\\r\\n21269,0\\r\\n21270,0\\r\\n21271,0\\r\\n21272,0\\r\\n21273,0\\r\\n21274,0\\r\\n21275,0\\r\\n21276,0\\r\\n21277,0\\r\\n21278,0\\r\\n21279,0\\r\\n21280,0\\r\\n21281,0\\r\\n21282,0\\r\\n21283,0\\r\\n21284,0\\r\\n21285,0\\r\\n21286,0\\r\\n21287,0\\r\\n21288,0\\r\\n21289,0\\r\\n21290,0\\r\\n21291,0\\r\\n21292,0\\r\\n21293,0\\r\\n21294,0\\r\\n21295,0\\r\\n21296,0\\r\\n21297,0\\r\\n21298,0\\r\\n21299,0\\r\\n21300,0\\r\\n21301,0\\r\\n21302,0\\r\\n21303,0\\r\\n21304,0\\r\\n21305,0\\r\\n21306,0\\r\\n21307,0\\r\\n21308,0\\r\\n21309,0\\r\\n21310,0\\r\\n21311,0\\r\\n21312,0\\r\\n21313,0\\r\\n21314,0\\r\\n21315,0\\r\\n21316,0\\r\\n21317,0\\r\\n21318,0\\r\\n21319,0\\r\\n21320,0\\r\\n21321,0\\r\\n21322,0\\r\\n21323,0\\r\\n21324,0\\r\\n21325,0\\r\\n21326,0\\r\\n21327,0\\r\\n21328,0\\r\\n21329,0\\r\\n21330,0\\r\\n21331,0\\r\\n21332,0\\r\\n21333,0\\r\\n21334,0\\r\\n21335,0\\r\\n21336,0\\r\\n21337,0\\r\\n21338,0\\r\\n21339,0\\r\\n21340,0\\r\\n21341,0\\r\\n21342,0\\r\\n21343,0\\r\\n21344,0\\r\\n21345,0\\r\\n21346,0\\r\\n21347,0\\r\\n21348,0\\r\\n21349,0\\r\\n21350,0\\r\\n21351,0\\r\\n21352,0\\r\\n21353,0\\r\\n21354,0\\r\\n21355,0\\r\\n21356,0\\r\\n21357,0\\r\\n21358,0\\r\\n21359,0\\r\\n21360,0\\r\\n21361,0\\r\\n21362,0\\r\\n21363,0\\r\\n21364,0\\r\\n21365,0\\r\\n21366,0\\r\\n21367,0\\r\\n21368,0\\r\\n21369,0\\r\\n21370,0\\r\\n21371,0\\r\\n21372,0\\r\\n21373,0\\r\\n21374,0\\r\\n21375,0\\r\\n21376,0\\r\\n21377,0\\r\\n21378,0\\r\\n21379,0\\r\\n21380,0\\r\\n21381,0\\r\\n21382,0\\r\\n21383,0\\r\\n21384,0\\r\\n21385,0\\r\\n21386,0\\r\\n21387,0\\r\\n21388,0\\r\\n21389,0\\r\\n21390,0\\r\\n21391,0\\r\\n21392,0\\r\\n21393,0\\r\\n21394,0\\r\\n21395,0\\r\\n21396,0\\r\\n21397,0\\r\\n21398,0\\r\\n21399,0\\r\\n21400,0\\r\\n21401,0\\r\\n21402,0\\r\\n21403,0\\r\\n21404,0\\r\\n21405,0\\r\\n21406,0\\r\\n21407,0\\r\\n21408,0\\r\\n21409,0\\r\\n21410,0\\r\\n21411,0\\r\\n21412,0\\r\\n21413,0\\r\\n21414,0\\r\\n21415,0\\r\\n21416,0\\r\\n21417,0\\r\\n21418,0\\r\\n21419,0\\r\\n21420,0\\r\\n21421,0\\r\\n21422,0\\r\\n21423,0\\r\\n21424,0\\r\\n21425,0\\r\\n21426,0\\r\\n21427,0\\r\\n21428,0\\r\\n21429,0\\r\\n21430,0\\r\\n21431,0\\r\\n21432,0\\r\\n21433,0\\r\\n21434,0\\r\\n21435,0\\r\\n21436,0\\r\\n21437,0\\r\\n21438,0\\r\\n21439,0\\r\\n21440,0\\r\\n21441,0\\r\\n21442,0\\r\\n21443,0\\r\\n21444,0\\r\\n21445,0\\r\\n21446,0\\r\\n21447,0\\r\\n21448,0\\r\\n21449,0\\r\\n21450,0\\r\\n21451,0\\r\\n21452,0\\r\\n21453,0\\r\\n21454,0\\r\\n21455,0\\r\\n21456,0\\r\\n21457,0\\r\\n21458,0\\r\\n21459,0\\r\\n21460,0\\r\\n21461,0\\r\\n21462,0\\r\\n21463,0\\r\\n21464,0\\r\\n21465,0\\r\\n21466,0\\r\\n21467,0\\r\\n21468,0\\r\\n21469,0\\r\\n21470,0\\r\\n21471,0\\r\\n21472,0\\r\\n21473,0\\r\\n21474,0\\r\\n21475,0\\r\\n21476,0\\r\\n21477,0\\r\\n21478,0\\r\\n21479,0\\r\\n21480,0\\r\\n21481,0\\r\\n21482,0\\r\\n21483,0\\r\\n21484,0\\r\\n21485,0\\r\\n21486,0\\r\\n21487,0\\r\\n21488,0\\r\\n21489,0\\r\\n21490,0\\r\\n21491,0\\r\\n21492,0\\r\\n21493,0\\r\\n21494,0\\r\\n21495,0\\r\\n21496,0\\r\\n21497,0\\r\\n21498,0\\r\\n21499,0\\r\\n21500,0\\r\\n21501,0\\r\\n21502,0\\r\\n21503,0\\r\\n21504,0\\r\\n21505,0\\r\\n21506,0\\r\\n21507,0\\r\\n21508,0\\r\\n21509,0\\r\\n21510,0\\r\\n21511,0\\r\\n21512,0\\r\\n21513,0\\r\\n21514,0\\r\\n21515,0\\r\\n21516,0\\r\\n21517,0\\r\\n21518,0\\r\\n21519,0\\r\\n21520,0\\r\\n21521,0\\r\\n21522,0\\r\\n21523,0\\r\\n21524,0\\r\\n21525,0\\r\\n21526,0\\r\\n21527,0\\r\\n21528,0\\r\\n21529,0\\r\\n21530,0\\r\\n21531,0\\r\\n21532,0\\r\\n21533,0\\r\\n21534,0\\r\\n21535,0\\r\\n21536,0\\r\\n21537,0\\r\\n21538,0\\r\\n21539,0\\r\\n21540,0\\r\\n21541,0\\r\\n21542,0\\r\\n21543,0\\r\\n21544,0\\r\\n21545,0\\r\\n21546,0\\r\\n21547,0\\r\\n21548,0\\r\\n21549,0\\r\\n21550,0\\r\\n21551,0\\r\\n21552,0\\r\\n21553,0\\r\\n21554,0\\r\\n21555,0\\r\\n21556,0\\r\\n21557,0\\r\\n21558,0\\r\\n21559,0\\r\\n21560,0\\r\\n21561,0\\r\\n21562,0\\r\\n21563,0\\r\\n21564,0\\r\\n21565,0\\r\\n21566,0\\r\\n21567,0\\r\\n21568,0\\r\\n21569,0\\r\\n21570,0\\r\\n21571,0\\r\\n21572,0\\r\\n21573,0\\r\\n21574,0\\r\\n21575,0\\r\\n21576,0\\r\\n21577,0\\r\\n21578,0\\r\\n21579,0\\r\\n21580,0\\r\\n21581,0\\r\\n21582,0\\r\\n21583,0\\r\\n21584,0\\r\\n21585,0\\r\\n21586,0\\r\\n21587,0\\r\\n21588,0\\r\\n21589,0\\r\\n21590,0\\r\\n21591,0\\r\\n21592,0\\r\\n21593,0\\r\\n21594,0\\r\\n21595,0\\r\\n21596,0\\r\\n21597,0\\r\\n21598,0\\r\\n21599,0\\r\\n21600,0\\r\\n21601,0\\r\\n21602,0\\r\\n21603,0\\r\\n21604,0\\r\\n21605,0\\r\\n21606,0\\r\\n21607,0\\r\\n21608,0\\r\\n21609,0\\r\\n21610,0\\r\\n21611,0\\r\\n21612,0\\r\\n21613,0\\r\\n21614,0\\r\\n21615,0\\r\\n21616,0\\r\\n21617,0\\r\\n21618,0\\r\\n21619,0\\r\\n21620,0\\r\\n21621,0\\r\\n21622,0\\r\\n21623,0\\r\\n21624,0\\r\\n21625,0\\r\\n21626,0\\r\\n21627,0\\r\\n21628,0\\r\\n21629,0\\r\\n21630,0\\r\\n21631,0\\r\\n21632,0\\r\\n21633,0\\r\\n21634,0\\r\\n21635,0\\r\\n21636,0\\r\\n21637,0\\r\\n21638,0\\r\\n21639,0\\r\\n21640,0\\r\\n21641,0\\r\\n21642,0\\r\\n21643,0\\r\\n21644,0\\r\\n21645,0\\r\\n21646,0\\r\\n21647,0\\r\\n21648,0\\r\\n21649,0\\r\\n21650,0\\r\\n21651,0\\r\\n21652,0\\r\\n21653,0\\r\\n21654,0\\r\\n21655,0\\r\\n21656,0\\r\\n21657,0\\r\\n21658,0\\r\\n21659,0\\r\\n21660,0\\r\\n21661,0\\r\\n21662,0\\r\\n21663,0\\r\\n21664,0\\r\\n21665,0\\r\\n21666,0\\r\\n21667,0\\r\\n21668,0\\r\\n21669,0\\r\\n21670,0\\r\\n21671,0\\r\\n21672,0\\r\\n21673,0\\r\\n21674,0\\r\\n21675,0\\r\\n21676,0\\r\\n21677,0\\r\\n21678,0\\r\\n21679,0\\r\\n21680,0\\r\\n21681,0\\r\\n21682,0\\r\\n21683,0\\r\\n21684,0\\r\\n21685,0\\r\\n21686,0\\r\\n21687,0\\r\\n21688,0\\r\\n21689,0\\r\\n21690,0\\r\\n21691,0\\r\\n21692,0\\r\\n21693,0\\r\\n21694,0\\r\\n21695,0\\r\\n21696,0\\r\\n21697,0\\r\\n21698,0\\r\\n21699,0\\r\\n21700,0\\r\\n21701,0\\r\\n21702,0\\r\\n21703,0\\r\\n21704,0\\r\\n21705,0\\r\\n21706,0\\r\\n21707,0\\r\\n21708,0\\r\\n21709,0\\r\\n21710,0\\r\\n21711,0\\r\\n21712,0\\r\\n21713,0\\r\\n21714,0\\r\\n21715,0\\r\\n21716,0\\r\\n21717,0\\r\\n21718,0\\r\\n21719,0\\r\\n21720,0\\r\\n21721,0\\r\\n21722,0\\r\\n21723,0\\r\\n21724,0\\r\\n21725,0\\r\\n21726,0\\r\\n21727,0\\r\\n21728,0\\r\\n21729,0\\r\\n21730,0\\r\\n21731,0\\r\\n21732,0\\r\\n21733,0\\r\\n21734,0\\r\\n21735,0\\r\\n21736,0\\r\\n21737,0\\r\\n21738,0\\r\\n21739,0\\r\\n21740,0\\r\\n21741,0\\r\\n21742,0\\r\\n21743,0\\r\\n21744,0\\r\\n21745,0\\r\\n21746,0\\r\\n21747,0\\r\\n21748,0\\r\\n21749,0\\r\\n21750,0\\r\\n21751,0\\r\\n21752,0\\r\\n21753,0\\r\\n21754,0\\r\\n21755,0\\r\\n21756,0\\r\\n21757,0\\r\\n21758,0\\r\\n21759,0\\r\\n21760,0\\r\\n21761,0\\r\\n21762,0\\r\\n21763,0\\r\\n21764,0\\r\\n21765,0\\r\\n21766,0\\r\\n21767,0\\r\\n21768,0\\r\\n21769,0\\r\\n21770,0\\r\\n21771,0\\r\\n21772,0\\r\\n21773,0\\r\\n21774,0\\r\\n21775,0\\r\\n21776,0\\r\\n21777,0\\r\\n21778,0\\r\\n21779,0\\r\\n21780,0\\r\\n21781,0\\r\\n21782,0\\r\\n21783,0\\r\\n21784,0\\r\\n21785,0\\r\\n21786,0\\r\\n21787,0\\r\\n21788,0\\r\\n21789,0\\r\\n21790,0\\r\\n21791,0\\r\\n21792,0\\r\\n21793,0\\r\\n21794,0\\r\\n21795,0\\r\\n21796,0\\r\\n21797,0\\r\\n21798,0\\r\\n21799,0\\r\\n21800,0\\r\\n21801,0\\r\\n21802,0\\r\\n21803,0\\r\\n21804,0\\r\\n21805,0\\r\\n21806,0\\r\\n21807,0\\r\\n21808,0\\r\\n21809,0\\r\\n21810,0\\r\\n21811,0\\r\\n21812,0\\r\\n21813,0\\r\\n21814,0\\r\\n21815,0\\r\\n21816,0\\r\\n21817,0\\r\\n21818,0\\r\\n21819,0\\r\\n21820,0\\r\\n21821,0\\r\\n21822,0\\r\\n21823,0\\r\\n21824,0\\r\\n21825,0\\r\\n21826,0\\r\\n21827,0\\r\\n21828,0\\r\\n21829,0\\r\\n21830,0\\r\\n21831,0\\r\\n21832,0\\r\\n21833,0\\r\\n21834,0\\r\\n21835,0\\r\\n21836,0\\r\\n21837,0\\r\\n21838,0\\r\\n21839,0\\r\\n21840,0\\r\\n21841,0\\r\\n21842,0\\r\\n21843,0\\r\\n21844,0\\r\\n21845,0\\r\\n21846,0\\r\\n21847,0\\r\\n21848,0\\r\\n21849,0\\r\\n21850,0\\r\\n21851,0\\r\\n21852,0\\r\\n21853,0\\r\\n21854,0\\r\\n21855,0\\r\\n21856,0\\r\\n21857,0\\r\\n21858,0\\r\\n21859,0\\r\\n21860,0\\r\\n21861,0\\r\\n21862,0\\r\\n21863,0\\r\\n21864,0\\r\\n21865,0\\r\\n21866,0\\r\\n21867,0\\r\\n21868,0\\r\\n21869,0\\r\\n21870,0\\r\\n21871,0\\r\\n21872,0\\r\\n21873,0\\r\\n21874,0\\r\\n21875,0\\r\\n21876,0\\r\\n21877,0\\r\\n21878,0\\r\\n21879,0\\r\\n21880,0\\r\\n21881,0\\r\\n21882,0\\r\\n21883,0\\r\\n21884,0\\r\\n21885,0\\r\\n21886,0\\r\\n21887,0\\r\\n21888,0\\r\\n21889,0\\r\\n21890,0\\r\\n21891,0\\r\\n21892,0\\r\\n21893,0\\r\\n21894,0\\r\\n21895,0\\r\\n21896,0\\r\\n21897,0\\r\\n21898,0\\r\\n21899,0\\r\\n21900,0\\r\\n21901,0\\r\\n21902,0\\r\\n21903,0\\r\\n21904,0\\r\\n21905,0\\r\\n21906,0\\r\\n21907,0\\r\\n21908,0\\r\\n21909,0\\r\\n21910,0\\r\\n21911,0\\r\\n21912,0\\r\\n21913,0\\r\\n21914,0\\r\\n21915,0\\r\\n21916,0\\r\\n21917,0\\r\\n21918,0\\r\\n21919,0\\r\\n21920,0\\r\\n21921,0\\r\\n21922,0\\r\\n21923,0\\r\\n21924,0\\r\\n21925,0\\r\\n21926,0\\r\\n21927,0\\r\\n21928,0\\r\\n21929,0\\r\\n21930,0\\r\\n21931,0\\r\\n21932,0\\r\\n21933,0\\r\\n21934,0\\r\\n21935,0\\r\\n21936,0\\r\\n21937,0\\r\\n21938,0\\r\\n21939,0\\r\\n21940,0\\r\\n21941,0\\r\\n21942,0\\r\\n21943,0\\r\\n21944,0\\r\\n21945,0\\r\\n21946,0\\r\\n21947,0\\r\\n21948,0\\r\\n21949,0\\r\\n21950,0\\r\\n21951,0\\r\\n21952,0\\r\\n21953,0\\r\\n21954,0\\r\\n21955,0\\r\\n21956,0\\r\\n21957,0\\r\\n21958,0\\r\\n21959,0\\r\\n21960,0\\r\\n21961,0\\r\\n21962,0\\r\\n21963,0\\r\\n21964,0\\r\\n21965,0\\r\\n21966,0\\r\\n21967,0\\r\\n21968,0\\r\\n21969,0\\r\\n21970,0\\r\\n21971,0\\r\\n21972,0\\r\\n21973,0\\r\\n21974,0\\r\\n21975,0\\r\\n21976,0\\r\\n21977,0\\r\\n21978,0\\r\\n21979,0\\r\\n21980,0\\r\\n21981,0\\r\\n21982,0\\r\\n21983,0\\r\\n21984,0\\r\\n21985,0\\r\\n21986,0\\r\\n21987,0\\r\\n21988,0\\r\\n21989,0\\r\\n21990,0\\r\\n21991,0\\r\\n21992,0\\r\\n21993,0\\r\\n21994,0\\r\\n21995,0\\r\\n21996,0\\r\\n21997,0\\r\\n21998,0\\r\\n21999,0\\r\\n22000,0\\r\\n22001,0\\r\\n22002,0\\r\\n22003,0\\r\\n22004,0\\r\\n22005,0\\r\\n22006,0\\r\\n22007,0\\r\\n22008,0\\r\\n22009,0\\r\\n22010,0\\r\\n22011,0\\r\\n22012,0\\r\\n22013,0\\r\\n22014,0\\r\\n22015,0\\r\\n22016,0\\r\\n22017,0\\r\\n22018,0\\r\\n22019,0\\r\\n22020,0\\r\\n22021,0\\r\\n22022,0\\r\\n22023,0\\r\\n22024,0\\r\\n22025,0\\r\\n22026,0\\r\\n22027,0\\r\\n22028,0\\r\\n22029,0\\r\\n22030,0\\r\\n22031,0\\r\\n22032,0\\r\\n22033,0\\r\\n22034,0\\r\\n22035,0\\r\\n22036,0\\r\\n22037,0\\r\\n22038,0\\r\\n22039,0\\r\\n22040,0\\r\\n22041,0\\r\\n22042,0\\r\\n22043,0\\r\\n22044,0\\r\\n22045,0\\r\\n22046,0\\r\\n22047,0\\r\\n22048,0\\r\\n22049,0\\r\\n22050,0\\r\\n22051,0\\r\\n22052,0\\r\\n22053,0\\r\\n22054,0\\r\\n22055,0\\r\\n22056,0\\r\\n22057,0\\r\\n22058,0\\r\\n22059,0\\r\\n22060,0\\r\\n22061,0\\r\\n22062,0\\r\\n22063,0\\r\\n22064,0\\r\\n22065,0\\r\\n22066,0\\r\\n22067,0\\r\\n22068,0\\r\\n22069,0\\r\\n22070,0\\r\\n22071,0\\r\\n22072,0\\r\\n22073,0\\r\\n22074,0\\r\\n22075,0\\r\\n22076,0\\r\\n22077,0\\r\\n22078,0\\r\\n22079,0\\r\\n22080,0\\r\\n22081,0\\r\\n22082,0\\r\\n22083,0\\r\\n22084,0\\r\\n22085,0\\r\\n22086,0\\r\\n22087,0\\r\\n22088,0\\r\\n22089,0\\r\\n22090,0\\r\\n22091,0\\r\\n22092,0\\r\\n22093,0\\r\\n22094,0\\r\\n22095,0\\r\\n22096,0\\r\\n22097,0\\r\\n22098,0\\r\\n22099,0\\r\\n22100,0\\r\\n22101,0\\r\\n22102,0\\r\\n22103,0\\r\\n22104,0\\r\\n22105,0\\r\\n22106,0\\r\\n22107,0\\r\\n22108,0\\r\\n22109,0\\r\\n22110,0\\r\\n22111,0\\r\\n22112,0\\r\\n22113,0\\r\\n22114,0\\r\\n22115,0\\r\\n22116,0\\r\\n22117,0\\r\\n22118,0\\r\\n22119,0\\r\\n22120,0\\r\\n22121,0\\r\\n22122,0\\r\\n22123,0\\r\\n22124,0\\r\\n22125,0\\r\\n22126,0\\r\\n22127,0\\r\\n22128,0\\r\\n22129,0\\r\\n22130,0\\r\\n22131,0\\r\\n22132,0\\r\\n22133,0\\r\\n22134,0\\r\\n22135,0\\r\\n22136,0\\r\\n22137,0\\r\\n22138,0\\r\\n22139,0\\r\\n22140,0\\r\\n22141,0\\r\\n22142,0\\r\\n22143,0\\r\\n22144,0\\r\\n22145,0\\r\\n22146,0\\r\\n22147,0\\r\\n22148,0\\r\\n22149,0\\r\\n22150,0\\r\\n22151,0\\r\\n22152,0\\r\\n22153,0\\r\\n22154,0\\r\\n22155,0\\r\\n22156,0\\r\\n22157,0\\r\\n22158,0\\r\\n22159,0\\r\\n22160,0\\r\\n22161,0\\r\\n22162,0\\r\\n22163,0\\r\\n22164,0\\r\\n22165,0\\r\\n22166,0\\r\\n22167,0\\r\\n22168,0\\r\\n22169,0\\r\\n22170,0\\r\\n22171,0\\r\\n22172,0\\r\\n22173,0\\r\\n22174,0\\r\\n22175,0\\r\\n22176,0\\r\\n22177,0\\r\\n22178,0\\r\\n22179,0\\r\\n22180,0\\r\\n22181,0\\r\\n22182,0\\r\\n22183,0\\r\\n22184,0\\r\\n22185,0\\r\\n22186,0\\r\\n22187,0\\r\\n22188,0\\r\\n22189,0\\r\\n22190,0\\r\\n22191,0\\r\\n22192,0\\r\\n22193,0\\r\\n22194,0\\r\\n22195,0\\r\\n22196,0\\r\\n22197,0\\r\\n22198,0\\r\\n22199,0\\r\\n22200,0\\r\\n22201,0\\r\\n22202,0\\r\\n22203,0\\r\\n22204,0\\r\\n22205,0\\r\\n22206,0\\r\\n22207,0\\r\\n22208,0\\r\\n22209,0\\r\\n22210,0\\r\\n22211,0\\r\\n22212,0\\r\\n22213,0\\r\\n22214,0\\r\\n22215,0\\r\\n22216,0\\r\\n22217,0\\r\\n22218,0\\r\\n22219,0\\r\\n22220,0\\r\\n22221,0\\r\\n22222,0\\r\\n22223,0\\r\\n22224,0\\r\\n22225,0\\r\\n22226,0\\r\\n22227,0\\r\\n22228,0\\r\\n22229,0\\r\\n22230,0\\r\\n22231,0\\r\\n22232,0\\r\\n22233,0\\r\\n22234,0\\r\\n22235,0\\r\\n22236,0\\r\\n22237,0\\r\\n22238,0\\r\\n22239,0\\r\\n22240,0\\r\\n22241,0\\r\\n22242,0\\r\\n22243,0\\r\\n22244,0\\r\\n22245,0\\r\\n22246,0\\r\\n22247,0\\r\\n22248,0\\r\\n22249,0\\r\\n22250,0\\r\\n22251,0\\r\\n22252,0\\r\\n22253,0\\r\\n22254,0\\r\\n22255,0\\r\\n22256,0\\r\\n22257,0\\r\\n22258,0\\r\\n22259,0\\r\\n22260,0\\r\\n22261,0\\r\\n22262,0\\r\\n22263,0\\r\\n22264,0\\r\\n22265,0\\r\\n22266,0\\r\\n22267,0\\r\\n22268,0\\r\\n22269,0\\r\\n22270,0\\r\\n22271,0\\r\\n22272,0\\r\\n22273,0\\r\\n22274,0\\r\\n22275,0\\r\\n22276,0\\r\\n22277,0\\r\\n22278,0\\r\\n22279,0\\r\\n22280,0\\r\\n22281,0\\r\\n22282,0\\r\\n22283,0\\r\\n22284,0\\r\\n22285,0\\r\\n22286,0\\r\\n22287,0\\r\\n22288,0\\r\\n22289,0\\r\\n22290,0\\r\\n22291,0\\r\\n22292,0\\r\\n22293,0\\r\\n22294,0\\r\\n22295,0\\r\\n22296,0\\r\\n22297,0\\r\\n22298,0\\r\\n22299,0\\r\\n22300,0\\r\\n22301,0\\r\\n22302,0\\r\\n22303,0\\r\\n22304,0\\r\\n22305,0\\r\\n22306,0\\r\\n22307,0\\r\\n22308,0\\r\\n22309,0\\r\\n22310,0\\r\\n22311,0\\r\\n22312,0\\r\\n22313,0\\r\\n22314,0\\r\\n22315,0\\r\\n22316,0\\r\\n22317,0\\r\\n22318,0\\r\\n22319,0\\r\\n22320,0\\r\\n22321,0\\r\\n22322,0\\r\\n22323,0\\r\\n22324,0\\r\\n22325,0\\r\\n22326,0\\r\\n22327,0\\r\\n22328,0\\r\\n22329,0\\r\\n22330,0\\r\\n22331,0\\r\\n22332,0\\r\\n22333,0\\r\\n22334,0\\r\\n22335,0\\r\\n22336,0\\r\\n22337,0\\r\\n22338,0\\r\\n22339,0\\r\\n22340,0\\r\\n22341,0\\r\\n22342,0\\r\\n22343,0\\r\\n22344,0\\r\\n22345,0\\r\\n22346,0\\r\\n22347,0\\r\\n22348,0\\r\\n22349,0\\r\\n22350,0\\r\\n22351,0\\r\\n22352,0\\r\\n22353,0\\r\\n22354,0\\r\\n22355,0\\r\\n22356,0\\r\\n22357,0\\r\\n22358,0\\r\\n22359,0\\r\\n22360,0\\r\\n22361,0\\r\\n22362,0\\r\\n22363,0\\r\\n22364,0\\r\\n22365,0\\r\\n22366,0\\r\\n22367,0\\r\\n22368,0\\r\\n22369,0\\r\\n22370,0\\r\\n22371,0\\r\\n22372,0\\r\\n22373,0\\r\\n22374,0\\r\\n22375,0\\r\\n22376,0\\r\\n22377,0\\r\\n22378,0\\r\\n22379,0\\r\\n22380,0\\r\\n22381,0\\r\\n22382,0\\r\\n22383,0\\r\\n22384,0\\r\\n22385,0\\r\\n22386,0\\r\\n22387,0\\r\\n22388,0\\r\\n22389,0\\r\\n22390,0\\r\\n22391,0\\r\\n22392,0\\r\\n22393,0\\r\\n22394,0\\r\\n22395,0\\r\\n22396,0\\r\\n22397,0\\r\\n22398,0\\r\\n22399,0\\r\\n22400,0\\r\\n22401,0\\r\\n22402,0\\r\\n22403,0\\r\\n22404,0\\r\\n22405,0\\r\\n22406,0\\r\\n22407,0\\r\\n22408,0\\r\\n22409,0\\r\\n22410,0\\r\\n22411,0\\r\\n22412,0\\r\\n22413,0\\r\\n22414,0\\r\\n22415,0\\r\\n22416,0\\r\\n22417,0\\r\\n22418,0\\r\\n22419,0\\r\\n22420,0\\r\\n22421,0\\r\\n22422,0\\r\\n22423,0\\r\\n22424,0\\r\\n22425,0\\r\\n22426,0\\r\\n22427,0\\r\\n22428,0\\r\\n22429,0\\r\\n22430,0\\r\\n22431,0\\r\\n22432,0\\r\\n22433,0\\r\\n22434,0\\r\\n22435,0\\r\\n22436,0\\r\\n22437,0\\r\\n22438,0\\r\\n22439,0\\r\\n22440,0\\r\\n22441,0\\r\\n22442,0\\r\\n22443,0\\r\\n22444,0\\r\\n22445,0\\r\\n22446,0\\r\\n22447,0\\r\\n22448,0\\r\\n22449,0\\r\\n22450,0\\r\\n22451,0\\r\\n22452,0\\r\\n22453,0\\r\\n22454,0\\r\\n22455,0\\r\\n22456,0\\r\\n22457,0\\r\\n22458,0\\r\\n22459,0\\r\\n22460,0\\r\\n22461,0\\r\\n22462,0\\r\\n22463,0\\r\\n22464,0\\r\\n22465,0\\r\\n22466,0\\r\\n22467,0\\r\\n22468,0\\r\\n22469,0\\r\\n22470,0\\r\\n22471,0\\r\\n22472,0\\r\\n22473,0\\r\\n22474,0\\r\\n22475,0\\r\\n22476,0\\r\\n22477,0\\r\\n22478,0\\r\\n22479,0\\r\\n22480,0\\r\\n22481,0\\r\\n22482,0\\r\\n22483,0\\r\\n22484,0\\r\\n22485,0\\r\\n22486,0\\r\\n22487,0\\r\\n22488,0\\r\\n22489,0\\r\\n22490,0\\r\\n22491,0\\r\\n22492,0\\r\\n22493,0\\r\\n22494,0\\r\\n22495,0\\r\\n22496,0\\r\\n22497,0\\r\\n22498,0\\r\\n22499,0\\r\\n22500,0\\r\\n22501,0\\r\\n22502,0\\r\\n22503,0\\r\\n22504,0\\r\\n22505,0\\r\\n22506,0\\r\\n22507,0\\r\\n22508,0\\r\\n22509,0\\r\\n22510,0\\r\\n22511,0\\r\\n22512,0\\r\\n22513,0\\r\\n22514,0\\r\\n22515,0\\r\\n22516,0\\r\\n22517,0\\r\\n22518,0\\r\\n22519,0\\r\\n22520,0\\r\\n22521,0\\r\\n22522,0\\r\\n22523,0\\r\\n22524,0\\r\\n22525,0\\r\\n22526,0\\r\\n22527,0\\r\\n22528,0\\r\\n22529,0\\r\\n22530,0\\r\\n22531,0\\r\\n22532,0\\r\\n22533,0\\r\\n22534,0\\r\\n22535,0\\r\\n22536,0\\r\\n22537,0\\r\\n22538,0\\r\\n22539,0\\r\\n22540,0\\r\\n22541,0\\r\\n22542,0\\r\\n22543,0\\r\\n22544,0\\r\\n22545,0\\r\\n22546,0\\r\\n22547,0\\r\\n22548,0\\r\\n22549,0\\r\\n22550,0\\r\\n22551,0\\r\\n22552,0\\r\\n22553,0\\r\\n22554,0\\r\\n22555,0\\r\\n22556,0\\r\\n22557,0\\r\\n22558,0\\r\\n22559,0\\r\\n22560,0\\r\\n22561,0\\r\\n22562,0\\r\\n22563,0\\r\\n22564,0\\r\\n22565,0\\r\\n22566,0\\r\\n22567,0\\r\\n22568,0\\r\\n22569,0\\r\\n22570,0\\r\\n22571,0\\r\\n22572,0\\r\\n22573,0\\r\\n22574,0\\r\\n22575,0\\r\\n22576,0\\r\\n22577,0\\r\\n22578,0\\r\\n22579,0\\r\\n22580,0\\r\\n22581,0\\r\\n22582,0\\r\\n22583,0\\r\\n22584,0\\r\\n22585,0\\r\\n22586,0\\r\\n22587,0\\r\\n22588,0\\r\\n22589,0\\r\\n22590,0\\r\\n22591,0\\r\\n22592,0\\r\\n22593,0\\r\\n22594,0\\r\\n22595,0\\r\\n22596,0\\r\\n22597,0\\r\\n22598,0\\r\\n22599,0\\r\\n22600,0\\r\\n22601,0\\r\\n22602,0\\r\\n22603,0\\r\\n22604,0\\r\\n22605,0\\r\\n22606,0\\r\\n22607,0\\r\\n22608,0\\r\\n22609,0\\r\\n22610,0\\r\\n22611,0\\r\\n22612,0\\r\\n22613,0\\r\\n22614,0\\r\\n22615,0\\r\\n22616,0\\r\\n22617,0\\r\\n22618,0\\r\\n22619,0\\r\\n22620,0\\r\\n22621,0\\r\\n22622,0\\r\\n22623,0\\r\\n22624,0\\r\\n22625,0\\r\\n22626,0\\r\\n22627,0\\r\\n22628,0\\r\\n22629,0\\r\\n22630,0\\r\\n22631,0\\r\\n22632,0\\r\\n22633,0\\r\\n22634,0\\r\\n22635,0\\r\\n22636,0\\r\\n22637,0\\r\\n22638,0\\r\\n22639,0\\r\\n22640,0\\r\\n22641,0\\r\\n22642,0\\r\\n22643,0\\r\\n22644,0\\r\\n22645,0\\r\\n22646,0\\r\\n22647,0\\r\\n22648,0\\r\\n22649,0\\r\\n22650,0\\r\\n22651,0\\r\\n22652,0\\r\\n22653,0\\r\\n22654,0\\r\\n22655,0\\r\\n22656,0\\r\\n22657,0\\r\\n22658,0\\r\\n22659,0\\r\\n22660,0\\r\\n22661,0\\r\\n22662,0\\r\\n22663,0\\r\\n22664,0\\r\\n22665,0\\r\\n22666,0\\r\\n22667,0\\r\\n22668,0\\r\\n22669,0\\r\\n22670,0\\r\\n22671,0\\r\\n22672,0\\r\\n22673,0\\r\\n22674,0\\r\\n22675,0\\r\\n22676,0\\r\\n22677,0\\r\\n22678,0\\r\\n22679,0\\r\\n22680,0\\r\\n22681,0\\r\\n22682,0\\r\\n22683,0\\r\\n22684,0\\r\\n22685,0\\r\\n22686,0\\r\\n22687,0\\r\\n22688,0\\r\\n22689,0\\r\\n22690,0\\r\\n22691,0\\r\\n22692,0\\r\\n22693,0\\r\\n22694,0\\r\\n22695,0\\r\\n22696,0\\r\\n22697,0\\r\\n22698,0\\r\\n22699,0\\r\\n22700,0\\r\\n22701,0\\r\\n22702,0\\r\\n22703,0\\r\\n22704,0\\r\\n22705,0\\r\\n22706,0\\r\\n22707,0\\r\\n22708,0\\r\\n22709,0\\r\\n22710,0\\r\\n22711,0\\r\\n22712,0\\r\\n22713,0\\r\\n22714,0\\r\\n22715,0\\r\\n22716,0\\r\\n22717,0\\r\\n22718,0\\r\\n22719,0\\r\\n22720,0\\r\\n22721,0\\r\\n22722,0\\r\\n22723,0\\r\\n22724,0\\r\\n22725,0\\r\\n22726,0\\r\\n22727,0\\r\\n22728,0\\r\\n22729,0\\r\\n22730,0\\r\\n22731,0\\r\\n22732,0\\r\\n22733,0\\r\\n22734,0\\r\\n22735,0\\r\\n22736,0\\r\\n22737,0\\r\\n22738,0\\r\\n22739,0\\r\\n22740,0\\r\\n22741,0\\r\\n22742,0\\r\\n22743,0\\r\\n22744,0\\r\\n22745,0\\r\\n22746,0\\r\\n22747,0\\r\\n22748,0\\r\\n22749,0\\r\\n22750,0\\r\\n22751,0\\r\\n22752,0\\r\\n22753,0\\r\\n22754,0\\r\\n22755,0\\r\\n22756,0\\r\\n22757,0\\r\\n22758,0\\r\\n22759,0\\r\\n22760,0\\r\\n22761,0\\r\\n22762,0\\r\\n22763,0\\r\\n22764,0\\r\\n22765,0\\r\\n22766,0\\r\\n22767,0\\r\\n22768,0\\r\\n22769,0\\r\\n22770,0\\r\\n22771,0\\r\\n22772,0\\r\\n22773,0\\r\\n22774,0\\r\\n22775,0\\r\\n22776,0\\r\\n22777,0\\r\\n22778,0\\r\\n22779,0\\r\\n22780,0\\r\\n22781,0\\r\\n22782,0\\r\\n22783,0\\r\\n22784,0\\r\\n22785,0\\r\\n22786,0\\r\\n22787,0\\r\\n22788,0\\r\\n22789,0\\r\\n22790,0\\r\\n22791,0\\r\\n22792,0\\r\\n22793,0\\r\\n22794,0\\r\\n22795,0\\r\\n22796,0\\r\\n22797,0\\r\\n22798,0\\r\\n22799,0\\r\\n22800,0\\r\\n22801,0\\r\\n22802,0\\r\\n22803,0\\r\\n22804,0\\r\\n22805,0\\r\\n22806,0\\r\\n22807,0\\r\\n22808,0\\r\\n22809,0\\r\\n22810,0\\r\\n22811,0\\r\\n22812,0\\r\\n22813,0\\r\\n22814,0\\r\\n22815,0\\r\\n22816,0\\r\\n22817,0\\r\\n22818,0\\r\\n22819,0\\r\\n22820,0\\r\\n22821,0\\r\\n22822,0\\r\\n22823,0\\r\\n22824,0\\r\\n22825,0\\r\\n22826,0\\r\\n22827,0\\r\\n22828,0\\r\\n22829,0\\r\\n22830,0\\r\\n22831,0\\r\\n22832,0\\r\\n22833,0\\r\\n22834,0\\r\\n22835,0\\r\\n22836,0\\r\\n22837,0\\r\\n22838,0\\r\\n22839,0\\r\\n22840,0\\r\\n22841,0\\r\\n22842,0\\r\\n22843,0\\r\\n22844,0\\r\\n22845,0\\r\\n22846,0\\r\\n22847,0\\r\\n22848,0\\r\\n22849,0\\r\\n22850,0\\r\\n22851,0\\r\\n22852,0\\r\\n22853,0\\r\\n22854,0\\r\\n22855,0\\r\\n22856,0\\r\\n22857,0\\r\\n22858,0\\r\\n22859,0\\r\\n22860,0\\r\\n22861,0\\r\\n22862,0\\r\\n22863,0\\r\\n22864,0\\r\\n22865,0\\r\\n22866,0\\r\\n22867,0\\r\\n22868,0\\r\\n22869,0\\r\\n22870,0\\r\\n22871,0\\r\\n22872,0\\r\\n22873,0\\r\\n22874,0\\r\\n22875,0\\r\\n22876,0\\r\\n22877,0\\r\\n22878,0\\r\\n22879,0\\r\\n22880,0\\r\\n22881,0\\r\\n22882,0\\r\\n22883,0\\r\\n22884,0\\r\\n22885,0\\r\\n22886,0\\r\\n22887,0\\r\\n22888,0\\r\\n22889,0\\r\\n22890,0\\r\\n22891,0\\r\\n22892,0\\r\\n22893,0\\r\\n22894,0\\r\\n22895,0\\r\\n22896,0\\r\\n22897,0\\r\\n22898,0\\r\\n22899,0\\r\\n22900,0\\r\\n22901,0\\r\\n22902,0\\r\\n22903,0\\r\\n22904,0\\r\\n22905,0\\r\\n22906,0\\r\\n22907,0\\r\\n22908,0\\r\\n22909,0\\r\\n22910,0\\r\\n22911,0\\r\\n22912,0\\r\\n22913,0\\r\\n22914,0\\r\\n22915,0\\r\\n22916,0\\r\\n22917,0\\r\\n22918,0\\r\\n22919,0\\r\\n22920,0\\r\\n22921,0\\r\\n22922,0\\r\\n22923,0\\r\\n22924,0\\r\\n22925,0\\r\\n22926,0\\r\\n22927,0\\r\\n22928,0\\r\\n22929,0\\r\\n22930,0\\r\\n22931,0\\r\\n22932,0\\r\\n22933,0\\r\\n22934,0\\r\\n22935,0\\r\\n22936,0\\r\\n22937,0\\r\\n22938,0\\r\\n22939,0\\r\\n22940,0\\r\\n22941,0\\r\\n22942,0\\r\\n22943,0\\r\\n22944,0\\r\\n22945,0\\r\\n22946,0\\r\\n22947,0\\r\\n22948,0\\r\\n22949,0\\r\\n22950,0\\r\\n22951,0\\r\\n22952,0\\r\\n22953,0\\r\\n22954,0\\r\\n22955,0\\r\\n22956,0\\r\\n22957,0\\r\\n22958,0\\r\\n22959,0\\r\\n22960,0\\r\\n22961,0\\r\\n22962,0\\r\\n22963,0\\r\\n22964,0\\r\\n22965,0\\r\\n22966,0\\r\\n22967,0\\r\\n22968,0\\r\\n22969,0\\r\\n22970,0\\r\\n22971,0\\r\\n22972,0\\r\\n22973,0\\r\\n22974,0\\r\\n22975,0\\r\\n22976,0\\r\\n22977,0\\r\\n22978,0\\r\\n22979,0\\r\\n22980,0\\r\\n22981,0\\r\\n22982,0\\r\\n22983,0\\r\\n22984,0\\r\\n22985,0\\r\\n22986,0\\r\\n22987,0\\r\\n22988,0\\r\\n22989,0\\r\\n22990,0\\r\\n22991,0\\r\\n22992,0\\r\\n22993,0\\r\\n22994,0\\r\\n22995,0\\r\\n22996,0\\r\\n22997,0\\r\\n22998,0\\r\\n22999,0\\r\\n23000,0\\r\\n23001,0\\r\\n23002,0\\r\\n23003,0\\r\\n23004,0\\r\\n23005,0\\r\\n23006,0\\r\\n23007,0\\r\\n23008,0\\r\\n23009,0\\r\\n23010,0\\r\\n23011,0\\r\\n23012,0\\r\\n23013,0\\r\\n23014,0\\r\\n23015,0\\r\\n23016,0\\r\\n23017,0\\r\\n23018,0\\r\\n23019,0\\r\\n23020,0\\r\\n23021,0\\r\\n23022,0\\r\\n23023,0\\r\\n23024,0\\r\\n23025,0\\r\\n23026,0\\r\\n23027,0\\r\\n23028,0\\r\\n23029,0\\r\\n23030,0\\r\\n23031,0\\r\\n23032,0\\r\\n23033,0\\r\\n23034,0\\r\\n23035,0\\r\\n23036,0\\r\\n23037,0\\r\\n23038,0\\r\\n23039,0\\r\\n23040,0\\r\\n23041,0\\r\\n23042,0\\r\\n23043,0\\r\\n23044,0\\r\\n23045,0\\r\\n23046,0\\r\\n23047,0\\r\\n23048,0\\r\\n23049,0\\r\\n23050,0\\r\\n23051,0\\r\\n23052,0\\r\\n23053,0\\r\\n23054,0\\r\\n23055,0\\r\\n23056,0\\r\\n23057,0\\r\\n23058,0\\r\\n23059,0\\r\\n23060,0\\r\\n23061,0\\r\\n23062,0\\r\\n23063,0\\r\\n23064,0\\r\\n23065,0\\r\\n23066,0\\r\\n23067,0\\r\\n23068,0\\r\\n23069,0\\r\\n23070,0\\r\\n23071,0\\r\\n23072,0\\r\\n23073,0\\r\\n23074,0\\r\\n23075,0\\r\\n23076,0\\r\\n23077,0\\r\\n23078,0\\r\\n23079,0\\r\\n23080,0\\r\\n23081,0\\r\\n23082,0\\r\\n23083,0\\r\\n23084,0\\r\\n23085,0\\r\\n23086,0\\r\\n23087,0\\r\\n23088,0\\r\\n23089,0\\r\\n23090,0\\r\\n23091,0\\r\\n23092,0\\r\\n23093,0\\r\\n23094,0\\r\\n23095,0\\r\\n23096,0\\r\\n23097,0\\r\\n23098,0\\r\\n23099,0\\r\\n23100,0\\r\\n23101,0\\r\\n23102,0\\r\\n23103,0\\r\\n23104,0\\r\\n23105,0\\r\\n23106,0\\r\\n23107,0\\r\\n23108,0\\r\\n23109,0\\r\\n23110,0\\r\\n23111,0\\r\\n23112,0\\r\\n23113,0\\r\\n23114,0\\r\\n23115,0\\r\\n23116,0\\r\\n23117,0\\r\\n23118,0\\r\\n23119,0\\r\\n23120,0\\r\\n23121,0\\r\\n23122,0\\r\\n23123,0\\r\\n23124,0\\r\\n23125,0\\r\\n23126,0\\r\\n23127,0\\r\\n23128,0\\r\\n23129,0\\r\\n23130,0\\r\\n23131,0\\r\\n23132,0\\r\\n23133,0\\r\\n23134,0\\r\\n23135,0\\r\\n23136,0\\r\\n23137,0\\r\\n23138,0\\r\\n23139,0\\r\\n23140,0\\r\\n23141,0\\r\\n23142,0\\r\\n23143,0\\r\\n23144,0\\r\\n23145,0\\r\\n23146,0\\r\\n23147,0\\r\\n23148,0\\r\\n23149,0\\r\\n23150,0\\r\\n23151,0\\r\\n23152,0\\r\\n23153,0\\r\\n23154,0\\r\\n23155,0\\r\\n23156,0\\r\\n23157,0\\r\\n23158,0\\r\\n23159,0\\r\\n23160,0\\r\\n23161,0\\r\\n23162,0\\r\\n23163,0\\r\\n23164,0\\r\\n23165,0\\r\\n23166,0\\r\\n23167,0\\r\\n23168,0\\r\\n23169,0\\r\\n23170,0\\r\\n23171,0\\r\\n23172,0\\r\\n23173,0\\r\\n23174,0\\r\\n23175,0\\r\\n23176,0\\r\\n23177,0\\r\\n23178,0\\r\\n23179,0\\r\\n23180,0\\r\\n23181,0\\r\\n23182,0\\r\\n23183,0\\r\\n23184,0\\r\\n23185,0\\r\\n23186,0\\r\\n23187,0\\r\\n23188,0\\r\\n23189,0\\r\\n23190,0\\r\\n23191,0\\r\\n23192,0\\r\\n23193,0\\r\\n23194,0\\r\\n23195,0\\r\\n23196,0\\r\\n23197,0\\r\\n23198,0\\r\\n23199,0\\r\\n23200,0\\r\\n23201,0\\r\\n23202,0\\r\\n23203,0\\r\\n23204,0\\r\\n23205,0\\r\\n23206,0\\r\\n23207,0\\r\\n23208,0\\r\\n23209,0\\r\\n23210,0\\r\\n23211,0\\r\\n23212,0\\r\\n23213,0\\r\\n23214,0\\r\\n23215,0\\r\\n23216,0\\r\\n23217,0\\r\\n23218,0\\r\\n23219,0\\r\\n23220,0\\r\\n23221,0\\r\\n23222,0\\r\\n23223,0\\r\\n23224,0\\r\\n23225,0\\r\\n23226,0\\r\\n23227,0\\r\\n23228,0\\r\\n23229,0\\r\\n23230,0\\r\\n23231,0\\r\\n23232,0\\r\\n23233,0\\r\\n23234,0\\r\\n23235,0\\r\\n23236,0\\r\\n23237,0\\r\\n23238,0\\r\\n23239,0\\r\\n23240,0\\r\\n23241,0\\r\\n23242,0\\r\\n23243,0\\r\\n23244,0\\r\\n23245,0\\r\\n23246,0\\r\\n23247,0\\r\\n23248,0\\r\\n23249,0\\r\\n23250,0\\r\\n23251,0\\r\\n23252,0\\r\\n23253,0\\r\\n23254,0\\r\\n23255,0\\r\\n23256,0\\r\\n23257,0\\r\\n23258,0\\r\\n23259,0\\r\\n23260,0\\r\\n23261,0\\r\\n23262,0\\r\\n23263,0\\r\\n23264,0\\r\\n23265,0\\r\\n23266,0\\r\\n23267,0\\r\\n23268,0\\r\\n23269,0\\r\\n23270,0\\r\\n23271,0\\r\\n23272,0\\r\\n23273,0\\r\\n23274,0\\r\\n23275,0\\r\\n23276,0\\r\\n23277,0\\r\\n23278,0\\r\\n23279,0\\r\\n23280,0\\r\\n23281,0\\r\\n23282,0\\r\\n23283,0\\r\\n23284,0\\r\\n23285,0\\r\\n23286,0\\r\\n23287,0\\r\\n23288,0\\r\\n23289,0\\r\\n23290,0\\r\\n23291,0\\r\\n23292,0\\r\\n23293,0\\r\\n23294,0\\r\\n23295,0\\r\\n23296,0\\r\\n23297,0\\r\\n23298,0\\r\\n23299,0\\r\\n23300,0\\r\\n23301,0\\r\\n23302,0\\r\\n23303,0\\r\\n23304,0\\r\\n23305,0\\r\\n23306,0\\r\\n23307,0\\r\\n23308,0\\r\\n23309,0\\r\\n23310,0\\r\\n23311,0\\r\\n23312,0\\r\\n23313,0\\r\\n23314,0\\r\\n23315,0\\r\\n23316,0\\r\\n23317,0\\r\\n23318,0\\r\\n23319,0\\r\\n23320,0\\r\\n23321,0\\r\\n23322,0\\r\\n23323,0\\r\\n23324,0\\r\\n23325,0\\r\\n23326,0\\r\\n23327,0\\r\\n23328,0\\r\\n23329,0\\r\\n23330,0\\r\\n23331,0\\r\\n23332,0\\r\\n23333,0\\r\\n23334,0\\r\\n23335,0\\r\\n23336,0\\r\\n23337,0\\r\\n23338,0\\r\\n23339,0\\r\\n23340,0\\r\\n23341,0\\r\\n23342,0\\r\\n23343,0\\r\\n23344,0\\r\\n23345,0\\r\\n23346,0\\r\\n23347,0\\r\\n23348,0\\r\\n23349,0\\r\\n23350,0\\r\\n23351,0\\r\\n23352,0\\r\\n23353,0\\r\\n23354,0\\r\\n23355,0\\r\\n23356,0\\r\\n23357,0\\r\\n23358,0\\r\\n23359,0\\r\\n23360,0\\r\\n23361,0\\r\\n23362,0\\r\\n23363,0\\r\\n23364,0\\r\\n23365,0\\r\\n23366,0\\r\\n23367,0\\r\\n23368,0\\r\\n23369,0\\r\\n23370,0\\r\\n23371,0\\r\\n23372,0\\r\\n23373,0\\r\\n23374,0\\r\\n23375,0\\r\\n23376,0\\r\\n23377,0\\r\\n23378,0\\r\\n23379,0\\r\\n23380,0\\r\\n23381,0\\r\\n23382,0\\r\\n23383,0\\r\\n23384,0\\r\\n23385,0\\r\\n23386,0\\r\\n23387,0\\r\\n23388,0\\r\\n23389,0\\r\\n23390,0\\r\\n23391,0\\r\\n23392,0\\r\\n23393,0\\r\\n23394,0\\r\\n23395,0\\r\\n23396,0\\r\\n23397,0\\r\\n23398,0\\r\\n23399,0\\r\\n23400,0\\r\\n23401,0\\r\\n23402,0\\r\\n23403,0\\r\\n23404,0\\r\\n23405,0\\r\\n23406,0\\r\\n23407,0\\r\\n23408,0\\r\\n23409,0\\r\\n23410,0\\r\\n23411,0\\r\\n23412,0\\r\\n23413,0\\r\\n23414,0\\r\\n23415,0\\r\\n23416,0\\r\\n23417,0\\r\\n23418,0\\r\\n23419,0\\r\\n23420,0\\r\\n23421,0\\r\\n23422,0\\r\\n23423,0\\r\\n23424,0\\r\\n23425,0\\r\\n23426,0\\r\\n23427,0\\r\\n23428,0\\r\\n23429,0\\r\\n23430,0\\r\\n23431,0\\r\\n23432,0\\r\\n23433,0\\r\\n23434,0\\r\\n23435,0\\r\\n23436,0\\r\\n23437,0\\r\\n23438,0\\r\\n23439,0\\r\\n23440,0\\r\\n23441,0\\r\\n23442,0\\r\\n23443,0\\r\\n23444,0\\r\\n23445,0\\r\\n23446,0\\r\\n23447,0\\r\\n23448,0\\r\\n23449,0\\r\\n23450,0\\r\\n23451,0\\r\\n23452,0\\r\\n23453,0\\r\\n23454,0\\r\\n23455,0\\r\\n23456,0\\r\\n23457,0\\r\\n23458,0\\r\\n23459,0\\r\\n23460,0\\r\\n23461,0\\r\\n23462,0\\r\\n23463,0\\r\\n23464,0\\r\\n23465,0\\r\\n23466,0\\r\\n23467,0\\r\\n23468,0\\r\\n23469,0\\r\\n23470,0\\r\\n23471,0\\r\\n23472,0\\r\\n23473,0\\r\\n23474,0\\r\\n23475,0\\r\\n23476,0\\r\\n23477,0\\r\\n23478,0\\r\\n23479,0\\r\\n23480,0\\r\\n23481,0\\r\\n23482,0\\r\\n23483,0\\r\\n23484,0\\r\\n23485,0\\r\\n23486,0\\r\\n23487,0\\r\\n23488,0\\r\\n23489,0\\r\\n23490,0\\r\\n23491,0\\r\\n23492,0\\r\\n23493,0\\r\\n23494,0\\r\\n23495,0\\r\\n23496,0\\r\\n23497,0\\r\\n23498,0\\r\\n23499,0\\r\\n23500,0\\r\\n23501,0\\r\\n23502,0\\r\\n23503,0\\r\\n23504,0\\r\\n23505,0\\r\\n23506,0\\r\\n23507,0\\r\\n23508,0\\r\\n23509,0\\r\\n23510,0\\r\\n23511,0\\r\\n23512,0\\r\\n23513,0\\r\\n23514,0\\r\\n23515,0\\r\\n23516,0\\r\\n23517,0\\r\\n23518,0\\r\\n23519,0\\r\\n23520,0\\r\\n23521,0\\r\\n23522,0\\r\\n23523,0\\r\\n23524,0\\r\\n23525,0\\r\\n23526,0\\r\\n23527,0\\r\\n23528,0\\r\\n23529,0\\r\\n23530,0\\r\\n23531,0\\r\\n23532,0\\r\\n23533,0\\r\\n23534,0\\r\\n23535,0\\r\\n23536,0\\r\\n23537,0\\r\\n23538,0\\r\\n23539,0\\r\\n23540,0\\r\\n23541,0\\r\\n23542,0\\r\\n23543,0\\r\\n23544,0\\r\\n23545,0\\r\\n23546,0\\r\\n23547,0\\r\\n23548,0\\r\\n23549,0\\r\\n23550,0\\r\\n23551,0\\r\\n23552,0\\r\\n23553,0\\r\\n23554,0\\r\\n23555,0\\r\\n23556,0\\r\\n23557,0\\r\\n23558,0\\r\\n23559,0\\r\\n23560,0\\r\\n23561,0\\r\\n23562,0\\r\\n23563,0\\r\\n23564,0\\r\\n23565,0\\r\\n23566,0\\r\\n23567,0\\r\\n23568,0\\r\\n23569,0\\r\\n23570,0\\r\\n23571,0\\r\\n23572,0\\r\\n23573,0\\r\\n23574,0\\r\\n23575,0\\r\\n23576,0\\r\\n23577,0\\r\\n23578,0\\r\\n23579,0\\r\\n23580,0\\r\\n23581,0\\r\\n23582,0\\r\\n23583,0\\r\\n23584,0\\r\\n23585,0\\r\\n23586,0\\r\\n23587,0\\r\\n23588,0\\r\\n23589,0\\r\\n23590,0\\r\\n23591,0\\r\\n23592,0\\r\\n23593,0\\r\\n23594,0\\r\\n23595,0\\r\\n23596,0\\r\\n23597,0\\r\\n23598,0\\r\\n23599,0\\r\\n23600,0\\r\\n23601,0\\r\\n23602,0\\r\\n23603,0\\r\\n23604,0\\r\\n23605,0\\r\\n23606,0\\r\\n23607,0\\r\\n23608,0\\r\\n23609,0\\r\\n23610,0\\r\\n23611,0\\r\\n23612,0\\r\\n23613,0\\r\\n23614,0\\r\\n23615,0\\r\\n23616,0\\r\\n23617,0\\r\\n23618,0\\r\\n23619,0\\r\\n23620,0\\r\\n23621,0\\r\\n23622,0\\r\\n23623,0\\r\\n23624,0\\r\\n23625,0\\r\\n23626,0\\r\\n23627,0\\r\\n23628,0\\r\\n23629,0\\r\\n23630,0\\r\\n23631,0\\r\\n23632,0\\r\\n23633,0\\r\\n23634,0\\r\\n23635,0\\r\\n23636,0\\r\\n23637,0\\r\\n23638,0\\r\\n23639,0\\r\\n23640,0\\r\\n23641,0\\r\\n23642,0\\r\\n23643,0\\r\\n23644,0\\r\\n23645,0\\r\\n23646,0\\r\\n23647,0\\r\\n23648,0\\r\\n23649,0\\r\\n23650,0\\r\\n23651,0\\r\\n23652,0\\r\\n23653,0\\r\\n23654,0\\r\\n23655,0\\r\\n23656,0\\r\\n23657,0\\r\\n23658,0\\r\\n23659,0\\r\\n23660,0\\r\\n23661,0\\r\\n23662,0\\r\\n23663,0\\r\\n23664,0\\r\\n23665,0\\r\\n23666,0\\r\\n23667,0\\r\\n23668,0\\r\\n23669,0\\r\\n23670,0\\r\\n23671,0\\r\\n23672,0\\r\\n23673,0\\r\\n23674,0\\r\\n23675,0\\r\\n23676,0\\r\\n23677,0\\r\\n23678,0\\r\\n23679,0\\r\\n23680,0\\r\\n23681,0\\r\\n23682,0\\r\\n23683,0\\r\\n23684,0\\r\\n23685,0\\r\\n23686,0\\r\\n23687,0\\r\\n23688,0\\r\\n23689,0\\r\\n23690,0\\r\\n23691,0\\r\\n23692,0\\r\\n23693,0\\r\\n23694,0\\r\\n23695,0\\r\\n23696,0\\r\\n23697,0\\r\\n23698,0\\r\\n23699,0\\r\\n23700,0\\r\\n23701,0\\r\\n23702,0\\r\\n23703,0\\r\\n23704,0\\r\\n23705,0\\r\\n23706,0\\r\\n23707,0\\r\\n23708,0\\r\\n23709,0\\r\\n23710,0\\r\\n23711,0\\r\\n23712,0\\r\\n23713,0\\r\\n23714,0\\r\\n23715,0\\r\\n23716,0\\r\\n23717,0\\r\\n23718,0\\r\\n23719,0\\r\\n23720,0\\r\\n23721,0\\r\\n23722,0\\r\\n23723,0\\r\\n23724,0\\r\\n23725,0\\r\\n23726,0\\r\\n23727,0\\r\\n23728,0\\r\\n23729,0\\r\\n23730,0\\r\\n23731,0\\r\\n23732,0\\r\\n23733,0\\r\\n23734,0\\r\\n23735,0\\r\\n23736,0\\r\\n23737,0\\r\\n23738,0\\r\\n23739,0\\r\\n23740,0\\r\\n23741,0\\r\\n23742,0\\r\\n23743,0\\r\\n23744,0\\r\\n23745,0\\r\\n23746,0\\r\\n23747,0\\r\\n23748,0\\r\\n23749,0\\r\\n23750,0\\r\\n23751,0\\r\\n23752,0\\r\\n23753,0\\r\\n23754,0\\r\\n23755,0\\r\\n23756,0\\r\\n23757,0\\r\\n23758,0\\r\\n23759,0\\r\\n23760,0\\r\\n23761,0\\r\\n23762,0\\r\\n23763,0\\r\\n23764,0\\r\\n23765,0\\r\\n23766,0\\r\\n23767,0\\r\\n23768,0\\r\\n23769,0\\r\\n23770,0\\r\\n23771,0\\r\\n23772,0\\r\\n23773,0\\r\\n23774,0\\r\\n23775,0\\r\\n23776,0\\r\\n23777,0\\r\\n23778,0\\r\\n23779,0\\r\\n23780,0\\r\\n23781,0\\r\\n23782,0\\r\\n23783,0\\r\\n23784,0\\r\\n23785,0\\r\\n23786,0\\r\\n23787,0\\r\\n23788,0\\r\\n23789,0\\r\\n23790,0\\r\\n23791,0\\r\\n23792,0\\r\\n23793,0\\r\\n23794,0\\r\\n23795,0\\r\\n23796,0\\r\\n23797,0\\r\\n23798,0\\r\\n23799,0\\r\\n23800,0\\r\\n23801,0\\r\\n23802,0\\r\\n23803,0\\r\\n23804,0\\r\\n23805,0\\r\\n23806,0\\r\\n23807,0\\r\\n23808,0\\r\\n23809,0\\r\\n23810,0\\r\\n23811,0\\r\\n23812,0\\r\\n23813,0\\r\\n23814,0\\r\\n23815,0\\r\\n23816,0\\r\\n23817,0\\r\\n23818,0\\r\\n23819,0\\r\\n23820,0\\r\\n23821,0\\r\\n23822,0\\r\\n23823,0\\r\\n23824,0\\r\\n23825,0\\r\\n23826,0\\r\\n23827,0\\r\\n23828,0\\r\\n23829,0\\r\\n23830,0\\r\\n23831,0\\r\\n23832,0\\r\\n23833,0\\r\\n23834,0\\r\\n23835,0\\r\\n23836,0\\r\\n23837,0\\r\\n23838,0\\r\\n23839,0\\r\\n23840,0\\r\\n23841,0\\r\\n23842,0\\r\\n23843,0\\r\\n23844,0\\r\\n23845,0\\r\\n23846,0\\r\\n23847,0\\r\\n23848,0\\r\\n23849,0\\r\\n23850,0\\r\\n23851,0\\r\\n23852,0\\r\\n23853,0\\r\\n23854,0\\r\\n23855,0\\r\\n23856,0\\r\\n23857,0\\r\\n23858,0\\r\\n23859,0\\r\\n23860,0\\r\\n23861,0\\r\\n23862,0\\r\\n23863,0\\r\\n23864,0\\r\\n23865,0\\r\\n23866,0\\r\\n23867,0\\r\\n23868,0\\r\\n23869,0\\r\\n23870,0\\r\\n23871,0\\r\\n23872,0\\r\\n23873,0\\r\\n23874,0\\r\\n23875,0\\r\\n23876,0\\r\\n23877,0\\r\\n23878,0\\r\\n23879,0\\r\\n23880,0\\r\\n23881,0\\r\\n23882,0\\r\\n23883,0\\r\\n23884,0\\r\\n23885,0\\r\\n23886,0\\r\\n23887,0\\r\\n23888,0\\r\\n23889,0\\r\\n23890,0\\r\\n23891,0\\r\\n23892,0\\r\\n23893,0\\r\\n23894,0\\r\\n23895,0\\r\\n23896,0\\r\\n23897,0\\r\\n23898,0\\r\\n23899,0\\r\\n23900,0\\r\\n23901,0\\r\\n23902,0\\r\\n23903,0\\r\\n23904,0\\r\\n23905,0\\r\\n23906,0\\r\\n23907,0\\r\\n23908,0\\r\\n23909,0\\r\\n23910,0\\r\\n23911,0\\r\\n23912,0\\r\\n23913,0\\r\\n23914,0\\r\\n23915,0\\r\\n23916,0\\r\\n23917,0\\r\\n23918,0\\r\\n23919,0\\r\\n23920,0\\r\\n23921,0\\r\\n23922,0\\r\\n23923,0\\r\\n23924,0\\r\\n23925,0\\r\\n23926,0\\r\\n23927,0\\r\\n23928,0\\r\\n23929,0\\r\\n23930,0\\r\\n23931,0\\r\\n23932,0\\r\\n23933,0\\r\\n23934,0\\r\\n23935,0\\r\\n23936,0\\r\\n23937,0\\r\\n23938,0\\r\\n23939,0\\r\\n23940,0\\r\\n23941,0\\r\\n23942,0\\r\\n23943,0\\r\\n23944,0\\r\\n23945,0\\r\\n23946,0\\r\\n23947,0\\r\\n23948,0\\r\\n23949,0\\r\\n23950,0\\r\\n23951,0\\r\\n23952,0\\r\\n23953,0\\r\\n23954,0\\r\\n23955,0\\r\\n23956,0\\r\\n23957,0\\r\\n23958,0\\r\\n23959,0\\r\\n23960,0\\r\\n23961,0\\r\\n23962,0\\r\\n23963,0\\r\\n23964,0\\r\\n23965,0\\r\\n23966,0\\r\\n23967,0\\r\\n23968,0\\r\\n23969,0\\r\\n23970,0\\r\\n23971,0\\r\\n23972,0\\r\\n23973,0\\r\\n23974,0\\r\\n23975,0\\r\\n23976,0\\r\\n23977,0\\r\\n23978,0\\r\\n23979,0\\r\\n23980,0\\r\\n23981,0\\r\\n23982,0\\r\\n23983,0\\r\\n23984,0\\r\\n23985,0\\r\\n23986,0\\r\\n23987,0\\r\\n23988,0\\r\\n23989,0\\r\\n23990,0\\r\\n23991,0\\r\\n23992,0\\r\\n23993,0\\r\\n23994,0\\r\\n23995,0\\r\\n23996,0\\r\\n23997,0\\r\\n23998,0\\r\\n23999,0\\r\\n24000,0\\r\\n24001,0\\r\\n24002,0\\r\\n24003,0\\r\\n24004,0\\r\\n24005,0\\r\\n24006,0\\r\\n24007,0\\r\\n24008,0\\r\\n24009,0\\r\\n24010,0\\r\\n24011,0\\r\\n24012,0\\r\\n24013,0\\r\\n24014,0\\r\\n24015,0\\r\\n24016,0\\r\\n24017,0\\r\\n24018,0\\r\\n24019,0\\r\\n24020,0\\r\\n24021,0\\r\\n24022,0\\r\\n24023,0\\r\\n24024,0\\r\\n24025,0\\r\\n24026,0\\r\\n24027,0\\r\\n24028,0\\r\\n24029,0\\r\\n24030,0\\r\\n24031,0\\r\\n24032,0\\r\\n24033,0\\r\\n24034,0\\r\\n24035,0\\r\\n24036,0\\r\\n24037,0\\r\\n24038,0\\r\\n24039,0\\r\\n24040,0\\r\\n24041,0\\r\\n24042,0\\r\\n24043,0\\r\\n24044,0\\r\\n24045,0\\r\\n24046,0\\r\\n24047,0\\r\\n24048,0\\r\\n24049,0\\r\\n24050,0\\r\\n24051,0\\r\\n24052,0\\r\\n24053,0\\r\\n24054,0\\r\\n24055,0\\r\\n24056,0\\r\\n24057,0\\r\\n24058,0\\r\\n24059,0\\r\\n24060,0\\r\\n24061,0\\r\\n24062,0\\r\\n24063,0\\r\\n24064,0\\r\\n24065,0\\r\\n24066,0\\r\\n24067,0\\r\\n24068,0\\r\\n24069,0\\r\\n24070,0\\r\\n24071,0\\r\\n24072,0\\r\\n24073,0\\r\\n24074,0\\r\\n24075,0\\r\\n24076,0\\r\\n24077,0\\r\\n24078,0\\r\\n24079,0\\r\\n24080,0\\r\\n24081,0\\r\\n24082,0\\r\\n24083,0\\r\\n24084,0\\r\\n24085,0\\r\\n24086,0\\r\\n24087,0\\r\\n24088,0\\r\\n24089,0\\r\\n24090,0\\r\\n24091,0\\r\\n24092,0\\r\\n24093,0\\r\\n24094,0\\r\\n24095,0\\r\\n24096,0\\r\\n24097,0\\r\\n24098,0\\r\\n24099,0\\r\\n24100,0\\r\\n24101,0\\r\\n24102,0\\r\\n24103,0\\r\\n24104,0\\r\\n24105,0\\r\\n24106,0\\r\\n24107,0\\r\\n24108,0\\r\\n24109,0\\r\\n24110,0\\r\\n24111,0\\r\\n24112,0\\r\\n24113,0\\r\\n24114,0\\r\\n24115,0\\r\\n24116,0\\r\\n24117,0\\r\\n24118,0\\r\\n24119,0\\r\\n24120,0\\r\\n24121,0\\r\\n24122,0\\r\\n24123,0\\r\\n24124,0\\r\\n24125,0\\r\\n24126,0\\r\\n24127,0\\r\\n24128,0\\r\\n24129,0\\r\\n24130,0\\r\\n24131,0\\r\\n24132,0\\r\\n24133,0\\r\\n24134,0\\r\\n24135,0\\r\\n24136,0\\r\\n24137,0\\r\\n24138,0\\r\\n24139,0\\r\\n24140,0\\r\\n24141,0\\r\\n24142,0\\r\\n24143,0\\r\\n24144,0\\r\\n24145,0\\r\\n24146,0\\r\\n24147,0\\r\\n24148,0\\r\\n24149,0\\r\\n24150,0\\r\\n24151,0\\r\\n24152,0\\r\\n24153,0\\r\\n24154,0\\r\\n24155,0\\r\\n24156,0\\r\\n24157,0\\r\\n24158,0\\r\\n24159,0\\r\\n24160,0\\r\\n24161,0\\r\\n24162,0\\r\\n24163,0\\r\\n24164,0\\r\\n24165,0\\r\\n24166,0\\r\\n24167,0\\r\\n24168,0\\r\\n24169,0\\r\\n24170,0\\r\\n24171,0\\r\\n24172,0\\r\\n24173,0\\r\\n24174,0\\r\\n24175,0\\r\\n24176,0\\r\\n24177,0\\r\\n24178,0\\r\\n24179,0\\r\\n24180,0\\r\\n24181,0\\r\\n24182,0\\r\\n24183,0\\r\\n24184,0\\r\\n24185,0\\r\\n24186,0\\r\\n24187,0\\r\\n24188,0\\r\\n24189,0\\r\\n24190,0\\r\\n24191,0\\r\\n24192,0\\r\\n24193,0\\r\\n24194,0\\r\\n24195,0\\r\\n24196,0\\r\\n24197,0\\r\\n24198,0\\r\\n24199,0\\r\\n24200,0\\r\\n24201,0\\r\\n24202,0\\r\\n24203,0\\r\\n24204,0\\r\\n24205,0\\r\\n24206,0\\r\\n24207,0\\r\\n24208,0\\r\\n24209,0\\r\\n24210,0\\r\\n24211,0\\r\\n24212,0\\r\\n24213,0\\r\\n24214,0\\r\\n24215,0\\r\\n24216,0\\r\\n24217,0\\r\\n24218,0\\r\\n24219,0\\r\\n24220,0\\r\\n24221,0\\r\\n24222,0\\r\\n24223,0\\r\\n24224,0\\r\\n24225,0\\r\\n24226,0\\r\\n24227,0\\r\\n24228,0\\r\\n24229,0\\r\\n24230,0\\r\\n24231,0\\r\\n24232,0\\r\\n24233,0\\r\\n24234,0\\r\\n24235,0\\r\\n24236,0\\r\\n24237,0\\r\\n24238,0\\r\\n24239,0\\r\\n24240,0\\r\\n24241,0\\r\\n24242,0\\r\\n24243,0\\r\\n24244,0\\r\\n24245,0\\r\\n24246,0\\r\\n24247,0\\r\\n24248,0\\r\\n24249,0\\r\\n24250,0\\r\\n24251,0\\r\\n24252,0\\r\\n24253,0\\r\\n24254,0\\r\\n24255,0\\r\\n24256,0\\r\\n24257,0\\r\\n24258,0\\r\\n24259,0\\r\\n24260,0\\r\\n24261,0\\r\\n24262,0\\r\\n24263,0\\r\\n24264,0\\r\\n24265,0\\r\\n24266,0\\r\\n24267,0\\r\\n24268,0\\r\\n24269,0\\r\\n24270,0\\r\\n24271,0\\r\\n24272,0\\r\\n24273,0\\r\\n24274,0\\r\\n24275,0\\r\\n24276,0\\r\\n24277,0\\r\\n24278,0\\r\\n24279,0\\r\\n24280,0\\r\\n24281,0\\r\\n24282,0\\r\\n24283,0\\r\\n24284,0\\r\\n24285,0\\r\\n24286,0\\r\\n24287,0\\r\\n24288,0\\r\\n24289,0\\r\\n24290,0\\r\\n24291,0\\r\\n24292,0\\r\\n24293,0\\r\\n24294,0\\r\\n24295,0\\r\\n24296,0\\r\\n24297,0\\r\\n24298,0\\r\\n24299,0\\r\\n24300,0\\r\\n24301,0\\r\\n24302,0\\r\\n24303,0\\r\\n24304,0\\r\\n24305,0\\r\\n24306,0\\r\\n24307,0\\r\\n24308,0\\r\\n24309,0\\r\\n24310,0\\r\\n24311,0\\r\\n24312,0\\r\\n24313,0\\r\\n24314,0\\r\\n24315,0\\r\\n24316,0\\r\\n24317,0\\r\\n24318,0\\r\\n24319,0\\r\\n24320,0\\r\\n24321,0\\r\\n24322,0\\r\\n24323,0\\r\\n24324,0\\r\\n24325,0\\r\\n24326,0\\r\\n24327,0\\r\\n24328,0\\r\\n24329,0\\r\\n24330,0\\r\\n24331,0\\r\\n24332,0\\r\\n24333,0\\r\\n24334,0\\r\\n24335,0\\r\\n24336,0\\r\\n24337,0\\r\\n24338,0\\r\\n24339,0\\r\\n24340,0\\r\\n24341,0\\r\\n24342,0\\r\\n24343,0\\r\\n24344,0\\r\\n24345,0\\r\\n24346,0\\r\\n24347,0\\r\\n24348,0\\r\\n24349,0\\r\\n24350,0\\r\\n24351,0\\r\\n24352,0\\r\\n24353,0\\r\\n24354,0\\r\\n24355,0\\r\\n24356,0\\r\\n24357,0\\r\\n24358,0\\r\\n24359,0\\r\\n24360,0\\r\\n24361,0\\r\\n24362,0\\r\\n24363,0\\r\\n24364,0\\r\\n24365,0\\r\\n24366,0\\r\\n24367,0\\r\\n24368,0\\r\\n24369,0\\r\\n24370,0\\r\\n24371,0\\r\\n24372,0\\r\\n24373,0\\r\\n24374,0\\r\\n24375,0\\r\\n24376,0\\r\\n24377,0\\r\\n24378,0\\r\\n24379,0\\r\\n24380,0\\r\\n24381,0\\r\\n24382,0\\r\\n24383,0\\r\\n24384,0\\r\\n24385,0\\r\\n24386,0\\r\\n24387,0\\r\\n24388,0\\r\\n24389,0\\r\\n24390,0\\r\\n24391,0\\r\\n24392,0\\r\\n24393,0\\r\\n24394,0\\r\\n24395,0\\r\\n24396,0\\r\\n24397,0\\r\\n24398,0\\r\\n24399,0\\r\\n24400,0\\r\\n24401,0\\r\\n24402,0\\r\\n24403,0\\r\\n24404,0\\r\\n24405,0\\r\\n24406,0\\r\\n24407,0\\r\\n24408,0\\r\\n24409,0\\r\\n24410,0\\r\\n24411,0\\r\\n24412,0\\r\\n24413,0\\r\\n24414,0\\r\\n24415,0\\r\\n24416,0\\r\\n24417,0\\r\\n24418,0\\r\\n24419,0\\r\\n24420,0\\r\\n24421,0\\r\\n24422,0\\r\\n24423,0\\r\\n24424,0\\r\\n24425,0\\r\\n24426,0\\r\\n24427,0\\r\\n24428,0\\r\\n24429,0\\r\\n24430,0\\r\\n24431,0\\r\\n24432,0\\r\\n24433,0\\r\\n24434,0\\r\\n24435,0\\r\\n24436,0\\r\\n24437,0\\r\\n24438,0\\r\\n24439,0\\r\\n24440,0\\r\\n24441,0\\r\\n24442,0\\r\\n24443,0\\r\\n24444,0\\r\\n24445,0\\r\\n24446,0\\r\\n24447,0\\r\\n24448,0\\r\\n24449,0\\r\\n24450,0\\r\\n24451,0\\r\\n24452,0\\r\\n24453,0\\r\\n24454,0\\r\\n24455,0\\r\\n24456,0\\r\\n24457,0\\r\\n24458,0\\r\\n24459,0\\r\\n24460,0\\r\\n24461,0\\r\\n24462,0\\r\\n24463,0\\r\\n24464,0\\r\\n24465,0\\r\\n24466,0\\r\\n24467,0\\r\\n24468,0\\r\\n24469,0\\r\\n24470,0\\r\\n24471,0\\r\\n24472,0\\r\\n24473,0\\r\\n24474,0\\r\\n24475,0\\r\\n24476,0\\r\\n24477,0\\r\\n24478,0\\r\\n24479,0\\r\\n24480,0\\r\\n24481,0\\r\\n24482,0\\r\\n24483,0\\r\\n24484,0\\r\\n24485,0\\r\\n24486,0\\r\\n24487,0\\r\\n24488,0\\r\\n24489,0\\r\\n24490,0\\r\\n24491,0\\r\\n24492,0\\r\\n24493,0\\r\\n24494,0\\r\\n24495,0\\r\\n24496,0\\r\\n24497,0\\r\\n24498,0\\r\\n24499,0\\r\\n24500,0\\r\\n24501,0\\r\\n24502,0\\r\\n24503,0\\r\\n24504,0\\r\\n24505,0\\r\\n24506,0\\r\\n24507,0\\r\\n24508,0\\r\\n24509,0\\r\\n24510,0\\r\\n24511,0\\r\\n24512,0\\r\\n24513,0\\r\\n24514,0\\r\\n24515,0\\r\\n24516,0\\r\\n24517,0\\r\\n24518,0\\r\\n24519,0\\r\\n24520,0\\r\\n24521,0\\r\\n24522,0\\r\\n24523,0\\r\\n24524,0\\r\\n24525,0\\r\\n24526,0\\r\\n24527,0\\r\\n24528,0\\r\\n24529,0\\r\\n24530,0\\r\\n24531,0\\r\\n24532,0\\r\\n24533,0\\r\\n24534,0\\r\\n24535,0\\r\\n24536,0\\r\\n24537,0\\r\\n24538,0\\r\\n24539,0\\r\\n24540,0\\r\\n24541,0\\r\\n24542,0\\r\\n24543,0\\r\\n24544,0\\r\\n24545,0\\r\\n24546,0\\r\\n24547,0\\r\\n24548,0\\r\\n24549,0\\r\\n24550,0\\r\\n24551,0\\r\\n24552,0\\r\\n24553,0\\r\\n24554,0\\r\\n24555,0\\r\\n24556,0\\r\\n24557,0\\r\\n24558,0\\r\\n24559,0\\r\\n24560,0\\r\\n24561,0\\r\\n24562,0\\r\\n24563,0\\r\\n24564,0\\r\\n24565,0\\r\\n24566,0\\r\\n24567,0\\r\\n24568,0\\r\\n24569,0\\r\\n24570,0\\r\\n24571,0\\r\\n24572,0\\r\\n24573,0\\r\\n24574,0\\r\\n24575,0\\r\\n24576,0\\r\\n24577,0\\r\\n24578,0\\r\\n24579,0\\r\\n24580,0\\r\\n24581,0\\r\\n24582,0\\r\\n24583,0\\r\\n24584,0\\r\\n24585,0\\r\\n24586,0\\r\\n24587,0\\r\\n24588,0\\r\\n24589,0\\r\\n24590,0\\r\\n24591,0\\r\\n24592,0\\r\\n24593,0\\r\\n24594,0\\r\\n24595,0\\r\\n24596,0\\r\\n24597,0\\r\\n24598,0\\r\\n24599,0\\r\\n24600,0\\r\\n24601,0\\r\\n24602,0\\r\\n24603,0\\r\\n24604,0\\r\\n24605,0\\r\\n24606,0\\r\\n24607,0\\r\\n24608,0\\r\\n24609,0\\r\\n24610,0\\r\\n24611,0\\r\\n24612,0\\r\\n24613,0\\r\\n24614,0\\r\\n24615,0\\r\\n24616,0\\r\\n24617,0\\r\\n24618,0\\r\\n24619,0\\r\\n24620,0\\r\\n24621,0\\r\\n24622,0\\r\\n24623,0\\r\\n24624,0\\r\\n24625,0\\r\\n24626,0\\r\\n24627,0\\r\\n24628,0\\r\\n24629,0\\r\\n24630,0\\r\\n24631,0\\r\\n24632,0\\r\\n24633,0\\r\\n24634,0\\r\\n24635,0\\r\\n24636,0\\r\\n24637,0\\r\\n24638,0\\r\\n24639,0\\r\\n24640,0\\r\\n24641,0\\r\\n24642,0\\r\\n24643,0\\r\\n24644,0\\r\\n24645,0\\r\\n24646,0\\r\\n24647,0\\r\\n24648,0\\r\\n24649,0\\r\\n24650,0\\r\\n24651,0\\r\\n24652,0\\r\\n24653,0\\r\\n24654,0\\r\\n24655,0\\r\\n24656,0\\r\\n24657,0\\r\\n24658,0\\r\\n24659,0\\r\\n24660,0\\r\\n24661,0\\r\\n24662,0\\r\\n24663,0\\r\\n24664,0\\r\\n24665,0\\r\\n24666,0\\r\\n24667,0\\r\\n24668,0\\r\\n24669,0\\r\\n24670,0\\r\\n24671,0\\r\\n24672,0\\r\\n24673,0\\r\\n24674,0\\r\\n24675,0\\r\\n24676,0\\r\\n24677,0\\r\\n24678,0\\r\\n24679,0\\r\\n24680,0\\r\\n24681,0\\r\\n24682,0\\r\\n24683,0\\r\\n24684,0\\r\\n24685,0\\r\\n24686,0\\r\\n24687,0\\r\\n24688,0\\r\\n24689,0\\r\\n24690,0\\r\\n24691,0\\r\\n24692,0\\r\\n24693,0\\r\\n24694,0\\r\\n24695,0\\r\\n24696,0\\r\\n24697,0\\r\\n24698,0\\r\\n24699,0\\r\\n24700,0\\r\\n24701,0\\r\\n24702,0\\r\\n24703,0\\r\\n24704,0\\r\\n24705,0\\r\\n24706,0\\r\\n24707,0\\r\\n24708,0\\r\\n24709,0\\r\\n24710,0\\r\\n24711,0\\r\\n24712,0\\r\\n24713,0\\r\\n24714,0\\r\\n24715,0\\r\\n24716,0\\r\\n24717,0\\r\\n24718,0\\r\\n24719,0\\r\\n24720,0\\r\\n24721,0\\r\\n24722,0\\r\\n24723,0\\r\\n24724,0\\r\\n24725,0\\r\\n24726,0\\r\\n24727,0\\r\\n24728,0\\r\\n24729,0\\r\\n24730,0\\r\\n24731,0\\r\\n24732,0\\r\\n24733,0\\r\\n24734,0\\r\\n24735,0\\r\\n24736,0\\r\\n24737,0\\r\\n24738,0\\r\\n24739,0\\r\\n24740,0\\r\\n24741,0\\r\\n24742,0\\r\\n24743,0\\r\\n24744,0\\r\\n24745,0\\r\\n24746,0\\r\\n24747,0\\r\\n24748,0\\r\\n24749,0\\r\\n24750,0\\r\\n24751,0\\r\\n24752,0\\r\\n24753,0\\r\\n24754,0\\r\\n24755,0\\r\\n24756,0\\r\\n24757,0\\r\\n24758,0\\r\\n24759,0\\r\\n24760,0\\r\\n24761,0\\r\\n24762,0\\r\\n24763,0\\r\\n24764,0\\r\\n24765,0\\r\\n24766,0\\r\\n24767,0\\r\\n24768,0\\r\\n24769,0\\r\\n24770,0\\r\\n24771,0\\r\\n24772,0\\r\\n24773,0\\r\\n24774,0\\r\\n24775,0\\r\\n24776,0\\r\\n24777,0\\r\\n24778,0\\r\\n24779,0\\r\\n24780,0\\r\\n24781,0\\r\\n24782,0\\r\\n24783,0\\r\\n24784,0\\r\\n24785,0\\r\\n24786,0\\r\\n24787,0\\r\\n24788,0\\r\\n24789,0\\r\\n24790,0\\r\\n24791,0\\r\\n24792,0\\r\\n24793,0\\r\\n24794,0\\r\\n24795,0\\r\\n24796,0\\r\\n24797,0\\r\\n24798,0\\r\\n24799,0\\r\\n24800,0\\r\\n24801,0\\r\\n24802,0\\r\\n24803,0\\r\\n24804,0\\r\\n24805,0\\r\\n24806,0\\r\\n24807,0\\r\\n24808,0\\r\\n24809,0\\r\\n24810,0\\r\\n24811,0\\r\\n24812,0\\r\\n24813,0\\r\\n24814,0\\r\\n24815,0\\r\\n24816,0\\r\\n24817,0\\r\\n24818,0\\r\\n24819,0\\r\\n24820,0\\r\\n24821,0\\r\\n24822,0\\r\\n24823,0\\r\\n24824,0\\r\\n24825,0\\r\\n24826,0\\r\\n24827,0\\r\\n24828,0\\r\\n24829,0\\r\\n24830,0\\r\\n24831,0\\r\\n24832,0\\r\\n24833,0\\r\\n24834,0\\r\\n24835,0\\r\\n24836,0\\r\\n24837,0\\r\\n24838,0\\r\\n24839,0\\r\\n24840,0\\r\\n24841,0\\r\\n24842,0\\r\\n24843,0\\r\\n24844,0\\r\\n24845,0\\r\\n24846,0\\r\\n24847,0\\r\\n24848,0\\r\\n24849,0\\r\\n24850,0\\r\\n24851,0\\r\\n24852,0\\r\\n24853,0\\r\\n24854,0\\r\\n24855,0\\r\\n24856,0\\r\\n24857,0\\r\\n24858,0\\r\\n24859,0\\r\\n24860,0\\r\\n24861,0\\r\\n24862,0\\r\\n24863,0\\r\\n24864,0\\r\\n24865,0\\r\\n24866,0\\r\\n24867,0\\r\\n24868,0\\r\\n24869,0\\r\\n24870,0\\r\\n24871,0\\r\\n24872,0\\r\\n24873,0\\r\\n24874,0\\r\\n24875,0\\r\\n24876,0\\r\\n24877,0\\r\\n24878,0\\r\\n24879,0\\r\\n24880,0\\r\\n24881,0\\r\\n24882,0\\r\\n24883,0\\r\\n24884,0\\r\\n24885,0\\r\\n24886,0\\r\\n24887,0\\r\\n24888,0\\r\\n24889,0\\r\\n24890,0\\r\\n24891,0\\r\\n24892,0\\r\\n24893,0\\r\\n24894,0\\r\\n24895,0\\r\\n24896,0\\r\\n24897,0\\r\\n24898,0\\r\\n24899,0\\r\\n24900,0\\r\\n24901,0\\r\\n24902,0\\r\\n24903,0\\r\\n24904,0\\r\\n24905,0\\r\\n24906,0\\r\\n24907,0\\r\\n24908,0\\r\\n24909,0\\r\\n24910,0\\r\\n24911,0\\r\\n24912,0\\r\\n24913,0\\r\\n24914,0\\r\\n24915,0\\r\\n24916,0\\r\\n24917,0\\r\\n24918,0\\r\\n24919,0\\r\\n24920,0\\r\\n24921,0\\r\\n24922,0\\r\\n24923,0\\r\\n24924,0\\r\\n24925,0\\r\\n24926,0\\r\\n24927,0\\r\\n24928,0\\r\\n24929,0\\r\\n24930,0\\r\\n24931,0\\r\\n24932,0\\r\\n24933,0\\r\\n24934,0\\r\\n24935,0\\r\\n24936,0\\r\\n24937,0\\r\\n24938,0\\r\\n24939,0\\r\\n24940,0\\r\\n24941,0\\r\\n24942,0\\r\\n24943,0\\r\\n24944,0\\r\\n24945,0\\r\\n24946,0\\r\\n24947,0\\r\\n24948,0\\r\\n24949,0\\r\\n24950,0\\r\\n24951,0\\r\\n24952,0\\r\\n24953,0\\r\\n24954,0\\r\\n24955,0\\r\\n24956,0\\r\\n24957,0\\r\\n24958,0\\r\\n24959,0\\r\\n24960,0\\r\\n24961,0\\r\\n24962,0\\r\\n24963,0\\r\\n24964,0\\r\\n24965,0\\r\\n24966,0\\r\\n24967,0\\r\\n24968,0\\r\\n24969,0\\r\\n24970,0\\r\\n24971,0\\r\\n24972,0\\r\\n24973,0\\r\\n24974,0\\r\\n24975,0\\r\\n24976,0\\r\\n24977,0\\r\\n24978,0\\r\\n24979,0\\r\\n24980,0\\r\\n24981,0\\r\\n24982,0\\r\\n24983,0\\r\\n24984,0\\r\\n24985,0\\r\\n24986,0\\r\\n24987,0\\r\\n24988,0\\r\\n24989,0\\r\\n24990,0\\r\\n24991,0\\r\\n24992,0\\r\\n24993,0\\r\\n24994,0\\r\\n24995,0\\r\\n24996,0\\r\\n24997,0\\r\\n24998,0\\r\\n24999,0\\r\\n25000,0\\r\\n25001,0\\r\\n25002,0\\r\\n25003,0\\r\\n25004,0\\r\\n25005,0\\r\\n25006,0\\r\\n25007,0\\r\\n25008,0\\r\\n25009,0\\r\\n25010,0\\r\\n25011,0\\r\\n25012,0\\r\\n25013,0\\r\\n25014,0\\r\\n25015,0\\r\\n25016,0\\r\\n25017,0\\r\\n25018,0\\r\\n25019,0\\r\\n25020,0\\r\\n25021,0\\r\\n25022,0\\r\\n25023,0\\r\\n25024,0\\r\\n25025,0\\r\\n25026,0\\r\\n25027,0\\r\\n25028,0\\r\\n25029,0\\r\\n25030,0\\r\\n25031,0\\r\\n25032,0\\r\\n25033,0\\r\\n25034,0\\r\\n25035,0\\r\\n25036,0\\r\\n25037,0\\r\\n25038,0\\r\\n25039,0\\r\\n25040,0\\r\\n25041,0\\r\\n25042,0\\r\\n25043,0\\r\\n25044,0\\r\\n25045,0\\r\\n25046,0\\r\\n25047,0\\r\\n25048,0\\r\\n25049,0\\r\\n25050,0\\r\\n25051,0\\r\\n25052,0\\r\\n25053,0\\r\\n25054,0\\r\\n25055,0\\r\\n25056,0\\r\\n25057,0\\r\\n25058,0\\r\\n25059,0\\r\\n25060,0\\r\\n25061,0\\r\\n25062,0\\r\\n25063,0\\r\\n25064,0\\r\\n25065,0\\r\\n25066,0\\r\\n25067,0\\r\\n25068,0\\r\\n25069,0\\r\\n25070,0\\r\\n25071,0\\r\\n25072,0\\r\\n25073,0\\r\\n25074,0\\r\\n25075,0\\r\\n25076,0\\r\\n25077,0\\r\\n25078,0\\r\\n25079,0\\r\\n25080,0\\r\\n25081,0\\r\\n25082,0\\r\\n25083,0\\r\\n25084,0\\r\\n25085,0\\r\\n25086,0\\r\\n25087,0\\r\\n25088,0\\r\\n25089,0\\r\\n25090,0\\r\\n25091,0\\r\\n25092,0\\r\\n25093,0\\r\\n25094,0\\r\\n25095,0\\r\\n25096,0\\r\\n25097,0\\r\\n25098,0\\r\\n25099,0\\r\\n25100,0\\r\\n25101,0\\r\\n25102,0\\r\\n25103,0\\r\\n25104,0\\r\\n25105,0\\r\\n25106,0\\r\\n25107,0\\r\\n25108,0\\r\\n25109,0\\r\\n25110,0\\r\\n25111,0\\r\\n25112,0\\r\\n25113,0\\r\\n25114,0\\r\\n25115,0\\r\\n25116,0\\r\\n25117,0\\r\\n25118,0\\r\\n25119,0\\r\\n25120,0\\r\\n25121,0\\r\\n25122,0\\r\\n25123,0\\r\\n25124,0\\r\\n25125,0\\r\\n25126,0\\r\\n25127,0\\r\\n25128,0\\r\\n25129,0\\r\\n25130,0\\r\\n25131,0\\r\\n25132,0\\r\\n25133,0\\r\\n25134,0\\r\\n25135,0\\r\\n25136,0\\r\\n25137,0\\r\\n25138,0\\r\\n25139,0\\r\\n25140,0\\r\\n25141,0\\r\\n25142,0\\r\\n25143,0\\r\\n25144,0\\r\\n25145,0\\r\\n25146,0\\r\\n25147,0\\r\\n25148,0\\r\\n25149,0\\r\\n25150,0\\r\\n25151,0\\r\\n25152,0\\r\\n25153,0\\r\\n25154,0\\r\\n25155,0\\r\\n25156,0\\r\\n25157,0\\r\\n25158,0\\r\\n25159,0\\r\\n25160,0\\r\\n25161,0\\r\\n25162,0\\r\\n25163,0\\r\\n25164,0\\r\\n25165,0\\r\\n25166,0\\r\\n25167,0\\r\\n25168,0\\r\\n25169,0\\r\\n25170,0\\r\\n25171,0\\r\\n25172,0\\r\\n25173,0\\r\\n25174,0\\r\\n25175,0\\r\\n25176,0\\r\\n25177,0\\r\\n25178,0\\r\\n25179,0\\r\\n25180,0\\r\\n25181,0\\r\\n25182,0\\r\\n25183,0\\r\\n25184,0\\r\\n25185,0\\r\\n25186,0\\r\\n25187,0\\r\\n25188,0\\r\\n25189,0\\r\\n25190,0\\r\\n25191,0\\r\\n25192,0\\r\\n25193,0\\r\\n25194,0\\r\\n25195,0\\r\\n25196,0\\r\\n25197,0\\r\\n25198,0\\r\\n25199,0\\r\\n25200,0\\r\\n25201,0\\r\\n25202,0\\r\\n25203,0\\r\\n25204,0\\r\\n25205,0\\r\\n25206,0\\r\\n25207,0\\r\\n25208,0\\r\\n25209,0\\r\\n25210,0\\r\\n25211,0\\r\\n25212,0\\r\\n25213,0\\r\\n25214,0\\r\\n25215,0\\r\\n25216,0\\r\\n25217,0\\r\\n25218,0\\r\\n25219,0\\r\\n25220,0\\r\\n25221,0\\r\\n25222,0\\r\\n25223,0\\r\\n25224,0\\r\\n25225,0\\r\\n25226,0\\r\\n25227,0\\r\\n25228,0\\r\\n25229,0\\r\\n25230,0\\r\\n25231,0\\r\\n25232,0\\r\\n25233,0\\r\\n25234,0\\r\\n25235,0\\r\\n25236,0\\r\\n25237,0\\r\\n25238,0\\r\\n25239,0\\r\\n25240,0\\r\\n25241,0\\r\\n25242,0\\r\\n25243,0\\r\\n25244,0\\r\\n25245,0\\r\\n25246,0\\r\\n25247,0\\r\\n25248,0\\r\\n25249,0\\r\\n25250,0\\r\\n25251,0\\r\\n25252,0\\r\\n25253,0\\r\\n25254,0\\r\\n25255,0\\r\\n25256,0\\r\\n25257,0\\r\\n25258,0\\r\\n25259,0\\r\\n25260,0\\r\\n25261,0\\r\\n25262,0\\r\\n25263,0\\r\\n25264,0\\r\\n25265,0\\r\\n25266,0\\r\\n25267,0\\r\\n25268,0\\r\\n25269,0\\r\\n25270,0\\r\\n25271,0\\r\\n25272,0\\r\\n25273,0\\r\\n25274,0\\r\\n25275,0\\r\\n25276,0\\r\\n25277,0\\r\\n25278,0\\r\\n25279,0\\r\\n25280,0\\r\\n25281,0\\r\\n25282,0\\r\\n25283,0\\r\\n25284,0\\r\\n25285,0\\r\\n25286,0\\r\\n25287,0\\r\\n25288,0\\r\\n25289,0\\r\\n25290,0\\r\\n25291,0\\r\\n25292,0\\r\\n25293,0\\r\\n25294,0\\r\\n25295,0\\r\\n25296,0\\r\\n25297,0\\r\\n25298,0\\r\\n25299,0\\r\\n25300,0\\r\\n25301,0\\r\\n25302,0\\r\\n25303,0\\r\\n25304,0\\r\\n25305,0\\r\\n25306,0\\r\\n25307,0\\r\\n25308,0\\r\\n25309,0\\r\\n25310,0\\r\\n25311,0\\r\\n25312,0\\r\\n25313,0\\r\\n25314,0\\r\\n25315,0\\r\\n25316,0\\r\\n25317,0\\r\\n25318,0\\r\\n25319,0\\r\\n25320,0\\r\\n25321,0\\r\\n25322,0\\r\\n25323,0\\r\\n25324,0\\r\\n25325,0\\r\\n25326,0\\r\\n25327,0\\r\\n25328,0\\r\\n25329,0\\r\\n25330,0\\r\\n25331,0\\r\\n25332,0\\r\\n25333,0\\r\\n25334,0\\r\\n25335,0\\r\\n25336,0\\r\\n25337,0\\r\\n25338,0\\r\\n25339,0\\r\\n25340,0\\r\\n25341,0\\r\\n25342,0\\r\\n25343,0\\r\\n25344,0\\r\\n25345,0\\r\\n25346,0\\r\\n25347,0\\r\\n25348,0\\r\\n25349,0\\r\\n25350,0\\r\\n25351,0\\r\\n25352,0\\r\\n25353,0\\r\\n25354,0\\r\\n25355,0\\r\\n25356,0\\r\\n25357,0\\r\\n25358,0\\r\\n25359,0\\r\\n25360,0\\r\\n25361,0\\r\\n25362,0\\r\\n25363,0\\r\\n25364,0\\r\\n25365,0\\r\\n25366,0\\r\\n25367,0\\r\\n25368,0\\r\\n25369,0\\r\\n25370,0\\r\\n25371,0\\r\\n25372,0\\r\\n25373,0\\r\\n25374,0\\r\\n25375,0\\r\\n25376,0\\r\\n25377,0\\r\\n25378,0\\r\\n25379,0\\r\\n25380,0\\r\\n25381,0\\r\\n25382,0\\r\\n25383,0\\r\\n25384,0\\r\\n25385,0\\r\\n25386,0\\r\\n25387,0\\r\\n25388,0\\r\\n25389,0\\r\\n25390,0\\r\\n25391,0\\r\\n25392,0\\r\\n25393,0\\r\\n25394,0\\r\\n25395,0\\r\\n25396,0\\r\\n25397,0\\r\\n25398,0\\r\\n25399,0\\r\\n25400,0\\r\\n25401,0\\r\\n25402,0\\r\\n25403,0\\r\\n25404,0\\r\\n25405,0\\r\\n25406,0\\r\\n25407,0\\r\\n25408,0\\r\\n25409,0\\r\\n25410,0\\r\\n25411,0\\r\\n25412,0\\r\\n25413,0\\r\\n25414,0\\r\\n25415,0\\r\\n25416,0\\r\\n25417,0\\r\\n25418,0\\r\\n25419,0\\r\\n25420,0\\r\\n25421,0\\r\\n25422,0\\r\\n25423,0\\r\\n25424,0\\r\\n25425,0\\r\\n25426,0\\r\\n25427,0\\r\\n25428,0\\r\\n25429,0\\r\\n25430,0\\r\\n25431,0\\r\\n25432,0\\r\\n25433,0\\r\\n25434,0\\r\\n25435,0\\r\\n25436,0\\r\\n25437,0\\r\\n25438,0\\r\\n25439,0\\r\\n25440,0\\r\\n25441,0\\r\\n25442,0\\r\\n25443,0\\r\\n25444,0\\r\\n25445,0\\r\\n25446,0\\r\\n25447,0\\r\\n25448,0\\r\\n25449,0\\r\\n25450,0\\r\\n25451,0\\r\\n25452,0\\r\\n25453,0\\r\\n25454,0\\r\\n25455,0\\r\\n25456,0\\r\\n25457,0\\r\\n25458,0\\r\\n25459,0\\r\\n25460,0\\r\\n25461,0\\r\\n25462,0\\r\\n25463,0\\r\\n25464,0\\r\\n25465,0\\r\\n25466,0\\r\\n25467,0\\r\\n25468,0\\r\\n25469,0\\r\\n25470,0\\r\\n25471,0\\r\\n25472,0\\r\\n25473,0\\r\\n25474,0\\r\\n25475,0\\r\\n25476,0\\r\\n25477,0\\r\\n25478,0\\r\\n25479,0\\r\\n25480,0\\r\\n25481,0\\r\\n25482,0\\r\\n25483,0\\r\\n25484,0\\r\\n25485,0\\r\\n25486,0\\r\\n25487,0\\r\\n25488,0\\r\\n25489,0\\r\\n25490,0\\r\\n25491,0\\r\\n25492,0\\r\\n25493,0\\r\\n25494,0\\r\\n25495,0\\r\\n25496,0\\r\\n25497,0\\r\\n25498,0\\r\\n25499,0\\r\\n25500,0\\r\\n25501,0\\r\\n25502,0\\r\\n25503,0\\r\\n25504,0\\r\\n25505,0\\r\\n25506,0\\r\\n25507,0\\r\\n25508,0\\r\\n25509,0\\r\\n25510,0\\r\\n25511,0\\r\\n25512,0\\r\\n25513,0\\r\\n25514,0\\r\\n25515,0\\r\\n25516,0\\r\\n25517,0\\r\\n25518,0\\r\\n25519,0\\r\\n25520,0\\r\\n25521,0\\r\\n25522,0\\r\\n25523,0\\r\\n25524,0\\r\\n25525,0\\r\\n25526,0\\r\\n25527,0\\r\\n25528,0\\r\\n25529,0\\r\\n25530,0\\r\\n25531,0\\r\\n25532,0\\r\\n25533,0\\r\\n25534,0\\r\\n25535,0\\r\\n25536,0\\r\\n25537,0\\r\\n25538,0\\r\\n25539,0\\r\\n25540,0\\r\\n25541,0\\r\\n25542,0\\r\\n25543,0\\r\\n25544,0\\r\\n25545,0\\r\\n25546,0\\r\\n25547,0\\r\\n25548,0\\r\\n25549,0\\r\\n25550,0\\r\\n25551,0\\r\\n25552,0\\r\\n25553,0\\r\\n25554,0\\r\\n25555,0\\r\\n25556,0\\r\\n25557,0\\r\\n25558,0\\r\\n25559,0\\r\\n25560,0\\r\\n25561,0\\r\\n25562,0\\r\\n25563,0\\r\\n25564,0\\r\\n25565,0\\r\\n25566,0\\r\\n25567,0\\r\\n25568,0\\r\\n25569,0\\r\\n25570,0\\r\\n25571,0\\r\\n25572,0\\r\\n25573,0\\r\\n25574,0\\r\\n25575,0\\r\\n25576,0\\r\\n25577,0\\r\\n25578,0\\r\\n25579,0\\r\\n25580,0\\r\\n25581,0\\r\\n25582,0\\r\\n25583,0\\r\\n25584,0\\r\\n25585,0\\r\\n25586,0\\r\\n25587,0\\r\\n25588,0\\r\\n25589,0\\r\\n25590,0\\r\\n25591,0\\r\\n25592,0\\r\\n25593,0\\r\\n25594,0\\r\\n25595,0\\r\\n25596,0\\r\\n25597,0\\r\\n25598,0\\r\\n25599,0\\r\\n25600,0\\r\\n25601,0\\r\\n25602,0\\r\\n25603,0\\r\\n25604,0\\r\\n25605,0\\r\\n25606,0\\r\\n25607,0\\r\\n25608,0\\r\\n25609,0\\r\\n25610,0\\r\\n25611,0\\r\\n25612,0\\r\\n25613,0\\r\\n25614,0\\r\\n25615,0\\r\\n25616,0\\r\\n25617,0\\r\\n25618,0\\r\\n25619,0\\r\\n25620,0\\r\\n25621,0\\r\\n25622,0\\r\\n25623,0\\r\\n25624,0\\r\\n25625,0\\r\\n25626,0\\r\\n25627,0\\r\\n25628,0\\r\\n25629,0\\r\\n25630,0\\r\\n25631,0\\r\\n25632,0\\r\\n25633,0\\r\\n25634,0\\r\\n25635,0\\r\\n25636,0\\r\\n25637,0\\r\\n25638,0\\r\\n25639,0\\r\\n25640,0\\r\\n25641,0\\r\\n25642,0\\r\\n25643,0\\r\\n25644,0\\r\\n25645,0\\r\\n25646,0\\r\\n25647,0\\r\\n25648,0\\r\\n25649,0\\r\\n25650,0\\r\\n25651,0\\r\\n25652,0\\r\\n25653,0\\r\\n25654,0\\r\\n25655,0\\r\\n25656,0\\r\\n25657,0\\r\\n25658,0\\r\\n25659,0\\r\\n25660,0\\r\\n25661,0\\r\\n25662,0\\r\\n25663,0\\r\\n25664,0\\r\\n25665,0\\r\\n25666,0\\r\\n25667,0\\r\\n25668,0\\r\\n25669,0\\r\\n25670,0\\r\\n25671,0\\r\\n25672,0\\r\\n25673,0\\r\\n25674,0\\r\\n25675,0\\r\\n25676,0\\r\\n25677,0\\r\\n25678,0\\r\\n25679,0\\r\\n25680,0\\r\\n25681,0\\r\\n25682,0\\r\\n25683,0\\r\\n25684,0\\r\\n25685,0\\r\\n25686,0\\r\\n25687,0\\r\\n25688,0\\r\\n25689,0\\r\\n25690,0\\r\\n25691,0\\r\\n25692,0\\r\\n25693,0\\r\\n25694,0\\r\\n25695,0\\r\\n25696,0\\r\\n25697,0\\r\\n25698,0\\r\\n25699,0\\r\\n25700,0\\r\\n25701,0\\r\\n25702,0\\r\\n25703,0\\r\\n25704,0\\r\\n25705,0\\r\\n25706,0\\r\\n25707,0\\r\\n25708,0\\r\\n25709,0\\r\\n25710,0\\r\\n25711,0\\r\\n25712,0\\r\\n25713,0\\r\\n25714,0\\r\\n25715,0\\r\\n25716,0\\r\\n25717,0\\r\\n25718,0\\r\\n25719,0\\r\\n25720,0\\r\\n25721,0\\r\\n25722,0\\r\\n25723,0\\r\\n25724,0\\r\\n25725,0\\r\\n25726,0\\r\\n25727,0\\r\\n25728,0\\r\\n25729,0\\r\\n25730,0\\r\\n25731,0\\r\\n25732,0\\r\\n25733,0\\r\\n25734,0\\r\\n25735,0\\r\\n25736,0\\r\\n25737,0\\r\\n25738,0\\r\\n25739,0\\r\\n25740,0\\r\\n25741,0\\r\\n25742,0\\r\\n25743,0\\r\\n25744,0\\r\\n25745,0\\r\\n25746,0\\r\\n25747,0\\r\\n25748,0\\r\\n25749,0\\r\\n25750,0\\r\\n25751,0\\r\\n25752,0\\r\\n25753,0\\r\\n25754,0\\r\\n25755,0\\r\\n25756,0\\r\\n25757,0\\r\\n25758,0\\r\\n25759,0\\r\\n25760,0\\r\\n25761,0\\r\\n25762,0\\r\\n25763,0\\r\\n25764,0\\r\\n25765,0\\r\\n25766,0\\r\\n25767,0\\r\\n25768,0\\r\\n25769,0\\r\\n25770,0\\r\\n25771,0\\r\\n25772,0\\r\\n25773,0\\r\\n25774,0\\r\\n25775,0\\r\\n25776,0\\r\\n25777,0\\r\\n25778,0\\r\\n25779,0\\r\\n25780,0\\r\\n25781,0\\r\\n25782,0\\r\\n25783,0\\r\\n25784,0\\r\\n25785,0\\r\\n25786,0\\r\\n25787,0\\r\\n25788,0\\r\\n25789,0\\r\\n25790,0\\r\\n25791,0\\r\\n25792,0\\r\\n25793,0\\r\\n25794,0\\r\\n25795,0\\r\\n25796,0\\r\\n25797,0\\r\\n25798,0\\r\\n25799,0\\r\\n25800,0\\r\\n25801,0\\r\\n25802,0\\r\\n25803,0\\r\\n25804,0\\r\\n25805,0\\r\\n25806,0\\r\\n25807,0\\r\\n25808,0\\r\\n25809,0\\r\\n25810,0\\r\\n25811,0\\r\\n25812,0\\r\\n25813,0\\r\\n25814,0\\r\\n25815,0\\r\\n25816,0\\r\\n25817,0\\r\\n25818,0\\r\\n25819,0\\r\\n25820,0\\r\\n25821,0\\r\\n25822,0\\r\\n25823,0\\r\\n25824,0\\r\\n25825,0\\r\\n25826,0\\r\\n25827,0\\r\\n25828,0\\r\\n25829,0\\r\\n25830,0\\r\\n25831,0\\r\\n25832,0\\r\\n25833,0\\r\\n25834,0\\r\\n25835,0\\r\\n25836,0\\r\\n25837,0\\r\\n25838,0\\r\\n25839,0\\r\\n25840,0\\r\\n25841,0\\r\\n25842,0\\r\\n25843,0\\r\\n25844,0\\r\\n25845,0\\r\\n25846,0\\r\\n25847,0\\r\\n25848,0\\r\\n25849,0\\r\\n25850,0\\r\\n25851,0\\r\\n25852,0\\r\\n25853,0\\r\\n25854,0\\r\\n25855,0\\r\\n25856,0\\r\\n25857,0\\r\\n25858,0\\r\\n25859,0\\r\\n25860,0\\r\\n25861,0\\r\\n25862,0\\r\\n25863,0\\r\\n25864,0\\r\\n25865,0\\r\\n25866,0\\r\\n25867,0\\r\\n25868,0\\r\\n25869,0\\r\\n25870,0\\r\\n25871,0\\r\\n25872,0\\r\\n25873,0\\r\\n25874,0\\r\\n25875,0\\r\\n25876,0\\r\\n25877,0\\r\\n25878,0\\r\\n25879,0\\r\\n25880,0\\r\\n25881,0\\r\\n25882,0\\r\\n25883,0\\r\\n25884,0\\r\\n25885,0\\r\\n25886,0\\r\\n25887,0\\r\\n25888,0\\r\\n25889,0\\r\\n25890,0\\r\\n25891,0\\r\\n25892,0\\r\\n25893,0\\r\\n25894,0\\r\\n25895,0\\r\\n25896,0\\r\\n25897,0\\r\\n25898,0\\r\\n25899,0\\r\\n25900,0\\r\\n25901,0\\r\\n25902,0\\r\\n25903,0\\r\\n25904,0\\r\\n25905,0\\r\\n25906,0\\r\\n25907,0\\r\\n25908,0\\r\\n25909,0\\r\\n25910,0\\r\\n25911,0\\r\\n25912,0\\r\\n25913,0\\r\\n25914,0\\r\\n25915,0\\r\\n25916,0\\r\\n25917,0\\r\\n25918,0\\r\\n25919,0\\r\\n25920,0\\r\\n25921,0\\r\\n25922,0\\r\\n25923,0\\r\\n25924,0\\r\\n25925,0\\r\\n25926,0\\r\\n25927,0\\r\\n25928,0\\r\\n25929,0\\r\\n25930,0\\r\\n25931,0\\r\\n25932,0\\r\\n25933,0\\r\\n25934,0\\r\\n25935,0\\r\\n25936,0\\r\\n25937,0\\r\\n25938,0\\r\\n25939,0\\r\\n25940,0\\r\\n25941,0\\r\\n25942,0\\r\\n25943,0\\r\\n25944,0\\r\\n25945,0\\r\\n25946,0\\r\\n25947,0\\r\\n25948,0\\r\\n25949,0\\r\\n25950,0\\r\\n25951,0\\r\\n25952,0\\r\\n25953,0\\r\\n25954,0\\r\\n25955,0\\r\\n25956,0\\r\\n25957,0\\r\\n25958,0\\r\\n25959,0\\r\\n25960,0\\r\\n25961,0\\r\\n25962,0\\r\\n25963,0\\r\\n25964,0\\r\\n25965,0\\r\\n25966,0\\r\\n25967,0\\r\\n25968,0\\r\\n25969,0\\r\\n25970,0\\r\\n25971,0\\r\\n25972,0\\r\\n25973,0\\r\\n25974,0\\r\\n25975,0\\r\\n25976,0\\r\\n25977,0\\r\\n25978,0\\r\\n25979,0\\r\\n25980,0\\r\\n25981,0\\r\\n25982,0\\r\\n25983,0\\r\\n25984,0\\r\\n25985,0\\r\\n25986,0\\r\\n25987,0\\r\\n25988,0\\r\\n25989,0\\r\\n25990,0\\r\\n25991,0\\r\\n25992,0\\r\\n25993,0\\r\\n25994,0\\r\\n25995,0\\r\\n25996,0\\r\\n25997,0\\r\\n25998,0\\r\\n25999,0\\r\\n26000,0\\r\\n26001,0\\r\\n26002,0\\r\\n26003,0\\r\\n26004,0\\r\\n26005,0\\r\\n26006,0\\r\\n26007,0\\r\\n26008,0\\r\\n26009,0\\r\\n26010,0\\r\\n26011,0\\r\\n26012,0\\r\\n26013,0\\r\\n26014,0\\r\\n26015,0\\r\\n26016,0\\r\\n26017,0\\r\\n26018,0\\r\\n26019,0\\r\\n26020,0\\r\\n26021,0\\r\\n26022,0\\r\\n26023,0\\r\\n26024,0\\r\\n26025,0\\r\\n26026,0\\r\\n26027,0\\r\\n26028,0\\r\\n26029,0\\r\\n26030,0\\r\\n26031,0\\r\\n26032,0\\r\\n26033,0\\r\\n26034,0\\r\\n26035,0\\r\\n26036,0\\r\\n26037,0\\r\\n26038,0\\r\\n26039,0\\r\\n26040,0\\r\\n26041,0\\r\\n26042,0\\r\\n26043,0\\r\\n26044,0\\r\\n26045,0\\r\\n26046,0\\r\\n26047,0\\r\\n26048,0\\r\\n26049,0\\r\\n26050,0\\r\\n26051,0\\r\\n26052,0\\r\\n26053,0\\r\\n26054,0\\r\\n26055,0\\r\\n26056,0\\r\\n26057,0\\r\\n26058,0\\r\\n26059,0\\r\\n26060,0\\r\\n26061,0\\r\\n26062,0\\r\\n26063,0\\r\\n26064,0\\r\\n26065,0\\r\\n26066,0\\r\\n26067,0\\r\\n26068,0\\r\\n26069,0\\r\\n26070,0\\r\\n26071,0\\r\\n26072,0\\r\\n26073,0\\r\\n26074,0\\r\\n26075,0\\r\\n26076,0\\r\\n26077,0\\r\\n26078,0\\r\\n26079,0\\r\\n26080,0\\r\\n26081,0\\r\\n26082,0\\r\\n26083,0\\r\\n26084,0\\r\\n26085,0\\r\\n26086,0\\r\\n26087,0\\r\\n26088,0\\r\\n26089,0\\r\\n26090,0\\r\\n26091,0\\r\\n26092,0\\r\\n26093,0\\r\\n26094,0\\r\\n26095,0\\r\\n26096,0\\r\\n26097,0\\r\\n26098,0\\r\\n26099,0\\r\\n26100,0\\r\\n26101,0\\r\\n26102,0\\r\\n26103,0\\r\\n26104,0\\r\\n26105,0\\r\\n26106,0\\r\\n26107,0\\r\\n26108,0\\r\\n26109,0\\r\\n26110,0\\r\\n26111,0\\r\\n26112,0\\r\\n26113,0\\r\\n26114,0\\r\\n26115,0\\r\\n26116,0\\r\\n26117,0\\r\\n26118,0\\r\\n26119,0\\r\\n26120,0\\r\\n26121,0\\r\\n26122,0\\r\\n26123,0\\r\\n26124,0\\r\\n26125,0\\r\\n26126,0\\r\\n26127,0\\r\\n26128,0\\r\\n26129,0\\r\\n26130,0\\r\\n26131,0\\r\\n26132,0\\r\\n26133,0\\r\\n26134,0\\r\\n26135,0\\r\\n26136,0\\r\\n26137,0\\r\\n26138,0\\r\\n26139,0\\r\\n26140,0\\r\\n26141,0\\r\\n26142,0\\r\\n26143,0\\r\\n26144,0\\r\\n26145,0\\r\\n26146,0\\r\\n26147,0\\r\\n26148,0\\r\\n26149,0\\r\\n26150,0\\r\\n26151,0\\r\\n26152,0\\r\\n26153,0\\r\\n26154,0\\r\\n26155,0\\r\\n26156,0\\r\\n26157,0\\r\\n26158,0\\r\\n26159,0\\r\\n26160,0\\r\\n26161,0\\r\\n26162,0\\r\\n26163,0\\r\\n26164,0\\r\\n26165,0\\r\\n26166,0\\r\\n26167,0\\r\\n26168,0\\r\\n26169,0\\r\\n26170,0\\r\\n26171,0\\r\\n26172,0\\r\\n26173,0\\r\\n26174,0\\r\\n26175,0\\r\\n26176,0\\r\\n26177,0\\r\\n26178,0\\r\\n26179,0\\r\\n26180,0\\r\\n26181,0\\r\\n26182,0\\r\\n26183,0\\r\\n26184,0\\r\\n26185,0\\r\\n26186,0\\r\\n26187,0\\r\\n26188,0\\r\\n26189,0\\r\\n26190,0\\r\\n26191,0\\r\\n26192,0\\r\\n26193,0\\r\\n26194,0\\r\\n26195,0\\r\\n26196,0\\r\\n26197,0\\r\\n26198,0\\r\\n26199,0\\r\\n26200,0\\r\\n26201,0\\r\\n26202,0\\r\\n26203,0\\r\\n26204,0\\r\\n26205,0\\r\\n26206,0\\r\\n26207,0\\r\\n26208,0\\r\\n26209,0\\r\\n26210,0\\r\\n26211,0\\r\\n26212,0\\r\\n26213,0\\r\\n26214,0\\r\\n26215,0\\r\\n26216,0\\r\\n26217,0\\r\\n26218,0\\r\\n26219,0\\r\\n26220,0\\r\\n26221,0\\r\\n26222,0\\r\\n26223,0\\r\\n26224,0\\r\\n26225,0\\r\\n26226,0\\r\\n26227,0\\r\\n26228,0\\r\\n26229,0\\r\\n26230,0\\r\\n26231,0\\r\\n26232,0\\r\\n26233,0\\r\\n26234,0\\r\\n26235,0\\r\\n26236,0\\r\\n26237,0\\r\\n26238,0\\r\\n26239,0\\r\\n26240,0\\r\\n26241,0\\r\\n26242,0\\r\\n26243,0\\r\\n26244,0\\r\\n26245,0\\r\\n26246,0\\r\\n26247,0\\r\\n26248,0\\r\\n26249,0\\r\\n26250,0\\r\\n26251,0\\r\\n26252,0\\r\\n26253,0\\r\\n26254,0\\r\\n26255,0\\r\\n26256,0\\r\\n26257,0\\r\\n26258,0\\r\\n26259,0\\r\\n26260,0\\r\\n26261,0\\r\\n26262,0\\r\\n26263,0\\r\\n26264,0\\r\\n26265,0\\r\\n26266,0\\r\\n26267,0\\r\\n26268,0\\r\\n26269,0\\r\\n26270,0\\r\\n26271,0\\r\\n26272,0\\r\\n26273,0\\r\\n26274,0\\r\\n26275,0\\r\\n26276,0\\r\\n26277,0\\r\\n26278,0\\r\\n26279,0\\r\\n26280,0\\r\\n26281,0\\r\\n26282,0\\r\\n26283,0\\r\\n26284,0\\r\\n26285,0\\r\\n26286,0\\r\\n26287,0\\r\\n26288,0\\r\\n26289,0\\r\\n26290,0\\r\\n26291,0\\r\\n26292,0\\r\\n26293,0\\r\\n26294,0\\r\\n26295,0\\r\\n26296,0\\r\\n26297,0\\r\\n26298,0\\r\\n26299,0\\r\\n26300,0\\r\\n26301,0\\r\\n26302,0\\r\\n26303,0\\r\\n26304,0\\r\\n26305,0\\r\\n26306,0\\r\\n26307,0\\r\\n26308,0\\r\\n26309,0\\r\\n26310,0\\r\\n26311,0\\r\\n26312,0\\r\\n26313,0\\r\\n26314,0\\r\\n26315,0\\r\\n26316,0\\r\\n26317,0\\r\\n26318,0\\r\\n26319,0\\r\\n26320,0\\r\\n26321,0\\r\\n26322,0\\r\\n26323,0\\r\\n26324,0\\r\\n26325,0\\r\\n26326,0\\r\\n26327,0\\r\\n26328,0\\r\\n26329,0\\r\\n26330,0\\r\\n26331,0\\r\\n26332,0\\r\\n26333,0\\r\\n26334,0\\r\\n26335,0\\r\\n26336,0\\r\\n26337,0\\r\\n26338,0\\r\\n26339,0\\r\\n26340,0\\r\\n26341,0\\r\\n26342,0\\r\\n26343,0\\r\\n26344,0\\r\\n26345,0\\r\\n26346,0\\r\\n26347,0\\r\\n26348,0\\r\\n26349,0\\r\\n26350,0\\r\\n26351,0\\r\\n26352,0\\r\\n26353,0\\r\\n26354,0\\r\\n26355,0\\r\\n26356,0\\r\\n26357,0\\r\\n26358,0\\r\\n26359,0\\r\\n26360,0\\r\\n26361,0\\r\\n26362,0\\r\\n26363,0\\r\\n26364,0\\r\\n26365,0\\r\\n26366,0\\r\\n26367,0\\r\\n26368,0\\r\\n26369,0\\r\\n26370,0\\r\\n26371,0\\r\\n26372,0\\r\\n26373,0\\r\\n26374,0\\r\\n26375,0\\r\\n26376,0\\r\\n26377,0\\r\\n26378,0\\r\\n26379,0\\r\\n26380,0\\r\\n26381,0\\r\\n26382,0\\r\\n26383,0\\r\\n26384,0\\r\\n26385,0\\r\\n26386,0\\r\\n26387,0\\r\\n26388,0\\r\\n26389,0\\r\\n26390,0\\r\\n26391,0\\r\\n26392,0\\r\\n26393,0\\r\\n26394,0\\r\\n26395,0\\r\\n26396,0\\r\\n26397,0\\r\\n26398,0\\r\\n26399,0\\r\\n26400,0\\r\\n26401,0\\r\\n26402,0\\r\\n26403,0\\r\\n26404,0\\r\\n26405,0\\r\\n26406,0\\r\\n26407,0\\r\\n26408,0\\r\\n26409,0\\r\\n26410,0\\r\\n26411,0\\r\\n26412,0\\r\\n26413,0\\r\\n26414,0\\r\\n26415,0\\r\\n26416,0\\r\\n26417,0\\r\\n26418,0\\r\\n26419,0\\r\\n26420,0\\r\\n26421,0\\r\\n26422,0\\r\\n26423,0\\r\\n26424,0\\r\\n26425,0\\r\\n26426,0\\r\\n26427,0\\r\\n26428,0\\r\\n26429,0\\r\\n26430,0\\r\\n26431,0\\r\\n26432,0\\r\\n26433,0\\r\\n26434,0\\r\\n26435,0\\r\\n26436,0\\r\\n26437,0\\r\\n26438,0\\r\\n26439,0\\r\\n26440,0\\r\\n26441,0\\r\\n26442,0\\r\\n26443,0\\r\\n26444,0\\r\\n26445,0\\r\\n26446,0\\r\\n26447,0\\r\\n26448,0\\r\\n26449,0\\r\\n26450,0\\r\\n26451,0\\r\\n26452,0\\r\\n26453,0\\r\\n26454,0\\r\\n26455,0\\r\\n26456,0\\r\\n26457,0\\r\\n26458,0\\r\\n26459,0\\r\\n26460,0\\r\\n26461,0\\r\\n26462,0\\r\\n26463,0\\r\\n26464,0\\r\\n26465,0\\r\\n26466,0\\r\\n26467,0\\r\\n26468,0\\r\\n26469,0\\r\\n26470,0\\r\\n26471,0\\r\\n26472,0\\r\\n26473,0\\r\\n26474,0\\r\\n26475,0\\r\\n26476,0\\r\\n26477,0\\r\\n26478,0\\r\\n26479,0\\r\\n26480,0\\r\\n26481,0\\r\\n26482,0\\r\\n26483,0\\r\\n26484,0\\r\\n26485,0\\r\\n26486,0\\r\\n26487,0\\r\\n26488,0\\r\\n26489,0\\r\\n26490,0\\r\\n26491,0\\r\\n26492,0\\r\\n26493,0\\r\\n26494,0\\r\\n26495,0\\r\\n26496,0\\r\\n26497,0\\r\\n26498,0\\r\\n26499,0\\r\\n26500,0\\r\\n26501,0\\r\\n26502,0\\r\\n26503,0\\r\\n26504,0\\r\\n26505,0\\r\\n26506,0\\r\\n26507,0\\r\\n26508,0\\r\\n26509,0\\r\\n26510,0\\r\\n26511,0\\r\\n26512,0\\r\\n26513,0\\r\\n26514,0\\r\\n26515,0\\r\\n26516,0\\r\\n26517,0\\r\\n26518,0\\r\\n26519,0\\r\\n26520,0\\r\\n26521,0\\r\\n26522,0\\r\\n26523,0\\r\\n26524,0\\r\\n26525,0\\r\\n26526,0\\r\\n26527,0\\r\\n26528,0\\r\\n26529,0\\r\\n26530,0\\r\\n26531,0\\r\\n26532,0\\r\\n26533,0\\r\\n26534,0\\r\\n26535,0\\r\\n26536,0\\r\\n26537,0\\r\\n26538,0\\r\\n26539,0\\r\\n26540,0\\r\\n26541,0\\r\\n26542,0\\r\\n26543,0\\r\\n26544,0\\r\\n26545,0\\r\\n26546,0\\r\\n26547,0\\r\\n26548,0\\r\\n26549,0\\r\\n26550,0\\r\\n26551,0\\r\\n26552,0\\r\\n26553,0\\r\\n26554,0\\r\\n26555,0\\r\\n26556,0\\r\\n26557,0\\r\\n26558,0\\r\\n26559,0\\r\\n26560,0\\r\\n26561,0\\r\\n26562,0\\r\\n26563,0\\r\\n26564,0\\r\\n26565,0\\r\\n26566,0\\r\\n26567,0\\r\\n26568,0\\r\\n26569,0\\r\\n26570,0\\r\\n26571,0\\r\\n26572,0\\r\\n26573,0\\r\\n26574,0\\r\\n26575,0\\r\\n26576,0\\r\\n26577,0\\r\\n26578,0\\r\\n26579,0\\r\\n26580,0\\r\\n26581,0\\r\\n26582,0\\r\\n26583,0\\r\\n26584,0\\r\\n26585,0\\r\\n26586,0\\r\\n26587,0\\r\\n26588,0\\r\\n26589,0\\r\\n26590,0\\r\\n26591,0\\r\\n26592,0\\r\\n26593,0\\r\\n26594,0\\r\\n26595,0\\r\\n26596,0\\r\\n26597,0\\r\\n26598,0\\r\\n26599,0\\r\\n26600,0\\r\\n26601,0\\r\\n26602,0\\r\\n26603,0\\r\\n26604,0\\r\\n26605,0\\r\\n26606,0\\r\\n26607,0\\r\\n26608,0\\r\\n26609,0\\r\\n26610,0\\r\\n26611,0\\r\\n26612,0\\r\\n26613,0\\r\\n26614,0\\r\\n26615,0\\r\\n26616,0\\r\\n26617,0\\r\\n26618,0\\r\\n26619,0\\r\\n26620,0\\r\\n26621,0\\r\\n26622,0\\r\\n26623,0\\r\\n26624,0\\r\\n26625,0\\r\\n26626,0\\r\\n26627,0\\r\\n26628,0\\r\\n26629,0\\r\\n26630,0\\r\\n26631,0\\r\\n26632,0\\r\\n26633,0\\r\\n26634,0\\r\\n26635,0\\r\\n26636,0\\r\\n26637,0\\r\\n26638,0\\r\\n26639,0\\r\\n26640,0\\r\\n26641,0\\r\\n26642,0\\r\\n26643,0\\r\\n26644,0\\r\\n26645,0\\r\\n26646,0\\r\\n26647,0\\r\\n26648,0\\r\\n26649,0\\r\\n26650,0\\r\\n26651,0\\r\\n26652,0\\r\\n26653,0\\r\\n26654,0\\r\\n26655,0\\r\\n26656,0\\r\\n26657,0\\r\\n26658,0\\r\\n26659,0\\r\\n26660,0\\r\\n26661,0\\r\\n26662,0\\r\\n26663,0\\r\\n26664,0\\r\\n26665,0\\r\\n26666,0\\r\\n26667,0\\r\\n26668,0\\r\\n26669,0\\r\\n26670,0\\r\\n26671,0\\r\\n26672,0\\r\\n26673,0\\r\\n26674,0\\r\\n26675,0\\r\\n26676,0\\r\\n26677,0\\r\\n26678,0\\r\\n26679,0\\r\\n26680,0\\r\\n26681,0\\r\\n26682,0\\r\\n26683,0\\r\\n26684,0\\r\\n26685,0\\r\\n26686,0\\r\\n26687,0\\r\\n26688,0\\r\\n26689,0\\r\\n26690,0\\r\\n26691,0\\r\\n26692,0\\r\\n26693,0\\r\\n26694,0\\r\\n26695,0\\r\\n26696,0\\r\\n26697,0\\r\\n26698,0\\r\\n26699,0\\r\\n26700,0\\r\\n26701,0\\r\\n26702,0\\r\\n26703,0\\r\\n26704,0\\r\\n26705,0\\r\\n26706,0\\r\\n26707,0\\r\\n26708,0\\r\\n26709,0\\r\\n26710,0\\r\\n26711,0\\r\\n26712,0\\r\\n26713,0\\r\\n26714,0\\r\\n26715,0\\r\\n26716,0\\r\\n26717,0\\r\\n26718,0\\r\\n26719,0\\r\\n26720,0\\r\\n26721,0\\r\\n26722,0\\r\\n26723,0\\r\\n26724,0\\r\\n26725,0\\r\\n26726,0\\r\\n26727,0\\r\\n26728,0\\r\\n26729,0\\r\\n26730,0\\r\\n26731,0\\r\\n26732,0\\r\\n26733,0\\r\\n26734,0\\r\\n26735,0\\r\\n26736,0\\r\\n26737,0\\r\\n26738,0\\r\\n26739,0\\r\\n26740,0\\r\\n26741,0\\r\\n26742,0\\r\\n26743,0\\r\\n26744,0\\r\\n26745,0\\r\\n26746,0\\r\\n26747,0\\r\\n26748,0\\r\\n26749,0\\r\\n26750,0\\r\\n26751,0\\r\\n26752,0\\r\\n26753,0\\r\\n26754,0\\r\\n26755,0\\r\\n26756,0\\r\\n26757,0\\r\\n26758,0\\r\\n26759,0\\r\\n26760,0\\r\\n26761,0\\r\\n26762,0\\r\\n26763,0\\r\\n26764,0\\r\\n26765,0\\r\\n26766,0\\r\\n26767,0\\r\\n26768,0\\r\\n26769,0\\r\\n26770,0\\r\\n26771,0\\r\\n26772,0\\r\\n26773,0\\r\\n26774,0\\r\\n26775,0\\r\\n26776,0\\r\\n26777,0\\r\\n26778,0\\r\\n26779,0\\r\\n26780,0\\r\\n26781,0\\r\\n26782,0\\r\\n26783,0\\r\\n26784,0\\r\\n26785,0\\r\\n26786,0\\r\\n26787,0\\r\\n26788,0\\r\\n26789,0\\r\\n26790,0\\r\\n26791,0\\r\\n26792,0\\r\\n26793,0\\r\\n26794,0\\r\\n26795,0\\r\\n26796,0\\r\\n26797,0\\r\\n26798,0\\r\\n26799,0\\r\\n26800,0\\r\\n26801,0\\r\\n26802,0\\r\\n26803,0\\r\\n26804,0\\r\\n26805,0\\r\\n26806,0\\r\\n26807,0\\r\\n26808,0\\r\\n26809,0\\r\\n26810,0\\r\\n26811,0\\r\\n26812,0\\r\\n26813,0\\r\\n26814,0\\r\\n26815,0\\r\\n26816,0\\r\\n26817,0\\r\\n26818,0\\r\\n26819,0\\r\\n26820,0\\r\\n26821,0\\r\\n26822,0\\r\\n26823,0\\r\\n26824,0\\r\\n26825,0\\r\\n26826,0\\r\\n26827,0\\r\\n26828,0\\r\\n26829,0\\r\\n26830,0\\r\\n26831,0\\r\\n26832,0\\r\\n26833,0\\r\\n26834,0\\r\\n26835,0\\r\\n26836,0\\r\\n26837,0\\r\\n26838,0\\r\\n26839,0\\r\\n26840,0\\r\\n26841,0\\r\\n26842,0\\r\\n26843,0\\r\\n26844,0\\r\\n26845,0\\r\\n26846,0\\r\\n26847,0\\r\\n26848,0\\r\\n26849,0\\r\\n26850,0\\r\\n26851,0\\r\\n26852,0\\r\\n26853,0\\r\\n26854,0\\r\\n26855,0\\r\\n26856,0\\r\\n26857,0\\r\\n26858,0\\r\\n26859,0\\r\\n26860,0\\r\\n26861,0\\r\\n26862,0\\r\\n26863,0\\r\\n26864,0\\r\\n26865,0\\r\\n26866,0\\r\\n26867,0\\r\\n26868,0\\r\\n26869,0\\r\\n26870,0\\r\\n26871,0\\r\\n26872,0\\r\\n26873,0\\r\\n26874,0\\r\\n26875,0\\r\\n26876,0\\r\\n26877,0\\r\\n26878,0\\r\\n26879,0\\r\\n26880,0\\r\\n26881,0\\r\\n26882,0\\r\\n26883,0\\r\\n26884,0\\r\\n26885,0\\r\\n26886,0\\r\\n26887,0\\r\\n26888,0\\r\\n26889,0\\r\\n26890,0\\r\\n26891,0\\r\\n26892,0\\r\\n26893,0\\r\\n26894,0\\r\\n26895,0\\r\\n26896,0\\r\\n26897,0\\r\\n26898,0\\r\\n26899,0\\r\\n26900,0\\r\\n26901,0\\r\\n26902,0\\r\\n26903,0\\r\\n26904,0\\r\\n26905,0\\r\\n26906,0\\r\\n26907,0\\r\\n26908,0\\r\\n26909,0\\r\\n26910,0\\r\\n26911,0\\r\\n26912,0\\r\\n26913,0\\r\\n26914,0\\r\\n26915,0\\r\\n26916,0\\r\\n26917,0\\r\\n26918,0\\r\\n26919,0\\r\\n26920,0\\r\\n26921,0\\r\\n26922,0\\r\\n26923,0\\r\\n26924,0\\r\\n26925,0\\r\\n26926,0\\r\\n26927,0\\r\\n26928,0\\r\\n26929,0\\r\\n26930,0\\r\\n26931,0\\r\\n26932,0\\r\\n26933,0\\r\\n26934,0\\r\\n26935,0\\r\\n26936,0\\r\\n26937,0\\r\\n26938,0\\r\\n26939,0\\r\\n26940,0\\r\\n26941,0\\r\\n26942,0\\r\\n26943,0\\r\\n26944,0\\r\\n26945,0\\r\\n26946,0\\r\\n26947,0\\r\\n26948,0\\r\\n26949,0\\r\\n26950,0\\r\\n26951,0\\r\\n26952,0\\r\\n26953,0\\r\\n26954,0\\r\\n26955,0\\r\\n26956,0\\r\\n26957,0\\r\\n26958,0\\r\\n26959,0\\r\\n26960,0\\r\\n26961,0\\r\\n26962,0\\r\\n26963,0\\r\\n26964,0\\r\\n26965,0\\r\\n26966,0\\r\\n26967,0\\r\\n26968,0\\r\\n26969,0\\r\\n26970,0\\r\\n26971,0\\r\\n26972,0\\r\\n26973,0\\r\\n26974,0\\r\\n26975,0\\r\\n26976,0\\r\\n26977,0\\r\\n26978,0\\r\\n26979,0\\r\\n26980,0\\r\\n26981,0\\r\\n26982,0\\r\\n26983,0\\r\\n26984,0\\r\\n26985,0\\r\\n26986,0\\r\\n26987,0\\r\\n26988,0\\r\\n26989,0\\r\\n26990,0\\r\\n26991,0\\r\\n26992,0\\r\\n26993,0\\r\\n26994,0\\r\\n26995,0\\r\\n26996,0\\r\\n26997,0\\r\\n26998,0\\r\\n26999,0\\r\\n27000,0\\r\\n27001,0\\r\\n27002,0\\r\\n27003,0\\r\\n27004,0\\r\\n27005,0\\r\\n27006,0\\r\\n27007,0\\r\\n27008,0\\r\\n27009,0\\r\\n27010,0\\r\\n27011,0\\r\\n27012,0\\r\\n27013,0\\r\\n27014,0\\r\\n27015,0\\r\\n27016,0\\r\\n27017,0\\r\\n27018,0\\r\\n27019,0\\r\\n27020,0\\r\\n27021,0\\r\\n27022,0\\r\\n27023,0\\r\\n27024,0\\r\\n27025,0\\r\\n27026,0\\r\\n27027,0\\r\\n27028,0\\r\\n27029,0\\r\\n27030,0\\r\\n27031,0\\r\\n27032,0\\r\\n27033,0\\r\\n27034,0\\r\\n27035,0\\r\\n27036,0\\r\\n27037,0\\r\\n27038,0\\r\\n27039,0\\r\\n27040,0\\r\\n27041,0\\r\\n27042,0\\r\\n27043,0\\r\\n27044,0\\r\\n27045,0\\r\\n27046,0\\r\\n27047,0\\r\\n27048,0\\r\\n27049,0\\r\\n27050,0\\r\\n27051,0\\r\\n27052,0\\r\\n27053,0\\r\\n27054,0\\r\\n27055,0\\r\\n27056,0\\r\\n27057,0\\r\\n27058,0\\r\\n27059,0\\r\\n27060,0\\r\\n27061,0\\r\\n27062,0\\r\\n27063,0\\r\\n27064,0\\r\\n27065,0\\r\\n27066,0\\r\\n27067,0\\r\\n27068,0\\r\\n27069,0\\r\\n27070,0\\r\\n27071,0\\r\\n27072,0\\r\\n27073,0\\r\\n27074,0\\r\\n27075,0\\r\\n27076,0\\r\\n27077,0\\r\\n27078,0\\r\\n27079,0\\r\\n27080,0\\r\\n27081,0\\r\\n27082,0\\r\\n27083,0\\r\\n27084,0\\r\\n27085,0\\r\\n27086,0\\r\\n27087,0\\r\\n27088,0\\r\\n27089,0\\r\\n27090,0\\r\\n27091,0\\r\\n27092,0\\r\\n27093,0\\r\\n27094,0\\r\\n27095,0\\r\\n27096,0\\r\\n27097,0\\r\\n27098,0\\r\\n27099,0\\r\\n27100,0\\r\\n27101,0\\r\\n27102,0\\r\\n27103,0\\r\\n27104,0\\r\\n27105,0\\r\\n27106,0\\r\\n27107,0\\r\\n27108,0\\r\\n27109,0\\r\\n27110,0\\r\\n27111,0\\r\\n27112,0\\r\\n27113,0\\r\\n27114,0\\r\\n27115,0\\r\\n27116,0\\r\\n27117,0\\r\\n27118,0\\r\\n27119,0\\r\\n27120,0\\r\\n27121,0\\r\\n27122,0\\r\\n27123,0\\r\\n27124,0\\r\\n27125,0\\r\\n27126,0\\r\\n27127,0\\r\\n27128,0\\r\\n27129,0\\r\\n27130,0\\r\\n27131,0\\r\\n27132,0\\r\\n27133,0\\r\\n27134,0\\r\\n27135,0\\r\\n27136,0\\r\\n27137,0\\r\\n27138,0\\r\\n27139,0\\r\\n27140,0\\r\\n27141,0\\r\\n27142,0\\r\\n27143,0\\r\\n27144,0\\r\\n27145,0\\r\\n27146,0\\r\\n27147,0\\r\\n27148,0\\r\\n27149,0\\r\\n27150,0\\r\\n27151,0\\r\\n27152,0\\r\\n27153,0\\r\\n27154,0\\r\\n27155,0\\r\\n27156,0\\r\\n27157,0\\r\\n27158,0\\r\\n27159,0\\r\\n27160,0\\r\\n27161,0\\r\\n27162,0\\r\\n27163,0\\r\\n27164,0\\r\\n27165,0\\r\\n27166,0\\r\\n27167,0\\r\\n27168,0\\r\\n27169,0\\r\\n27170,0\\r\\n27171,0\\r\\n27172,0\\r\\n27173,0\\r\\n27174,0\\r\\n27175,0\\r\\n27176,0\\r\\n27177,0\\r\\n27178,0\\r\\n27179,0\\r\\n27180,0\\r\\n27181,0\\r\\n27182,0\\r\\n27183,0\\r\\n27184,0\\r\\n27185,0\\r\\n27186,0\\r\\n27187,0\\r\\n27188,0\\r\\n27189,0\\r\\n27190,0\\r\\n27191,0\\r\\n27192,0\\r\\n27193,0\\r\\n27194,0\\r\\n27195,0\\r\\n27196,0\\r\\n27197,0\\r\\n27198,0\\r\\n27199,0\\r\\n27200,0\\r\\n27201,0\\r\\n27202,0\\r\\n27203,0\\r\\n27204,0\\r\\n27205,0\\r\\n27206,0\\r\\n27207,0\\r\\n27208,0\\r\\n27209,0\\r\\n27210,0\\r\\n27211,0\\r\\n27212,0\\r\\n27213,0\\r\\n27214,0\\r\\n27215,0\\r\\n27216,0\\r\\n27217,0\\r\\n27218,0\\r\\n27219,0\\r\\n27220,0\\r\\n27221,0\\r\\n27222,0\\r\\n27223,0\\r\\n27224,0\\r\\n27225,0\\r\\n27226,0\\r\\n27227,0\\r\\n27228,0\\r\\n27229,0\\r\\n27230,0\\r\\n27231,0\\r\\n27232,0\\r\\n27233,0\\r\\n27234,0\\r\\n27235,0\\r\\n27236,0\\r\\n27237,0\\r\\n27238,0\\r\\n27239,0\\r\\n27240,0\\r\\n27241,0\\r\\n27242,0\\r\\n27243,0\\r\\n27244,0\\r\\n27245,0\\r\\n27246,0\\r\\n27247,0\\r\\n27248,0\\r\\n27249,0\\r\\n27250,0\\r\\n27251,0\\r\\n27252,0\\r\\n27253,0\\r\\n27254,0\\r\\n27255,0\\r\\n27256,0\\r\\n27257,0\\r\\n27258,0\\r\\n27259,0\\r\\n27260,0\\r\\n27261,0\\r\\n27262,0\\r\\n27263,0\\r\\n27264,0\\r\\n27265,0\\r\\n27266,0\\r\\n27267,0\\r\\n27268,0\\r\\n27269,0\\r\\n27270,0\\r\\n27271,0\\r\\n27272,0\\r\\n27273,0\\r\\n27274,0\\r\\n27275,0\\r\\n27276,0\\r\\n27277,0\\r\\n27278,0\\r\\n27279,0\\r\\n27280,0\\r\\n27281,0\\r\\n27282,0\\r\\n27283,0\\r\\n27284,0\\r\\n27285,0\\r\\n27286,0\\r\\n27287,0\\r\\n27288,0\\r\\n27289,0\\r\\n27290,0\\r\\n27291,0\\r\\n27292,0\\r\\n27293,0\\r\\n27294,0\\r\\n27295,0\\r\\n27296,0\\r\\n27297,0\\r\\n27298,0\\r\\n27299,0\\r\\n27300,0\\r\\n27301,0\\r\\n27302,0\\r\\n27303,0\\r\\n27304,0\\r\\n27305,0\\r\\n27306,0\\r\\n27307,0\\r\\n27308,0\\r\\n27309,0\\r\\n27310,0\\r\\n27311,0\\r\\n27312,0\\r\\n27313,0\\r\\n27314,0\\r\\n27315,0\\r\\n27316,0\\r\\n27317,0\\r\\n27318,0\\r\\n27319,0\\r\\n27320,0\\r\\n27321,0\\r\\n27322,0\\r\\n27323,0\\r\\n27324,0\\r\\n27325,0\\r\\n27326,0\\r\\n27327,0\\r\\n27328,0\\r\\n27329,0\\r\\n27330,0\\r\\n27331,0\\r\\n27332,0\\r\\n27333,0\\r\\n27334,0\\r\\n27335,0\\r\\n27336,0\\r\\n27337,0\\r\\n27338,0\\r\\n27339,0\\r\\n27340,0\\r\\n27341,0\\r\\n27342,0\\r\\n27343,0\\r\\n27344,0\\r\\n27345,0\\r\\n27346,0\\r\\n27347,0\\r\\n27348,0\\r\\n27349,0\\r\\n27350,0\\r\\n27351,0\\r\\n27352,0\\r\\n27353,0\\r\\n27354,0\\r\\n27355,0\\r\\n27356,0\\r\\n27357,0\\r\\n27358,0\\r\\n27359,0\\r\\n27360,0\\r\\n27361,0\\r\\n27362,0\\r\\n27363,0\\r\\n27364,0\\r\\n27365,0\\r\\n27366,0\\r\\n27367,0\\r\\n27368,0\\r\\n27369,0\\r\\n27370,0\\r\\n27371,0\\r\\n27372,0\\r\\n27373,0\\r\\n27374,0\\r\\n27375,0\\r\\n27376,0\\r\\n27377,0\\r\\n27378,0\\r\\n27379,0\\r\\n27380,0\\r\\n27381,0\\r\\n27382,0\\r\\n27383,0\\r\\n27384,0\\r\\n27385,0\\r\\n27386,0\\r\\n27387,0\\r\\n27388,0\\r\\n27389,0\\r\\n27390,0\\r\\n27391,0\\r\\n27392,0\\r\\n27393,0\\r\\n27394,0\\r\\n27395,0\\r\\n27396,0\\r\\n27397,0\\r\\n27398,0\\r\\n27399,0\\r\\n27400,0\\r\\n27401,0\\r\\n27402,0\\r\\n27403,0\\r\\n27404,0\\r\\n27405,0\\r\\n27406,0\\r\\n27407,0\\r\\n27408,0\\r\\n27409,0\\r\\n27410,0\\r\\n27411,0\\r\\n27412,0\\r\\n27413,0\\r\\n27414,0\\r\\n27415,0\\r\\n27416,0\\r\\n27417,0\\r\\n27418,0\\r\\n27419,0\\r\\n27420,0\\r\\n27421,0\\r\\n27422,0\\r\\n27423,0\\r\\n27424,0\\r\\n27425,0\\r\\n27426,0\\r\\n27427,0\\r\\n27428,0\\r\\n27429,0\\r\\n27430,0\\r\\n27431,0\\r\\n27432,0\\r\\n27433,0\\r\\n27434,0\\r\\n27435,0\\r\\n27436,0\\r\\n27437,0\\r\\n27438,0\\r\\n27439,0\\r\\n27440,0\\r\\n27441,0\\r\\n27442,0\\r\\n27443,0\\r\\n27444,0\\r\\n27445,0\\r\\n27446,0\\r\\n27447,0\\r\\n27448,0\\r\\n27449,0\\r\\n27450,0\\r\\n27451,0\\r\\n27452,0\\r\\n27453,0\\r\\n27454,0\\r\\n27455,0\\r\\n27456,0\\r\\n27457,0\\r\\n27458,0\\r\\n27459,0\\r\\n27460,0\\r\\n27461,0\\r\\n27462,0\\r\\n27463,0\\r\\n27464,0\\r\\n27465,0\\r\\n27466,0\\r\\n27467,0\\r\\n27468,0\\r\\n27469,0\\r\\n27470,0\\r\\n27471,0\\r\\n27472,0\\r\\n27473,0\\r\\n27474,0\\r\\n27475,0\\r\\n27476,0\\r\\n27477,0\\r\\n27478,0\\r\\n27479,0\\r\\n27480,0\\r\\n27481,0\\r\\n27482,0\\r\\n27483,0\\r\\n27484,0\\r\\n27485,0\\r\\n27486,0\\r\\n27487,0\\r\\n27488,0\\r\\n27489,0\\r\\n27490,0\\r\\n27491,0\\r\\n27492,0\\r\\n27493,0\\r\\n27494,0\\r\\n27495,0\\r\\n27496,0\\r\\n27497,0\\r\\n27498,0\\r\\n27499,0\\r\\n27500,0\\r\\n27501,0\\r\\n27502,0\\r\\n27503,0\\r\\n27504,0\\r\\n27505,0\\r\\n27506,0\\r\\n27507,0\\r\\n27508,0\\r\\n27509,0\\r\\n27510,0\\r\\n27511,0\\r\\n27512,0\\r\\n27513,0\\r\\n27514,0\\r\\n27515,0\\r\\n27516,0\\r\\n27517,0\\r\\n27518,0\\r\\n27519,0\\r\\n27520,0\\r\\n27521,0\\r\\n27522,0\\r\\n27523,0\\r\\n27524,0\\r\\n27525,0\\r\\n27526,0\\r\\n27527,0\\r\\n27528,0\\r\\n27529,0\\r\\n27530,0\\r\\n27531,0\\r\\n27532,0\\r\\n27533,0\\r\\n27534,0\\r\\n27535,0\\r\\n27536,0\\r\\n27537,0\\r\\n27538,0\\r\\n27539,0\\r\\n27540,0\\r\\n27541,0\\r\\n27542,0\\r\\n27543,0\\r\\n27544,0\\r\\n27545,0\\r\\n27546,0\\r\\n27547,0\\r\\n27548,0\\r\\n27549,0\\r\\n27550,0\\r\\n27551,0\\r\\n27552,0\\r\\n27553,0\\r\\n27554,0\\r\\n27555,0\\r\\n27556,0\\r\\n27557,0\\r\\n27558,0\\r\\n27559,0\\r\\n27560,0\\r\\n27561,0\\r\\n27562,0\\r\\n27563,0\\r\\n27564,0\\r\\n27565,0\\r\\n27566,0\\r\\n27567,0\\r\\n27568,0\\r\\n27569,0\\r\\n27570,0\\r\\n27571,0\\r\\n27572,0\\r\\n27573,0\\r\\n27574,0\\r\\n27575,0\\r\\n27576,0\\r\\n27577,0\\r\\n27578,0\\r\\n27579,0\\r\\n27580,0\\r\\n27581,0\\r\\n27582,0\\r\\n27583,0\\r\\n27584,0\\r\\n27585,0\\r\\n27586,0\\r\\n27587,0\\r\\n27588,0\\r\\n27589,0\\r\\n27590,0\\r\\n27591,0\\r\\n27592,0\\r\\n27593,0\\r\\n27594,0\\r\\n27595,0\\r\\n27596,0\\r\\n27597,0\\r\\n27598,0\\r\\n27599,0\\r\\n27600,0\\r\\n27601,0\\r\\n27602,0\\r\\n27603,0\\r\\n27604,0\\r\\n27605,0\\r\\n27606,0\\r\\n27607,0\\r\\n27608,0\\r\\n27609,0\\r\\n27610,0\\r\\n27611,0\\r\\n27612,0\\r\\n27613,0\\r\\n27614,0\\r\\n27615,0\\r\\n27616,0\\r\\n27617,0\\r\\n27618,0\\r\\n27619,0\\r\\n27620,0\\r\\n27621,0\\r\\n27622,0\\r\\n27623,0\\r\\n27624,0\\r\\n27625,0\\r\\n27626,0\\r\\n27627,0\\r\\n27628,0\\r\\n27629,0\\r\\n27630,0\\r\\n27631,0\\r\\n27632,0\\r\\n27633,0\\r\\n27634,0\\r\\n27635,0\\r\\n27636,0\\r\\n27637,0\\r\\n27638,0\\r\\n27639,0\\r\\n27640,0\\r\\n27641,0\\r\\n27642,0\\r\\n27643,0\\r\\n27644,0\\r\\n27645,0\\r\\n27646,0\\r\\n27647,0\\r\\n27648,0\\r\\n27649,0\\r\\n27650,0\\r\\n27651,0\\r\\n27652,0\\r\\n27653,0\\r\\n27654,0\\r\\n27655,0\\r\\n27656,0\\r\\n27657,0\\r\\n27658,0\\r\\n27659,0\\r\\n27660,0\\r\\n27661,0\\r\\n27662,0\\r\\n27663,0\\r\\n27664,0\\r\\n27665,0\\r\\n27666,0\\r\\n27667,0\\r\\n27668,0\\r\\n27669,0\\r\\n27670,0\\r\\n27671,0\\r\\n27672,0\\r\\n27673,0\\r\\n27674,0\\r\\n27675,0\\r\\n27676,0\\r\\n27677,0\\r\\n27678,0\\r\\n27679,0\\r\\n27680,0\\r\\n27681,0\\r\\n27682,0\\r\\n27683,0\\r\\n27684,0\\r\\n27685,0\\r\\n27686,0\\r\\n27687,0\\r\\n27688,0\\r\\n27689,0\\r\\n27690,0\\r\\n27691,0\\r\\n27692,0\\r\\n27693,0\\r\\n27694,0\\r\\n27695,0\\r\\n27696,0\\r\\n27697,0\\r\\n27698,0\\r\\n27699,0\\r\\n27700,0\\r\\n27701,0\\r\\n27702,0\\r\\n27703,0\\r\\n27704,0\\r\\n27705,0\\r\\n27706,0\\r\\n27707,0\\r\\n27708,0\\r\\n27709,0\\r\\n27710,0\\r\\n27711,0\\r\\n27712,0\\r\\n27713,0\\r\\n27714,0\\r\\n27715,0\\r\\n27716,0\\r\\n27717,0\\r\\n27718,0\\r\\n27719,0\\r\\n27720,0\\r\\n27721,0\\r\\n27722,0\\r\\n27723,0\\r\\n27724,0\\r\\n27725,0\\r\\n27726,0\\r\\n27727,0\\r\\n27728,0\\r\\n27729,0\\r\\n27730,0\\r\\n27731,0\\r\\n27732,0\\r\\n27733,0\\r\\n27734,0\\r\\n27735,0\\r\\n27736,0\\r\\n27737,0\\r\\n27738,0\\r\\n27739,0\\r\\n27740,0\\r\\n27741,0\\r\\n27742,0\\r\\n27743,0\\r\\n27744,0\\r\\n27745,0\\r\\n27746,0\\r\\n27747,0\\r\\n27748,0\\r\\n27749,0\\r\\n27750,0\\r\\n27751,0\\r\\n27752,0\\r\\n27753,0\\r\\n27754,0\\r\\n27755,0\\r\\n27756,0\\r\\n27757,0\\r\\n27758,0\\r\\n27759,0\\r\\n27760,0\\r\\n27761,0\\r\\n27762,0\\r\\n27763,0\\r\\n27764,0\\r\\n27765,0\\r\\n27766,0\\r\\n27767,0\\r\\n27768,0\\r\\n27769,0\\r\\n27770,0\\r\\n27771,0\\r\\n27772,0\\r\\n27773,0\\r\\n27774,0\\r\\n27775,0\\r\\n27776,0\\r\\n27777,0\\r\\n27778,0\\r\\n27779,0\\r\\n27780,0\\r\\n27781,0\\r\\n27782,0\\r\\n27783,0\\r\\n27784,0\\r\\n27785,0\\r\\n27786,0\\r\\n27787,0\\r\\n27788,0\\r\\n27789,0\\r\\n27790,0\\r\\n27791,0\\r\\n27792,0\\r\\n27793,0\\r\\n27794,0\\r\\n27795,0\\r\\n27796,0\\r\\n27797,0\\r\\n27798,0\\r\\n27799,0\\r\\n27800,0\\r\\n27801,0\\r\\n27802,0\\r\\n27803,0\\r\\n27804,0\\r\\n27805,0\\r\\n27806,0\\r\\n27807,0\\r\\n27808,0\\r\\n27809,0\\r\\n27810,0\\r\\n27811,0\\r\\n27812,0\\r\\n27813,0\\r\\n27814,0\\r\\n27815,0\\r\\n27816,0\\r\\n27817,0\\r\\n27818,0\\r\\n27819,0\\r\\n27820,0\\r\\n27821,0\\r\\n27822,0\\r\\n27823,0\\r\\n27824,0\\r\\n27825,0\\r\\n27826,0\\r\\n27827,0\\r\\n27828,0\\r\\n27829,0\\r\\n27830,0\\r\\n27831,0\\r\\n27832,0\\r\\n27833,0\\r\\n27834,0\\r\\n27835,0\\r\\n27836,0\\r\\n27837,0\\r\\n27838,0\\r\\n27839,0\\r\\n27840,0\\r\\n27841,0\\r\\n27842,0\\r\\n27843,0\\r\\n27844,0\\r\\n27845,0\\r\\n27846,0\\r\\n27847,0\\r\\n27848,0\\r\\n27849,0\\r\\n27850,0\\r\\n27851,0\\r\\n27852,0\\r\\n27853,0\\r\\n27854,0\\r\\n27855,0\\r\\n27856,0\\r\\n27857,0\\r\\n27858,0\\r\\n27859,0\\r\\n27860,0\\r\\n27861,0\\r\\n27862,0\\r\\n27863,0\\r\\n27864,0\\r\\n27865,0\\r\\n27866,0\\r\\n27867,0\\r\\n27868,0\\r\\n27869,0\\r\\n27870,0\\r\\n27871,0\\r\\n27872,0\\r\\n27873,0\\r\\n27874,0\\r\\n27875,0\\r\\n27876,0\\r\\n27877,0\\r\\n27878,0\\r\\n27879,0\\r\\n27880,0\\r\\n27881,0\\r\\n27882,0\\r\\n27883,0\\r\\n27884,0\\r\\n27885,0\\r\\n27886,0\\r\\n27887,0\\r\\n27888,0\\r\\n27889,0\\r\\n27890,0\\r\\n27891,0\\r\\n27892,0\\r\\n27893,0\\r\\n27894,0\\r\\n27895,0\\r\\n27896,0\\r\\n27897,0\\r\\n27898,0\\r\\n27899,0\\r\\n27900,0\\r\\n27901,0\\r\\n27902,0\\r\\n27903,0\\r\\n27904,0\\r\\n27905,0\\r\\n27906,0\\r\\n27907,0\\r\\n27908,0\\r\\n27909,0\\r\\n27910,0\\r\\n27911,0\\r\\n27912,0\\r\\n27913,0\\r\\n27914,0\\r\\n27915,0\\r\\n27916,0\\r\\n27917,0\\r\\n27918,0\\r\\n27919,0\\r\\n27920,0\\r\\n27921,0\\r\\n27922,0\\r\\n27923,0\\r\\n27924,0\\r\\n27925,0\\r\\n27926,0\\r\\n27927,0\\r\\n27928,0\\r\\n27929,0\\r\\n27930,0\\r\\n27931,0\\r\\n27932,0\\r\\n27933,0\\r\\n27934,0\\r\\n27935,0\\r\\n27936,0\\r\\n27937,0\\r\\n27938,0\\r\\n27939,0\\r\\n27940,0\\r\\n27941,0\\r\\n27942,0\\r\\n27943,0\\r\\n27944,0\\r\\n27945,0\\r\\n27946,0\\r\\n27947,0\\r\\n27948,0\\r\\n27949,0\\r\\n27950,0\\r\\n27951,0\\r\\n27952,0\\r\\n27953,0\\r\\n27954,0\\r\\n27955,0\\r\\n27956,0\\r\\n27957,0\\r\\n27958,0\\r\\n27959,0\\r\\n27960,0\\r\\n27961,0\\r\\n27962,0\\r\\n27963,0\\r\\n27964,0\\r\\n27965,0\\r\\n27966,0\\r\\n27967,0\\r\\n27968,0\\r\\n27969,0\\r\\n27970,0\\r\\n27971,0\\r\\n27972,0\\r\\n27973,0\\r\\n27974,0\\r\\n27975,0\\r\\n27976,0\\r\\n27977,0\\r\\n27978,0\\r\\n27979,0\\r\\n27980,0\\r\\n27981,0\\r\\n27982,0\\r\\n27983,0\\r\\n27984,0\\r\\n27985,0\\r\\n27986,0\\r\\n27987,0\\r\\n27988,0\\r\\n27989,0\\r\\n27990,0\\r\\n27991,0\\r\\n27992,0\\r\\n27993,0\\r\\n27994,0\\r\\n27995,0\\r\\n27996,0\\r\\n27997,0\\r\\n27998,0\\r\\n27999,0\\r\\n28000,0\\r\\n'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "metadata": {
        "id": "xyPDArMSAjIw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "submit=pd.read_csv('sample_submission.csv')\n",
        "fin=submit.iloc[:,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "upZkhLOXBp-P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1784
        },
        "outputId": "d767d1c3-57dc-4b97-c676-1d70c6cbf68c"
      },
      "cell_type": "code",
      "source": [
        "net.eval()\n",
        "print(8*'#','Started Predicting',8*'#')\n",
        "for i in range(len(testdata)):\n",
        "  b=dataset.iloc[i,:]\n",
        "  a=numpy.array(b)\n",
        "  a=torch.from_numpy(a)\n",
        "  a=a.type(torch.FloatTensor)\n",
        "  a=a.reshape(1,1,28,28)\n",
        "  a=a/256\n",
        "  label,_=net(a)\n",
        "  _, predicted = torch.max(label, 1)\n",
        "  predicted=predicted.data.numpy()\n",
        "  fin.iloc[i,1]=predicted\n",
        "  z=(i+1)/len(testdata)*100\n",
        "  if z%1==0:\n",
        "    print(4*'#',z,' % Complete',4*'#')\n",
        "  \n",
        "  \n",
        "  \n",
        "  "
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "######## Started Predicting ########\n",
            "#### 1.0  % Complete ####\n",
            "#### 2.0  % Complete ####\n",
            "#### 3.0  % Complete ####\n",
            "#### 4.0  % Complete ####\n",
            "#### 5.0  % Complete ####\n",
            "#### 6.0  % Complete ####\n",
            "#### 8.0  % Complete ####\n",
            "#### 9.0  % Complete ####\n",
            "#### 10.0  % Complete ####\n",
            "#### 11.0  % Complete ####\n",
            "#### 12.0  % Complete ####\n",
            "#### 13.0  % Complete ####\n",
            "#### 15.0  % Complete ####\n",
            "#### 16.0  % Complete ####\n",
            "#### 17.0  % Complete ####\n",
            "#### 18.0  % Complete ####\n",
            "#### 19.0  % Complete ####\n",
            "#### 20.0  % Complete ####\n",
            "#### 21.0  % Complete ####\n",
            "#### 22.0  % Complete ####\n",
            "#### 23.0  % Complete ####\n",
            "#### 24.0  % Complete ####\n",
            "#### 25.0  % Complete ####\n",
            "#### 26.0  % Complete ####\n",
            "#### 27.0  % Complete ####\n",
            "#### 30.0  % Complete ####\n",
            "#### 31.0  % Complete ####\n",
            "#### 32.0  % Complete ####\n",
            "#### 33.0  % Complete ####\n",
            "#### 34.0  % Complete ####\n",
            "#### 35.0  % Complete ####\n",
            "#### 36.0  % Complete ####\n",
            "#### 37.0  % Complete ####\n",
            "#### 38.0  % Complete ####\n",
            "#### 39.0  % Complete ####\n",
            "#### 40.0  % Complete ####\n",
            "#### 41.0  % Complete ####\n",
            "#### 42.0  % Complete ####\n",
            "#### 43.0  % Complete ####\n",
            "#### 44.0  % Complete ####\n",
            "#### 45.0  % Complete ####\n",
            "#### 46.0  % Complete ####\n",
            "#### 47.0  % Complete ####\n",
            "#### 48.0  % Complete ####\n",
            "#### 49.0  % Complete ####\n",
            "#### 50.0  % Complete ####\n",
            "#### 51.0  % Complete ####\n",
            "#### 52.0  % Complete ####\n",
            "#### 53.0  % Complete ####\n",
            "#### 54.0  % Complete ####\n",
            "#### 59.0  % Complete ####\n",
            "#### 60.0  % Complete ####\n",
            "#### 61.0  % Complete ####\n",
            "#### 62.0  % Complete ####\n",
            "#### 63.0  % Complete ####\n",
            "#### 64.0  % Complete ####\n",
            "#### 65.0  % Complete ####\n",
            "#### 66.0  % Complete ####\n",
            "#### 67.0  % Complete ####\n",
            "#### 68.0  % Complete ####\n",
            "#### 69.0  % Complete ####\n",
            "#### 70.0  % Complete ####\n",
            "#### 71.0  % Complete ####\n",
            "#### 72.0  % Complete ####\n",
            "#### 73.0  % Complete ####\n",
            "#### 74.0  % Complete ####\n",
            "#### 75.0  % Complete ####\n",
            "#### 76.0  % Complete ####\n",
            "#### 77.0  % Complete ####\n",
            "#### 78.0  % Complete ####\n",
            "#### 79.0  % Complete ####\n",
            "#### 80.0  % Complete ####\n",
            "#### 81.0  % Complete ####\n",
            "#### 82.0  % Complete ####\n",
            "#### 83.0  % Complete ####\n",
            "#### 84.0  % Complete ####\n",
            "#### 85.0  % Complete ####\n",
            "#### 86.0  % Complete ####\n",
            "#### 87.0  % Complete ####\n",
            "#### 88.0  % Complete ####\n",
            "#### 89.0  % Complete ####\n",
            "#### 90.0  % Complete ####\n",
            "#### 91.0  % Complete ####\n",
            "#### 92.0  % Complete ####\n",
            "#### 93.0  % Complete ####\n",
            "#### 94.0  % Complete ####\n",
            "#### 95.0  % Complete ####\n",
            "#### 96.0  % Complete ####\n",
            "#### 97.0  % Complete ####\n",
            "#### 98.0  % Complete ####\n",
            "#### 99.0  % Complete ####\n",
            "#### 100.0  % Complete ####\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8XnGCoxgJ8B6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fin.to_csv('Final.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aNHGdZE5Kbiv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "files.download('Final.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}