# Capsule-Net-on-MNIST

 CNNs (convolutional neural networks) are awesome. They are one of the reasons deep learning is so popular today. They can do amazing things that people used to think computers would not be capable of doing for a long, long time. Nonetheless, they have their limits and they have fundamental drawbacks.
#  Hinton: “The pooling operation used in convolutional neural networks is a big mistake and the fact that it works so well is a disaster.”
Capsules introduce a new building block that can be used in deep learning to better model hierarchical relationships inside of internal knowledge representation of a neural network. Intuition behind them is very simple and elegant.
Sara Sabour, Nicholas Frost and Geoffrey Hinton released a paper titled “Dynamic Routing Between Capsules” 4 days back.
Now when one of the Godfathers of Deep Learning “Geoffrey Hinton” is releasing a paper it is bound to be ground breaking.
# Basic Architecture:
![498](https://user-images.githubusercontent.com/39593019/49949768-76acbf80-ff1c-11e8-99d1-2a67a2673de9.png)

# Requirements
Python 3
PyTorch
TorchVision
TorchNet

Benchmarks
Highest accuracy was more than 99% on the 380th epoch. 
The model may achieve a higher accuracy as shown by the trend of the test accuracy.
